uri,name,type,n_variable,n_words,n_words_unique,n_characters,avg_char_per_word,n_loop,n_ifthen,arg_name,arg_type,arg_value,line,docs,list_functions,n_functions
repos/datasets/benchmarks/benchmark_array_xd.py:benchmark_array_xd,benchmark_array_xd,function,20,99,54,1462,14.77,3,0,[],[],[],92,[],"['tempfile.TemporaryDirectory', 'datasets.Features', 'Array2D', 'generate_examples', 'write', 'read_func', 'datasets.Sequence', 'open', 'f.write']",9
repos/datasets/benchmarks/benchmark_array_xd.py:read_batch_formatted_as_numpy,read_batch_formatted_as_numpy,function,9,20,19,231,11.55,1,0,"['feats', 'tmp_dir']","[None, None]","[None, None]",63,[],"['dataset.set_format', 'range', 'len']",3
repos/datasets/benchmarks/benchmark_array_xd.py:read_batch_unformated,read_batch_unformated,function,8,19,18,203,10.68,1,0,"['feats', 'tmp_dir']","[None, None]","[None, None]",53,[],"['range', 'len']",2
repos/datasets/benchmarks/benchmark_array_xd.py:read_col_formatted_as_numpy,read_col_formatted_as_numpy,function,7,13,13,181,13.92,1,0,"['feats', 'tmp_dir']","[None, None]","[None, None]",83,[],['dataset.set_format'],1
repos/datasets/benchmarks/benchmark_array_xd.py:read_col_unformated,read_col_unformated,function,6,12,12,153,12.75,1,0,"['feats', 'tmp_dir']","[None, None]","[None, None]",74,[],[],0
repos/datasets/benchmarks/benchmark_array_xd.py:read_formatted_as_numpy,read_formatted_as_numpy,function,4,12,12,171,14.25,1,0,"['feats', 'tmp_dir']","[None, None]","[None, None]",43,[],['dataset.set_format'],1
repos/datasets/benchmarks/benchmark_array_xd.py:read_unformated,read_unformated,function,3,11,11,143,13.0,1,0,"['feats', 'tmp_dir']","[None, None]","[None, None]",34,[],[],0
repos/datasets/benchmarks/benchmark_array_xd.py:write,write,function,11,17,17,219,12.88,1,0,"['my_features', 'dummy_data', 'tmp_dir']","[None, None, None]","[None, None, None]",25,[],"['ArrowWriter', 'my_features.encode_example', 'writer.write', 'writer.finalize']",4
repos/datasets/benchmarks/benchmark_getitem_100B.py:benchmark_table_100B,benchmark_table_100B,function,10,28,28,436,15.57,1,0,[],[],[],63,[],"['print', 'generate_100B_dataset', 'func', 'open', 'f.write']",5
repos/datasets/benchmarks/benchmark_getitem_100B.py:generate_100B_dataset,generate_100B_dataset,function,5,12,11,164,13.67,0,0,"['num_examples', 'chunk_size']","[' int', ' int']","[None, None]",19,[],"['pa.concat_tables', 'datasets.Dataset']",2
repos/datasets/benchmarks/benchmark_getitem_100B.py:get_batch_of_1024_random_rows,get_batch_of_1024_random_rows,function,2,5,5,48,9.6,0,0,['dataset'],[' datasets.Dataset'],[None],59,[],"['dataset[RandIter', 'len']",2
repos/datasets/benchmarks/benchmark_getitem_100B.py:get_batch_of_1024_rows,get_batch_of_1024_rows,function,2,8,7,54,6.75,0,0,['dataset'],[' datasets.Dataset'],[None],54,[],"['dataset[range', 'len']",2
repos/datasets/benchmarks/benchmark_getitem_100B.py:get_first_row,get_first_row,function,2,2,2,12,6.0,0,0,['dataset'],[' datasets.Dataset'],[None],44,[],[],0
repos/datasets/benchmarks/benchmark_getitem_100B.py:get_last_row,get_last_row,function,2,2,2,13,6.5,0,0,['dataset'],[' datasets.Dataset'],[None],49,[],[],0
repos/datasets/benchmarks/benchmark_getitem_100B.py:RandIter,RandIter,class,14,24,18,268,11.17,0,0,[],[],[],26,[],[],0
repos/datasets/benchmarks/benchmark_getitem_100B.py:RandIter:__iter__,RandIter:__iter__,method,2,2,2,32,16.0,0,0,['self'],[None],[None],36,[],['iter'],1
repos/datasets/benchmarks/benchmark_getitem_100B.py:RandIter:__len__,RandIter:__len__,method,2,2,2,15,7.5,0,0,['self'],[None],[None],39,[],[],0
repos/datasets/benchmarks/benchmark_getitem_100B.py:RandIter:__post_init__,RandIter:__post_init__,method,4,6,6,123,20.5,0,0,['self'],[None],[None],32,[],['rng.integers'],1
repos/datasets/benchmarks/benchmark_indices_mapping.py:benchmark_indices_mapping,benchmark_indices_mapping,function,13,42,40,560,13.33,1,0,[],[],[],41,[],"['tempfile.TemporaryDirectory', 'print', 'datasets.Features', 'datasets.Value', 'generate_example_dataset', 'func', 'open', 'f.write']",8
repos/datasets/benchmarks/benchmark_indices_mapping.py:select,select,function,2,4,4,41,10.25,0,0,['dataset'],[' datasets.Dataset'],[None],16,[],"['dataset.select', 'len']",2
repos/datasets/benchmarks/benchmark_indices_mapping.py:shard,shard,function,4,7,7,68,9.71,1,0,"['dataset', 'num_shards']","[' datasets.Dataset', None]","[None, '10']",36,[],"['range', 'dataset.shard']",2
repos/datasets/benchmarks/benchmark_indices_mapping.py:shuffle,shuffle,function,2,2,2,19,9.5,0,0,['dataset'],[' datasets.Dataset'],[None],26,[],['dataset.shuffle'],1
repos/datasets/benchmarks/benchmark_indices_mapping.py:sort,sort,function,2,2,2,25,12.5,0,0,['dataset'],[' datasets.Dataset'],[None],21,[],['dataset.sort'],1
repos/datasets/benchmarks/benchmark_indices_mapping.py:train_test_split,train_test_split,function,2,2,2,31,15.5,0,0,['dataset'],[' datasets.Dataset'],[None],31,[],['dataset.train_test_split'],1
repos/datasets/benchmarks/benchmark_iterating.py:benchmark_iterating,benchmark_iterating,function,22,189,85,2072,10.96,2,0,[],[],[],42,[],"['tempfile.TemporaryDirectory', 'print', 'datasets.Features', 'datasets.Sequence', 'datasets.Value', 'generate_example_dataset', 'str', 'kwargs.values', 'func', 'dataset.shuffle', 'open', 'f.write']",12
repos/datasets/benchmarks/benchmark_iterating.py:read,read,function,4,6,6,33,5.5,1,0,"['dataset', 'length']","[' datasets.Dataset', None]","[None, None]",17,[],['range'],1
repos/datasets/benchmarks/benchmark_iterating.py:read_batch,read_batch,function,5,11,10,65,5.91,1,0,"['dataset', 'length', 'batch_size']","[' datasets.Dataset', None, None]","[None, None, None]",23,[],"['range', 'len']",2
repos/datasets/benchmarks/benchmark_iterating.py:read_formatted,read_formatted,function,6,8,8,70,8.75,1,0,"['dataset', 'length', 'type']","[' datasets.Dataset', None, None]","[None, None, None]",29,[],"['dataset.formatted_as', 'range']",2
repos/datasets/benchmarks/benchmark_iterating.py:read_formatted_batch,read_formatted_batch,function,7,13,12,96,7.38,1,0,"['dataset', 'length', 'batch_size', 'type']","[' datasets.Dataset', None, None, None]","[None, None, None, None]",36,[],"['dataset.formatted_as', 'range']",2
repos/datasets/benchmarks/benchmark_map_filter.py:benchmark_map_filter,benchmark_map_filter,function,28,103,56,1319,12.81,0,0,[],[],[],27,[],"['tempfile.TemporaryDirectory', 'datasets.Features', 'datasets.Value', 'generate_example_dataset', 'tokenize', 'tokenizer', 'map', 'dataset.formatted_as', 'filter', 'open', 'f.write']",11
repos/datasets/benchmarks/benchmark_map_filter.py:filter,filter,function,2,2,2,26,13.0,0,0,"['dataset', '**kwargs']","[' datasets.Dataset', None]","[None, None]",23,[],['dataset.filter'],1
repos/datasets/benchmarks/benchmark_map_filter.py:map,map,function,2,2,2,23,11.5,0,0,"['dataset', '**kwargs']","[' datasets.Dataset', None]","[None, None]",18,[],['dataset.map'],1
repos/datasets/benchmarks/format.py:format_json_to_md,format_json_to_md,function,31,120,71,1008,8.4,2,4,"['input_json_file', 'output_md_file']","[None, None]","[None, None]",5,[],"['open', 'json.load', 'sorted', 'benchmark_name.split', 'output_md.append', 'metric_vals.get', 'isinstance', 'f.writelines']",8
repos/datasets/benchmarks/utils.py:generate_example_dataset,generate_example_dataset,function,18,44,43,554,12.59,1,1,"['dataset_path', 'features', 'num_examples', 'seq_shapes']","[None, None, None, None]","[None, None, '100', 'None']",47,[],"['generate_examples', 'ArrowWriter', 'features.encode_example', 'writer.write', 'writer.finalize', 'ValueError']",6
repos/datasets/benchmarks/utils.py:generate_examples,generate_examples,function,19,61,47,588,9.64,3,2,"['features', 'num_examples', 'seq_shapes']","[' dict', None, None]","[None, '100', 'None']",22,[],"['range', 'enumerate', 'isinstance', 'dummy_data.append']",4
repos/datasets/benchmarks/utils.py:get_duration,get_duration,function,9,17,16,179,10.53,0,0,['func'],[None],[None],10,[],"['wrapper', 'timeit.default_timer', 'func']",3
repos/datasets/metrics/accuracy/accuracy.py:Accuracy,Accuracy,class,8,43,32,697,16.21,0,1,[],[],[],80,[],[],0
repos/datasets/metrics/accuracy/accuracy.py:Accuracy:_compute,Accuracy:_compute,method,1,10,10,116,11.6,0,0,"['self', 'predictions', 'references', 'normalize', 'sample_weight']","[None, None, None, None, None]","[None, None, None, 'True', 'None']",100,[],"['float', 'accuracy_score']",2
repos/datasets/metrics/accuracy/accuracy.py:Accuracy:_info,Accuracy:_info,method,6,25,19,488,19.52,0,1,['self'],[None],[None],81,[],"['datasets.MetricInfo', 'datasets.Sequence', 'datasets.Value']",3
repos/datasets/metrics/bertscore/bertscore.py:filter_logging_context,filter_logging_context,function,7,23,22,245,10.65,0,1,[],[],[],26,[],"['filter_log', 'logger.addFilter', 'logger.removeFilter']",3
repos/datasets/metrics/bertscore/bertscore.py:BERTScore,BERTScore,class,40,205,161,2620,12.78,0,7,[],[],[],100,[],[],0
repos/datasets/metrics/bertscore/bertscore.py:BERTScore:_compute,BERTScore:_compute,method,28,125,96,1464,11.71,0,4,"['self', 'predictions', 'references', 'lang', 'model_type', 'num_layers', 'verbose', 'idf', 'device', 'batch_size', 'nthreads', 'all_layers', 'rescale_with_baseline', 'baseline_path', 'use_fast_tokenizer', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, 'None', 'None', 'None', 'False', 'False', 'None', '64', '4', 'False', 'False', 'None', 'False', None]",120,[],"['version.parse', 'functools.partial', 'ImportWarning', 'get_hash', 'filter_logging_context', 'hasattr', 'scorer', 'P.tolist', 'R.tolist', 'F.tolist']",10
repos/datasets/metrics/bertscore/bertscore.py:BERTScore:_info,BERTScore:_info,method,5,23,23,495,21.52,0,0,['self'],[None],[None],101,[],"['datasets.MetricInfo', 'datasets.Value', 'datasets.Sequence']",3
repos/datasets/metrics/bertscore/bertscore.py:BERTScore:add,BERTScore:add,method,3,8,8,114,14.25,0,1,"['self', 'prediction', 'reference', '**kwargs']","[None, None, None, None]","[None, 'None', 'None', None]",201,"['        """"""Add one prediction and reference for the metric\'s stack.""""""\n']","['isinstance', 'super']",2
repos/datasets/metrics/bertscore/bertscore.py:BERTScore:add_batch,BERTScore:add_batch,method,3,19,16,161,8.47,0,2,"['self', 'predictions', 'references', '**kwargs']","[None, None, None, None]","[None, 'None', 'None', None]",193,"['        """"""Add a batch of predictions and references for the metric\'s stack.""""""\n']","['isinstance', 'super']",2
repos/datasets/metrics/bleu/bleu.py:Bleu,Bleu,class,9,63,56,1032,16.38,0,0,[],[],[],93,[],[],0
repos/datasets/metrics/bleu/bleu.py:Bleu:_compute,Bleu:_compute,method,3,29,29,353,12.17,0,0,"['self', 'predictions', 'references', 'max_order', 'smooth']","[None, None, None, None, None]","[None, None, None, '4', 'False']",114,[],['compute_bleu'],1
repos/datasets/metrics/bleu/bleu.py:Bleu:_info,Bleu:_info,method,5,26,23,595,22.88,0,0,['self'],[None],[None],94,[],"['datasets.MetricInfo', 'datasets.Sequence']",2
repos/datasets/metrics/bleurt/bleurt.py:BLEURT,BLEURT,class,20,103,84,1369,13.29,0,2,[],[],[],79,[],[],0
repos/datasets/metrics/bleurt/bleurt.py:BLEURT:_compute,BLEURT:_compute,method,3,6,6,94,15.67,0,0,"['self', 'predictions', 'references']","[None, None, None]","[None, None, None]",120,[],[],0
repos/datasets/metrics/bleurt/bleurt.py:BLEURT:_download_and_prepare,BLEURT:_download_and_prepare,method,10,68,55,704,10.35,0,2,"['self', 'dl_manager']","[None, None]","[None, None]",96,[],"['logger.warning', 'datasets.load_metric', 'KeyError', 'dl_manager.download_and_extract', 'score.BleurtScorer']",5
repos/datasets/metrics/bleurt/bleurt.py:BLEURT:_info,BLEURT:_info,method,5,20,18,468,23.4,0,0,['self'],[None],[None],80,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/cer/cer.py:CER,CER,class,16,67,50,1071,15.99,1,1,[],[],[],118,[],[],0
repos/datasets/metrics/cer/cer.py:CER:_compute,CER:_compute,method,10,39,27,526,13.49,1,1,"['self', 'predictions', 'references', 'concatenate_texts']","[None, None, None, None]","[None, None, None, 'False']",137,[],"['jiwer.compute_measures', 'zip']",2
repos/datasets/metrics/cer/cer.py:CER:_info,CER:_info,method,5,21,19,462,22.0,0,0,['self'],[None],[None],119,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/cer/test_cer.py:TestCER,TestCER,class,23,224,80,2712,12.11,0,0,[],[],[],22,[],[],0
repos/datasets/metrics/cer/test_cer.py:TestCER:test_cer_case_senstive,TestCER:test_cer_case_senstive,method,5,13,13,159,12.23,0,0,['self'],[None],[None],23,[],"['cer.compute', 'self.assertTrue']",2
repos/datasets/metrics/cer/test_cer.py:TestCER:test_cer_del,TestCER:test_cer_del,method,5,11,11,149,13.55,0,0,['self'],[None],[None],64,[],"['cer.compute', 'self.assertTrue']",2
repos/datasets/metrics/cer/test_cer.py:TestCER:test_cer_empty,TestCER:test_cer_empty,method,5,8,8,112,14.0,0,0,['self'],[None],[None],120,[],"['self.assertRaises', 'cer.compute']",2
repos/datasets/metrics/cer/test_cer.py:TestCER:test_cer_equal,TestCER:test_cer_equal,method,4,7,7,117,16.71,0,0,['self'],[None],[None],78,[],"['cer.compute', 'self.assertEqual']",2
repos/datasets/metrics/cer/test_cer.py:TestCER:test_cer_insert,TestCER:test_cer_insert,method,5,11,11,147,13.36,0,0,['self'],[None],[None],71,[],"['cer.compute', 'self.assertTrue']",2
repos/datasets/metrics/cer/test_cer.py:TestCER:test_cer_list_of_seqs,TestCER:test_cer_list_of_seqs,method,9,32,20,333,10.41,0,0,['self'],[None],[None],83,[],"['cer.compute', 'self.assertEqual', 'self.assertTrue']",3
repos/datasets/metrics/cer/test_cer.py:TestCER:test_cer_sub,TestCER:test_cer_sub,method,5,11,11,148,13.45,0,0,['self'],[None],[None],57,[],"['cer.compute', 'self.assertTrue']",2
repos/datasets/metrics/cer/test_cer.py:TestCER:test_cer_unicode,TestCER:test_cer_unicode,method,6,38,25,452,11.89,0,0,['self'],[None],[None],103,[],"['cer.compute', 'self.assertTrue', 'self.assertFalse']",3
repos/datasets/metrics/cer/test_cer.py:TestCER:test_cer_whitespace,TestCER:test_cer_whitespace,method,6,50,16,594,11.88,0,0,['self'],[None],[None],30,[],"['cer.compute', 'self.assertTrue']",2
repos/datasets/metrics/cer/test_cer.py:TestCER:test_correlated_sentences,TestCER:test_correlated_sentences,method,5,23,20,212,9.22,0,0,['self'],[None],[None],94,[],"['cer.compute', 'self.assertTrue']",2
repos/datasets/metrics/chrf/chrf.py:ChrF,ChrF,class,21,132,109,1481,11.22,2,2,[],[],[],127,[],[],0
repos/datasets/metrics/chrf/chrf.py:ChrF:_compute,ChrF:_compute,method,13,51,44,535,10.49,2,1,"['self', 'predictions', 'references', 'char_order', 'word_order', 'beta', 'lowercase', 'whitespace', 'eps_smoothing', '']","[None, None, None, ' int ', ' int ', ' int ', ' bool ', ' bool ', ' bool ', None]","[None, None, None, ' CHRF.CHAR_ORDER', ' CHRF.WORD_ORDER', ' CHRF.BETA', ' False', ' False', ' False', None]",151,[],"['len', 'any', 'ValueError', 'range', 'CHRF', 'sb_chrf.corpus_score']",6
repos/datasets/metrics/chrf/chrf.py:ChrF:_info,ChrF:_info,method,7,55,53,727,13.22,0,1,['self'],[None],[None],128,[],"['version.parse', 'ImportWarning', 'datasets.MetricInfo', 'datasets.Value', 'datasets.Sequence']",5
repos/datasets/metrics/code_eval/code_eval.py:estimate_pass_at_k,estimate_pass_at_k,function,11,47,39,361,7.68,0,2,"['num_samples', 'num_correct', 'k']","[None, None, None]","[None, None, None]",197,"['    """"""Estimates pass@k of each problem and returns them in an array.""""""\n']","['estimator', 'np.prod', 'np.arange', 'isinstance', 'itertools.repeat', 'len', 'iter', 'np.array', 'int', 'zip']",10
repos/datasets/metrics/code_eval/code_eval.py:CodeEval,CodeEval,class,48,135,109,1569,11.62,4,2,[],[],[],134,[],[],0
repos/datasets/metrics/code_eval/code_eval.py:CodeEval:_compute,CodeEval:_compute,method,40,106,82,1044,9.85,4,2,"['self', 'predictions', 'references', 'k', '10', '100]', 'num_workers', 'timeout']","[None, None, None, None, None, None, None, None]","[None, None, None, '[1', None, None, '4', '3.0']",154,"['        """"""Returns the scores""""""\n']","['os.getenv', 'ValueError', 'NotImplementedError', 'ThreadPoolExecutor', 'Counter', 'defaultdict', 'enumerate', 'executor.submit', 'futures.append', 'as_completed', 'future.result', 'results.values', 'result.sort', 'total.append', 'correct.append', 'np.array', 'estimate_pass_at_k']",17
repos/datasets/metrics/code_eval/code_eval.py:CodeEval:_info,CodeEval:_info,method,7,18,18,427,23.72,0,0,['self'],[None],[None],135,[],"['datasets.MetricInfo', 'datasets.Sequence', 'datasets.Value']",3
repos/datasets/metrics/code_eval/execute.py:chdir,chdir,function,8,18,17,125,6.94,0,1,['root'],[None],[None],142,[],"['os.getcwd', 'os.chdir']",2
repos/datasets/metrics/code_eval/execute.py:check_correctness,check_correctness,function,12,32,31,350,10.94,0,2,"['check_program', 'timeout', 'task_id', 'completion_id']","[None, None, None, None]","[None, None, None, None]",28,"['    """"""\n', '    Evaluates the functional correctness of a completion by running the test\n', '    suite provided in the problem.\n', '\n', '    :param completion_id: an optional completion ID so we can match\n', '        the results later even if execution finishes asynchronously.\n', '    """"""\n']","['multiprocessing.Manager', 'manager.list', 'multiprocessing.Process', 'p.start', 'p.join', 'p.is_alive', 'p.kill', 'result.append']",8
repos/datasets/metrics/code_eval/execute.py:create_tempdir,create_tempdir,function,4,8,7,76,9.5,0,0,[],[],[],110,[],"['tempfile.TemporaryDirectory', 'chdir']",2
repos/datasets/metrics/code_eval/execute.py:reliability_guard,reliability_guard,function,51,114,65,1158,10.16,0,2,['maximum_memory_bytes'],[None],['None'],156,"['    """"""\n', '    This disables various destructive functions and prevents the generated code\n', '    from interfering with the test (e.g. fork bomb, killing other processes,\n', '    removing filesystem files, etc.)\n', '\n', '    WARNING\n', '    This function is NOT a security sandbox. Untrusted code, including, model-\n', '    generated code, should not be blindly executed outside of one. See the\n', ""    Codex paper for more information about OpenAI's code sandbox, and proceed\n"", '    with caution.\n', '    """"""\n']","['resource.setrlimit', 'platform.uname', 'faulthandler.disable']",3
repos/datasets/metrics/code_eval/execute.py:swallow_io,swallow_io,function,6,9,7,140,15.56,0,0,[],[],[],101,[],"['WriteOnlyStringIO', 'contextlib.redirect_stdout', 'contextlib.redirect_stderr', 'redirect_stdin']",4
repos/datasets/metrics/code_eval/execute.py:time_limit,time_limit,function,5,15,14,216,14.4,0,0,['seconds'],[None],[None],88,[],"['signal_handler', 'TimeoutException', 'signal.setitimer', 'signal.signal']",4
repos/datasets/metrics/code_eval/execute.py:unsafe_execute,unsafe_execute,function,19,39,34,389,9.97,0,0,"['check_program', 'result', 'timeout']","[None, None, None]","[None, None, None]",56,[],"['create_tempdir', 'reliability_guard', 'swallow_io', 'time_limit', 'exec', 'result.append']",6
repos/datasets/metrics/code_eval/execute.py:TimeoutException,TimeoutException,class,0,1,1,4,4.0,0,0,[],[],[],116,[],[],0
repos/datasets/metrics/code_eval/execute.py:WriteOnlyStringIO,WriteOnlyStringIO,class,6,24,11,183,7.62,0,0,[],[],[],120,[],[],0
repos/datasets/metrics/code_eval/execute.py:redirect_stdin,redirect_stdin,class,1,2,2,15,7.5,0,0,[],[],[],137,[],[],0
repos/datasets/metrics/code_eval/execute.py:WriteOnlyStringIO:read,WriteOnlyStringIO:read,method,1,2,2,12,6.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",123,[],[],0
repos/datasets/metrics/code_eval/execute.py:WriteOnlyStringIO:readable,WriteOnlyStringIO:readable,method,1,2,2,11,5.5,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",132,"['        """"""Returns True if the IO object can be read.""""""\n']",[],0
repos/datasets/metrics/code_eval/execute.py:WriteOnlyStringIO:readline,WriteOnlyStringIO:readline,method,1,2,2,12,6.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",126,[],[],0
repos/datasets/metrics/code_eval/execute.py:WriteOnlyStringIO:readlines,WriteOnlyStringIO:readlines,method,1,2,2,12,6.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",129,[],[],0
repos/datasets/metrics/comet/comet.py:COMET,COMET,class,19,80,66,1205,15.06,0,3,[],[],[],109,[],[],0
repos/datasets/metrics/comet/comet.py:COMET:_compute,COMET:_compute,method,9,34,31,285,8.38,0,2,"['self', 'sources', 'predictions', 'references', 'gpus', 'progress_bar']","[None, None, None, None, None, None]","[None, None, None, None, 'None', 'False']",137,[],['zip'],1
repos/datasets/metrics/comet/comet.py:COMET:_download_and_prepare,COMET:_download_and_prepare,method,3,8,7,194,24.25,0,1,"['self', 'dl_manager']","[None, None]","[None, None]",131,[],['comet.load_from_checkpoint'],1
repos/datasets/metrics/comet/comet.py:COMET:_info,COMET:_info,method,5,26,22,586,22.54,0,0,['self'],[None],[None],110,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/coval/coval.py:check_gold_parse_annotation,check_gold_parse_annotation,function,8,26,19,196,7.54,1,3,['key_lines'],[None],[None],257,[],"['line.startswith', 'len', 'line.split']",3
repos/datasets/metrics/coval/coval.py:evaluate,evaluate,function,16,70,56,669,9.56,1,2,"['key_lines', 'sys_lines', 'metrics', 'NP_only', 'remove_nested', 'keep_singletons', 'min_span']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",228,[],"['get_coref_infos', 'evaluator.evaluate_documents', 'output_scores.update', 'logger.info', 'name.ljust']",5
repos/datasets/metrics/coval/coval.py:get_coref_infos,get_coref_infos,function,33,145,83,1845,12.72,0,5,"['key_lines', 'sys_lines', 'NP_only', 'remove_nested', 'keep_singletons', 'min_span', 'doc']","[None, None, None, None, None, None, None]","[None, None, 'False', 'False', 'True', 'False', '""dummy_doc""']",168,[],"['reader.get_doc_mentions', 'reader.set_annotated_parse_trees', 'reader.remove_nested_coref_mentions', 'reader.get_mention_assignments', 'logger.info']",5
repos/datasets/metrics/coval/coval.py:Coval,Coval,class,14,74,68,1122,15.16,0,2,[],[],[],272,[],[],0
repos/datasets/metrics/coval/coval.py:Coval:_compute,Coval:_compute,method,8,42,40,518,12.33,0,2,"['self', 'predictions', 'references', 'keep_singletons', 'NP_only', 'min_span', 'remove_nested']","[None, None, None, None, None, None, None]","[None, None, None, 'True', 'False', 'False', 'False']",292,[],"['util.check_gold_parse_annotation', 'NotImplementedError', 'evaluate']",3
repos/datasets/metrics/coval/coval.py:Coval:_info,Coval:_info,method,5,20,19,473,23.65,0,0,['self'],[None],[None],273,[],"['datasets.MetricInfo', 'datasets.Sequence']",2
repos/datasets/metrics/cuad/cuad.py:CUAD,CUAD,class,14,79,51,920,11.65,1,0,[],[],[],70,[],[],0
repos/datasets/metrics/cuad/cuad.py:CUAD:_compute,CUAD:_compute,method,7,40,27,308,7.7,1,0,"['self', 'predictions', 'references']","[None, None, None]","[None, None, None]",97,[],['evaluate'],1
repos/datasets/metrics/cuad/cuad.py:CUAD:_info,CUAD:_info,method,6,33,24,553,16.76,0,0,['self'],[None],[None],71,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/cuad/evaluate.py:compute_precision_recall,compute_precision_recall,function,17,133,55,836,6.29,4,8,"['predictions', 'ground_truths', 'qa_id']","[None, None, None]","[None, None, None]",51,[],"['len', 'get_jaccard']",2
repos/datasets/metrics/cuad/evaluate.py:evaluate,evaluate,function,36,119,86,1088,9.14,4,2,"['dataset', 'predictions']","[None, None]","[None, None]",150,[],"['print', 'compute_precision_recall', 'precisions.append', 'recalls.append', 'metric_max_over_ground_truths', 'sorted', 'recalls.sort', 'get_aupr', 'get_prec_at_recall']",9
repos/datasets/metrics/cuad/evaluate.py:exact_match_score,exact_match_score,function,2,3,3,66,22.0,0,0,"['prediction', 'ground_truth']","[None, None]","[None, None]",134,[],['normalize_answer'],1
repos/datasets/metrics/cuad/evaluate.py:get_aupr,get_aupr,function,6,11,9,132,12.0,0,1,"['precisions', 'recalls']","[None, None]","[None, None]",115,[],"['process_precisions', 'np.trapz', 'np.isnan']",3
repos/datasets/metrics/cuad/evaluate.py:get_jaccard,get_jaccard,function,14,42,34,501,11.93,1,0,"['prediction', 'ground_truth']","[None, None]","[None, None]",15,[],"['ground_truth.replace', 'prediction.replace', 'ground_truth.lower', 'prediction.lower', 'set', 'ground_truth.intersection', 'ground_truth.union', 'len']",8
repos/datasets/metrics/cuad/evaluate.py:get_prec_at_recall,get_prec_at_recall,function,10,18,15,191,10.61,1,1,"['precisions', 'recalls', 'recall_thresh']","[None, None, None]","[None, None, None]",123,"['    """"""Assumes recalls are sorted in increasing order""""""\n']","['process_precisions', 'zip']",2
repos/datasets/metrics/cuad/evaluate.py:metric_max_over_ground_truths,metric_max_over_ground_truths,function,16,34,25,195,5.74,2,2,"['metric_fn', 'predictions', 'ground_truths']","[None, None, None]","[None, None, None]",138,[],['metric_fn'],1
repos/datasets/metrics/cuad/evaluate.py:normalize_answer,normalize_answer,function,8,33,23,315,9.55,0,0,['s'],[None],[None],32,"['    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n']","['remove_articles', 're.sub', 'white_space_fix', 'remove_punc', 'set', 'lower', 'text.lower']",7
repos/datasets/metrics/cuad/evaluate.py:process_precisions,process_precisions,function,9,15,14,177,11.8,1,0,['precisions'],[None],[None],103,"['    """"""\n', ""    Processes precisions to ensure that precision and recall don't both get worse.\n"", '    Assumes the list precision is sorted in order of recalls\n', '    """"""\n']","['range', 'len', 'max']",3
repos/datasets/metrics/exact_match/exact_match.py:ExactMatch,ExactMatch,class,22,96,64,1240,12.92,1,4,[],[],[],88,[],[],0
repos/datasets/metrics/exact_match/exact_match.py:ExactMatch:_compute,ExactMatch:_compute,method,17,67,39,810,12.09,1,4,"['self', 'predictions', 'references', 'regexes_to_ignore', 'ignore_case', 'ignore_punctuation', 'ignore_numbers', '']","[None, None, None, None, None, None, None, None]","[None, None, None, 'None', 'False', 'False', 'False', None]",103,[],"['np.array', 'np.asarray', 'np.mean']",3
repos/datasets/metrics/exact_match/exact_match.py:ExactMatch:_info,ExactMatch:_info,method,4,17,15,275,16.18,0,0,['self'],[None],[None],89,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/f1/f1.py:F1,F1,class,10,52,39,769,14.79,0,1,[],[],[],99,[],[],0
repos/datasets/metrics/f1/f1.py:F1:_compute,F1:_compute,method,3,17,17,168,9.88,0,0,"['self', 'predictions', 'references', 'labels', 'pos_label', 'average', 'sample_weight']","[None, None, None, None, None, None, None]","[None, None, None, 'None', '1', '""binary""', 'None']",119,[],"['f1_score', 'float']",2
repos/datasets/metrics/f1/f1.py:F1:_info,F1:_info,method,6,25,19,482,19.28,0,1,['self'],[None],[None],100,[],"['datasets.MetricInfo', 'datasets.Sequence', 'datasets.Value']",3
repos/datasets/metrics/frugalscore/frugalscore.py:FRUGALSCORE,FRUGALSCORE,class,33,122,106,1644,13.48,0,3,[],[],[],57,[],[],0
repos/datasets/metrics/frugalscore/frugalscore.py:FRUGALSCORE:_compute,FRUGALSCORE:_compute,method,22,81,74,955,11.79,0,2,"['self', 'predictions', 'references', 'batch_size', 'max_length', 'device', '']","[None, None, None, None, None, None, None]","[None, None, None, '32', '128', 'None', None]",80,"['        """"""Returns the scores""""""\n']","['len', 'TrainingArguments', 'tokenize_function', 'self.tokenizer', 'raw_datasets.map', 'tokenized_datasets.remove_columns', 'Trainer', 'trainer.predict', 'list']",9
repos/datasets/metrics/frugalscore/frugalscore.py:FRUGALSCORE:_download_and_prepare,FRUGALSCORE:_download_and_prepare,method,6,12,11,256,21.33,0,1,"['self', 'dl_manager']","[None, None]","[None, None]",72,[],"['AutoModelForSequenceClassification.from_pretrained', 'AutoTokenizer.from_pretrained']",2
repos/datasets/metrics/frugalscore/frugalscore.py:FRUGALSCORE:_info,FRUGALSCORE:_info,method,4,15,14,281,18.73,0,0,['self'],[None],[None],58,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/glue/glue.py:acc_and_f1,acc_and_f1,function,4,13,13,115,8.85,0,0,"['preds', 'labels']","[None, None]","[None, None]",86,[],"['simple_accuracy', 'float']",2
repos/datasets/metrics/glue/glue.py:pearson_and_spearman,pearson_and_spearman,function,3,13,12,154,11.85,0,0,"['preds', 'labels']","[None, None]","[None, None]",95,[],['float'],1
repos/datasets/metrics/glue/glue.py:simple_accuracy,simple_accuracy,function,1,3,3,35,11.67,0,0,"['preds', 'labels']","[None, None]","[None, None]",82,[],['float'],1
repos/datasets/metrics/glue/glue.py:Glue,Glue,class,14,138,83,1323,9.59,0,2,[],[],[],105,[],[],0
repos/datasets/metrics/glue/glue.py:Glue:_compute,Glue:_compute,method,7,62,45,595,9.6,0,1,"['self', 'predictions', 'references']","[None, None, None]","[None, None, None]",141,[],"['matthews_corrcoef', 'pearson_and_spearman', 'acc_and_f1', 'simple_accuracy', 'KeyError']",5
repos/datasets/metrics/glue/glue.py:Glue:_info,Glue:_info,method,8,70,61,669,9.56,0,1,['self'],[None],[None],106,[],"['KeyError', 'datasets.MetricInfo', 'datasets.Value']",3
repos/datasets/metrics/google_bleu/google_bleu.py:GoogleBleu,GoogleBleu,class,6,52,43,648,12.46,0,0,[],[],[],176,[],[],0
repos/datasets/metrics/google_bleu/google_bleu.py:GoogleBleu:_compute,GoogleBleu:_compute,method,2,10,10,134,13.4,0,0,"['self', 'predictions', 'references', 'min_len', 'max_len', '']","[None, ' List[List[List[str]]]', ' List[List[str]]', ' int ', ' int ', None]","[None, None, None, ' 1', ' 4', None]",192,[],['gleu_score.corpus_gleu'],1
repos/datasets/metrics/google_bleu/google_bleu.py:GoogleBleu:_info,GoogleBleu:_info,method,3,21,18,353,16.81,0,0,['self'],[None],[None],177,[],"['datasets.MetricInfo', 'datasets.Sequence']",2
repos/datasets/metrics/indic_glue/indic_glue.py:acc_and_f1,acc_and_f1,function,4,13,13,115,8.85,0,0,"['preds', 'labels']","[None, None]","[None, None]",78,[],"['simple_accuracy', 'float']",2
repos/datasets/metrics/indic_glue/indic_glue.py:precision_at_10,precision_at_10,function,14,30,25,370,12.33,0,0,"['en_sentvecs', 'in_sentvecs']","[None, None]","[None, None]",87,[],"['np.array', 'np.mean', 'cdist', 'sim.argsort', 'np.any', 'float']",6
repos/datasets/metrics/indic_glue/indic_glue.py:simple_accuracy,simple_accuracy,function,1,3,3,35,11.67,0,0,"['preds', 'labels']","[None, None]","[None, None]",74,[],['float'],1
repos/datasets/metrics/indic_glue/indic_glue.py:IndicGlue,IndicGlue,class,14,146,81,1442,9.88,0,5,[],[],[],104,[],[],0
repos/datasets/metrics/indic_glue/indic_glue.py:IndicGlue:_compute,IndicGlue:_compute,method,6,62,53,551,8.89,0,1,"['self', 'predictions', 'references']","[None, None, None]","[None, None, None]",146,[],"['precision_at_10', 'acc_and_f1', 'simple_accuracy', 'KeyError']",4
repos/datasets/metrics/indic_glue/indic_glue.py:IndicGlue:_info,IndicGlue:_info,method,9,78,65,832,10.67,0,4,['self'],[None],[None],105,[],"['KeyError', 'datasets.MetricInfo', 'datasets.Value', 'datasets.Sequence']",4
repos/datasets/metrics/mae/mae.py:Mae,Mae,class,11,46,34,799,17.37,0,1,[],[],[],84,[],[],0
repos/datasets/metrics/mae/mae.py:Mae:_compute,Mae:_compute,method,3,8,8,129,16.12,0,0,"['self', 'predictions', 'references', 'sample_weight', 'multioutput']","[None, None, None, None, None]","[None, None, None, 'None', '""uniform_average""']",108,[],['mean_absolute_error'],1
repos/datasets/metrics/mae/mae.py:Mae:_get_feature_types,Mae:_get_feature_types,method,4,18,12,250,13.89,0,1,['self'],[None],[None],96,[],"['datasets.Sequence', 'datasets.Value']",2
repos/datasets/metrics/mae/mae.py:Mae:_info,Mae:_info,method,3,10,10,282,28.2,0,0,['self'],[None],[None],85,[],['datasets.MetricInfo'],1
repos/datasets/metrics/mahalanobis/mahalanobis.py:Mahalanobis,Mahalanobis,class,20,90,68,964,10.71,0,3,[],[],[],60,[],[],0
repos/datasets/metrics/mahalanobis/mahalanobis.py:Mahalanobis:_compute,Mahalanobis:_compute,method,17,70,51,685,9.79,0,3,"['self', 'X', 'reference_distribution']","[None, None, None]","[None, None, None]",73,[],"['np.array', 'len', 'ValueError', 'np.mean', 'np.cov', 'np.dot']",6
repos/datasets/metrics/mahalanobis/mahalanobis.py:Mahalanobis:_info,Mahalanobis:_info,method,2,14,14,218,15.57,0,0,['self'],[None],[None],61,[],"['datasets.MetricInfo', 'datasets.Sequence']",2
repos/datasets/metrics/matthews_correlation/matthews_correlation.py:MatthewsCorrelation,MatthewsCorrelation,class,6,31,26,525,16.94,0,0,[],[],[],82,[],[],0
repos/datasets/metrics/matthews_correlation/matthews_correlation.py:MatthewsCorrelation:_compute,MatthewsCorrelation:_compute,method,1,7,7,110,15.71,0,0,"['self', 'predictions', 'references', 'sample_weight']","[None, None, None, None]","[None, None, None, 'None']",99,[],['float'],1
repos/datasets/metrics/matthews_correlation/matthews_correlation.py:MatthewsCorrelation:_info,MatthewsCorrelation:_info,method,4,17,16,337,19.82,0,0,['self'],[None],[None],83,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/mauve/mauve.py:Mauve,Mauve,class,9,69,63,1401,20.3,0,0,[],[],[],88,[],[],0
repos/datasets/metrics/mauve/mauve.py:Mauve:_compute,Mauve:_compute,method,3,23,22,557,24.22,0,0,"['self', 'predictions', 'references', 'p_features', 'q_features', 'p_tokens', 'q_tokens', 'num_buckets', 'pca_max_data', 'kmeans_explained_var', 'kmeans_num_redo', 'kmeans_max_iter', 'featurize_model_name', 'device_id', 'max_text_length', 'divergence_curve_discretization_size', 'mauve_scaling_factor', 'verbose', 'seed', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, 'None', 'None', 'None', 'None', '""auto""', '-1', '0.9', '5', '500', '""gpt2-large""', '-1', '1024', '25', '5', 'True', '25', None]",108,[],['compute_mauve'],1
repos/datasets/metrics/mauve/mauve.py:Mauve:_info,Mauve:_info,method,5,22,20,454,20.64,0,0,['self'],[None],[None],89,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/mean_iou/mean_iou.py:intersect_and_union,intersect_and_union,function,26,68,47,721,10.6,1,2,"['pred_label', 'label', 'num_labels', 'ignore_index', 'label_map', 'int]] ', 'reduce_labels', '']","[None, None, None, ' bool', ' Optional[Dict[int', None, ' bool ', None]","[None, None, None, None, None, ' None', ' False', None]",95,"['    """"""Calculate intersection and Union.\n', '\n', '    Args:\n', '        pred_label (`ndarray`):\n', '            Prediction segmentation map of shape (height, width).\n', '        label (`ndarray`):\n', '            Ground truth segmentation map of shape (height, width).\n', '        num_labels (`int`):\n', '            Number of categories.\n', '        ignore_index (`int`):\n', '            Index that will be ignored during evaluation.\n', '        label_map (`dict`, *optional*):\n', '            Mapping old labels to new labels. The parameter will work only when label is str.\n', '        reduce_labels (`bool`, *optional*, defaults to `False`):\n', '            Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0 is used for background,\n', '            and background itself is not included in all classes of a dataset (e.g. ADE20k). The background label will be replaced by 255.\n', '\n', '     Returns:\n', '         area_intersect (`ndarray`):\n', '            The intersection of prediction and ground truth histogram on all classes.\n', '         area_union (`ndarray`):\n', '            The union of prediction and ground truth histogram on all classes.\n', '         area_pred_label (`ndarray`):\n', '            The prediction histogram on all classes.\n', '         area_label (`ndarray`):\n', '            The ground truth histogram on all classes.\n', '    """"""\n']","['label_map.items', 'np.array', 'np.not_equal', 'np.histogram']",4
repos/datasets/metrics/mean_iou/mean_iou.py:mean_iou,mean_iou,function,19,49,46,644,13.14,0,1,"['results', 'gt_seg_maps', 'num_labels', 'ignore_index', 'nan_to_num', 'label_map', 'int]] ', 'reduce_labels', '']","[None, None, None, ' bool', ' Optional[int] ', ' Optional[Dict[int', None, ' bool ', None]","[None, None, None, None, ' None', None, ' None', ' False', None]",209,"['    """"""Calculate Mean Intersection and Union (mIoU).\n', '\n', '    Args:\n', '        results (`ndarray`):\n', '            List of prediction segmentation maps, each of shape (height, width).\n', '        gt_seg_maps (`ndarray`):\n', '            List of ground truth segmentation maps, each of shape (height, width).\n', '        num_labels (`int`):\n', '            Number of categories.\n', '        ignore_index (`int`):\n', '            Index that will be ignored during evaluation.\n', '        nan_to_num (`int`, *optional*):\n', '            If specified, NaN values will be replaced by the number defined by the user.\n', '        label_map (`dict`, *optional*):\n', '            Mapping old labels to new labels. The parameter will work only when label is str.\n', '        reduce_labels (`bool`, *optional*, defaults to `False`):\n', '            Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0 is used for background,\n', '            and background itself is not included in all classes of a dataset (e.g. ADE20k). The background label will be replaced by 255.\n', '\n', '    Returns:\n', '        `Dict[str, float | ndarray]` comprising various elements:\n', '        - *mean_iou* (`float`):\n', '            Mean Intersection-over-Union (IoU averaged over all categories).\n', '        - *mean_accuracy* (`float`):\n', '            Mean accuracy (averaged over all categories).\n', '        - *overall_accuracy* (`float`):\n', '            Overall accuracy on all images.\n', '        - *per_category_accuracy* (`ndarray` of shape `(num_labels,)`):\n', '            Per category accuracy.\n', '        - *per_category_iou* (`ndarray` of shape `(num_labels,)`):\n', '            Per category IoU.\n', '    """"""\n']","['total_intersect_and_union', 'total_area_intersect.sum', 'total_area_label.sum', 'np.nanmean', 'np.nan_to_num', 'metrics.items']",6
repos/datasets/metrics/mean_iou/mean_iou.py:total_intersect_and_union,total_intersect_and_union,function,20,47,32,647,13.77,1,0,"['results', 'gt_seg_maps', 'num_labels', 'ignore_index', 'label_map', 'int]] ', 'reduce_labels', '']","[None, None, None, ' bool', ' Optional[Dict[int', None, ' bool ', None]","[None, None, None, None, None, ' None', ' False', None]",159,"['    """"""Calculate Total Intersection and Union, by calculating `intersect_and_union` for each (predicted, ground truth) pair.\n', '\n', '    Args:\n', '        results (`ndarray`):\n', '            List of prediction segmentation maps, each of shape (height, width).\n', '        gt_seg_maps (`ndarray`):\n', '            List of ground truth segmentation maps, each of shape (height, width).\n', '        num_labels (`int`):\n', '            Number of categories.\n', '        ignore_index (`int`):\n', '            Index that will be ignored during evaluation.\n', '        label_map (`dict`, *optional*):\n', '            Mapping old labels to new labels. The parameter will work only when label is str.\n', '        reduce_labels (`bool`, *optional*, defaults to `False`):\n', '            Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0 is used for background,\n', '            and background itself is not included in all classes of a dataset (e.g. ADE20k). The background label will be replaced by 255.\n', '\n', '     Returns:\n', '         total_area_intersect (`ndarray`):\n', '            The intersection of prediction and ground truth histogram on all classes.\n', '         total_area_union (`ndarray`):\n', '            The union of prediction and ground truth histogram on all classes.\n', '         total_area_pred_label (`ndarray`):\n', '            The prediction histogram on all classes.\n', '         total_area_label (`ndarray`):\n', '            The ground truth histogram on all classes.\n', '    """"""\n']","['np.zeros', 'zip', 'intersect_and_union']",3
repos/datasets/metrics/mean_iou/mean_iou.py:MeanIoU,MeanIoU,class,8,51,45,853,16.73,0,0,[],[],[],274,[],[],0
repos/datasets/metrics/mean_iou/mean_iou.py:MeanIoU:_compute,MeanIoU:_compute,method,3,12,11,207,17.25,0,0,"['self', 'predictions', 'references', 'num_labels', 'ignore_index', 'nan_to_num', 'label_map', 'int]] ', 'reduce_labels', '']","[None, None, None, ' int', ' bool', ' Optional[int] ', ' Optional[Dict[int', None, ' bool ', None]","[None, None, None, None, None, ' None', None, ' None', ' False', None]",292,[],['mean_iou'],1
repos/datasets/metrics/mean_iou/mean_iou.py:MeanIoU:_info,MeanIoU:_info,method,4,17,16,450,26.47,0,0,['self'],[None],[None],275,[],"['datasets.MetricInfo', 'datasets.Sequence']",2
repos/datasets/metrics/meteor/meteor.py:Meteor,Meteor,class,15,81,59,1158,14.3,0,3,[],[],[],87,[],[],0
repos/datasets/metrics/meteor/meteor.py:Meteor:_compute,Meteor:_compute,method,5,37,26,370,10.0,0,1,"['self', 'predictions', 'references', 'alpha', 'beta', 'gamma']","[None, None, None, None, None, None]","[None, None, None, '0.9', '3', '0.5']",115,[],"['version.Version', 'meteor_score.single_meteor_score', 'word_tokenize', 'zip', 'np.mean']",5
repos/datasets/metrics/meteor/meteor.py:Meteor:_download_and_prepare,Meteor:_download_and_prepare,method,5,11,9,167,15.18,0,2,"['self', 'dl_manager']","[None, None]","[None, None]",106,[],"['nltk.download', 'version.Version']",2
repos/datasets/metrics/meteor/meteor.py:Meteor:_info,Meteor:_info,method,5,21,19,491,23.38,0,0,['self'],[None],[None],88,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/mse/mse.py:Mse,Mse,class,11,50,36,816,16.32,0,1,[],[],[],88,[],[],0
repos/datasets/metrics/mse/mse.py:Mse:_compute,Mse:_compute,method,3,11,11,134,12.18,0,0,"['self', 'predictions', 'references', 'sample_weight', 'multioutput', 'squared']","[None, None, None, None, None, None]","[None, None, None, 'None', '""uniform_average""', 'True']",112,[],['mean_squared_error'],1
repos/datasets/metrics/mse/mse.py:Mse:_get_feature_types,Mse:_get_feature_types,method,4,18,12,250,13.89,0,1,['self'],[None],[None],100,[],"['datasets.Sequence', 'datasets.Value']",2
repos/datasets/metrics/mse/mse.py:Mse:_info,Mse:_info,method,3,10,10,281,28.1,0,0,['self'],[None],[None],89,[],['datasets.MetricInfo'],1
repos/datasets/metrics/pearsonr/pearsonr.py:Pearsonr,Pearsonr,class,9,37,32,580,15.68,0,1,[],[],[],86,[],[],0
repos/datasets/metrics/pearsonr/pearsonr.py:Pearsonr:_compute,Pearsonr:_compute,method,4,15,13,176,11.73,0,1,"['self', 'predictions', 'references', 'return_pvalue']","[None, None, None, None]","[None, None, None, 'False']",101,[],"['pearsonr', 'float']",2
repos/datasets/metrics/pearsonr/pearsonr.py:Pearsonr:_info,Pearsonr:_info,method,4,15,14,325,21.67,0,0,['self'],[None],[None],87,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/perplexity/perplexity.py:Perplexity,Perplexity,class,55,268,196,2922,10.9,1,7,[],[],[],88,[],[],0
repos/datasets/metrics/perplexity/perplexity.py:Perplexity:_compute,Perplexity:_compute,method,51,242,173,2548,10.53,1,7,"['self', 'input_texts', 'model_id', 'batch_size', 'add_start_token', 'device']","[None, None, None, ' int ', ' bool ', None]","[None, None, None, ' 16', ' True', 'None']",102,[],"['AutoModelForCausalLM.from_pretrained', 'model.to', 'AutoTokenizer.from_pretrained', 'list', 'len', 'tokenizer.add_special_tokens', 'tokenizer', 'torch.all', 'torch.ge', 'CrossEntropyLoss', 'logging.tqdm', 'min', 'torch.tensor', 'encoded_batch.size', 'torch.cat', 'torch.no_grad', 'model', 'torch.exp2', 'shift_attention_mask_batch.sum', 'perplexity_batch.tolist', 'np.mean']",21
repos/datasets/metrics/perplexity/perplexity.py:Perplexity:_info,Perplexity:_info,method,3,13,13,261,20.08,0,0,['self'],[None],[None],89,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/precision/precision.py:Precision,Precision,class,10,56,43,856,15.29,0,1,[],[],[],105,[],[],0
repos/datasets/metrics/precision/precision.py:Precision:_compute,Precision:_compute,method,3,18,18,217,12.06,0,0,"['self', 'predictions', 'references', 'labels', 'pos_label', 'average', 'sample_weight', 'zero_division', '']","[None, None, None, None, None, None, None, None, None]","[None, None, None, 'None', '1', '""binary""', 'None', '""warn""', None]",125,[],"['precision_score', 'float']",2
repos/datasets/metrics/precision/precision.py:Precision:_info,Precision:_info,method,6,25,19,489,19.56,0,1,['self'],[None],[None],106,[],"['datasets.MetricInfo', 'datasets.Sequence', 'datasets.Value']",3
repos/datasets/metrics/recall/recall.py:Recall,Recall,class,10,56,43,847,15.12,0,1,[],[],[],95,[],[],0
repos/datasets/metrics/recall/recall.py:Recall:_compute,Recall:_compute,method,3,18,18,211,11.72,0,0,"['self', 'predictions', 'references', 'labels', 'pos_label', 'average', 'sample_weight', 'zero_division', '']","[None, None, None, None, None, None, None, None, None]","[None, None, None, 'None', '1', '""binary""', 'None', '""warn""', None]",115,[],"['recall_score', 'float']",2
repos/datasets/metrics/recall/recall.py:Recall:_info,Recall:_info,method,6,25,19,486,19.44,0,1,['self'],[None],[None],96,[],"['datasets.MetricInfo', 'datasets.Sequence', 'datasets.Value']",3
repos/datasets/metrics/roc_auc/roc_auc.py:ROCAUC,ROCAUC,class,9,61,42,967,15.85,0,2,[],[],[],145,[],[],0
repos/datasets/metrics/roc_auc/roc_auc.py:ROCAUC:_compute,ROCAUC:_compute,method,2,13,13,170,13.08,0,0,"['self', 'references', 'prediction_scores', 'average', 'sample_weight', 'max_fpr', 'multi_class', 'labels', '']","[None, None, None, None, None, None, None, None, None]","[None, None, None, '""macro""', 'None', 'None', '""raise""', 'None', None]",170,[],['roc_auc_score'],1
repos/datasets/metrics/roc_auc/roc_auc.py:ROCAUC:_info,ROCAUC:_info,method,6,35,22,642,18.34,0,2,['self'],[None],[None],146,[],"['datasets.MetricInfo', 'datasets.Sequence', 'datasets.Value']",3
repos/datasets/metrics/rouge/rouge.py:Rouge,Rouge,class,27,82,63,1109,13.52,3,4,[],[],[],87,[],[],0
repos/datasets/metrics/rouge/rouge.py:Rouge:_compute,Rouge:_compute,method,21,52,37,511,9.83,3,4,"['self', 'predictions', 'references', 'rouge_types', 'use_aggregator', 'use_stemmer']","[None, None, None, None, None, None]","[None, None, None, 'None', 'True', 'False']",106,[],"['rouge_scorer.RougeScorer', 'scoring.BootstrapAggregator', 'zip', 'scorer.score', 'aggregator.add_scores', 'scores.append', 'aggregator.aggregate']",7
repos/datasets/metrics/rouge/rouge.py:Rouge:_info,Rouge:_info,method,5,21,19,484,23.05,0,0,['self'],[None],[None],88,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/sacrebleu/sacrebleu.py:Sacrebleu,Sacrebleu,class,20,136,114,1740,12.79,2,2,[],[],[],105,[],[],0
repos/datasets/metrics/sacrebleu/sacrebleu.py:Sacrebleu:_compute,Sacrebleu:_compute,method,12,65,56,720,11.08,2,1,"['self', 'predictions', 'references', 'smooth_method', 'smooth_value', 'force', 'lowercase', 'tokenize', 'use_effective_order', '']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, '""exp""', 'None', 'False', 'False', 'None', 'False', None]",131,[],"['len', 'any', 'ValueError', 'range', 'scb.corpus_bleu']",5
repos/datasets/metrics/sacrebleu/sacrebleu.py:Sacrebleu:_info,Sacrebleu:_info,method,7,57,55,844,14.81,0,1,['self'],[None],[None],106,[],"['version.parse', 'ImportWarning', 'datasets.MetricInfo', 'datasets.Value', 'datasets.Sequence']",5
repos/datasets/metrics/sari/sari.py:SARIngram,SARIngram,function,52,188,94,2173,11.56,7,7,"['sgrams', 'cgrams', 'rgramslist', 'numref']","[None, None, None, None]","[None, None, None, None]",81,[],"['Counter', 'sgramcounter.items', 'cgramcounter.items', 'len', 'sum', 'set']",6
repos/datasets/metrics/sari/sari.py:SARIsent,SARIsent,function,55,260,116,1882,7.24,4,9,"['ssent', 'csent', 'rsents']","[None, None, None]","[None, None, None]",160,[],"['len', 'ssent.split', 'csent.split', 'rsent.split', 'r1gramslist.append', 'range', 'r2grams.append', 'r3grams.append', 'r4grams.append', 'r2gramslist.append', 'r3gramslist.append', 'r4gramslist.append', 's2grams.append', 's3grams.append', 's4grams.append', 'c2grams.append', 'c3grams.append', 'c4grams.append', 'SARIngram', 'sum']",20
repos/datasets/metrics/sari/sari.py:normalize,normalize,function,12,40,27,592,14.8,0,4,"['sentence', 'lowercase', 'tokenizer', 'return_str']","[None, ' bool ', ' str ', ' bool ']","[None, ' True', ' ""13a""', ' True']",229,[],"['sentence.lower', 'version.parse', 'sacremoses.MosesTokenizer', 'normalized_sent.split']",4
repos/datasets/metrics/sari/sari.py:Sari,Sari,class,15,72,61,999,13.88,1,1,[],[],[],259,[],[],0
repos/datasets/metrics/sari/sari.py:Sari:_compute,Sari:_compute,method,8,40,35,350,8.75,1,1,"['self', 'sources', 'predictions', 'references']","[None, None, None, None]","[None, None, None, None]",279,[],"['len', 'ValueError', 'zip', 'SARIsent', 'normalize']",5
repos/datasets/metrics/sari/sari.py:Sari:_info,Sari:_info,method,6,25,23,582,23.28,0,0,['self'],[None],[None],260,[],"['datasets.MetricInfo', 'datasets.Value', 'datasets.Sequence']",3
repos/datasets/metrics/seqeval/seqeval.py:Seqeval,Seqeval,class,25,120,103,1609,13.41,1,1,[],[],[],102,[],[],0
repos/datasets/metrics/seqeval/seqeval.py:Seqeval:_compute,Seqeval:_compute,method,19,75,69,910,12.13,1,1,"['self', 'predictions', 'references', 'suffix', 'scheme', 'mode', 'sample_weight', 'zero_division', 'int] ', '']","[None, None, None, ' bool ', ' Optional[str] ', ' Optional[str] ', ' Optional[List[int]] ', ' Union[str', None, None]","[None, None, None, ' False', ' None', ' None', ' None', None, ' ""warn""', None]",119,[],"['importlib.import_module', 'getattr', 'ValueError', 'classification_report', 'report.pop', 'report.items', 'accuracy_score']",7
repos/datasets/metrics/seqeval/seqeval.py:Seqeval:_info,Seqeval:_info,method,5,21,18,487,23.19,0,0,['self'],[None],[None],103,[],"['datasets.MetricInfo', 'datasets.Sequence']",2
repos/datasets/metrics/squad/evaluate.py:evaluate,evaluate,function,18,68,46,571,8.4,3,1,"['dataset', 'predictions']","[None, None]","[None, None]",55,[],"['print', 'metric_max_over_ground_truths']",2
repos/datasets/metrics/squad/evaluate.py:exact_match_score,exact_match_score,function,2,3,3,66,22.0,0,0,"['prediction', 'ground_truth']","[None, None]","[None, None]",43,[],['normalize_answer'],1
repos/datasets/metrics/squad/evaluate.py:f1_score,f1_score,function,11,31,24,372,12.0,0,1,"['prediction', 'ground_truth']","[None, None]","[None, None]",30,[],"['normalize_answer', 'Counter', 'sum', 'len']",4
repos/datasets/metrics/squad/evaluate.py:metric_max_over_ground_truths,metric_max_over_ground_truths,function,8,12,12,175,14.58,1,0,"['metric_fn', 'prediction', 'ground_truths']","[None, None, None]","[None, None, None]",47,[],"['metric_fn', 'scores_for_ground_truths.append', 'max']",3
repos/datasets/metrics/squad/evaluate.py:normalize_answer,normalize_answer,function,8,33,23,315,9.55,0,0,['s'],[None],[None],11,"['    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n']","['remove_articles', 're.sub', 'white_space_fix', 'remove_punc', 'set', 'lower', 'text.lower']",7
repos/datasets/metrics/squad/squad.py:Squad,Squad,class,14,77,53,900,11.69,1,0,[],[],[],67,[],[],0
repos/datasets/metrics/squad/squad.py:Squad:_compute,Squad:_compute,method,7,40,27,308,7.7,1,0,"['self', 'predictions', 'references']","[None, None, None]","[None, None, None]",91,[],['evaluate'],1
repos/datasets/metrics/squad/squad.py:Squad:_info,Squad:_info,method,6,31,26,533,17.19,0,0,['self'],[None],[None],68,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/squad_v2/evaluate.py:apply_no_ans_threshold,apply_no_ans_threshold,function,9,21,19,174,8.29,1,1,"['scores', 'na_probs', 'qid_to_has_ans', 'na_prob_thresh']","[None, None, None, None]","[None, None, None, None]",127,[],"['scores.items', 'float']",2
repos/datasets/metrics/squad_v2/evaluate.py:compute_exact,compute_exact,function,1,3,3,61,20.33,0,0,"['a_gold', 'a_pred']","[None, None]","[None, None]",86,[],"['int', 'normalize_answer']",2
repos/datasets/metrics/squad_v2/evaluate.py:compute_f1,compute_f1,function,11,40,30,376,9.4,0,2,"['a_gold', 'a_pred']","[None, None]","[None, None]",90,[],"['get_tokens', 'collections.Counter', 'sum', 'len', 'int']",5
repos/datasets/metrics/squad_v2/evaluate.py:find_all_best_thresh,find_all_best_thresh,function,6,20,16,300,15.0,0,0,"['main_eval', 'preds', 'exact_raw', 'f1_raw', 'na_probs', 'qid_to_has_ans']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",271,[],['find_best_thresh'],1
repos/datasets/metrics/squad_v2/evaluate.py:find_best_thresh,find_best_thresh,function,18,59,40,440,7.46,1,4,"['preds', 'scores', 'na_probs', 'qid_to_has_ans']","[None, None, None, None]","[None, None, None, None]",248,[],"['sum', 'sorted', 'enumerate', 'len']",4
repos/datasets/metrics/squad_v2/evaluate.py:get_raw_scores,get_raw_scores,function,24,60,38,443,7.38,4,3,"['dataset', 'preds']","[None, None]","[None, None]",106,[],"['normalize_answer', 'print', 'max']",3
repos/datasets/metrics/squad_v2/evaluate.py:get_tokens,get_tokens,function,3,7,6,50,7.14,0,1,['s'],[None],[None],80,[],['normalize_answer'],1
repos/datasets/metrics/squad_v2/evaluate.py:histogram_na_prob,histogram_na_prob,function,13,33,31,344,10.42,1,1,"['na_probs', 'qid_list', 'image_dir', 'name']","[None, None, None, None]","[None, None, None, None]",235,[],"['np.ones_like', 'float', 'plt.hist', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.savefig', 'plt.clf']",8
repos/datasets/metrics/squad_v2/evaluate.py:main,main,function,37,129,84,1429,11.08,3,6,[],[],[],280,[],"['open', 'json.load', 'make_qid_to_has_ans', 'qid_to_has_ans.items', 'get_raw_scores', 'apply_no_ans_threshold', 'make_eval_dict', 'merge_eval', 'find_all_best_thresh', 'run_precision_recall_analysis', 'histogram_na_prob', 'json.dump', 'print']",13
repos/datasets/metrics/squad_v2/evaluate.py:make_eval_dict,make_eval_dict,function,4,46,26,376,8.17,0,1,"['exact_scores', 'f1_scores', 'qid_list']","[None, None, None]","[None, None, 'None']",138,[],"['len', 'collections.OrderedDict', 'sum']",3
repos/datasets/metrics/squad_v2/evaluate.py:make_precision_recall_eval,make_precision_recall_eval,function,19,59,47,499,8.46,1,3,"['scores', 'na_probs', 'num_true_pos', 'qid_to_has_ans', 'out_image', 'title']","[None, None, None, None, None, None]","[None, None, None, None, 'None', 'None']",176,[],"['sorted', 'enumerate', 'float', 'len', 'precisions.append', 'recalls.append', 'plot_pr_curve']",7
repos/datasets/metrics/squad_v2/evaluate.py:make_qid_to_has_ans,make_qid_to_has_ans,function,10,18,13,158,8.78,3,0,['dataset'],[None],[None],52,[],['bool'],1
repos/datasets/metrics/squad_v2/evaluate.py:merge_eval,merge_eval,function,3,6,6,54,9.0,1,0,"['main_eval', 'new_eval', 'prefix']","[None, None, None]","[None, None, None]",159,[],[],0
repos/datasets/metrics/squad_v2/evaluate.py:normalize_answer,normalize_answer,function,8,32,23,309,9.66,0,0,['s'],[None],[None],61,"['    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n']","['remove_articles', 'ARTICLES_REGEX.sub', 'white_space_fix', 'remove_punc', 'set', 'lower', 'text.lower']",7
repos/datasets/metrics/squad_v2/evaluate.py:parse_args,parse_args,function,7,79,67,862,10.91,0,1,[],[],[],25,[],"['argparse.ArgumentParser', 'parser.add_argument', 'len', 'parser.print_help', 'sys.exit', 'parser.parse_args']",6
repos/datasets/metrics/squad_v2/evaluate.py:plot_pr_curve,plot_pr_curve,function,9,19,16,267,14.05,0,0,"['precisions', 'recalls', 'out_image', 'title']","[None, None, None, None]","[None, None, None, None]",164,[],"['plt.step', 'plt.fill_between', 'plt.xlabel', 'plt.ylabel', 'plt.xlim', 'plt.ylim', 'plt.title', 'plt.savefig', 'plt.clf']",9
repos/datasets/metrics/squad_v2/evaluate.py:run_precision_recall_analysis,run_precision_recall_analysis,function,17,82,56,919,11.21,0,2,"['main_eval', 'exact_raw', 'f1_raw', 'na_probs', 'qid_to_has_ans', 'out_image_dir']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",199,[],"['os.makedirs', 'sum', 'qid_to_has_ans.values', 'make_precision_recall_eval', 'float', 'qid_to_has_ans.items', 'merge_eval']",7
repos/datasets/metrics/squad_v2/squad_v2.py:SquadV2,SquadV2,class,30,127,91,1704,13.42,2,2,[],[],[],89,[],[],0
repos/datasets/metrics/squad_v2/squad_v2.py:SquadV2:_compute,SquadV2:_compute,method,23,87,61,1037,11.92,2,2,"['self', 'predictions', 'references', 'no_answer_threshold']","[None, None, None, None]","[None, None, None, '1.0']",114,[],"['make_qid_to_has_ans', 'qid_to_has_ans.items', 'get_raw_scores', 'apply_no_ans_threshold', 'make_eval_dict', 'merge_eval', 'find_all_best_thresh', 'dict']",8
repos/datasets/metrics/squad_v2/squad_v2.py:SquadV2:_info,SquadV2:_info,method,6,33,26,584,17.7,0,0,['self'],[None],[None],90,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/super_glue/record_evaluation.py:evaluate,evaluate,function,21,69,50,625,9.06,2,2,"['dataset', 'predictions']","[None, None]","[None, None]",58,[],"['print', 'metric_max_over_ground_truths', 'int', 'correct_ids.append']",4
repos/datasets/metrics/super_glue/record_evaluation.py:exact_match_score,exact_match_score,function,2,3,3,66,22.0,0,0,"['prediction', 'ground_truth']","[None, None]","[None, None]",46,[],['normalize_answer'],1
repos/datasets/metrics/super_glue/record_evaluation.py:f1_score,f1_score,function,11,31,24,372,12.0,0,1,"['prediction', 'ground_truth']","[None, None]","[None, None]",33,[],"['normalize_answer', 'Counter', 'sum', 'len']",4
repos/datasets/metrics/super_glue/record_evaluation.py:metric_max_over_ground_truths,metric_max_over_ground_truths,function,8,12,12,175,14.58,1,0,"['metric_fn', 'prediction', 'ground_truths']","[None, None, None]","[None, None, None]",50,[],"['metric_fn', 'scores_for_ground_truths.append', 'max']",3
repos/datasets/metrics/super_glue/record_evaluation.py:normalize_answer,normalize_answer,function,8,33,23,315,9.55,0,0,['s'],[None],[None],14,"['    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n']","['remove_articles', 're.sub', 'white_space_fix', 'remove_punc', 'set', 'lower', 'text.lower']",7
repos/datasets/metrics/super_glue/super_glue.py:acc_and_f1,acc_and_f1,function,4,14,14,130,9.29,0,0,"['preds', 'labels', 'f1_avg']","[None, None, None]","[None, None, '""binary""']",112,[],"['simple_accuracy', 'float']",2
repos/datasets/metrics/super_glue/super_glue.py:evaluate_multirc,evaluate_multirc,function,26,69,58,755,10.94,2,1,"['ids_preds', 'labels']","[None, None]","[None, None]",121,"['    """"""\n', '    Computes F1 score and Exact Match for MultiRC predictions.\n', '    """"""\n']","['zip', 'question_map.items', 'f1_score', 'f1s.append', 'int', 'len', 'ems.append', 'float', 'sum']",9
repos/datasets/metrics/super_glue/super_glue.py:simple_accuracy,simple_accuracy,function,1,3,3,35,11.67,0,0,"['preds', 'labels']","[None, None]","[None, None]",108,[],['float'],1
repos/datasets/metrics/super_glue/super_glue.py:SuperGlue,SuperGlue,class,21,220,116,2163,9.83,1,3,[],[],[],147,[],[],0
repos/datasets/metrics/super_glue/super_glue.py:SuperGlue:_compute,SuperGlue:_compute,method,12,92,69,829,9.01,1,1,"['self', 'predictions', 'references']","[None, None, None]","[None, None, None]",212,[],"['matthews_corrcoef', 'acc_and_f1', 'evaluate_record', 'evaluate_multirc', 'simple_accuracy', 'KeyError']",6
repos/datasets/metrics/super_glue/super_glue.py:SuperGlue:_get_feature_types,SuperGlue:_get_feature_types,method,4,60,25,682,11.37,0,1,['self'],[None],[None],176,[],"['datasets.Value', 'datasets.Sequence']",2
repos/datasets/metrics/super_glue/super_glue.py:SuperGlue:_info,SuperGlue:_info,method,7,60,54,563,9.38,0,1,['self'],[None],[None],148,[],"['KeyError', 'datasets.MetricInfo']",2
repos/datasets/metrics/ter/ter.py:Ter,Ter,class,21,122,99,1439,11.8,2,2,[],[],[],154,[],[],0
repos/datasets/metrics/ter/ter.py:Ter:_compute,Ter:_compute,method,13,47,40,557,11.85,2,1,"['self', 'predictions', 'references', 'normalized', 'ignore_punct', 'support_zh_ja_chars', 'case_sensitive', '']","[None, None, None, ' bool ', ' bool ', ' bool ', ' bool ', None]","[None, None, None, ' False', ' False', ' False', ' False', None]",178,[],"['len', 'any', 'ValueError', 'range', 'TER', 'sb_ter.corpus_score']",6
repos/datasets/metrics/ter/ter.py:Ter:_info,Ter:_info,method,7,55,53,711,12.93,0,1,['self'],[None],[None],155,[],"['version.parse', 'ImportWarning', 'datasets.MetricInfo', 'datasets.Value', 'datasets.Sequence']",5
repos/datasets/metrics/wer/wer.py:WER,WER,class,16,59,46,847,14.36,1,1,[],[],[],77,[],[],0
repos/datasets/metrics/wer/wer.py:WER:_compute,WER:_compute,method,10,32,23,378,11.81,1,1,"['self', 'predictions', 'references', 'concatenate_texts']","[None, None, None, None]","[None, 'None', 'None', 'False']",95,[],"['compute_measures', 'zip']",2
repos/datasets/metrics/wer/wer.py:WER:_info,WER:_info,method,5,20,18,376,18.8,0,0,['self'],[None],[None],78,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/wiki_split/wiki_split.py:SARIngram,SARIngram,function,52,188,94,2173,11.56,7,7,"['sgrams', 'cgrams', 'rgramslist', 'numref']","[None, None, None, None]","[None, None, None, None]",108,[],"['Counter', 'sgramcounter.items', 'cgramcounter.items', 'len', 'sum', 'set']",6
repos/datasets/metrics/wiki_split/wiki_split.py:SARIsent,SARIsent,function,55,260,116,1882,7.24,4,9,"['ssent', 'csent', 'rsents']","[None, None, None]","[None, None, None]",187,[],"['len', 'ssent.split', 'csent.split', 'rsent.split', 'r1gramslist.append', 'range', 'r2grams.append', 'r3grams.append', 'r4grams.append', 'r2gramslist.append', 'r3gramslist.append', 'r4gramslist.append', 's2grams.append', 's3grams.append', 's4grams.append', 'c2grams.append', 'c3grams.append', 'c4grams.append', 'SARIngram', 'sum']",20
repos/datasets/metrics/wiki_split/wiki_split.py:compute_em,compute_em,function,2,17,15,126,7.41,0,0,"['predictions', 'references']","[None, None]","[None, None]",103,[],"['zip', 'len']",2
repos/datasets/metrics/wiki_split/wiki_split.py:compute_exact,compute_exact,function,1,3,3,61,20.33,0,0,"['a_gold', 'a_pred']","[None, None]","[None, None]",99,[],"['int', 'normalize_answer']",2
repos/datasets/metrics/wiki_split/wiki_split.py:compute_sacrebleu,compute_sacrebleu,function,12,42,35,498,11.86,2,1,"['predictions', 'references', 'smooth_method', 'smooth_value', 'force', 'lowercase', 'use_effective_order', '']","[None, None, None, None, None, None, None, None]","[None, None, '""exp""', 'None', 'False', 'False', 'False', None]",295,[],"['len', 'any', 'ValueError', 'range', 'sacrebleu.corpus_bleu']",5
repos/datasets/metrics/wiki_split/wiki_split.py:compute_sari,compute_sari,function,8,39,33,341,8.74,1,1,"['sources', 'predictions', 'references']","[None, None, None]","[None, None, None]",285,[],"['len', 'ValueError', 'zip', 'SARIsent', 'normalize']",5
repos/datasets/metrics/wiki_split/wiki_split.py:normalize,normalize,function,12,40,27,592,14.8,0,4,"['sentence', 'lowercase', 'tokenizer', 'return_str']","[None, ' bool ', ' str ', ' bool ']","[None, ' True', ' ""13a""', ' True']",256,[],"['sentence.lower', 'version.parse', 'sacremoses.MosesTokenizer', 'normalized_sent.split']",4
repos/datasets/metrics/wiki_split/wiki_split.py:normalize_answer,normalize_answer,function,10,36,26,350,9.72,0,0,['s'],[None],[None],79,"['    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n']","['remove_articles', 're.compile', 're.sub', 'white_space_fix', 'remove_punc', 'set', 'lower', 'text.lower']",8
repos/datasets/metrics/wiki_split/wiki_split.py:WikiSplit,WikiSplit,class,9,50,43,1222,24.44,0,0,[],[],[],321,[],[],0
repos/datasets/metrics/wiki_split/wiki_split.py:WikiSplit:_compute,WikiSplit:_compute,method,3,14,11,299,21.36,0,0,"['self', 'sources', 'predictions', 'references']","[None, None, None, None]","[None, None, None, None]",347,[],"['result.update', 'compute_sari', 'compute_sacrebleu', 'compute_em']",4
repos/datasets/metrics/wiki_split/wiki_split.py:WikiSplit:_info,WikiSplit:_info,method,5,29,27,856,29.52,0,0,['self'],[None],[None],322,[],"['datasets.MetricInfo', 'datasets.Value', 'datasets.Sequence']",3
repos/datasets/metrics/xnli/xnli.py:simple_accuracy,simple_accuracy,function,1,3,3,28,9.33,0,0,"['preds', 'labels']","[None, None]","[None, None]",63,[],[],0
repos/datasets/metrics/xnli/xnli.py:Xnli,Xnli,class,9,37,29,476,12.86,0,0,[],[],[],68,[],[],0
repos/datasets/metrics/xnli/xnli.py:Xnli:_compute,Xnli:_compute,method,2,4,4,58,14.5,0,0,"['self', 'predictions', 'references']","[None, None, None]","[None, None, None]",85,[],['simple_accuracy'],1
repos/datasets/metrics/xnli/xnli.py:Xnli:_info,Xnli:_info,method,6,27,21,359,13.3,0,0,['self'],[None],[None],69,[],"['datasets.MetricInfo', 'datasets.Value']",2
repos/datasets/metrics/xtreme_s/xtreme_s.py:bleu,bleu,function,20,122,94,1004,8.23,3,2,"['preds', 'labels', 'smooth_method', 'smooth_value', 'force', 'lowercase', 'tokenize', 'use_effective_order', '']","[None, None, None, None, None, None, None, None, None]","[None, None, '""exp""', 'None', 'False', 'False', 'None', 'False', None]",143,[],"['list', 'ValueError', 'version.parse', 'ImportWarning', 'len', 'any', 'range', 'scb.corpus_bleu']",8
repos/datasets/metrics/xtreme_s/xtreme_s.py:f1_and_simple_accuracy,f1_and_simple_accuracy,function,2,10,10,117,11.7,0,0,"['preds', 'labels']","[None, None]","[None, None]",136,[],"['float', 'simple_accuracy']",2
repos/datasets/metrics/xtreme_s/xtreme_s.py:simple_accuracy,simple_accuracy,function,1,3,3,35,11.67,0,0,"['preds', 'labels']","[None, None]","[None, None]",132,[],['float'],1
repos/datasets/metrics/xtreme_s/xtreme_s.py:wer_and_cer,wer_and_cer,function,19,100,73,969,9.69,1,2,"['preds', 'labels', 'concatenate_texts', 'config_name']","[None, None, None, None]","[None, None, None, None]",187,[],"['ValueError', 'compute_measures', 'compute_score', 'zip']",4
repos/datasets/metrics/xtreme_s/xtreme_s.py:XtremeS,XtremeS,class,26,136,91,1658,12.19,0,4,[],[],[],221,[],[],0
repos/datasets/metrics/xtreme_s/xtreme_s.py:XtremeS:_compute,XtremeS:_compute,method,18,89,65,1094,12.29,0,3,"['self', 'predictions', 'references', 'bleu_kwargs', 'wer_kwargs']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",240,[],"['simple_accuracy', 'f1_and_simple_accuracy', 'bleu_kwargs.pop', 'bleu', 'wer_kwargs.pop', 'wer_and_cer', 'KeyError']",7
repos/datasets/metrics/xtreme_s/xtreme_s.py:XtremeS:_info,XtremeS:_info,method,9,39,35,472,12.1,0,1,['self'],[None],[None],222,[],"['KeyError', 'datasets.MetricInfo', 'datasets.Value']",3
repos/datasets/src/datasets/arrow_dataset.py:_check_column_names,_check_column_names,function,7,32,27,230,7.19,1,2,['column_names'],[' List[str]'],[None],649,"['    """"""Check the column names to make sure they don\'t contain duplicates.""""""\n']","['Counter', 'all', 'counter.values', 'ValueError']",4
repos/datasets/src/datasets/arrow_dataset.py:_check_table,_check_table,function,5,22,19,189,8.59,0,1,['table'],[None],[None],637,"['    """"""We check the table type to make sure it\'s an instance of :class:`datasets.table.Table`""""""\n']","['isinstance', 'InMemoryTable', 'TypeError']",3
repos/datasets/src/datasets/arrow_dataset.py:_check_valid_indices_value,_check_valid_indices_value,function,1,23,19,108,4.7,0,1,"['index', 'size']","[None, None]","[None, None]",657,[],['IndexError'],1
repos/datasets/src/datasets/arrow_dataset.py:_concatenate_map_style_datasets,_concatenate_map_style_datasets,function,47,243,125,2184,8.99,6,14,"['dsets', 'info', 'split', 'axis', '']","[' List[Dataset]', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' int ', None]","[None, ' None', ' None', ' 0', None]",6203,"['    """"""\n', '    Converts a list of :class:`Dataset` with the same schema into a single :class:`Dataset`.\n', '    When you concatenate on axis 0, missing data are filled with None values.\n', '\n', '    Args:\n', '        dsets (`List[datasets.Dataset]`): List of Datasets to concatenate.\n', '        info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '        split (:class:`NamedSplit`, optional): Name of the dataset split.\n', '        axis (``{0, 1}``, default ``0``, meaning over rows):\n', '            Axis to concatenate over, where ``0`` means over rows (vertically) and ``1`` means over columns\n', '            (horizontally).\n', '\n', '            *New in version 1.6.0*\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> ds3 = _concatenate_map_style_datasets([ds1, ds2])\n', '    ```\n', '    """"""\n']","['any', '_check_if_features_can_be_aligned', 'all', 'ValueError', '_check_column_names', 'logger.info', 'apply_offset_to_indices_table', 'pc.add', 'pa.scalar', 'InMemoryTable.from_arrays', 'range', 'indices_tables.append', 'len', 'concat_tables', 'InMemoryTable.from_batches', 'pa.int64', '_align_features', 'update_metadata_with_features', 'features.items', 'DatasetInfo.from_merge', 'update_fingerprint', 'Dataset', 'concatenated_dataset.set_format']",23
repos/datasets/src/datasets/arrow_dataset.py:_interleave_map_style_datasets,_interleave_map_style_datasets,function,30,115,92,1299,11.3,2,5,"['datasets', 'probabilities', 'seed', 'info', 'split', 'stopping_strategy', '""all_exhausted""] ', '**kwargs', '']","[' List[""Dataset""]', ' Optional[List[float]] ', ' Optional[int] ', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Literal[""first_exhausted""', None, None, None]","[None, ' None', ' None', ' None', ' None', None, ' ""first_exhausted""', None, None]",6317,"['    """"""\n', '    Interleave several map-style datasets (sources) into a single map-style dataset.\n', '    The new dataset is constructed by alternating between the sources to get the examples.\n', '    If `probabilities = None` (default) the new dataset is constructed by cycling between each source to get the examples.\n', '    If `probabilities` is not `None, the new dataset is constructed by getting examples from a random source at a time according to the provided probabilities.\n', '\n', '    Args:\n', '        datasets (`List[Dataset]`): list of datasets to interleave\n', '        probabilities (`List[float]`, optional, default None): If specified, the new dataset is constructed by sampling\n', '            examples from one source at a time according to these probabilities.\n', '        seed (`int`, optional, default None): The random seed used to choose a source for each example.\n', '        info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '        split (:class:`NamedSplit`, optional): Name of the dataset split.\n', '        stopping_strategy (`str`, defaults to `first_exhausted`):\n', '            Two strategies are proposed right now.\n', '            By default, `first_exhausted` is an undersampling strategy, i.e the dataset construction is stopped as soon as one dataset has ran out of samples.\n', '            If the strategy is `all_exhausted`,  we use an oversampling strategy, i.e the dataset construction is stopped as soon as every samples of every dataset has been added at least once.\n', '            Note that if the strategy is `all_exhausted`, the interleaved dataset size can get enormous:\n', '            - with no probabilities, the resulting dataset will have max_length_datasets*nb_dataset samples.\n', '            - with given probabilities, the resulting dataset will have more samples if some datasets have really low probability of visiting.\n', '        **kwargs (additional keyword arguments): Keyword arguments to be passed to :meth:`datasets.Datasets.select` when selecting the indices used to interleave the datasets.\n', '\n', '    Output:\n', '        :class:`datasets.Dataset`\n', '    """"""\n']","['ValueError', '_concatenate_map_style_datasets', 'np.cumsum', 'np.arange', 'np.mod', 'np.array', 'np.full', 'iter_random_indices', 'rng.choice', 'len', 'bool_strategy_func', 'indices.append', 'concatenated_datasets.select']",13
repos/datasets/src/datasets/arrow_dataset.py:_split_by_node_map_style_dataset,_split_by_node_map_style_dataset,function,2,4,4,69,17.25,0,0,"['dataset', 'rank', 'world_size']","[' Dataset', ' int', ' int']","[None, None, None]",6423,"['    """"""\n', '    Split a dataset for the node at rank `rank` in a pool of nodes of size `world_size`.\n', '    Each node is assigned a chunk of data, e.g. rank 0 is given the first chunk of the dataset.\n', '    To maximize data loading throughput, chunks are made of contiguous data on disk if possible.\n', '\n', '    Args:\n', '        dataset ([`Dataset`]):\n', '            The dataset to split by node.\n', '        rank (`int`):\n', '            Rank of the current node.\n', '        world_size (`int`):\n', '            Total number of nodes.\n', '\n', '    Returns:\n', '        [`Dataset`]: The dataset to be used on the node at rank `rank`.\n', '    """"""\n']",['dataset.shard'],1
repos/datasets/src/datasets/arrow_dataset.py:get_indices_from_mask_function,get_indices_from_mask_function,function,35,120,64,1115,9.29,5,9,"['function', 'batched', 'with_indices', 'with_rank', 'input_columns', 'List[str]]]', 'indices_mapping', '*args', '**fn_kwargs', '']","[' Callable', ' bool', ' bool', ' bool', ' Optional[Union[str', None, ' Optional[Table] ', None, None, None]","[None, None, None, None, None, None, ' None', None, None, None]",6446,[],"['function', 'len', 'range', 'mask.append', 'zip', 'pa.array', 'indices_mapping.column', 'indices_array.to_pylist']",8
repos/datasets/src/datasets/arrow_dataset.py:transmit_format,transmit_format,function,25,99,81,1124,11.35,1,3,['func'],[None],[None],548,"['    """"""Wrapper for dataset transforms that recreate a new Dataset to transmit the format of the original dataset to the new dataset""""""\n']","['wrapper', 'kwargs.pop', 'set', 'func', 'list', 'isinstance', 'self_format.copy', 'sorted', 'dataset.set_format']",9
repos/datasets/src/datasets/arrow_dataset.py:transmit_tasks,transmit_tasks,function,14,61,51,569,9.33,1,2,['func'],[None],[None],591,"['    """"""Wrapper for dataset transforms that recreate a new Dataset to transmit the task templates of the original dataset to the new dataset""""""\n']","['wrapper', 'kwargs.pop', 'func', 'list', 'isinstance', 'all']",6
repos/datasets/src/datasets/arrow_dataset.py:update_metadata_with_features,update_metadata_with_features,function,15,38,31,573,15.08,0,2,"['table', 'features']","[' Table', ' Features']","[None, None]",621,"['    """"""To be used in dataset transforms that modify the features of the dataset, in order to update the features stored in the metadata of its schema.""""""\n']","['Features', 'ArrowWriter._build_metadata', 'json.loads', 'asdict', 'json.dumps', 'table.replace_schema_metadata']",6
repos/datasets/src/datasets/arrow_dataset.py:Dataset,Dataset,class,915,8869,2897,92064,10.38,59,324,[],[],[],668,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin,DatasetInfoMixin,class,34,119,57,1218,10.24,0,0,[],[],[],159,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetTransformationNotAllowedError,DatasetTransformationNotAllowedError,class,0,1,1,4,4.0,0,0,[],[],[],544,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:NonExistentDatasetError,NonExistentDatasetError,class,0,1,1,4,4.0,0,0,[],[],[],662,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:TensorflowDatasetMixin,TensorflowDatasetMixin,class,111,770,394,6767,8.79,16,35,[],[],[],235,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__del__,Dataset:__del__,method,4,10,7,83,8.3,0,2,['self'],[None],[None],1421,[],['hasattr'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__enter__,Dataset:__enter__,method,1,2,2,10,5.0,0,0,['self'],[None],[None],1427,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__exit__,Dataset:__exit__,method,1,1,1,14,14.0,0,0,"['self', 'exc_type', 'exc_val', 'exc_tb']","[None, None, None, None]","[None, None, None, None]",1430,[],['self.__del__'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__getitem__,Dataset:__getitem__,method,8,16,14,142,8.88,1,0,"['self', 'key', 'slice', 'Iterable[int]]) -> Dict', 'key', 'key)', 'keys']","[None, ' Union[int', None, '  # noqa: F811...@overloadself', ' str) -> List:  # noqa: F811...self', '  # noqa: F811key)self', ' List) -> List:']","[None, None, None, None, None, None, None]",2852,"['        """"""Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).""""""\n']","['self.__getitem__', 'len', 'batch.items', 'range']",4
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__getitem__,Dataset:__getitem__,method,8,16,14,142,8.88,1,0,"['self', 'key', 'slice', 'Iterable[int]]) -> Dict', 'key', 'key)', 'keys']","[None, ' Union[int', None, '  # noqa: F811...@overloadself', ' str) -> List:  # noqa: F811...self', '  # noqa: F811key)self', ' List) -> List:']","[None, None, None, None, None, None, None]",2856,"['        """"""Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).""""""\n']","['self.__getitem__', 'len', 'batch.items', 'range']",4
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__getitem__,Dataset:__getitem__,method,8,16,14,142,8.88,1,0,"['self', 'key', 'slice', 'Iterable[int]]) -> Dict', 'key', 'key)', 'keys']","[None, ' Union[int', None, '  # noqa: F811...@overloadself', ' str) -> List:  # noqa: F811...self', '  # noqa: F811key)self', ' List) -> List:']","[None, None, None, None, None, None, None]",2859,"['        """"""Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).""""""\n']","['self.__getitem__', 'len', 'batch.items', 'range']",4
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__getitems__,Dataset:__getitems__,method,8,16,14,142,8.88,1,0,"['self', 'keys']","[None, ' List']","[None, None]",2863,"['        """"""Can be used to get a batch using a list of integers indices.""""""\n']","['self.__getitem__', 'len', 'batch.items', 'range']",4
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__init__,Dataset:__init__,method,45,200,125,2018,10.09,0,9,"['self', 'arrow_table', 'info', 'split', 'indices_table', 'fingerprint', '']","[None, ' Table', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[Table] ', ' Optional[str] ', None]","[None, None, ' None', ' None', ' None', ' None', None]",671,[],"['info.copy', 'DatasetInfo', 'DatasetInfoMixin.__init__', 'IndexableMixin.__init__', '_check_table', 'maybe_register_dataset_for_temp_dir_deletion', 'json.loads', 'Features.from_arrow_schema', 'ValueError', 'generate_fingerprint', '_check_column_names', 'update_metadata_with_features']",12
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__iter__,Dataset:__iter__,method,16,50,41,608,12.16,3,2,['self'],[None],[None],2425,"['        """"""Iterate through the examples.\n', '\n', '        If a formatting is set with [`Dataset.set_format`] rows will be returned with the\n', '        selected format.\n', '        """"""\n']","['get_formatter', 'table_iter', 'range', 'pa_subtable.slice', 'format_table', 'self._getitem']",6
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__len__,Dataset:__len__,method,2,2,2,19,9.5,0,0,['self'],[None],[None],2408,"['        """"""Number of rows in the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.__len__\n', '        <bound method Dataset.__len__ of Dataset({\n', ""            features: ['text', 'label'],\n"", '            num_rows: 1066\n', '        })>\n', '        ```\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__repr__,Dataset:__repr__,method,2,6,6,96,16.0,0,0,['self'],[None],[None],2486,[],"['f""Dataset']",1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:__setstate__,Dataset:__setstate__,method,3,4,4,89,22.25,0,0,"['self', 'state']","[None, None]","[None, None]",1416,[],['maybe_register_dataset_for_temp_dir_deletion'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_build_local_temp_path,Dataset:_build_local_temp_path,method,5,7,7,154,22.0,0,0,['uri_or_path'],[' str'],[None],1634,"['        """"""\n', '        Builds and returns a Path concatenating a local temporary dir with the dir path (or absolute/relative\n', '        path extracted from the uri) passed.\n', '\n', '        Args:\n', '            uri_or_path (`str`): Path (e.g. `""dataset/train""`) or remote URI (e.g.\n', '                `""s3://my-bucket/dataset/train""`) to concatenate.\n', '\n', '        Returns:\n', '            :class:`Path`: the concatenated path (temp dir + path)\n', '        """"""\n']","['Path', 'get_temporary_cache_files_directory', 'src_dataset_path.relative_to']",3
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_estimate_nbytes,Dataset:_estimate_nbytes,method,19,75,50,725,9.67,1,4,['self'],[None],[None],5226,[],"['require_decoding', 'extra_nbytes_visitor', 'isinstance', 'array.to_pylist', 'xgetsize', 'array.field', 'self.with_format', 'table_visitor', 'len']",9
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_generate_tables_from_cache_file,Dataset:_generate_tables_from_cache_file,method,4,8,7,132,16.5,1,0,['filename'],[' str'],[None],5265,[],['enumerate'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_generate_tables_from_shards,Dataset:_generate_tables_from_shards,method,5,12,8,120,10.0,2,0,"['shards', 'batch_size']","[' List[""Dataset""]', ' int']","[None, None]",5259,[],"['enumerate', 'shard.with_format']",2
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_get_cache_file_path,Dataset:_get_cache_file_path,method,9,22,17,361,16.41,0,1,"['self', 'fingerprint']","[None, None]","[None, None]",2906,[],"['is_caching_enabled', 'generate_random_fingerprint', 'get_temporary_cache_files_directory']",3
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_getitem,Dataset:_getitem,method,14,78,55,824,10.56,0,2,"['self', 'key', 'slice', 'str', 'ListLike[int]]', '**kwargs']","[None, ' Union[int', None, None, None, None]","[None, None, None, None, None, None]",2831,"['        """"""\n', '        Can be used to index columns (by string names) or rows (by integer, slice, or list-like of integer indices)\n', '        """"""\n']","['isinstance', 'TypeError', 'get_formatter', 'query_table', 'format_table']",5
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_map_single,Dataset:_map_single,method,132,784,373,7738,9.87,6,45,"['shard', 'function', 'with_indices', 'with_rank', 'input_columns', 'batched', 'batch_size', 'drop_last_batch', 'remove_columns', 'keep_in_memory', 'cache_file_name', 'writer_batch_size', 'features', 'disable_nullable', 'fn_kwargs', 'new_fingerprint', 'rank', 'offset', '']","[' ""Dataset""', ' Optional[Callable] ', ' bool ', ' bool ', ' Optional[List[str]] ', ' bool ', ' Optional[int] ', ' bool ', ' Optional[List[str]] ', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[Features] ', ' bool ', ' Optional[dict] ', ' Optional[str] ', ' Optional[int] ', ' int ', None]","[None, ' None', ' False', ' False', ' None', ' False', ' 1000', ' False', ' None', ' False', ' None', ' 1000', ' None', ' False', ' None', ' None', ' None', ' 0', None]",3278,"['        """"""Apply a function to all the elements in the table (individually or in batches)\n', '        and update the table (if function does update examples).\n', '\n', '        Args:\n', '            shard (`datasets.Dataset`): Dataset to map the transform on.\n', '            function (`Callable`): with one of the following signature:\n', '                - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False` and `with_rank=False`\n', '                - `function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '                - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False` and `with_rank=False`\n', '                - `function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '\n', '                For advanced usage, the function can also return a `pyarrow.Table`.\n', '                Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n', '                If no function is provided, default to identity function: lambda x: x\n', '            with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx[, rank]): ...`.\n', '            with_rank (`bool`, default `False`): Provide process rank to `function`. Note that in this case the signature of `function` should be `def function(example[, idx], rank): ...`.\n', '            input_columns (`Optional[List[str]]`, defaults to `None`): The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n', '            batch_size (`int`, optional, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n', '                `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n', '            drop_last_batch (`bool`, default: `False`): Whether a last batch smaller than the batch_size should be\n', '                dropped instead of being processed by the function.\n', '            remove_columns (`Optional[List[str]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n', '            cache_file_name (`str`, optional, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            features (`Optional[datasets.Features]`, defaults to `None`): Use a specific Features to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (`bool`, defaults to `False`): Disallow null values in the table.\n', '            fn_kwargs (`Dict`, optional, defaults to `None`): Keyword arguments to be passed to `function`\n', '            new_fingerprint (`str`, optional, defaults to `None`): the new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '            rank: (`int`, optional, defaults to `None`): If specified, this is the process rank when doing multiprocessing\n', '            offset: (`int`, defaults to 0): If specified, this is an offset applied to the indices passed to `function` if `with_indices=True`.\n', '        """"""\n']","['get_formatter', 'NumExamplesMismatchError', 'validate_function_output', 'isinstance', 'TypeError', 'all', 'processed_inputs.values', 'apply_function_on_filtered_inputs', 'format_table', 'range', 'function', 'dict', 'pa_inputs.itercolumns', 'inputs_to_merge.pop', 'processed_inputs.pop', 'len', 'init_buffer_and_writer', 'pa.BufferOutputStream', 'ArrowWriter', 'logger.info', 'tempfile.NamedTemporaryFile', 'contextlib.ExitStack', 'shard.with_format', 'enumerate', 'zip', 'arrow_formatted_shard.iter', 'time.time', 'stack.enter_context', 'writer.write_row', 'writer.write', 'list', 'check_same_num_examples=len', 'DatasetTransformationNotAllowedError', 'writer.write_table', 'writer.write_batch', 'writer.finalize', 'close_stream=bool', 'tmp_file.close', 'os.remove', 'shutil.move', 'os.umask', 'os.chmod', 'Dataset.from_file', 'Dataset.from_buffer']",44
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_new_dataset_with_indices,Dataset:_new_dataset_with_indices,method,13,51,38,520,10.2,0,3,"['self', 'indices_cache_file_name', 'indices_buffer', 'fingerprint', '']","[None, ' Optional[str] ', ' Optional[pa.Buffer] ', ' Optional[str] ', None]","[None, ' None', ' None', ' None', None]",3789,"['        """"""Return a new Dataset obtained by adding indices (provided in indices_cache_file_name or in a buffer) to the\n', '        current Dataset.\n', '        """"""\n']","['ValueError', 'MemoryMappedTable.from_file', 'InMemoryTable.from_buffer', 'Dataset']",4
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_push_parquet_shards_to_hub,Dataset:_push_parquet_shards_to_hub,method,35,108,87,1405,13.01,2,3,"['self', 'repo_id', 'data_dir', 'split', 'token', 'revision', 'create_pr', 'max_shard_size', 'str]] ', 'num_shards', 'embed_external_files', '']","[None, ' str', ' str ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[bool] ', ' Optional[Union[int', None, ' Optional[int] ', ' bool ', None]","[None, None, ' ""data""', ' None', ' None', ' None', ' False', None, ' None', ' None', ' True', None]",5388,"['        """"""Pushes the dataset shards as Parquet files to the hub.\n', '\n', '        Returns:\n', '            additions (`List[CommitOperation]`): list of the `CommitOperationAdd` of the uploaded shards\n', '            uploaded_size (`int`): number of uploaded bytes to the repository\n', '            dataset_nbytes (`int`): approximate size in bytes of the uploaded dataset afer uncompression\n', '        """"""\n']","['require_decoding', 'self._estimate_nbytes', 'convert_file_size_to_int', 'int', 'max', 'range', 'shards_with_embedded_external_files', 'shard.with_format', 'shard.map', 'HfApi', 'hf_tqdm', 'enumerate', 'BytesIO', 'shard.to_parquet', 'buffer.tell', 'CommitOperationAdd', 'api.preupload_lfs_files', 'additions.append']",18
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_save_to_disk_single,Dataset:_save_to_disk_single,method,18,49,37,646,13.18,1,1,"['job_id', 'shard', 'fpath', 'storage_options']","[' int', ' ""Dataset""', ' str', ' Optional[dict]']","[None, None, None, None]",1607,[],"['ArrowWriter', 'time.time', 'shard.with_format', 'writer.write_table', 'len', 'writer.finalize', 'writer.close']",7
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_select_contiguous,Dataset:_select_contiguous,method,11,67,52,623,9.3,0,3,"['self', 'start', 'length', 'new_fingerprint', '']","[None, ' int', ' int', ' Optional[str] ', None]","[None, None, None, ' None', None]",3911,"['        """"""Create a new dataset with rows from a contiguous slice of data.\n', '        The slice is defined by that start index and its length.\n', '\n', '        Args:\n', '            start (`int`): start index.\n', '            length (`int`): length of the slice to select.\n', '            new_fingerprint (`str`, optional, default `None`): the new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds._select_contiguous(0, 4)\n', '        Dataset({\n', ""            features: ['text', 'label'],\n"", '            num_rows: 4\n', '        })\n', '        ```\n', '        """"""\n']","['len', 'DatasetTransformationNotAllowedError', '_check_valid_indices_value', 'Dataset']",4
repos/datasets/src/datasets/arrow_dataset.py:Dataset:_select_with_indices_mapping,Dataset:_select_with_indices_mapping,method,36,182,122,2057,11.3,0,11,"['self', 'indices', 'keep_in_memory', 'indices_cache_file_name', 'writer_batch_size', 'new_fingerprint', '']","[None, ' Iterable', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', None]","[None, None, ' False', ' None', ' 1000', ' None', None]",3967,"['        """"""Create a new dataset with rows selected following the list/array of indices.\n', '        The new dataset is made by creating a new indices mapping on top of the main arrow table.\n', '\n', '        Args:\n', '            indices (sequence, iterable, range, ndarray or Series): List or 1D-array of integer indices for indexing.\n', '            keep_in_memory (`bool`, default `False`): Keep the indices mapping in memory instead of writing it to a cache file.\n', '            indices_cache_file_name (`str`, optional, default `None`): Provide the name of a path for the cache file. It is used to store the\n', '                indices mapping instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, default `1000`): Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n', '            new_fingerprint (`str`, optional, default `None`): the new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds._select_with_indices_mapping(range(4))\n', '        Dataset({\n', ""            features: ['text', 'label'],\n"", '            num_rows: 4\n', '        })\n', '        ```\n', '        """"""\n']","['ValueError', 'len', 'DatasetTransformationNotAllowedError', 'pa.BufferOutputStream', 'ArrowWriter', 'logger.info', 'tempfile.NamedTemporaryFile', 'isinstance', 'list', '_check_valid_indices_value', 'self._select_contiguous', 'pa.array', 'writer.write_table', 'writer.finalize', 'close_stream=bool', 'tmp_file.close', 'os.remove', 'shutil.move', 'os.umask', 'os.chmod', 'self._new_dataset_with_indices']",21
repos/datasets/src/datasets/arrow_dataset.py:Dataset:add_column,Dataset:add_column,method,13,30,29,491,16.37,0,0,"['self', 'name', 'column', 'np.array]', 'new_fingerprint']","[None, ' str', ' Union[list', None, ' str']","[None, None, None, None, None]",5826,"['        """"""Add column to Dataset.\n', '\n', '        <Added version=""1.7""/>\n', '\n', '        Args:\n', '            name (`str`):\n', '                Column name.\n', '            column (`list` or `np.array`):\n', '                Column data to be added.\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> more_text = ds[""text""]\n', '        >>> ds.add_column(name=""text_2"", column=more_text)\n', '        Dataset({\n', ""            features: ['text', 'label', 'text_2'],\n"", '            num_rows: 1066\n', '        })\n', '        ```\n', '        """"""\n']","['InMemoryTable.from_pydict', '_check_column_names', 'self.flatten_indices', 'concat_tables', 'update_metadata_with_features', 'Dataset']",6
repos/datasets/src/datasets/arrow_dataset.py:Dataset:add_elasticsearch_index,Dataset:add_elasticsearch_index,method,0,2,2,3,1.5,0,0,"['self', 'column', 'index_name', 'host', 'port', 'es_client', '# noqa', 'es_index_config', '']","[None, ' str', ' Optional[str] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""elasticsearch.Elasticsearch""] ', ' F821es_index_name: Optional[str] ', ' Optional[dict] ', None]","[None, None, ' None', ' None', ' None', ' None', ' None', ' None', None]",6003,"['        """"""Add a text index using ElasticSearch for fast retrieval. This is done in-place.\n', '\n', '        Args:\n', '            column (`str`):\n', '                The column of the documents to add to the index.\n', '            index_name (`str`, *optional*):\n', '                The `index_name`/identifier of the index.\n', '                This is the index name that is used to call [`~Dataset.get_nearest_examples`] or [`~Dataset.search`].\n', '                By default it corresponds to `column`.\n', '            host (`str`, *optional*, defaults to `localhost`):\n', '                Host of where ElasticSearch is running.\n', '            port (`str`, *optional*, defaults to `9200`):\n', '                Port of where ElasticSearch is running.\n', '            es_client (`elasticsearch.Elasticsearch`, *optional*):\n', '                The elasticsearch client used to create the index if host and port are `None`.\n', '            es_index_name (`str`, *optional*):\n', '                The elasticsearch index name used to create the index.\n', '            es_index_config (`dict`, *optional*):\n', '                The configuration of the elasticsearch index.\n', '                Default config is:\n', '                    ```\n', '                    {\n', '                        ""settings"": {\n', '                            ""number_of_shards"": 1,\n', '                            ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},\n', '                        },\n', '                        ""mappings"": {\n', '                            ""properties"": {\n', '                                ""text"": {\n', '                                    ""type"": ""text"",\n', '                                    ""analyzer"": ""standard"",\n', '                                    ""similarity"": ""BM25""\n', '                                },\n', '                            }\n', '                        },\n', '                    }\n', '                    ```\n', '        Example:\n', '\n', '        ```python\n', '        >>> es_client = elasticsearch.Elasticsearch()\n', ""        >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n"", '        >>> ds.add_elasticsearch_index(column=\'line\', es_client=es_client, es_index_name=""my_es_index"")\n', ""        >>> scores, retrieved_examples = ds.get_nearest_examples('line', 'my new query', k=10)\n"", '        ```\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:add_faiss_index,Dataset:add_faiss_index,method,13,17,17,314,18.47,0,0,"['self', 'column', 'index_name', 'device', 'string_factory', 'metric_type', 'custom_index', '# noqa', 'train_size', 'faiss_verbose', 'dtype', '']","[None, ' str', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', ' F821batch_size: int ', ' Optional[int] ', ' bool ', None, None]","[None, None, ' None', ' None', ' None', ' None', ' None', ' 1000', ' None', ' False', 'np.float32', None]",5864,"['        """"""Add a dense index using Faiss for fast retrieval.\n', '        By default the index is done over the vectors of the specified column.\n', '        You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n', '        You can find more information about Faiss here:\n', '\n', '        - For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)\n', '\n', '        Args:\n', '            column (`str`):\n', '                The column of the vectors to add to the index.\n', '            index_name (`str`, *optional*):\n', '                The `index_name`/identifier of the index.\n', '                This is the `index_name` that is used to call [`~datasets.Dataset.get_nearest_examples`] or [`~datasets.Dataset.search`].\n', '                By default it corresponds to `column`.\n', '            device (`Union[int, List[int]]`, *optional*):\n', '                If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n', '                If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n', '            string_factory (`str`, *optional*):\n', '                This is passed to the index factory of Faiss to create the index.\n', '                Default index class is `IndexFlat`.\n', '            metric_type (`int`, *optional*):\n', '                Type of metric. Ex: `faiss.METRIC_INNER_PRODUCT` or `faiss.METRIC_L2`.\n', '            custom_index (`faiss.Index`, *optional*):\n', '                Custom Faiss index that you already have instantiated and configured for your needs.\n', '            batch_size (`int`):\n', '                Size of the batch to use while adding vectors to the `FaissIndex`. Default value is `1000`.\n', '                <Added version=""2.4.0""/>\n', '            train_size (`int`, *optional*):\n', '                If the index needs a training step, specifies how many vectors will be used to train the index.\n', '            faiss_verbose (`bool`, defaults to `False`):\n', '                Enable the verbosity of the Faiss index.\n', '            dtype (`data-type`):\n', '                The dtype of the numpy arrays that are indexed.\n', '                Default is `np.float32`.\n', '\n', '        Example:\n', '\n', '        ```python\n', ""        >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n"", ""        >>> ds_with_embeddings = ds.map(lambda example: {'embeddings': embed(example['line']}))\n"", ""        >>> ds_with_embeddings.add_faiss_index(column='embeddings')\n"", '        >>> # query\n', ""        >>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', embed('my new query'), k=10)\n"", '        >>> # save index\n', ""        >>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n"", '\n', ""        >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n"", '        >>> # load index\n', ""        >>> ds.load_faiss_index('embeddings', 'my_index.faiss')\n"", '        >>> # query\n', ""        >>> scores, retrieved_examples = ds.get_nearest_examples('embeddings', embed('my new query'), k=10)\n"", '        ```\n', '        """"""\n']","['self.formatted_as', 'super']",2
repos/datasets/src/datasets/arrow_dataset.py:Dataset:add_faiss_index_from_external_arrays,Dataset:add_faiss_index_from_external_arrays,method,10,11,11,290,26.36,0,0,"['self', 'external_arrays', 'index_name', 'device', 'string_factory', 'metric_type', 'custom_index', '# noqa', 'train_size', 'faiss_verbose', 'dtype', '']","[None, ' np.array', ' str', ' Optional[int] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', ' F821batch_size: int ', ' Optional[int] ', ' bool ', None, None]","[None, None, None, ' None', ' None', ' None', ' None', ' 1000', ' None', ' False', 'np.float32', None]",5944,"['        """"""Add a dense index using Faiss for fast retrieval.\n', '        The index is created using the vectors of `external_arrays`.\n', '        You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n', '        You can find more information about Faiss here:\n', '\n', '        - For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)\n', '\n', '        Args:\n', '            external_arrays (`np.array`):\n', '                If you want to use arrays from outside the lib for the index, you can set `external_arrays`.\n', '                It will use `external_arrays` to create the Faiss index instead of the arrays in the given `column`.\n', '            index_name (`str`):\n', '                The `index_name`/identifier of the index.\n', '                This is the `index_name` that is used to call [`~datasets.Dataset.get_nearest_examples`] or [`~datasets.Dataset.search`].\n', '            device (Optional `Union[int, List[int]]`, *optional*):\n', '                If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n', '                If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n', '            string_factory (`str`, *optional*):\n', '                This is passed to the index factory of Faiss to create the index.\n', '                Default index class is `IndexFlat`.\n', '            metric_type (`int`, *optional*):\n', '                Type of metric. Ex: `faiss.faiss.METRIC_INNER_PRODUCT` or `faiss.METRIC_L2`.\n', '            custom_index (`faiss.Index`, *optional*):\n', '                Custom Faiss index that you already have instantiated and configured for your needs.\n', '            batch_size (`int`, *optional*):\n', '                Size of the batch to use while adding vectors to the FaissIndex. Default value is 1000.\n', '                <Added version=""2.4.0""/>\n', '            train_size (`int`, *optional*):\n', '                If the index needs a training step, specifies how many vectors will be used to train the index.\n', '            faiss_verbose (`bool`, defaults to False):\n', '                Enable the verbosity of the Faiss index.\n', '            dtype (`numpy.dtype`):\n', '                The dtype of the numpy arrays that are indexed. Default is np.float32.\n', '        """"""\n']",['super'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:add_item,Dataset:add_item,method,20,56,52,834,14.89,0,1,"['self', 'item', 'new_fingerprint']","[None, ' dict', ' str']","[None, None, None]",6073,"['        """"""Add item to Dataset.\n', '\n', '        <Added version=""1.7""/>\n', '\n', '        Args:\n', '            item (`dict`):\n', '                Item data to be added.\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', ""        >>> new_review = {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n"", '        >>> ds = ds.add_item(new_review)\n', '        >>> ds[-1]\n', ""        {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n"", '        ```\n', '        """"""\n']","['InMemoryTable.from_pydict', 'item.items', '_align_features', 'Features.from_arrow_schema', 'concat_tables', 'item_table.cast', 'pa.array', 'InMemoryTable.from_arrays', 'update_metadata_with_features', 'Dataset']",10
repos/datasets/src/datasets/arrow_dataset.py:Dataset:align_labels_with_mapping,Dataset:align_labels_with_mapping,method,24,180,94,1700,9.44,4,6,"['self', 'label2id', 'label_column']","[None, ' Dict', ' str']","[None, None, None]",6125,"['        """"""Align the dataset\'s label ID and label name mapping to match an input `label2id` mapping.\n', ""        This is useful when you want to ensure that a model's predicted labels are aligned with the dataset.\n"", '        The alignment in done using the lowercase label names.\n', '\n', '        Args:\n', '            label2id (`dict`):\n', '                The label name to ID mapping to align the dataset with.\n', '            label_column (`str`):\n', '                The column name of labels to align on.\n', '\n', '        Example:\n', '\n', '        ```python\n', ""        >>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n"", '        >>> ds = load_dataset(""glue"", ""mnli"", split=""train"")\n', '        >>> # mapping to align with\n', ""        >>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n"", '        >>> ds_aligned = ds.align_labels_with_mapping(label2id, ""label"")\n', '        ```\n', '\n', '        """"""\n']","['ValueError', 'isinstance', 'dict', 'list', 'label2id.items', 'process_label_ids', 'int2str_function', 'ClassLabel', 'Sequence', 'self.map']",10
repos/datasets/src/datasets/arrow_dataset.py:Dataset:cache_files,Dataset:cache_files,method,4,17,16,189,11.12,0,1,['self'],[None],[None],1811,"['        """"""The cache files containing the Apache Arrow table backing the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.cache_files\n', ""        [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]\n"", '        ```\n', '        """"""\n']",['list_table_cache_files'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:cast,Dataset:cast,method,20,48,40,619,12.9,0,1,"['self', 'features', 'batch_size', 'keep_in_memory', 'load_from_cache_file', 'cache_file_name', 'writer_batch_size', 'num_proc', '']","[None, ' Features', ' Optional[int] ', ' bool ', ' Optional[bool] ', ' Optional[str] ', ' Optional[int] ', ' Optional[int] ', None]","[None, None, ' 1000', ' False', ' None', ' None', ' 1000', ' None', None]",2043,"['        """"""\n', '        Cast the dataset to a new set of features.\n', '\n', '        Args:\n', '            features ([`Features`]):\n', '                New features to cast the dataset to.\n', '                The name of the fields in the features must match the current column names.\n', '                The type of the data must also be convertible from one type to the other.\n', '                For non-trivial conversion, e.g. `str` <-> `ClassLabel` you should use [`~datasets.Dataset.map`] to update the Dataset.\n', '            batch_size (`int`, defaults to `1000`):\n', '                Number of examples per batch provided to cast.\n', '                If `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to cast.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            load_from_cache_file (`bool`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_name (`str`, *optional*, defaults to `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running [`~datasets.Dataset.map`].\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', ""                Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '\n', '        Returns:\n', '            [`Dataset`]: A copy of the dataset with casted features.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset, ClassLabel, Value\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.features\n', ""        {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", '        >>> new_features = ds.features.copy()\n', ""        >>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n"", ""        >>> new_features['text'] = Value('large_string')\n"", '        >>> ds = ds.cast(new_features)\n', '        >>> ds.features\n', ""        {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n"", ""         'text': Value(dtype='large_string', id=None)}\n"", '        ```\n', '        """"""\n']","['sorted', 'ValueError', 'self.with_format', 'dataset.map', 'partial', 'dataset.with_format']",6
repos/datasets/src/datasets/arrow_dataset.py:Dataset:cast_column,Dataset:cast_column,method,15,23,20,374,16.26,0,1,"['self', 'column', 'feature', 'new_fingerprint']","[None, ' str', ' FeatureType', ' Optional[str] ']","[None, None, None, ' None']",2127,"['        """"""Cast column to feature for decoding.\n', '\n', '        Args:\n', '            column (`str`):\n', '                Column name.\n', '            feature (`FeatureType`):\n', '                Target feature.\n', '            new_fingerprint (`str`, *optional*):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.features\n', ""        {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", ""        >>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n"", '        >>> ds.features\n', ""        {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", '        ```\n', '        """"""\n']","['hasattr', 'copy.deepcopy', 'update_metadata_with_features', 'self.cast']",4
repos/datasets/src/datasets/arrow_dataset.py:Dataset:class_encode_column,Dataset:class_encode_column,method,26,128,75,1083,8.46,1,3,"['self', 'column', 'include_nulls']","[None, ' str', ' bool ']","[None, None, ' False']",1923,"['        """"""Casts the given column as [`~datasets.features.ClassLabel`] and updates the table.\n', '\n', '        Args:\n', '            column (`str`):\n', '                The name of the column to cast (list all the column names with [`~datasets.Dataset.column_names`])\n', '            include_nulls (`bool`, defaults to `False`):\n', '                Whether to include null values in the class labels. If `True`, the null values will be encoded as the `""None""` class label.\n', '\n', '                <Added version=""1.14.2""/>\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""boolq"", split=""validation"")\n', '        >>> ds.features\n', ""        {'answer': Value(dtype='bool', id=None),\n"", ""         'passage': Value(dtype='string', id=None),\n"", ""         'question': Value(dtype='string', id=None)}\n"", ""        >>> ds = ds.class_encode_column('answer')\n"", '        >>> ds.features\n', ""        {'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n"", ""         'passage': Value(dtype='string', id=None),\n"", ""         'question': Value(dtype='string', id=None)}\n"", '        ```\n', '        """"""\n']","['ValueError', 'isinstance', 'self.unique', 'stringify_column', 'str', 'self.map', 'sorted', 'dset.unique', 'ClassLabel', 'cast_to_class_labels', 'dst_feat.str2int', 'dset.map']",12
repos/datasets/src/datasets/arrow_dataset.py:Dataset:cleanup_cache_files,Dataset:cleanup_cache_files,method,17,55,44,685,12.45,2,3,['self'],[None],[None],2869,"['        """"""Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is\n', '        one.\n', '\n', '        Be careful when running this command that no other process is currently using other cache files.\n', '\n', '        Returns:\n', '            `int`: Number of removed files.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.cleanup_cache_files()\n', '        10\n', '        ```\n', '        """"""\n']","['logger.info', 'os.listdir', 'f_name.startswith', 'f_name.endswith', 'files_to_remove.append', 'os.remove', 'len']",7
repos/datasets/src/datasets/arrow_dataset.py:Dataset:column_names,Dataset:column_names,method,2,2,2,29,14.5,0,0,['self'],[None],[None],1861,"['        """"""Names of the columns in the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.column_names\n', ""        ['text', 'label']\n"", '        ```\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:data,Dataset:data,method,2,2,2,16,8.0,0,0,['self'],[None],[None],1791,"['        """"""The Apache Arrow table backing the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.data\n', '        MemoryMappedTable\n', '        text: string\n', '        label: int64\n', '        ----\n', '        text: [[""compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children ."",""the soundtrack alone is worth the price of admission ."",""rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue ."",""beneath the film\'s obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve ."",""bielinsky is a filmmaker of impressive talent ."",""so beautifully acted and directed , it\'s clear that washington most certainly has a new career ahead of him if he so chooses ."",""a visual spectacle full of stunning images and effects ."",""a gentle and engrossing character study ."",""it\'s enough to watch huppert scheming , with her small , intelligent eyes as steady as any noir villain , and to enjoy the perfectly pitched web of tension that chabrol spins ."",""an engrossing portrait of uncompromising artists trying to create something original against the backdrop of a corporate music industry that only seems to care about the bottom line ."",...,""ultimately , jane learns her place as a girl , softens up and loses some of the intensity that made her an interesting character to begin with ."",""ah-nuld\'s action hero days might be over ."",""it\'s clear why deuces wild , which was shot two years ago , has been gathering dust on mgm\'s shelf ."",""feels like nothing quite so much as a middle-aged moviemaker\'s attempt to surround himself with beautiful , half-naked women ."",""when the precise nature of matthew\'s predicament finally comes into sharp focus , the revelation fails to justify the build-up ."",""this picture is murder by numbers , and as easy to be bored by as your abc\'s , despite a few whopping shootouts ."",""hilarious musical comedy though stymied by accents thick as mud ."",""if you are into splatter movies , then you will probably have a reasonably good time with the salton sea ."",""a dull , simple-minded and stereotypical tale of drugs , death and mind-numbing indifference on the inner-city streets ."",""the feature-length stretch . . . strains the show\'s concept .""]]\n', '        label: [[1,1,1,1,1,1,1,1,1,1,...,0,0,0,0,0,0,0,0,0,0]]\n', '        ```\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:export,Dataset:export,method,47,243,153,2464,10.14,1,6,"['self', 'filename', 'format', '']","[None, ' str', ' str ', None]","[None, None, ' ""tfrecord""', None]",4760,"['        """"""Writes the Arrow dataset to a TFRecord file.\n', '\n', '        The dataset must already be in tensorflow format. The records will be written with\n', '        keys from `dataset._format_columns`.\n', '\n', '        Args:\n', '            filename (`str`): The filename, including the `.tfrecord` extension, to write to.\n', '            format (`str`, optional, default `""tfrecord""`): The type of output file. Currently this is a no-op, as\n', '                TFRecords are the only option. This enables a more flexible function signature later.\n', '        """"""\n']","['logger.error', '_bytes_feature', '_float_feature', '_int64_feature', '_feature', 'isinstance', 'ValueError', 'np.dtype', 'len', 'hasattr', 'np.issubdtype', 'serialize_example', 'ex.items', 'example_proto.SerializeToString', 'tf_serialize_example', 'tf.py_function', 'tf.reshape', 'generator', 'filename.endswith', 'logger.info', 'writer.write']",21
repos/datasets/src/datasets/arrow_dataset.py:Dataset:features,Dataset:features,method,8,24,20,144,6.0,0,1,['self'],[None],[None],740,[],"['super', 'ValueError']",2
repos/datasets/src/datasets/arrow_dataset.py:Dataset:filter,Dataset:filter,method,32,86,78,998,11.6,0,3,"['self', 'function', 'with_indices', 'with_rank', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'keep_in_memory', 'load_from_cache_file', 'cache_file_name', 'writer_batch_size', 'fn_kwargs', 'num_proc', 'suffix_template', 'new_fingerprint', 'desc', '']","[None, ' Optional[Callable] ', ' bool ', ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' bool ', ' Optional[bool] ', ' Optional[str] ', ' Optional[int] ', ' Optional[dict] ', ' Optional[int] ', ' str ', ' Optional[str] ', ' Optional[str] ', None]","[None, ' None', ' False', ' False', None, ' None', ' False', ' 1000', ' False', ' None', ' None', ' 1000', ' None', ' None', ' ""_{rank:05d}_of_{num_proc:05d}""', ' None', ' None', None]",3613,"['        """"""Apply a filter function to all the elements in the table in batches\n', '        and update the table so that the dataset only includes examples according to the filter function.\n', '\n', '        Args:\n', '            function (`Callable`): Callable with one of the following signatures:\n', '\n', '                - `function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False` and `with_rank=False`\n', '                - `function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '                - `function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False` and `with_rank=False`\n', '                - `function(batch: Dict[str, List], *extra_args) -> List[bool]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '\n', '                If no function is provided, defaults to an always `True` function: `lambda x: True`.\n', '            with_indices (`bool`, defaults to `False`):\n', '                Provide example indices to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example, idx[, rank]): ...`.\n', '            with_rank (`bool`, defaults to `False`):\n', '                Provide process rank to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example[, idx], rank): ...`.\n', '            input_columns (`str` or `List[str]`, *optional*):\n', '                The columns to be passed into `function` as\n', '                positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`):\n', '                Provide batch of examples to `function`.\n', '            batch_size (`int`, *optional*, defaults to `1000`):\n', '                Number of examples per batch provided to `function` if\n', '                `batched = True`. If `batched = False`, one example per batch is passed to `function`.\n', '                If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_name (`str`, *optional*):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            fn_kwargs (`dict`, *optional*):\n', '                Keyword arguments to be passed to `function`.\n', '            num_proc (`int`, *optional*):\n', ""                Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '            suffix_template (`str`):\n', '                If `cache_file_name` is specified, then this suffix will be added at the end of the base name of each.\n', '                For example, if `cache_file_name` is `""processed.arrow""`, then for `rank = 1` and `num_proc = 4`,\n', '                the resulting file would be `""processed_00001_of_00004.arrow""` for the default suffix (default\n', '                `_{rank:05d}_of_{num_proc:05d}`).\n', '            new_fingerprint (`str`, *optional*):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '            desc (`str`, *optional*, defaults to `None`):\n', '                Meaningful description to be displayed alongside with the progress bar while filtering examples.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.filter(lambda x: x[""label""] == 1)\n', '        Dataset({\n', ""            features: ['text', 'label'],\n"", '            num_rows: 533\n', '        })\n', '        ```\n', '        """"""\n']","['len', 'DatasetTransformationNotAllowedError', 'self.map', 'function=partial', 'features=Features', 'Value', 'copy.deepcopy']",7
repos/datasets/src/datasets/arrow_dataset.py:Dataset:flatten,Dataset:flatten,method,16,49,37,570,11.63,1,1,"['self', 'new_fingerprint', 'max_depth']","[None, ' Optional[str] ', None]","[None, ' None', '16']",1999,"['        """"""Flatten the table.\n', '        Each column with a struct type is flattened into one column per struct field.\n', '        Other columns are left unchanged.\n', '\n', '        Args:\n', '            new_fingerprint (`str`, *optional*):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '\n', '        Returns:\n', '            [`Dataset`]: A copy of the dataset with flattened columns.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""squad"", split=""train"")\n', '        >>> ds.features\n', ""        {'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n"", ""         'context': Value(dtype='string', id=None),\n"", ""         'id': Value(dtype='string', id=None),\n"", ""         'question': Value(dtype='string', id=None),\n"", ""         'title': Value(dtype='string', id=None)}\n"", '        >>> ds.flatten()\n', '        Dataset({\n', ""            features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n"", '            num_rows: 87599\n', '        })\n', '        ```\n', '        """"""\n']","['copy.deepcopy', 'range', 'any', 'Features', 'update_metadata_with_features', 'logger.info']",6
repos/datasets/src/datasets/arrow_dataset.py:Dataset:flatten_indices,Dataset:flatten_indices,method,2,17,17,276,16.24,0,0,"['self', 'keep_in_memory', 'cache_file_name', 'writer_batch_size', 'features', 'disable_nullable', 'num_proc', 'new_fingerprint', '']","[None, ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[Features] ', ' bool ', ' Optional[int] ', ' Optional[str] ', None]","[None, ' False', ' None', ' 1000', ' None', ' False', ' None', ' None', None]",3743,"['        """"""Create and cache a new Dataset by flattening the indices mapping.\n', '\n', '        Args:\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            cache_file_name (`str`, *optional*, default `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            features (`Optional[datasets.Features]`, defaults to `None`):\n', '                Use a specific [`Features`] to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (`bool`, defaults to `False`):\n', '                Allow null values in the table.\n', '            num_proc (`int`, optional, default `None`):\n', '                Max number of processes when generating cache. Already cached shards are loaded sequentially\n', '            new_fingerprint (`str`, *optional*, defaults to `None`):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '        """"""\n']",['self.map'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:format,Dataset:format,method,1,17,17,200,11.76,0,0,['self'],[None],[None],2490,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:formatted_as,Dataset:formatted_as,method,10,19,19,339,17.84,0,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",2499,"['        """"""To be used in a `with` statement. Set `__getitem__` return format (type and columns).\n', '\n', '        Args:\n', '            type (`str`, *optional*):\n', ""                Output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n"", '                `None` means `__getitem__`` returns python objects (default).\n', '            columns (`List[str]`, *optional*):\n', '                Columns to format in the output.\n', '                `None` means `__getitem__` returns all columns (default).\n', '            output_all_columns (`bool`, defaults to `False`):\n', '                Keep un-formatted columns as well in the output (as python objects).\n', '            **format_kwargs (additional keyword arguments):\n', '                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '        """"""\n']",['self.set_format'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_buffer,Dataset:from_buffer,method,6,17,15,206,12.12,0,1,"['cls', 'buffer', 'info', 'split', 'indices_buffer', '']","[None, ' pa.Buffer', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[pa.Buffer] ', None]","[None, None, ' None', ' None', ' None', None]",787,"['        """"""Instantiate a Dataset backed by an Arrow buffer.\n', '\n', '        Args:\n', '            buffer (`pyarrow.Buffer`):\n', '                Arrow buffer.\n', '            info (`DatasetInfo`, *optional*):\n', '                Dataset information, like description, citation, etc.\n', '            split (`NamedSplit`, *optional*):\n', '                Name of the dataset split.\n', '            indices_buffer (`pyarrow.Buffer`, *optional*):\n', '                Indices Arrow buffer.\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '        """"""\n']","['InMemoryTable.from_buffer', 'cls']",2
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_csv,Dataset:from_csv,method,4,14,14,194,13.86,0,0,"['path_or_paths', 'List[PathLike]]', 'split', 'features', 'cache_dir', 'keep_in_memory', 'num_proc', '**kwargs', '']","[' Union[PathLike', None, ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' Optional[int] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' None', None, None]",1006,"['        """"""Create Dataset from CSV file(s).\n', '\n', '        Args:\n', '            path_or_paths (`path-like` or list of `path-like`):\n', '                Path(s) of the CSV file(s).\n', '            split ([`NamedSplit`], *optional*):\n', '                Split name to be assigned to the dataset.\n', '            features ([`Features`], *optional*):\n', '                Dataset features.\n', '            cache_dir (`str`, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.8.0""/>\n', '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to [`pandas.read_csv`].\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> ds = Dataset.from_csv('path/to/dataset.csv')\n"", '        ```\n', '        """"""\n']",['CsvDatasetReader'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_dict,Dataset:from_dict,method,24,124,62,988,7.97,2,8,"['cls', 'mapping', 'features', 'info', 'split', '']","[None, ' dict', ' Optional[Features] ', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', None]","[None, None, ' None', ' None', ' None', None]",923,"['        """"""\n', '        Convert `dict` to a `pyarrow.Table` to create a [`Dataset`].\n', '\n', '        Args:\n', '            mapping (`Mapping`):\n', '                Mapping of strings to Arrays or Python lists.\n', '            features ([`Features`], *optional*):\n', '                Dataset features.\n', '            info (`DatasetInfo`, *optional*):\n', '                Dataset information, like description, citation, etc.\n', '            split (`NamedSplit`, *optional*):\n', '                Name of the dataset split.\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '        """"""\n']","['ValueError', 'mapping.items', 'isinstance', 'cast_array_to_feature', 'OptimizedTypedSequence', 'features.encode_column', 'InMemoryTable.from_pydict', 'DatasetInfo', 'Features', 'generate_from_arrow_type', 'data.get_inferred_type', 'cls']",12
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_file,Dataset:from_file,method,6,21,19,281,13.38,0,1,"['cls', 'filename', 'info', 'split', 'indices_filename', 'in_memory', '']","[None, ' str', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[str] ', ' bool ', None]","[None, None, ' None', ' None', ' None', ' False', None]",747,"['        """"""Instantiate a Dataset backed by an Arrow table at filename.\n', '\n', '        Args:\n', '            filename (`str`):\n', '                File name of the dataset.\n', '            info (`DatasetInfo`, *optional*):\n', '                Dataset information, like description, citation, etc.\n', '            split (`NamedSplit`, *optional*):\n', '                Name of the dataset split.\n', '            indices_filename (`str`, *optional*):\n', '                File names of the indices.\n', '            in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '        """"""\n']","['ArrowReader.read_table', 'cls']",2
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_generator,Dataset:from_generator,method,4,14,14,238,17.0,0,0,"['generator', 'features', 'cache_dir', 'keep_in_memory', 'gen_kwargs', 'num_proc', '**kwargs', '']","[' Callable', ' Optional[Features] ', ' str ', ' bool ', ' Optional[dict] ', ' Optional[int] ', None, None]","[None, ' None', ' None', ' False', ' None', ' None', None, None]",1059,"['        """"""Create a Dataset from a generator.\n', '\n', '        Args:\n', '            generator (:`Callable`):\n', '                A generator function that `yields` examples.\n', '            features ([`Features`], *optional*):\n', '                Dataset features.\n', '            cache_dir (`str`, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            gen_kwargs(`dict`, *optional*):\n', '                Keyword arguments to be passed to the `generator` callable.\n', '                You can define a sharded dataset by passing the list of shards in `gen_kwargs` and setting `num_proc` greater than 1.\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n', '                If `num_proc` is greater than one, then all list values in `gen_kwargs` must be the same length. These values will be split between calls to the generator. The number of shards will be the minimum of the shortest list in `gen_kwargs` and `num_proc`.\n', '\n', '                <Added version=""2.7.0""/>\n', '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to :[`GeneratorConfig`].\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> def gen():\n', '        ...     yield {""text"": ""Good"", ""label"": 0}\n', '        ...     yield {""text"": ""Bad"", ""label"": 1}\n', '        ...\n', '        >>> ds = Dataset.from_generator(gen)\n', '        ```\n', '\n', '        ```py\n', '        >>> def gen(shards):\n', '        ...     for shard in shards:\n', '        ...         with open(shard) as f:\n', '        ...             for line in f:\n', '        ...                 yield {""line"": line}\n', '        ...\n', '        >>> shards = [f""data{i}.txt"" for i in range(32)]\n', '        >>> ds = Dataset.from_generator(gen, gen_kwargs={""shards"": shards})\n', '        ```\n', '        """"""\n']",['GeneratorDatasetInputStream'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_json,Dataset:from_json,method,4,15,15,210,14.0,0,0,"['path_or_paths', 'List[PathLike]]', 'split', 'features', 'cache_dir', 'keep_in_memory', 'field', 'num_proc', '**kwargs', '']","[' Union[PathLike', None, ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' Optional[str] ', ' Optional[int] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' None', ' None', None, None]",1128,"['        """"""Create Dataset from JSON or JSON Lines file(s).\n', '\n', '        Args:\n', '            path_or_paths (`path-like` or list of `path-like`):\n', '                Path(s) of the JSON or JSON Lines file(s).\n', '            split ([`NamedSplit`], *optional*):\n', '                Split name to be assigned to the dataset.\n', '            features ([`Features`], *optional*):\n', '                 Dataset features.\n', '            cache_dir (`str`, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            field (`str`, *optional*):\n', '                Field name of the JSON file where the dataset is contained in.\n', '            num_proc (`int`, *optional* defaults to `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.8.0""/>\n', '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to [`JsonConfig`].\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> ds = Dataset.from_json('path/to/dataset.json')\n"", '        ```\n', '        """"""\n']",['JsonDatasetReader'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_list,Dataset:from_list,method,3,20,17,115,5.75,0,0,"['cls', 'mapping', 'features', 'info', 'split', '']","[None, ' List[dict]', ' Optional[Features] ', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', None]","[None, None, ' None', ' None', ' None', None]",979,"['        """"""\n', '        Convert a list of dicts to a `pyarrow.Table` to create a [`Dataset`]`.\n', '\n', '        Note that the keys of the first entry will be used to determine the dataset columns,\n', '        regardless of what is passed to features.\n', '\n', '        Args:\n', '            mapping (`List[dict]`): A list of mappings of strings to row values.\n', '            features (`Features`, optional): Dataset features.\n', '            info (`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '            split (`NamedSplit`, optional): Name of the dataset split.\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '        """"""\n']",['cls.from_dict'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_pandas,Dataset:from_pandas,method,11,65,34,484,7.45,0,5,"['cls', 'df', 'features', 'info', 'split', 'preserve_index', '']","[None, ' pd.DataFrame', ' Optional[Features] ', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[bool] ', None]","[None, None, ' None', ' None', ' None', ' None', None]",819,"['        """"""\n', '        Convert `pandas.DataFrame` to a `pyarrow.Table` to create a [`Dataset`].\n', '\n', '        The column types in the resulting Arrow Table are inferred from the dtypes of the `pandas.Series` in the\n', '        DataFrame. In the case of non-object Series, the NumPy dtype is translated to its Arrow equivalent. In the\n', '        case of `object`, we need to guess the datatype by looking at the Python objects in this Series.\n', '\n', ""        Be aware that Series of the `object` dtype don't carry enough information to always lead to a meaningful Arrow\n"", '        type. In the case that we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only\n', '        contains `None/nan` objects, the type is set to `null`. This behavior can be avoided by constructing explicit\n', '        features and passing it to this function.\n', '\n', '        Args:\n', '            df (`pandas.DataFrame`):\n', '                Dataframe that contains the dataset.\n', '            features ([`Features`], *optional*):\n', '                Dataset features.\n', '            info (`DatasetInfo`, *optional*):\n', '                Dataset information, like description, citation, etc.\n', '            split (`NamedSplit`, *optional*):\n', '                Name of the dataset split.\n', '            preserve_index (`bool`, *optional*):\n', '                Whether to store the index as an additional column in the resulting Dataset.\n', '                The default of `None` will store the index as a column, except for `RangeIndex` which is stored as metadata only.\n', '                Use `preserve_index=True` to force it to be stored as a column.\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds = Dataset.from_pandas(df)\n', '        ```\n', '        """"""\n']","['ValueError', 'DatasetInfo', 'InMemoryTable.from_pandas', 'table.cast', 'cls']",5
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_parquet,Dataset:from_parquet,method,4,15,15,223,14.87,0,0,"['path_or_paths', 'List[PathLike]]', 'split', 'features', 'cache_dir', 'keep_in_memory', 'columns', 'num_proc', '**kwargs', '']","[' Union[PathLike', None, ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' Optional[List[str]] ', ' Optional[int] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' None', ' None', None, None]",1185,"['        """"""Create Dataset from Parquet file(s).\n', '\n', '        Args:\n', '            path_or_paths (`path-like` or list of `path-like`):\n', '                Path(s) of the Parquet file(s).\n', '            split (`NamedSplit`, *optional*):\n', '                Split name to be assigned to the dataset.\n', '            features (`Features`, *optional*):\n', '                Dataset features.\n', '            cache_dir (`str`, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            columns (`List[str]`, *optional*):\n', '                If not `None`, only these columns will be read from the file.\n', ""                A column name may be a prefix of a nested field, e.g. 'a' will select\n"", ""                'a.b', 'a.c', and 'a.d.e'.\n"", '            num_proc (`int`, *optional*, defaults to `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.8.0""/>\n', '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to [`ParquetConfig`].\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> ds = Dataset.from_parquet('path/to/dataset.parquet')\n"", '        ```\n', '        """"""\n']",['ParquetDatasetReader'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_polars,Dataset:from_polars,method,11,62,32,446,7.19,0,5,"['cls', 'df', 'features', 'info', 'split', '']","[None, ' ""pl.DataFrame""', ' Optional[Features] ', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', None]","[None, None, ' None', ' None', ' None', None]",881,"['        """"""\n', '        Collect the underlying arrow arrays in an Arrow Table.\n', '\n', '        This operation is mostly zero copy.\n', '\n', '        Data types that do copy:\n', '            * CategoricalType\n', '\n', '        Args:\n', '            df (`polars.DataFrame`): DataFrame to convert to Arrow Table\n', '            features (`Features`, optional): Dataset features.\n', '            info (`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '            split (`NamedSplit`, optional): Name of the dataset split.\n', '\n', '        Examples:\n', '        ```py\n', '        >>> ds = Dataset.from_polars(df)\n', '        ```\n', '        """"""\n']","['ValueError', 'DatasetInfo', 'InMemoryTable', 'table.cast', 'cls']",5
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_spark,Dataset:from_spark,method,6,27,27,356,13.19,0,1,"['df', 'split', 'features', 'keep_in_memory', 'cache_dir', 'working_dir', 'load_from_cache_file', '**kwargs', '']","[' ""pyspark.sql.DataFrame""', ' Optional[NamedSplit] ', ' Optional[Features] ', ' bool ', ' str ', ' str ', ' bool ', None, None]","[None, ' None', ' None', ' False', ' None', ' None', ' True', None, None]",1297,"['        """"""Create a Dataset from Spark DataFrame. Dataset downloading is distributed over Spark workers.\n', '\n', '        Args:\n', '            df (`pyspark.sql.DataFrame`):\n', '                The DataFrame containing the desired data.\n', '            split (`NamedSplit`, *optional*):\n', '                Split name to be assigned to the dataset.\n', '            features (`Features`, *optional*):\n', '                Dataset features.\n', '            cache_dir (`str`, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data. When using a multi-node Spark cluster, the cache_dir must be accessible to both\n', '                workers and the driver.\n', '            keep_in_memory (`bool`):\n', '                Whether to copy the data in-memory.\n', '            working_dir (`str`, *optional*)\n', '                Intermediate directory for each Spark worker to write data to before moving it to `cache_dir`. Setting\n', '                a non-NFS intermediate directory may improve performance.\n', '            load_from_cache_file (`bool`):\n', '                Whether to load the dataset from the cache if possible.\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> df = spark.createDataFrame(\n', '        >>>     data=[[1, ""Elia""], [2, ""Teo""], [3, ""Fang""]],\n', '        >>>     columns=[""id"", ""name""],\n', '        >>> )\n', '        >>> ds = Dataset.from_spark(df)\n', '        ```\n', '        """"""\n']","['EnvironmentError', 'SparkDatasetReader']",2
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_sql,Dataset:from_sql,method,4,13,13,157,12.08,0,0,"['sql', '""sqlalchemy.sql.Selectable""]', 'con', '""sqlalchemy.engine.Connection""', '""sqlalchemy.engine.Engine""', '""sqlite3.Connection""]', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[' Union[str', None, ' Union[str', None, None, None, ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, None, None, None, None, ' None', ' None', ' False', None, None]",1359,"['        """"""Create Dataset from SQL query or database table.\n', '\n', '        Args:\n', '            sql (`str` or `sqlalchemy.sql.Selectable`):\n', '                SQL query to be executed or a table name.\n', '            con (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`):\n', '                A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) used to instantiate a database connection or a SQLite3/SQLAlchemy connection object.\n', '            features ([`Features`], *optional*):\n', '                Dataset features.\n', '            cache_dir (`str`, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to [`SqlConfig`].\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> # Fetch a database table\n', '        >>> ds = Dataset.from_sql(""test_data"", ""postgres:///db_name"")\n', '        >>> # Execute a SQL query on the table\n', '        >>> ds = Dataset.from_sql(""SELECT sentence FROM test_data"", ""postgres:///db_name"")\n', '        >>> # Use a Selectable object to specify the query\n', '        >>> from sqlalchemy import select, text\n', '        >>> stmt = select([text(""sentence"")]).select_from(text(""test_data""))\n', '        >>> ds = Dataset.from_sql(stmt, ""postgres:///db_name"")\n', '        ```\n', '\n', '        <Tip>\n', '\n', '        The returned dataset can only be cached if `con` is specified as URI string.\n', '\n', '        </Tip>\n', '        """"""\n']",['SqlDatasetReader'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:from_text,Dataset:from_text,method,4,14,14,197,14.07,0,0,"['path_or_paths', 'List[PathLike]]', 'split', 'features', 'cache_dir', 'keep_in_memory', 'num_proc', '**kwargs', '']","[' Union[PathLike', None, ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' Optional[int] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' None', None, None]",1244,"['        """"""Create Dataset from text file(s).\n', '\n', '        Args:\n', '            path_or_paths (`path-like` or list of `path-like`):\n', '                Path(s) of the text file(s).\n', '            split (`NamedSplit`, *optional*):\n', '                Split name to be assigned to the dataset.\n', '            features (`Features`, *optional*):\n', '                Dataset features.\n', '            cache_dir (`str`, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.8.0""/>\n', '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to [`TextConfig`].\n', '\n', '        Returns:\n', '            [`Dataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> ds = Dataset.from_text('path/to/dataset.txt')\n"", '        ```\n', '        """"""\n']",['TextDatasetReader'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:iter,Dataset:iter,method,19,56,45,641,11.45,2,3,"['self', 'batch_size', 'drop_last_batch']","[None, ' int', ' bool ']","[None, None, ' False']",2454,"['        """"""Iterate through the batches of size `batch_size`.\n', '\n', '        If a formatting is set with [`~datasets.Dataset.set_format`] rows will be returned with the\n', '        selected format.\n', '\n', '        Args:\n', '            batch_size (:obj:`int`): size of each batch to yield.\n', '            drop_last_batch (:obj:`bool`, default `False`): Whether a last batch smaller than the batch_size should be\n', '                dropped\n', '        """"""\n']","['get_formatter', 'table_iter', 'format_table', 'range', 'self._getitem', 'slice']",6
repos/datasets/src/datasets/arrow_dataset.py:Dataset:load_from_disk,Dataset:load_from_disk,method,49,323,159,3549,10.99,0,10,"['dataset_path', 'fs', 'keep_in_memory', 'storage_options', '']","[' str', None, ' Optional[bool] ', ' Optional[dict] ', None]","[None, '""deprecated""', ' None', ' None', None]",1651,"['        """"""\n', '        Loads a dataset that was previously saved using [`save_to_disk`] from a dataset directory, or from a\n', '        filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n', '\n', '        Args:\n', '            dataset_path (`str`):\n', '                Path (e.g. `""dataset/train""`) or remote URI (e.g. `""s3//my-bucket/dataset/train""`)\n', '                of the dataset directory where the dataset will be loaded from.\n', '            fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n', '                Instance of the remote filesystem where the dataset will be saved to.\n', '\n', '                <Deprecated version=""2.8.0"">\n', '\n', '                `fs` was deprecated in version 2.8.0 and will be removed in 3.0.0.\n', '                Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`\n', '\n', '                </Deprecated>\n', '\n', '            keep_in_memory (`bool`, defaults to `None`):\n', '                Whether to copy the dataset in-memory. If `None`, the\n', '                dataset will not be copied in-memory unless explicitly enabled by setting\n', '                `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n', '                [improve performance](../cache#improve-performance) section.\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.8.0""/>\n', '\n', '        Returns:\n', '            [`Dataset`] or [`DatasetDict`]:\n', '            - If `dataset_path` is a path of a dataset directory, the dataset requested.\n', '            - If `dataset_path` is a path of a dataset dict directory, a `datasets.DatasetDict` with each split.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds = load_from_disk(""path/to/dataset/directory"")\n', '        ```\n', '        """"""\n']","['warnings.warn', 'url_to_fs', 'posixpath.join', 'fs.isfile', 'FileNotFoundError', 'is_remote_filesystem', 'Dataset._build_local_temp_path', 'fs.download', 'dest_dataset_path.as_posix', 'open', 'json.load', 'DatasetInfo.from_dict', 'estimate_dataset_size', 'Path', 'is_small_dataset', 'concat_tables', 'thread_map', 'disable=len', 'Split', 'Dataset', 'dataset.with_format']",21
repos/datasets/src/datasets/arrow_dataset.py:Dataset:map,Dataset:map,method,105,647,362,6662,10.3,6,34,"['self', 'function', 'with_indices', 'with_rank', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'drop_last_batch', 'remove_columns', 'List[str]]] ', 'keep_in_memory', 'load_from_cache_file', 'cache_file_name', 'writer_batch_size', 'features', 'disable_nullable', 'fn_kwargs', 'num_proc', 'suffix_template', 'new_fingerprint', 'desc', '']","[None, ' Optional[Callable] ', ' bool ', ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[bool] ', ' Optional[str] ', ' Optional[int] ', ' Optional[Features] ', ' bool ', ' Optional[dict] ', ' Optional[int] ', ' str ', ' Optional[str] ', ' Optional[str] ', None]","[None, ' None', ' False', ' False', None, ' None', ' False', ' 1000', ' False', None, ' None', ' False', ' None', ' None', ' 1000', ' None', ' False', ' None', ' None', ' ""_{rank:05d}_of_{num_proc:05d}""', ' None', ' None', None]",2918,"['        """"""\n', '        Apply a function to all the examples in the table (individually or in batches) and update the table.\n', '        If your function returns a column that already exists, then it overwrites it.\n', '\n', '        You can specify whether the function should be batched or not with the `batched` parameter:\n', '\n', '        - If batched is `False`, then the function takes 1 example in and should return 1 example.\n', '          An example is a dictionary, e.g. `{""text"": ""Hello there !""}`.\n', '        - If batched is `True` and `batch_size` is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.\n', '          A batch is a dictionary, e.g. a batch of 1 example is `{""text"": [""Hello there !""]}`.\n', '        - If batched is `True` and `batch_size` is `n > 1`, then the function takes a batch of `n` examples as input and can return a batch with `n` examples, or with an arbitrary number of examples.\n', '          Note that the last batch may have less than `n` examples.\n', '          A batch is a dictionary, e.g. a batch of `n` examples is `{""text"": [""Hello there !""] * n}`.\n', '\n', '        Args:\n', '            function (`Callable`): Function with one of the following signatures:\n', '\n', '                - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False` and `with_rank=False`\n', '                - `function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '                - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False` and `with_rank=False`\n', '                - `function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '\n', '                For advanced usage, the function can also return a `pyarrow.Table`.\n', '                Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n', '                If no function is provided, default to identity function: `lambda x: x`.\n', '            with_indices (`bool`, defaults to `False`):\n', '                Provide example indices to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example, idx[, rank]): ...`.\n', '            with_rank (`bool`, defaults to `False`):\n', '                Provide process rank to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example[, idx], rank): ...`.\n', '            input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\n', '                The columns to be passed into `function`\n', '                as positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`):\n', '                Provide batch of examples to `function`.\n', '            batch_size (`int`, *optional*, defaults to `1000`):\n', '                Number of examples per batch provided to `function` if `batched=True`.\n', '                If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\n', '            drop_last_batch (`bool`, defaults to `False`):\n', '                Whether a last batch smaller than the batch_size should be\n', '                dropped instead of being processed by the function.\n', '            remove_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\n', '                Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_name (`str`, *optional*, defaults to `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            features (`Optional[datasets.Features]`, defaults to `None`):\n', '                Use a specific Features to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (`bool`, defaults to `False`):\n', '                Disallow null values in the table.\n', '            fn_kwargs (`Dict`, *optional*, defaults to `None`):\n', '                Keyword arguments to be passed to `function`.\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', '                Max number of processes when generating cache. Already cached shards are loaded sequentially.\n', '            suffix_template (`str`):\n', '                If `cache_file_name` is specified, then this suffix\n', '                will be added at the end of the base name of each. Defaults to `""_{rank:05d}_of_{num_proc:05d}""`. For example, if `cache_file_name` is ""processed.arrow"", then for\n', '                `rank=1` and `num_proc=4`, the resulting file would be `""processed_00001_of_00004.arrow""` for the default suffix.\n', '            new_fingerprint (`str`, *optional*, defaults to `None`):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '            desc (`str`, *optional*, defaults to `None`):\n', '                Meaningful description to be displayed alongside with the progress bar while mapping examples.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> def add_prefix(example):\n', '        ...     example[""text""] = ""Review: "" + example[""text""]\n', '        ...     return example\n', '        >>> ds = ds.map(add_prefix)\n', '        >>> ds[0:3][""text""]\n', ""        ['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\n"", ""         'Review: the soundtrack alone is worth the price of admission .',\n"", ""         'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']\n"", '\n', '        # process a batch of examples\n', '        >>> ds = ds.map(lambda example: tokenizer(example[""text""]), batched=True)\n', '        # set number of processors\n', '        >>> ds = ds.map(add_prefix, num_proc=4)\n', '        ```\n', '        """"""\n']","['ValueError', 'len', 'Dataset', 'self.remove_columns', 'isinstance', 'set', 'is_caching_enabled', 'logger.warning', 'format_transform_for_fingerprint', 'format_kwargs_for_fingerprint', 'update_fingerprint', 'validate_fingerprint', 'self._get_cache_file_path', 'load_processed_shard_from_cache', 'Dataset.from_file', 'logger.info', 'hf_tqdm', 'Dataset._map_single', 'logger.debug', 'pbar.update', 'format_cache_file_name', 'cache_file_name.rindex', 'suffix_template.format', 'suffix_template.replace', 'format_new_fingerprint', 'deepcopy', 'prev_env.get', 'self.shard', 'range', 'sum', 'Pool', 'iflatmap_unordered', '_concatenate_map_style_datasets', 'any', 'zip']",35
repos/datasets/src/datasets/arrow_dataset.py:Dataset:num_columns,Dataset:num_columns,method,2,2,2,28,14.0,0,0,['self'],[None],[None],1829,"['        """"""Number of columns in the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.num_columns\n', '        2\n', '        ```\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:num_rows,Dataset:num_rows,method,4,9,8,80,8.89,0,1,['self'],[None],[None],1844,"['        """"""Number of rows in the dataset (same as [`Dataset.__len__`]).\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.num_rows\n', '        1066\n', '        ```\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:prepare_for_task,Dataset:prepare_for_task,method,22,123,78,1120,9.11,3,4,"['self', 'task', 'TaskTemplate]', 'id']","[None, ' Union[str', None, ' int ']","[None, None, None, ' 0']",2781,"['        """"""\n', ""        Prepare a dataset for the given task by casting the dataset's [`Features`] to standardized column names and types as detailed in [`datasets.tasks`](./task_templates).\n"", '\n', '        Casts [`datasets.DatasetInfo.features`] according to a task-specific schema. Intended for single-use only, so all task templates are removed from [`datasets.DatasetInfo.task_templates`] after casting.\n', '\n', '        Args:\n', '            task (`Union[str, TaskTemplate]`):\n', '                The task to prepare the dataset for during training and evaluation. If `str`, supported tasks include:\n', '\n', '                - `""text-classification""`\n', '                - `""question-answering""`\n', '\n', '                If [`TaskTemplate`], must be one of the task templates in [`datasets.tasks`](./task_templates).\n', '            id (`int`, defaults to `0`):\n', '                The id required to unambiguously identify the task template when multiple task templates of the same type are supported.\n', '        """"""\n']","['isinstance', 'ValueError', 'len', 'enumerate', 'template.align_with_features', 'self.remove_columns', 'dataset.rename_columns', 'dataset.cast']",8
repos/datasets/src/datasets/arrow_dataset.py:Dataset:push_to_hub,Dataset:push_to_hub,method,139,664,390,7873,11.86,4,26,"['self', 'repo_id', 'config_name', 'set_default', 'split', 'data_dir', 'commit_message', 'commit_description', 'private', 'token', 'revision', 'branch', 'create_pr', 'max_shard_size', 'str]] ', 'num_shards', 'embed_external_files', '']","[None, ' str', ' str ', ' Optional[bool] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[bool] ', ' Optional[str] ', ' Optional[str] ', None, ' Optional[bool] ', ' Optional[Union[int', None, ' Optional[int] ', ' bool ', None]","[None, None, ' ""default""', ' None', ' None', ' None', ' None', ' None', ' False', ' None', ' None', '""deprecated""', ' False', None, ' None', ' None', ' True', None]",5466,"['        """"""Pushes the dataset to the hub as a Parquet dataset.\n', '        The dataset is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n', '\n', '        The resulting Parquet files are self-contained by default. If your dataset contains [`Image`] or [`Audio`]\n', '        data, the Parquet files will store the bytes of your images or audio files.\n', '        You can disable this by setting `embed_external_files` to `False`.\n', '\n', '        Args:\n', '            repo_id (`str`):\n', '                The ID of the repository to push to in the following format: `<user>/<dataset_name>` or\n', '                `<org>/<dataset_name>`. Also accepts `<dataset_name>`, which will default to the namespace\n', '                of the logged-in user.\n', '            config_name (`str`, defaults to ""default""):\n', '                The configuration name (or subset) of a dataset. Defaults to ""default"".\n', '            set_default (`bool`, *optional*):\n', '                Whether to set this configuration as the default one. Otherwise, the default configuration is the one\n', '                named ""default"".\n', '            split (`str`, *optional*):\n', '                The name of the split that will be given to that dataset. Defaults to `self.split`.\n', '            data_dir (`str`, *optional*):\n', '                Directory name that will contain the uploaded data files. Defaults to the `config_name` if different\n', '                from ""default"", else ""data"".\n', '\n', '                <Added version=""2.17.0""/>\n', '            commit_message (`str`, *optional*):\n', '                Message to commit while pushing. Will default to `""Upload dataset""`.\n', '            commit_description (`str`, *optional*):\n', '                Description of the commit that will be created.\n', '                Additionally, description of the PR if a PR is created (`create_pr` is True).\n', '\n', '                <Added version=""2.16.0""/>\n', '            private (`bool`, *optional*, defaults to `False`):\n', '                Whether the dataset repository should be set to private or not. Only affects repository creation:\n', '                a repository that already exists will not be affected by that parameter.\n', '            token (`str`, *optional*):\n', '                An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n', '                to the token saved locally when logging in with `huggingface-cli login`. Will raise an error\n', '                if no token is passed and the user is not logged-in.\n', '            revision (`str`, *optional*):\n', '                Branch to push the uploaded files to. Defaults to the `""main""` branch.\n', '\n', '                <Added version=""2.15.0""/>\n', '            branch (`str`, *optional*):\n', '                The git branch on which to push the dataset. This defaults to the default branch as specified\n', '                in your repository, which defaults to `""main""`.\n', '\n', '                <Deprecated version=""2.15.0"">\n', '\n', '                `branch` was deprecated in favor of `revision` in version 2.15.0 and will be removed in 3.0.0.\n', '\n', '                </Deprecated>\n', '            create_pr (`bool`, *optional*, defaults to `False`):\n', '                Whether to create a PR with the uploaded files or directly commit.\n', '\n', '                <Added version=""2.15.0""/>\n', '            max_shard_size (`int` or `str`, *optional*, defaults to `""500MB""`):\n', '                The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by\n', '                a unit (like `""5MB""`).\n', '            num_shards (`int`, *optional*):\n', '                Number of shards to write. By default, the number of shards depends on `max_shard_size`.\n', '\n', '                <Added version=""2.8.0""/>\n', '            embed_external_files (`bool`, defaults to `True`):\n', '                Whether to embed file bytes in the shards.\n', '                In particular, this will do the following before the push for the fields of type:\n', '\n', '                - [`Audio`] and [`Image`]: remove local path information and embed file content in the Parquet files.\n', '\n', '        Return:\n', '            huggingface_hub.CommitInfo\n', '\n', '        Example:\n', '\n', '        ```python\n', '        >>> dataset.push_to_hub(""<organization>/<dataset_id>"")\n', '        >>> dataset_dict.push_to_hub(""<organization>/<dataset_id>"", private=True)\n', '        >>> dataset.push_to_hub(""<organization>/<dataset_id>"", max_shard_size=""1GB"")\n', '        >>> dataset.push_to_hub(""<organization>/<dataset_id>"", num_shards=1024)\n', '        ```\n', '\n', '        If your dataset has multiple splits (e.g. train/validation/test):\n', '\n', '        ```python\n', '        >>> train_dataset.push_to_hub(""<organization>/<dataset_id>"", split=""train"")\n', '        >>> val_dataset.push_to_hub(""<organization>/<dataset_id>"", split=""validation"")\n', '        >>> # later\n', '        >>> dataset = load_dataset(""<organization>/<dataset_id>"")\n', '        >>> train_dataset = dataset[""train""]\n', '        >>> val_dataset = dataset[""validation""]\n', '        ```\n', '\n', '        If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):\n', '\n', '        ```python\n', '        >>> english_dataset.push_to_hub(""<organization>/<dataset_id>"", ""en"")\n', '        >>> french_dataset.push_to_hub(""<organization>/<dataset_id>"", ""fr"")\n', '        >>> # later\n', '        >>> english_dataset = load_dataset(""<organization>/<dataset_id>"", ""en"")\n', '        >>> french_dataset = load_dataset(""<organization>/<dataset_id>"", ""fr"")\n', '        ```\n', '        """"""\n']","['ValueError', 'str', 're.match', 'warnings.warn', 'HfApi', 'api.create_repo', 'api.create_branch', 'self._push_parquet_shards_to_hub', 'api.list_repo_tree', 'isinstance', 'deletions.append', 'fnmatch.fnmatch', 'PUSH_TO_HUB_WITHOUT_METADATA_CONFIGS_SPLIT_PATTERN_SHARDED.replace', 'string_to_dict', 'glob_pattern_to_regex', 'repo_splits.append', 'repo_id.split', 'SplitDict', 'SplitInfo', 'num_examples=len', 'api.hf_hub_download', 'DatasetCard.load', 'MetadataConfigs.from_dataset_card_data', 'DatasetInfosDict.from_dataset_card_data', 'DatasetCardData', 'MetadataConfigs', 'open', 'json.load', 'dataset_infos.get', 'DatasetInfo.from_dict', 'logger.info', 'list', 'sanitize_patterns', 'len', 'data_files_to_dump.items', 'metadata_configs.get_default_config_name', 'asdict', 'BytesIO', 'buffer.write', 'additions.append', 'CommitOperationAdd', 'DatasetInfosDict', 'DatasetCard', 'path_or_fileobj=str', 'api.create_commit', 'math.ceil', 'range']",47
repos/datasets/src/datasets/arrow_dataset.py:Dataset:remove_columns,Dataset:remove_columns,method,15,44,38,543,12.34,1,2,"['self', 'column_names', 'List[str]]', 'new_fingerprint']","[None, ' Union[str', None, ' Optional[str] ']","[None, None, None, ' None']",2171,"['        """"""\n', '        Remove one or several column(s) in the dataset and the features associated to them.\n', '\n', '        You can also remove a column using [`~datasets.Dataset.map`] with `remove_columns` but the present method\n', ""        doesn't copy the data of the remaining columns and is thus faster.\n"", '\n', '        Args:\n', '            column_names (`Union[str, List[str]]`):\n', '                Name of the column(s) to remove.\n', '            new_fingerprint (`str`, *optional*):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '\n', '        Returns:\n', '            [`Dataset`]: A copy of the dataset object without the columns to remove.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', ""        >>> ds = ds.remove_columns('label')\n"", '        Dataset({\n', ""            features: ['text'],\n"", '            num_rows: 1066\n', '        })\n', '        >>> ds = ds.remove_columns(column_names=ds.column_names) # Removing all the columns returns an empty dataset with the `num_rows` property set to 0\n', '        Dataset({\n', '            features: [],\n', '            num_rows: 0\n', '        })\n', '        ```\n', '        """"""\n']","['copy.deepcopy', 'isinstance', 'set', 'ValueError', 'update_metadata_with_features']",5
repos/datasets/src/datasets/arrow_dataset.py:Dataset:rename_column,Dataset:rename_column,method,20,117,62,1076,9.2,1,5,"['self', 'original_column_name', 'new_column_name', 'new_fingerprint']","[None, ' str', ' str', ' Optional[str] ']","[None, None, None, ' None']",2226,"['        """"""\n', '        Rename a column in the dataset, and move the features associated to the original column under the new column\n', '        name.\n', '\n', '        Args:\n', '            original_column_name (`str`):\n', '                Name of the column to rename.\n', '            new_column_name (`str`):\n', '                New name for the column.\n', '            new_fingerprint (`str`, *optional*):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '\n', '        Returns:\n', '            [`Dataset`]: A copy of the dataset with a renamed column.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', ""        >>> ds = ds.rename_column('label', 'label_new')\n"", '        Dataset({\n', ""            features: ['text', 'label_new'],\n"", '            num_rows: 1066\n', '        })\n', '        ```\n', '        """"""\n']","['copy.deepcopy', 'ValueError', 'rename', 'Features', 'update_metadata_with_features']",5
repos/datasets/src/datasets/arrow_dataset.py:Dataset:rename_columns,Dataset:rename_columns,method,23,119,82,1231,10.34,2,5,"['self', 'column_mapping', 'str]', 'new_fingerprint']","[None, ' Dict[str', None, ' Optional[str] ']","[None, None, None, ' None']",2293,"['        """"""\n', '        Rename several columns in the dataset, and move the features associated to the original columns under\n', '        the new column names.\n', '\n', '        Args:\n', '            column_mapping (`Dict[str, str]`):\n', '                A mapping of columns to rename to their new names\n', '            new_fingerprint (`str`, *optional*):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '\n', '        Returns:\n', '            [`Dataset`]: A copy of the dataset with renamed columns\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', ""        >>> ds = ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n"", '        Dataset({\n', ""            features: ['text_new', 'label_new'],\n"", '            num_rows: 1066\n', '        })\n', '        ```\n', '        """"""\n']","['copy.deepcopy', 'set', 'ValueError', 'len', 'column_mapping.values', 'rename', 'Features', 'update_metadata_with_features']",8
repos/datasets/src/datasets/arrow_dataset.py:Dataset:reset_format,Dataset:reset_format,method,1,1,1,17,17.0,0,0,['self'],[None],[None],2609,"['        """"""Reset `__getitem__` return format to python objects and all columns.\n', '\n', '        Same as `self.set_format()`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n', ""        >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n"", ""        >>> ds.set_format(type='numpy', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n"", '        >>> ds.format\n', ""        {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': 'numpy'}\n"", '        >>> ds.reset_format()\n', '        >>> ds.format\n', ""        {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': None}\n"", '        ```\n', '        """"""\n']",['self.set_format'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:save_to_disk,Dataset:save_to_disk,method,70,357,217,3476,9.74,8,11,"['self', 'dataset_path', 'fs', 'max_shard_size', 'int]] ', 'num_shards', 'num_proc', 'storage_options', '']","[None, ' PathLike', None, ' Optional[Union[str', None, ' Optional[int] ', ' Optional[int] ', ' Optional[dict] ', None]","[None, None, '""deprecated""', None, ' None', ' None', ' None', ' None', None]",1434,"['        """"""\n', '        Saves a dataset to a dataset directory, or in a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n', '\n', '        For [`Image`] and [`Audio`] data:\n', '\n', '        All the Image() and Audio() data are stored in the arrow files.\n', '        If you want to store paths or urls, please use the Value(""string"") type.\n', '\n', '        Args:\n', '            dataset_path (`str`):\n', '                Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)\n', '                of the dataset directory where the dataset will be saved to.\n', '            fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n', '                Instance of the remote filesystem where the dataset will be saved to.\n', '\n', '                <Deprecated version=""2.8.0"">\n', '\n', '                `fs` was deprecated in version 2.8.0 and will be removed in 3.0.0.\n', '                Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`\n', '\n', '                </Deprecated>\n', '\n', '            max_shard_size (`int` or `str`, *optional*, defaults to `""500MB""`):\n', '                The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n', '                (like `""50MB""`).\n', '            num_shards (`int`, *optional*):\n', '                Number of shards to write. By default the number of shards depends on `max_shard_size` and `num_proc`.\n', '\n', '                <Added version=""2.8.0""/>\n', '            num_proc (`int`, *optional*):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.8.0""/>\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.8.0""/>\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds.save_to_disk(""path/to/dataset/directory"")\n', '        >>> ds.save_to_disk(""path/to/dataset/directory"", max_shard_size=""1GB"")\n', '        >>> ds.save_to_disk(""path/to/dataset/directory"", num_shards=1024)\n', '        ```\n', '        """"""\n']","['ValueError', 'warnings.warn', 'self.list_indexes', 'self._estimate_nbytes', 'convert_file_size_to_int', 'int', 'max', 'url_to_fs', 'is_remote_filesystem', 'Path', 'PermissionError', 'fs.makedirs', 'str', 'range', 'json.dumps', 'TypeError', 'asdict', 'hf_tqdm', 'total=len', 'self.shard', 'posixpath.join', 'Pool', 'iflatmap_unordered', 'pbar.set_description', 'logger.debug', 'pbar.update', 'Dataset._save_to_disk_single', 'fs.open', 'json.dump', 'sorted']",30
repos/datasets/src/datasets/arrow_dataset.py:Dataset:select,Dataset:select,method,23,120,89,1275,10.62,0,9,"['self', 'indices', 'keep_in_memory', 'indices_cache_file_name', 'writer_batch_size', 'new_fingerprint', '']","[None, ' Iterable', ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', None]","[None, None, ' False', ' None', ' 1000', ' None', None]",3822,"['        """"""Create a new dataset with rows selected following the list/array of indices.\n', '\n', '        Args:\n', '            indices (`range`, `list`, `iterable`, `ndarray` or `Series`):\n', '                Range, list or 1D-array of integer indices for indexing.\n', '                If the indices correspond to a contiguous range, the Arrow table is simply sliced.\n', '                However passing a list of indices that are not contiguous creates indices mapping, which is much less efficient,\n', '                but still faster than recreating an Arrow table made of the requested rows.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the indices mapping in memory instead of writing it to a cache file.\n', '            indices_cache_file_name (`str`, *optional*, defaults to `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                indices mapping instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            new_fingerprint (`str`, *optional*, defaults to `None`):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.select(range(4))\n', '        Dataset({\n', ""            features: ['text', 'label'],\n"", '            num_rows: 4\n', '        })\n', '        ```\n', '        """"""\n']","['ValueError', 'len', 'DatasetTransformationNotAllowedError', 'isinstance', 'indices.to_numpy', 'list', '_is_range_contiguous', 'self._select_contiguous', 'next', 'itertools.count', 'all', 'zip', 'self._select_with_indices_mapping']",13
repos/datasets/src/datasets/arrow_dataset.py:Dataset:select_columns,Dataset:select_columns,method,14,46,39,575,12.5,0,2,"['self', 'column_names', 'List[str]]', 'new_fingerprint']","[None, ' Union[str', None, ' Optional[str] ']","[None, None, None, ' None']",2362,"['        """"""Select one or several column(s) in the dataset and the features\n', '        associated to them.\n', '\n', '        Args:\n', '            column_names (`Union[str, List[str]]`):\n', '                Name of the column(s) to keep.\n', '            new_fingerprint (`str`, *optional*):\n', '                The new fingerprint of the dataset after transform. If `None`,\n', '                the new fingerprint is computed using a hash of the previous\n', '                fingerprint, and the transform arguments.\n', '\n', '        Returns:\n', '            [`Dataset`]: A copy of the dataset object which only consists of\n', '            selected columns.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', ""        >>> ds.select_columns(['text'])\n"", '        Dataset({\n', ""            features: ['text'],\n"", '            num_rows: 1066\n', '        })\n', '        ```\n', '        """"""\n']","['isinstance', 'set', 'ValueError', 'copy.deepcopy', 'Features', 'update_metadata_with_features']",6
repos/datasets/src/datasets/arrow_dataset.py:Dataset:set_format,Dataset:set_format,method,17,120,84,989,8.24,0,6,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",2531,"['        """"""Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n', '        The format `type` (for example ""numpy"") is used to format batches when using `__getitem__`.\n', ""        It's also possible to use custom transforms for formatting using [`~datasets.Dataset.set_transform`].\n"", '\n', '        Args:\n', '            type (`str`, *optional*):\n', ""                Either output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n"", '                `None` means `__getitem__` returns python objects (default).\n', '            columns (`List[str]`, *optional*):\n', '                Columns to format in the output.\n', '                `None` means `__getitem__` returns all columns (default).\n', '            output_all_columns (`bool`, defaults to `False`):\n', '                Keep un-formatted columns as well in the output (as python objects).\n', '            **format_kwargs (additional keyword arguments):\n', '                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '\n', '        It is possible to call [`~datasets.Dataset.map`] after calling `set_format`. Since `map` may add new columns, then the list of formatted columns\n', '        gets updated. In this case, if you apply `map` on a dataset to add a new column, then this column will be formatted as:\n', '\n', '            ```\n', '            new formatted columns = (all columns - previously unformatted columns)\n', '            ```\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n', ""        >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n"", ""        >>> ds.set_format(type='numpy', columns=['text', 'label'])\n"", '        >>> ds.format\n', ""        {'type': 'numpy',\n"", ""        'format_kwargs': {},\n"", ""        'columns': ['text', 'label'],\n"", ""        'output_all_columns': False}\n"", '        ```\n', '        """"""\n']","['format_kwargs.update', 'self.set_format', 'get_format_type_from_alias', 'get_formatter', 'isinstance', 'list', 'set', 'ValueError', 'columns.copy', 'logger.debug', '__getitem__', 'str']",12
repos/datasets/src/datasets/arrow_dataset.py:Dataset:set_transform,Dataset:set_transform,method,1,4,4,99,24.75,0,0,"['self', 'transform', 'columns', 'output_all_columns', '']","[None, ' Optional[Callable]', ' Optional[List] ', ' bool ', None]","[None, None, ' None', ' False', None]",2638,"['        """"""Set `__getitem__` return format using this transform. The transform is applied on-the-fly on batches when `__getitem__` is called.\n', '        As [`~datasets.Dataset.set_format`], this can be reset using [`~datasets.Dataset.reset_format`].\n', '\n', '        Args:\n', '            transform (`Callable`, *optional*):\n', '                User-defined formatting transform, replaces the format defined by [`~datasets.Dataset.set_format`].\n', '                A formatting function is a callable that takes a batch (as a `dict`) as input and returns a batch.\n', '                This function is applied right before returning the objects in `__getitem__`.\n', '            columns (`List[str]`, *optional*):\n', '                Columns to format in the output.\n', '                If specified, then the input batch of the transform only contains those columns.\n', '            output_all_columns (`bool`, defaults to `False`):\n', '                Keep un-formatted columns as well in the output (as python objects).\n', '                If set to True, then the other un-formatted columns are kept with the output of the transform.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', ""        >>> tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"", '        >>> def encode(batch):\n', ""        ...     return tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')\n"", '        >>> ds.set_transform(encode)\n', '        >>> ds[0]\n', ""        {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"", '         1, 1]),\n', ""         'input_ids': tensor([  101, 29353,  2135, 15102,  1996,  9428, 20868,  2890,  8663,  6895,\n"", '                 20470,  2571,  3663,  2090,  4603,  3017,  3008,  1998,  2037, 24211,\n', '                 5637,  1998, 11690,  2336,  1012,   102]),\n', ""         'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n"", '                 0, 0])}\n', '        ```\n', '        """"""\n']",['self.set_format'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:shape,Dataset:shape,method,3,10,9,102,10.2,0,1,['self'],[None],[None],1876,"['        """"""Shape of the dataset (number of columns, number of rows).\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.shape\n', '        (1066, 2)\n', '        ```\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:shard,Dataset:shard,method,14,53,41,438,8.26,0,2,"['self', 'num_shards', 'index', 'contiguous', 'keep_in_memory', 'indices_cache_file_name', 'writer_batch_size', '']","[None, ' int', ' int', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', None]","[None, None, None, ' False', ' False', ' None', ' 1000', None]",4683,"['        """"""Return the `index`-nth shard from dataset split into `num_shards` pieces.\n', '\n', '        This shards deterministically. `dset.shard(n, i)` will contain all elements of dset whose\n', '        index mod `n = i`.\n', '\n', '        `dset.shard(n, i, contiguous=True)` will instead split dset into contiguous chunks,\n', '        so it can be easily concatenated back together after processing. If `n % i == l`, then the\n', '        first `l` shards will have length `(n // i) + 1`, and the remaining shards will have length `(n // i)`.\n', '        `datasets.concatenate([dset.shard(n, i, contiguous=True) for i in range(n)])` will return\n', '        a dataset with the same order as the original.\n', '\n', '        Be sure to shard before using any randomizing operator (such as `shuffle`).\n', '        It is best if the shard operator is used early in the dataset pipeline.\n', '\n', '\n', '        Args:\n', '            num_shards (`int`):\n', '                How many shards to split the dataset into.\n', '            index (`int`):\n', '                Which shard to select and return.\n', '            contiguous: (`bool`, defaults to `False`):\n', '                Whether to select contiguous blocks of indices for shards.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            indices_cache_file_name (`str`, *optional*):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                indices of each shard instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds\n', '        Dataset({\n', ""            features: ['text', 'label'],\n"", '            num_rows: 1066\n', '        })\n', '        >>> ds.shard(num_shards=2, index=0)\n', '        Dataset({\n', ""            features: ['text', 'label'],\n"", '            num_rows: 533\n', '        })\n', '        ```\n', '        """"""\n']","['ValueError', 'len', 'min', 'range', 'np.arange', 'self.select']",6
repos/datasets/src/datasets/arrow_dataset.py:Dataset:shuffle,Dataset:shuffle,method,0,1,1,4,4.0,0,0,"['self', 'seed', 'generator', 'keep_in_memory', 'load_from_cache_file', 'indices_cache_file_name', 'writer_batch_size', 'new_fingerprint', '']","[None, ' Optional[int] ', ' Optional[np.random.Generator] ', ' bool ', ' Optional[bool] ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', None]","[None, ' None', ' None', ' False', ' None', ' None', ' 1000', ' None', None]",4272,"['        """"""Create a new Dataset where the rows are shuffled.\n', '\n', '        Currently shuffling uses numpy random generators.\n', ""        You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n"", '\n', '        Shuffling takes the list of indices `[0:len(my_dataset)]` and shuffles it to create an indices mapping.\n', '        However as soon as your [`Dataset`] has an indices mapping, the speed can become 10x slower.\n', ""        This is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you aren't reading contiguous chunks of data anymore.\n"", ""        To restore the speed, you'd need to rewrite the entire dataset on your disk again using [`Dataset.flatten_indices`], which removes the indices mapping.\n"", '        This may take a lot of time depending of the size of your dataset though:\n', '\n', '        ```python\n', '        my_dataset[0]  # fast\n', '        my_dataset = my_dataset.shuffle(seed=42)\n', '        my_dataset[0]  # up to 10x slower\n', '        my_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\n', '        my_dataset[0]  # fast again\n', '        ```\n', '\n', '        In this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approximate shuffling method [`IterableDataset.shuffle`].\n', '        It only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal:\n', '\n', '        ```python\n', '        my_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=128)\n', '        for example in enumerate(my_iterable_dataset):  # fast\n', '            pass\n', '\n', '        shuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\n', '\n', '        for example in enumerate(shuffled_iterable_dataset):  # as fast as before\n', '            pass\n', '        ```\n', '\n', '        Args:\n', '            seed (`int`, *optional*):\n', '                A seed to initialize the default BitGenerator if `generator=None`.\n', '                If `None`, then fresh, unpredictable entropy will be pulled from the OS.\n', '                If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n', '            generator (`numpy.random.Generator`, *optional*):\n', '                Numpy random Generator to use to compute the permutation of the dataset rows.\n', '                If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n', '            keep_in_memory (`bool`, default `False`):\n', '                Keep the shuffled indices in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the shuffled indices\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_name (`str`, *optional*):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                shuffled indices instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            new_fingerprint (`str`, *optional*, defaults to `None`):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', ""        >>> ds['label'][:10]\n"", '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n', '\n', '        # set a seed\n', '        >>> shuffled_ds = ds.shuffle(seed=42)\n', ""        >>> shuffled_ds['label'][:10]\n"", '        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n', '        ```\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:Dataset:skip,Dataset:skip,method,2,3,3,37,12.33,0,0,"['self', 'n']","[None, ' int']","[None, None]",4070,"['        """"""\n', '        Create a new [`Dataset`] that skips the first `n` elements.\n', '\n', '        Args:\n', '            n (`int`):\n', '                Number of elements to skip.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"")\n', '        >>> list(ds.take(3))\n', ""        [{'label': 1,\n"", '         \'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'},\n', ""         {'label': 1,\n"", '         \'text\': \'the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\'},\n', ""         {'label': 1, 'text': 'effective but too-tepid biopic'}]\n"", '        >>> ds = ds.skip(1)\n', '        >>> list(ds.take(3))\n', ""        [{'label': 1,\n"", '         \'text\': \'the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\'},\n', ""         {'label': 1, 'text': 'effective but too-tepid biopic'},\n"", ""         {'label': 1,\n"", ""         'text': 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .'}]\n"", '        ```\n', '        """"""\n']","['self.select', 'len']",2
repos/datasets/src/datasets/arrow_dataset.py:Dataset:sort,Dataset:sort,method,25,225,154,2068,9.19,1,13,"['self', 'column_names', 'Sequence_[str]]', 'reverse', 'Sequence_[bool]] ', 'kind', 'null_placement', 'keep_in_memory', 'load_from_cache_file', 'indices_cache_file_name', 'writer_batch_size', 'new_fingerprint', '']","[None, ' Union[str', None, ' Union[bool', None, None, ' str ', ' bool ', ' Optional[bool] ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', None]","[None, None, None, None, ' False', '""deprecated""', ' ""at_end""', ' False', ' None', ' None', ' 1000', ' None', None]",4125,"['        """"""Create a new dataset sorted according to a single or multiple columns.\n', '\n', '        Args:\n', '            column_names (`Union[str, Sequence[str]]`):\n', '                Column name(s) to sort by.\n', '            reverse (`Union[bool, Sequence[bool]]`, defaults to `False`):\n', '                If `True`, sort by descending order rather than ascending. If a single bool is provided,\n', '                the value is applied to the sorting of all column names. Otherwise a list of bools with the\n', '                same length and order as column_names must be provided.\n', '            kind (`str`, *optional*):\n', '                Pandas algorithm for sorting selected in `{quicksort, mergesort, heapsort, stable}`,\n', '                The default is `quicksort`. Note that both `stable` and `mergesort` use `timsort` under the covers and, in general,\n', '                the actual implementation will vary with data type. The `mergesort` option is retained for backwards compatibility.\n', '                <Deprecated version=""2.8.0"">\n', '\n', '                `kind` was deprecated in version 2.10.0 and will be removed in 3.0.0.\n', '\n', '                </Deprecated>\n', '            null_placement (`str`, defaults to `at_end`):\n', '                Put `None` values at the beginning if `at_start` or `first` or at the end if `at_end` or `last`\n', '\n', '                <Added version=""1.14.2""/>\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the sorted indices in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the sorted indices\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_name (`str`, *optional*, defaults to `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                sorted indices instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                Higher value gives smaller cache files, lower value consume less temporary memory.\n', '            new_fingerprint (`str`, *optional*, defaults to `None`):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', ""        >>> ds = load_dataset('rotten_tomatoes', split='validation')\n"", ""        >>> ds['label'][:10]\n"", '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n', ""        >>> sorted_ds = ds.sort('label')\n"", ""        >>> sorted_ds['label'][:10]\n"", '        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n', ""        >>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])\n"", ""        >>> another_sorted_ds['label'][:10]\n"", '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n', '        ```\n', '        """"""\n']","['len', 'DatasetTransformationNotAllowedError', 'warnings.warn', 'isinstance', 'ValueError', 'is_caching_enabled', 'self._get_cache_file_path', 'logger.info', 'self._new_dataset_with_indices', 'query_table', 'key=slice', 'zip', 'pc.sort_indices', 'self.select']",14
repos/datasets/src/datasets/arrow_dataset.py:Dataset:take,Dataset:take,method,2,2,2,27,13.5,0,0,"['self', 'n']","[None, ' int']","[None, None]",4100,"['        """"""\n', '        Create a new [`Dataset`] with only the first `n` elements.\n', '\n', '        Args:\n', '            n (`int`):\n', '                Number of elements to take.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"")\n', '        >>> small_ds = ds.take(2)\n', '        >>> list(small_ds)\n', ""        [{'label': 1,\n"", '         \'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'},\n', ""         {'label': 1,\n"", '         \'text\': \'the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\'}]\n', '        ```\n', '        """"""\n']",['self.select'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:to_csv,Dataset:to_csv,method,4,13,13,178,13.69,0,0,"['self', 'path_or_buf', 'BinaryIO]', 'batch_size', 'num_proc', 'storage_options', '**to_csv_kwargs', '']","[None, ' Union[PathLike', None, ' Optional[int] ', ' Optional[int] ', ' Optional[dict] ', None, None]","[None, None, None, ' None', ' None', ' None', None, None]",4849,"['        """"""Exports the dataset to csv\n', '\n', '        Args:\n', '            path_or_buf (`PathLike` or `FileOrBuffer`):\n', '                Either a path to a file (e.g. `file.csv`), a remote URI (e.g. `hf://datasets/username/my_dataset_name/data.csv`),\n', '                or a BinaryIO, where the dataset will be saved to in the specified format.\n', '            batch_size (`int`, *optional*):\n', '                Size of the batch to load in memory and write at once.\n', '                Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '            num_proc (`int`, *optional*):\n', ""                Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing. `batch_size` in this case defaults to\n', '                `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n', '                value if you have sufficient compute power.\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.19.0""/>\n', '            **to_csv_kwargs (additional keyword arguments):\n', ""                Parameters to pass to pandas's [`pandas.DataFrame.to_csv`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).\n"", '\n', '                <Changed version=""2.10.0"">\n', '\n', '                Now, `index` defaults to `False` if not specified.\n', '\n', '                If you would like to write the index, pass `index=True` and also set a name for the index column by\n', '                passing `index_label`.\n', '\n', '                </Changed>\n', '\n', '        Returns:\n', '            `int`: The number of characters or bytes written.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds.to_csv(""path/to/dataset/directory"")\n', '        ```\n', '        """"""\n']",['CsvDatasetWriter'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:to_dict,Dataset:to_dict,method,10,65,50,574,8.83,1,3,"['self', 'batch_size', 'batched']","[None, ' Optional[int] ', None]","[None, ' None', '""deprecated""']",4908,"['        """"""Returns the dataset as a Python dict. Can also return a generator for large datasets.\n', '\n', '        Args:\n', '            batched (`bool`):\n', '                Set to `True` to return a generator that yields the dataset as batches\n', '                of `batch_size` rows. Defaults to `False` (returns the whole datasets once).\n', '\n', '                <Deprecated version=""2.11.0"">\n', '\n', '                Use `.iter(batch_size=batch_size)` followed by `.to_dict()` on the individual batches instead.\n', '\n', '                </Deprecated>\n', '\n', '            batch_size (`int`, *optional*): The size (number of rows) of the batches if `batched` is `True`.\n', '                Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '\n', '        Returns:\n', '            `dict` or `Iterator[dict]`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds.to_dict()\n', '        ```\n', '        """"""\n']","['warnings.warn', 'query_table', 'key=slice', 'len', 'range']",5
repos/datasets/src/datasets/arrow_dataset.py:Dataset:to_iterable_dataset,Dataset:to_iterable_dataset,method,14,131,95,1028,7.85,0,4,"['self', 'num_shards']","[None, ' Optional[int] ']","[None, ' 1']",5269,"['        """"""Get an [`datasets.IterableDataset`] from a map-style [`datasets.Dataset`].\n', '        This is equivalent to loading a dataset in streaming mode with [`datasets.load_dataset`], but much faster since the data is streamed from local files.\n', '\n', '        Contrary to map-style datasets, iterable datasets are lazy and can only be iterated over (e.g. using a for loop).\n', '        Since they are read sequentially in training loops, iterable datasets are much faster than map-style datasets.\n', '        All the transformations applied to iterable datasets like filtering or processing are done on-the-fly when you start iterating over the dataset.\n', '\n', '        Still, it is possible to shuffle an iterable dataset using [`datasets.IterableDataset.shuffle`].\n', '        This is a fast approximate shuffling that works best if you have multiple shards and if you specify a buffer size that is big enough.\n', '\n', ""        To get the best speed performance, make sure your dataset doesn't have an indices mapping.\n"", '        If this is the case, the data are not read contiguously, which can be slow sometimes.\n', '        You can use `ds = ds.flatten_indices()` to write your dataset in contiguous chunks of data and have optimal speed before switching to an iterable dataset.\n', '\n', '        Args:\n', '            num_shards (`int`, default to `1`):\n', '                Number of shards to define when instantiating the iterable dataset. This is especially useful for big datasets to be able to shuffle properly,\n', '                and also to enable fast parallel loading using a PyTorch DataLoader or in distributed setups for example.\n', '                Shards are defined using [`datasets.Dataset.shard`]: it simply slices the data without writing anything on disk.\n', '\n', '        Returns:\n', '            [`datasets.IterableDataset`]\n', '\n', '        Example:\n', '\n', '        Basic usage:\n', '        ```python\n', '        >>> ids = ds.to_iterable_dataset()\n', '        >>> for example in ids:\n', '        ...     pass\n', '        ```\n', '\n', '        With lazy filtering and processing:\n', '        ```python\n', '        >>> ids = ds.to_iterable_dataset()\n', '        >>> ids = ids.filter(filter_fn).map(process_fn)  # will filter and process on-the-fly when you start iterating over the iterable dataset\n', '        >>> for example in ids:\n', '        ...     pass\n', '        ```\n', '\n', '        With sharding to enable efficient shuffling:\n', '        ```python\n', '        >>> ids = ds.to_iterable_dataset(num_shards=64)  # the dataset is split into 64 shards to be iterated over\n', '        >>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer for fast approximate shuffling when you start iterating\n', '        >>> for example in ids:\n', '        ...     pass\n', '        ```\n', '\n', '        With a PyTorch DataLoader:\n', '        ```python\n', '        >>> import torch\n', '        >>> ids = ds.to_iterable_dataset(num_shards=64)\n', '        >>> ids = ids.filter(filter_fn).map(process_fn)\n', '        >>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards to each worker to load, filter and process when you start iterating\n', '        >>> for example in ids:\n', '        ...     pass\n', '        ```\n', '\n', '        With a PyTorch DataLoader and shuffling:\n', '        ```python\n', '        >>> import torch\n', '        >>> ids = ds.to_iterable_dataset(num_shards=64)\n', '        >>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n', '        >>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating\n', '        >>> for example in ids:\n', '        ...     pass\n', '        ```\n', '\n', '        In a distributed setup like PyTorch DDP with a PyTorch DataLoader and shuffling\n', '        ```python\n', '        >>> from datasets.distributed import split_dataset_by_node\n', '        >>> ids = ds.to_iterable_dataset(num_shards=512)\n', '        >>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n', '        >>> ids = split_dataset_by_node(ds, world_size=8, rank=0)  # will keep only 512 / 8 = 64 shards from the shuffled lists of shards when you start iterating\n', ""        >>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from this node's list of shards to each worker when you start iterating\n"", '        >>> for example in ids:\n', '        ...     pass\n', '        ```\n', '\n', '        With shuffling and multiple epochs:\n', '        ```python\n', '        >>> ids = ds.to_iterable_dataset(num_shards=64)\n', '        >>> ids = ids.shuffle(buffer_size=10_000, seed=42)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n', '        >>> for epoch in range(n_epochs):\n', '        ...     ids.set_epoch(epoch)  # will use effective_seed = seed + epoch to shuffle the shards and for the shuffle buffer when you start iterating\n', '        ...     for example in ids:\n', '        ...         pass\n', '        ```\n', '        Feel free to also use [`IterableDataset.set_epoch`] when using a PyTorch DataLoader or in distributed setups.\n', '        """"""\n']","['NotImplementedError', 'my_dataset.with_format', 'len', 'ValueError', 'logger.info', 'ds.flatten_indices', 'self.shard', 'range', 'ArrowExamplesIterable', 'IterableDataset', 'info=DatasetInfo']",11
repos/datasets/src/datasets/arrow_dataset.py:Dataset:to_json,Dataset:to_json,method,4,13,13,182,14.0,0,0,"['self', 'path_or_buf', 'BinaryIO]', 'batch_size', 'num_proc', 'storage_options', '**to_json_kwargs', '']","[None, ' Union[PathLike', None, ' Optional[int] ', ' Optional[int] ', ' Optional[dict] ', None, None]","[None, None, None, ' None', ' None', ' None', None, None]",4977,"['        """"""Export the dataset to JSON Lines or JSON.\n', '\n', '        The default output format is [JSON Lines](https://jsonlines.org/).\n', '        To export to [JSON](https://www.json.org), pass `lines=False` argument and the desired `orient`.\n', '\n', '        Args:\n', '            path_or_buf (`PathLike` or `FileOrBuffer`):\n', '                Either a path to a file (e.g. `file.json`), a remote URI (e.g. `hf://datasets/username/my_dataset_name/data.json`),\n', '                or a BinaryIO, where the dataset will be saved to in the specified format.\n', '            batch_size (`int`, *optional*):\n', '                Size of the batch to load in memory and write at once.\n', '                Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '            num_proc (`int`, *optional*):\n', ""                Number of processes for multiprocessing. By default, it doesn't\n"", '                use multiprocessing. `batch_size` in this case defaults to\n', '                `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n', '                value if you have sufficient compute power.\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.19.0""/>\n', '            **to_json_kwargs (additional keyword arguments):\n', ""                Parameters to pass to pandas's [`pandas.DataFrame.to_json`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).\n"", '                Default arguments are `lines=True` and `orient=""records"".\n', '\n', '                <Changed version=""2.11.0"">\n', '\n', '                The parameter `index` defaults to `False` if `orient` is `""split""` or `""table""`.\n', '\n', '                If you would like to write the index, pass `index=True`.\n', '\n', '                </Changed>\n', '\n', '        Returns:\n', '            `int`: The number of characters or bytes written.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds.to_json(""path/to/dataset/directory/filename.jsonl"")\n', '        ```\n', '        """"""\n']",['JsonDatasetWriter'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:to_list,Dataset:to_list,method,3,7,7,97,13.86,0,0,['self'],[None],[None],4959,"['        """"""Returns the dataset as a Python list.\n', '\n', '        Returns:\n', '            `list`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds.to_list()\n', '        ```\n', '        """"""\n']","['query_table', 'key=slice', 'len']",3
repos/datasets/src/datasets/arrow_dataset.py:Dataset:to_pandas,Dataset:to_pandas,method,8,33,24,404,12.24,1,2,"['self', 'batch_size', 'batched']","[None, ' Optional[int] ', ' bool ']","[None, ' None', ' False']",5039,"['        """"""Returns the dataset as a `pandas.DataFrame`. Can also return a generator for large datasets.\n', '\n', '        Args:\n', '            batched (`bool`):\n', '                Set to `True` to return a generator that yields the dataset as batches\n', '                of `batch_size` rows. Defaults to `False` (returns the whole datasets once).\n', '            batch_size (`int`, *optional*):\n', '                The size (number of rows) of the batches if `batched` is `True`.\n', '                Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '\n', '        Returns:\n', '            `pandas.DataFrame` or `Iterator[pandas.DataFrame]`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds.to_pandas()\n', '        ```\n', '        """"""\n']","['query_table', 'key=slice', 'len', 'range']",4
repos/datasets/src/datasets/arrow_dataset.py:Dataset:to_parquet,Dataset:to_parquet,method,4,12,12,174,14.5,0,0,"['self', 'path_or_buf', 'BinaryIO]', 'batch_size', 'storage_options', '**parquet_writer_kwargs', '']","[None, ' Union[PathLike', None, ' Optional[int] ', ' Optional[dict] ', None, None]","[None, None, None, ' None', ' None', None, None]",5138,"['        """"""Exports the dataset to parquet\n', '\n', '        Args:\n', '            path_or_buf (`PathLike` or `FileOrBuffer`):\n', '                Either a path to a file (e.g. `file.parquet`), a remote URI (e.g. `hf://datasets/username/my_dataset_name/data.parquet`),\n', '                or a BinaryIO, where the dataset will be saved to in the specified format.\n', '            batch_size (`int`, *optional*):\n', '                Size of the batch to load in memory and write at once.\n', '                Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.19.0""/>\n', '            **parquet_writer_kwargs (additional keyword arguments):\n', ""                Parameters to pass to PyArrow's `pyarrow.parquet.ParquetWriter`.\n"", '\n', '        Returns:\n', '            `int`: The number of characters or bytes written.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds.to_parquet(""path/to/dataset/directory"")\n', '        ```\n', '        """"""\n']",['ParquetDatasetWriter'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:to_polars,Dataset:to_polars,method,17,75,45,647,8.63,1,5,"['self', 'batch_size', 'batched', 'schema_overrides', 'rechunk', '']","[None, ' Optional[int] ', ' bool ', ' Optional[dict] ', ' bool ', None]","[None, ' None', ' False', ' None', ' True', None]",5078,"['        """"""Returns the dataset as a `polars.DataFrame`. Can also return a generator for large datasets.\n', '\n', '        Args:\n', '            batched (`bool`):\n', '                Set to `True` to return a generator that yields the dataset as batches\n', '                of `batch_size` rows. Defaults to `False` (returns the whole datasets once).\n', '            batch_size (`int`, *optional*):\n', '                The size (number of rows) of the batches if `batched` is `True`.\n', '                Defaults to `genomicsml.datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '            schema_overrides (`dict`, *optional*):\n', '                Support type specification or override of one or more columns; note that\n', '                any dtypes inferred from the schema param will be overridden.\n', '            rechunk (`bool`):\n', '                Make sure that all data is in contiguous memory. Defaults to `True`.\n', '        Returns:\n', '            `polars.DataFrame` or `Iterator[polars.DataFrame]`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds.to_polars()\n', '        ```\n', '        """"""\n']","['pl.from_arrow', 'query_table', 'key=slice', 'len', 'range', 'ValueError']",6
repos/datasets/src/datasets/arrow_dataset.py:Dataset:to_sql,Dataset:to_sql,method,4,10,10,121,12.1,0,0,"['self', 'name', 'con', '""sqlalchemy.engine.Connection""', '""sqlalchemy.engine.Engine""', '""sqlite3.Connection""]', 'batch_size', '**sql_writer_kwargs', '']","[None, ' str', ' Union[str', None, None, None, ' Optional[int] ', None, None]","[None, None, None, None, None, None, ' None', None, None]",5177,"['        """"""Exports the dataset to a SQL database.\n', '\n', '        Args:\n', '            name (`str`):\n', '                Name of SQL table.\n', '            con (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`):\n', '                A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) or a SQLite3/SQLAlchemy connection object used to write to a database.\n', '            batch_size (`int`, *optional*):\n', '                Size of the batch to load in memory and write at once.\n', '                Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n', '            **sql_writer_kwargs (additional keyword arguments):\n', ""                Parameters to pass to pandas's [`pandas.DataFrame.to_sql`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html).\n"", '\n', '                <Changed version=""2.11.0"">\n', '\n', '                Now, `index` defaults to `False` if not specified.\n', '\n', '                If you would like to write the index, pass `index=True` and also set a name for the index column by\n', '                passing `index_label`.\n', '\n', '                </Changed>\n', '\n', '        Returns:\n', '            `int`: The number of records written.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> # con provided as a connection URI string\n', '        >>> ds.to_sql(""data"", ""sqlite:///my_own_db.sql"")\n', '        >>> # con provided as a sqlite3 connection object\n', '        >>> import sqlite3\n', '        >>> con = sqlite3.connect(""my_own_db.sql"")\n', '        >>> with con:\n', '        ...     ds.to_sql(""data"", con)\n', '        ```\n', '        """"""\n']",['SqlDatasetWriter'],1
repos/datasets/src/datasets/arrow_dataset.py:Dataset:train_test_split,Dataset:train_test_split,method,60,572,271,5009,8.76,0,28,"['self', 'test_size', 'int', 'None] ', 'train_size', 'int', 'None] ', 'shuffle', 'stratify_by_column', 'seed', 'generator', 'keep_in_memory', 'load_from_cache_file', 'train_indices_cache_file_name', 'test_indices_cache_file_name', 'writer_batch_size', 'train_new_fingerprint', 'test_new_fingerprint', '']","[None, ' Union[float', None, None, ' Union[float', None, None, ' bool ', ' Optional[str] ', ' Optional[int] ', ' Optional[np.random.Generator] ', ' bool ', ' Optional[bool] ', ' Optional[str] ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', ' Optional[str] ', None]","[None, None, None, ' None', None, None, ' None', ' True', ' None', ' None', ' None', ' False', ' None', ' None', ' None', ' 1000', ' None', ' None', None]",4407,"['        """"""Return a dictionary ([`datasets.DatasetDict`]) with two random train and test subsets (`train` and `test` `Dataset` splits).\n', '        Splits are created from the dataset according to `test_size`, `train_size` and `shuffle`.\n', '\n', '        This method is similar to scikit-learn `train_test_split`.\n', '\n', '        Args:\n', '            test_size (`numpy.random.Generator`, *optional*):\n', '                Size of the test split\n', '                If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the test split.\n', '                If `int`, represents the absolute number of test samples.\n', '                If `None`, the value is set to the complement of the train size.\n', '                If `train_size` is also `None`, it will be set to `0.25`.\n', '            train_size (`numpy.random.Generator`, *optional*):\n', '                Size of the train split\n', '                If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the train split.\n', '                If `int`, represents the absolute number of train samples.\n', '                If `None`, the value is automatically set to the complement of the test size.\n', '            shuffle (`bool`, *optional*, defaults to `True`):\n', '                Whether or not to shuffle the data before splitting.\n', '            stratify_by_column (`str`, *optional*, defaults to `None`):\n', '                The column name of labels to be used to perform stratified split of data.\n', '            seed (`int`, *optional*):\n', '                A seed to initialize the default BitGenerator if `generator=None`.\n', '                If `None`, then fresh, unpredictable entropy will be pulled from the OS.\n', '                If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n', '            generator (`numpy.random.Generator`, *optional*):\n', '                Numpy random Generator to use to compute the permutation of the dataset rows.\n', '                If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the splits indices in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the splits indices\n', '                can be identified, use it instead of recomputing.\n', '            train_cache_file_name (`str`, *optional*):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                train split indices instead of the automatically generated cache file name.\n', '            test_cache_file_name (`str`, *optional*):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                test split indices instead of the automatically generated cache file name.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            train_new_fingerprint (`str`, *optional*, defaults to `None`):\n', '                The new fingerprint of the train set after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '            test_new_fingerprint (`str`, *optional*, defaults to `None`):\n', '                The new fingerprint of the test set after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds = ds.train_test_split(test_size=0.2, shuffle=True)\n', '        DatasetDict({\n', '            train: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 852\n', '            })\n', '            test: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 214\n', '            })\n', '        })\n', '\n', '        # set a seed\n', '        >>> ds = ds.train_test_split(test_size=0.2, seed=42)\n', '\n', '        # stratified split\n', '        >>> ds = load_dataset(""imdb"",split=""train"")\n', '        Dataset({\n', ""            features: ['text', 'label'],\n"", '            num_rows: 25000\n', '        })\n', '        >>> ds = ds.train_test_split(test_size=0.2, stratify_by_column=""label"")\n', '        DatasetDict({\n', '            train: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 20000\n', '            })\n', '            test: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 5000\n', '            })\n', '        })\n', '        ```\n', '        """"""\n']","['len', 'DatasetTransformationNotAllowedError', 'DatasetDict', 'isinstance', 'ValueError', 'ceil', 'float', 'floor', 'int', 'is_caching_enabled', 'self._get_cache_file_path', 'logger.info', 'self._new_dataset_with_indices', 'np.arange', 'next', 'stratified_shuffle_split_generate_indices', 'self.with_format', 'str', 'generator.permutation', 'self.select']",20
repos/datasets/src/datasets/arrow_dataset.py:Dataset:unique,Dataset:unique,method,11,28,23,297,10.61,0,2,"['self', 'column']","[None, ' str']","[None, None]",1892,"['        """"""Return a list of the unique elements in a column.\n', '\n', '        This is implemented in the low-level backend and as such, very fast.\n', '\n', '        Args:\n', '            column (`str`):\n', '                Column name (list all the column names with [`~datasets.Dataset.column_names`]).\n', '\n', '        Returns:\n', '            `list`: List of unique elements in the given column.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', ""        >>> ds.unique('label')\n"", '        [1, 0]\n', '        ```\n', '        """"""\n']","['ValueError', 'self.flatten_indices']",2
repos/datasets/src/datasets/arrow_dataset.py:Dataset:with_format,Dataset:with_format,method,4,8,7,141,17.62,0,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",2681,"['        """"""Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n', '        The format `type` (for example ""numpy"") is used to format batches when using `__getitem__`.\n', '\n', ""        It's also possible to use custom transforms for formatting using [`~datasets.Dataset.with_transform`].\n"", '\n', '        Contrary to [`~datasets.Dataset.set_format`], `with_format` returns a new [`Dataset`] object.\n', '\n', '        Args:\n', '            type (`str`, *optional*):\n', ""                Either output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n"", '                `None` means `__getitem__` returns python objects (default).\n', '            columns (`List[str]`, *optional*):\n', '                Columns to format in the output.\n', '                `None` means `__getitem__` returns all columns (default).\n', '            output_all_columns (`bool`, defaults to `False`):\n', '                Keep un-formatted columns as well in the output (as python objects).\n', '            **format_kwargs (additional keyword arguments):\n', '                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n', ""        >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n"", '        >>> ds.format\n', ""        {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': None}\n"", ""        >>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n"", '        >>> ds.format\n', ""        {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': 'tensorflow'}\n"", '        ```\n', '        """"""\n']","['copy.deepcopy', 'dataset.set_format']",2
repos/datasets/src/datasets/arrow_dataset.py:Dataset:with_transform,Dataset:with_transform,method,4,7,6,138,19.71,0,0,"['self', 'transform', 'columns', 'output_all_columns', '']","[None, ' Optional[Callable]', ' Optional[List] ', ' bool ', None]","[None, None, ' None', ' False', None]",2732,"['        """"""Set `__getitem__` return format using this transform. The transform is applied on-the-fly on batches when `__getitem__` is called.\n', '\n', '        As [`~datasets.Dataset.set_format`], this can be reset using [`~datasets.Dataset.reset_format`].\n', '\n', '        Contrary to [`~datasets.Dataset.set_transform`], `with_transform` returns a new [`Dataset`] object.\n', '\n', '        Args:\n', '            transform (`Callable`, `optional`):\n', '                User-defined formatting transform, replaces the format defined by [`~datasets.Dataset.set_format`].\n', '                A formatting function is a callable that takes a batch (as a `dict`) as input and returns a batch.\n', '                This function is applied right before returning the objects in `__getitem__`.\n', '            columns (`List[str]`, `optional`):\n', '                Columns to format in the output.\n', '                If specified, then the input batch of the transform only contains those columns.\n', '            output_all_columns (`bool`, defaults to `False`):\n', '                Keep un-formatted columns as well in the output (as python objects).\n', '                If set to `True`, then the other un-formatted columns are kept with the output of the transform.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n', '        >>> def encode(example):\n', '        ...     return tokenizer(example[""text""], padding=True, truncation=True, return_tensors=\'pt\')\n', '        >>> ds = ds.with_transform(encode)\n', '        >>> ds[0]\n', ""        {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"", '         1, 1, 1, 1, 1]),\n', ""         'input_ids': tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,\n"", '                 1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,\n', '                 1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102]),\n', ""         'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n"", '                 0, 0, 0, 0, 0])}\n', '        ```\n', '        """"""\n']","['copy.deepcopy', 'dataset.set_transform']",2
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:__init__,DatasetInfoMixin:__init__,method,4,4,4,33,8.25,0,0,"['self', 'info', 'split']","[None, ' DatasetInfo', ' Optional[NamedSplit]']","[None, None, None]",164,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:builder_name,DatasetInfoMixin:builder_name,method,2,2,2,29,14.5,0,0,['self'],[None],[None],179,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:citation,DatasetInfoMixin:citation,method,2,2,2,25,12.5,0,0,['self'],[None],[None],183,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:config_name,DatasetInfoMixin:config_name,method,2,2,2,28,14.0,0,0,['self'],[None],[None],187,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:dataset_size,DatasetInfoMixin:dataset_size,method,2,2,2,29,14.5,0,0,['self'],[None],[None],191,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:description,DatasetInfoMixin:description,method,2,2,2,28,14.0,0,0,['self'],[None],[None],195,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:download_checksums,DatasetInfoMixin:download_checksums,method,2,2,2,35,17.5,0,0,['self'],[None],[None],199,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:download_size,DatasetInfoMixin:download_size,method,2,2,2,30,15.0,0,0,['self'],[None],[None],203,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:features,DatasetInfoMixin:features,method,2,9,8,70,7.78,0,0,['self'],[None],[None],207,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:homepage,DatasetInfoMixin:homepage,method,2,2,2,25,12.5,0,0,['self'],[None],[None],211,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:info,DatasetInfoMixin:info,method,2,2,2,16,8.0,0,0,['self'],[None],[None],169,"['        """"""[`~datasets.DatasetInfo`] object containing all the metadata in the dataset.""""""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:license,DatasetInfoMixin:license,method,2,2,2,24,12.0,0,0,['self'],[None],[None],215,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:size_in_bytes,DatasetInfoMixin:size_in_bytes,method,2,2,2,30,15.0,0,0,['self'],[None],[None],219,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:split,DatasetInfoMixin:split,method,2,2,2,17,8.5,0,0,['self'],[None],[None],174,"['        """"""[`~datasets.NamedSplit`] object corresponding to a named dataset split.""""""\n']",[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:supervised_keys,DatasetInfoMixin:supervised_keys,method,2,2,2,32,16.0,0,0,['self'],[None],[None],223,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:task_templates,DatasetInfoMixin:task_templates,method,2,2,2,31,15.5,0,0,['self'],[None],[None],227,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:DatasetInfoMixin:version,DatasetInfoMixin:version,method,2,2,2,24,12.0,0,0,['self'],[None],[None],231,[],[],0
repos/datasets/src/datasets/arrow_dataset.py:TensorflowDatasetMixin:_get_output_signature,TensorflowDatasetMixin:_get_output_signature,method,62,219,147,2038,9.31,10,9,"['dataset', 'collate_fn', 'collate_fn_args', 'cols_to_retain', 'batch_size', 'num_test_batches', '']","[' ""Dataset""', ' Callable', ' dict', ' Optional[List[str]] ', ' Optional[int] ', ' int ', None]","[None, None, None, ' None', ' None', ' 20', None]",239,"['        """"""Private method used by `to_tf_dataset()` to find the shapes and dtypes of samples from this dataset\n', '           after being passed through the collate_fn. Tensorflow needs an exact signature for tf.numpy_function, so\n', ""           the only way to do this is to run test batches - the collator may add or rename columns, so we can't figure\n"", '           it out just by inspecting the dataset.\n', '\n', '        Args:\n', '            dataset (`Dataset`): Dataset to load samples from.\n', '            collate_fn(`bool`): Shuffle the dataset order when loading. Recommended True for training, False for\n', '                validation/evaluation.\n', '            collate_fn(`Callable`): A function or callable object (such as a `DataCollator`) that will collate\n', '                lists of samples into a batch.\n', '            collate_fn_args (`Dict`): A `dict` of keyword arguments to be passed to the\n', '                `collate_fn`.\n', '            batch_size (`int`, optional): The size of batches loaded from the dataset. Used for shape inference.\n', '                Can be None, which indicates that batch sizes can be variable.\n', '            num_test_batches (`int`): The number of batches to load from the dataset for shape inference.\n', '\n', '        Returns:\n', '            `dict`: Dict mapping column names to tf.Tensorspec objects\n', '            `dict`: Dict mapping column names to np.dtype objects\n', '        """"""\n']","['ImportError', 'len', 'ValueError', 'min', 'list', 'range', 'sample', 'test_batch.items', 'collate_fn', 'test_batches.append', 'isinstance', 'np_arrays.append', 'np.issubdtype', 'RuntimeError', 'static_shape.append', 'tf.TensorSpec']",16
repos/datasets/src/datasets/arrow_dataset.py:TensorflowDatasetMixin:to_tf_dataset,TensorflowDatasetMixin:to_tf_dataset,method,59,493,255,4159,8.44,6,26,"['self', 'batch_size', 'columns', 'List[str]]] ', 'shuffle', 'collate_fn', 'drop_remainder', 'collate_fn_args', 'Any]] ', 'label_cols', 'List[str]]] ', 'prefetch', 'num_workers', 'num_test_batches', '']","[None, ' Optional[int] ', ' Optional[Union[str', None, ' bool ', ' Optional[Callable] ', ' bool ', ' Optional[Dict[str', None, ' Optional[Union[str', None, ' bool ', ' int ', ' int ', None]","[None, ' None', None, ' None', ' False', ' None', ' False', None, ' None', None, ' None', ' True', ' 0', ' 20', None]",336,"['        """"""Create a `tf.data.Dataset` from the underlying Dataset. This `tf.data.Dataset` will load and collate batches from\n', '        the Dataset, and is suitable for passing to methods like `model.fit()` or `model.predict()`. The dataset will yield\n', '        `dicts` for both inputs and labels unless the `dict` would contain only a single key, in which case a raw\n', '        `tf.Tensor` is yielded instead.\n', '\n', '        Args:\n', '            batch_size (`int`, *optional*):\n', ""                Size of batches to load from the dataset. Defaults to `None`, which implies that the dataset won't be\n"", '                batched, but the returned dataset can be batched later with `tf_dataset.batch(batch_size)`.\n', '            columns (`List[str]` or `str`, *optional*):\n', '                Dataset column(s) to load in the `tf.data.Dataset`.\n', '                Column names that are created by the `collate_fn` and that do not exist in the original dataset can be used.\n', '            shuffle(`bool`, defaults to `False`):\n', '                Shuffle the dataset order when loading. Recommended `True` for training, `False` for\n', '                validation/evaluation.\n', '            drop_remainder(`bool`, defaults to `False`):\n', '                Drop the last incomplete batch when loading. Ensures\n', '                that all batches yielded by the dataset will have the same length on the batch dimension.\n', '            collate_fn(`Callable`, *optional*):\n', '                A function or callable object (such as a `DataCollator`) that will collate\n', '                lists of samples into a batch.\n', '            collate_fn_args (`Dict`, *optional*):\n', '                An optional `dict` of keyword arguments to be passed to the\n', '                `collate_fn`.\n', '            label_cols (`List[str]` or `str`, defaults to `None`):\n', '                Dataset column(s) to load as labels.\n', '                Note that many models compute loss internally rather than letting Keras do it, in which case\n', ""                passing the labels here is optional, as long as they're in the input `columns`.\n"", '            prefetch (`bool`, defaults to `True`):\n', '                Whether to run the dataloader in a separate thread and maintain\n', '                a small buffer of batches for training. Improves performance by allowing data to be loaded in the\n', '                background while the model is training.\n', '            num_workers (`int`, defaults to `0`):\n', '                Number of workers to use for loading the dataset. Only supported on Python versions >= 3.8.\n', '            num_test_batches (`int`, defaults to `20`):\n', '                Number of batches to use to infer the output signature of the dataset.\n', '                The higher this number, the more accurate the signature will be, but the longer it will take to\n', '                create the dataset.\n', '\n', '        Returns:\n', '            `tf.data.Dataset`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> ds_train = ds[""train""].to_tf_dataset(\n', ""        ...    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n"", '        ...    shuffle=True,\n', '        ...    batch_size=16,\n', '        ...    collate_fn=data_collator,\n', '        ... )\n', '        ```\n', '        """"""\n']","['ImportError', 'len', 'isinstance', 'warnings.warn', 'logger.warning', 'to_tf_dataset', 'ValueError', 'list', 'self.with_format', 'dataset._get_output_signature', 'dataset_to_tf', 'NotImplementedError', 'multiprocess_dataset_to_tf', 'split_features_and_labels', 'input_batch.items', 'tf_dataset.map', 'tf_dataset.prefetch', 'cleanup_callback', 'dataset.__del__']",19
repos/datasets/src/datasets/arrow_reader.py:_pct_to_abs_closest,_pct_to_abs_closest,function,1,4,4,45,11.25,0,0,"['boundary', 'num_examples']","[None, None]","[None, None]",466,[],['int'],1
repos/datasets/src/datasets/arrow_reader.py:_pct_to_abs_pct1,_pct_to_abs_pct1,function,6,34,34,208,6.12,0,1,"['boundary', 'num_examples']","[None, None]","[None, None]",455,[],"['ValueError', 'math.trunc']",2
repos/datasets/src/datasets/arrow_reader.py:_rel_to_abs_instr,_rel_to_abs_instr,function,18,93,48,646,6.95,0,9,"['rel_instr', 'name2len']","[None, None]","[None, None]",470,"['    """"""Returns _AbsoluteInstruction instance for given RelativeInstruction.\n', '\n', '    Args:\n', '        rel_instr: RelativeInstruction instance.\n', '        name2len: dict {split_name: num_examples}.\n', '    """"""\n']","['ValueError', 'pct_to_abs', 'max', 'min', '_AbsoluteInstruction']",5
repos/datasets/src/datasets/arrow_reader.py:_str_to_read_instruction,_str_to_read_instruction,function,11,34,28,363,10.68,0,2,['spec'],[None],[None],440,"['    """"""Returns ReadInstruction for given string.""""""\n']","['_SUB_SPEC_RE.match', 'ValueError', 'res.group', 'ReadInstruction', 'from_=int', 'to=int']",6
repos/datasets/src/datasets/arrow_reader.py:make_file_instructions,make_file_instructions,function,50,201,112,1742,8.67,6,11,"['name', 'split_infos', 'instruction', '""ReadInstruction""]', 'filetype_suffix', 'prefix_path', '']","[' str', ' List[""SplitInfo""]', ' Union[str', None, ' Optional[str] ', ' Optional[str] ', None]","[None, None, None, None, ' None', ' None', None]",96,"['    """"""Returns instructions of the split dict.\n', '\n', '    Args:\n', '        name (`str`): Name of the dataset.\n', '        split_infos (`list` of `[SplitInfo]`): Dataset splits information.\n', '        instruction ([`ReadInstruction`] or `str`): Reading instruction for a dataset.\n', ""        filetype_suffix (`str`, *optional*): Suffix of dataset files, e.g. 'arrow' or 'parquet'.\n"", '        prefix_path (`str`, *optional*): Prefix of dataset files, e.g. directory name.\n', '\n', '    Returns:\n', '        [`FileInstructions`]\n', '    """"""\n']","['isinstance', 'TypeError', 'ValueError', 'filenames_for_dataset_split', 'ReadInstruction.from_spec', 'instruction.to_absolute', 'file_instructions.append', 'zip', 'FileInstructions']",9
repos/datasets/src/datasets/arrow_reader.py:ArrowReader,ArrowReader,class,19,81,55,728,8.99,0,3,[],[],[],328,[],[],0
repos/datasets/src/datasets/arrow_reader.py:BaseReader,BaseReader,class,65,286,200,3251,11.37,4,6,[],[],[],171,[],[],0
repos/datasets/src/datasets/arrow_reader.py:FileInstructions,FileInstructions,class,3,4,4,45,11.25,0,0,[],[],[],82,[],[],0
repos/datasets/src/datasets/arrow_reader.py:MissingFilesOnHfGcsError,MissingFilesOnHfGcsError,class,0,1,1,4,4.0,0,0,[],[],[],75,[],[],0
repos/datasets/src/datasets/arrow_reader.py:ParquetReader,ParquetReader,class,13,59,42,531,9.0,0,1,[],[],[],375,[],[],0
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction,ReadInstruction,class,51,220,137,2109,9.59,1,5,[],[],[],499,[],[],0
repos/datasets/src/datasets/arrow_reader.py:_AbsoluteInstruction,_AbsoluteInstruction,class,5,14,11,70,5.0,0,0,[],[],[],407,[],[],0
repos/datasets/src/datasets/arrow_reader.py:_RelativeInstruction,_RelativeInstruction,class,17,151,68,907,6.01,0,5,[],[],[],416,[],[],0
repos/datasets/src/datasets/arrow_reader.py:ArrowReader:__init__,ArrowReader:__init__,method,2,4,4,57,14.25,0,0,"['self', 'path', 'info']","[None, ' str', ' Optional[""DatasetInfo""]']","[None, None, None]",176,"['        """"""Initializes ArrowReader.\n', '\n', '        Args:\n', '            path (str): path where tfrecords are stored.\n', '            info (DatasetInfo): info about the dataset.\n', '        """"""\n']",['super'],1
repos/datasets/src/datasets/arrow_reader.py:ArrowReader:_get_table_from_filename,ArrowReader:_get_table_from_filename,method,9,51,32,382,7.49,0,2,"['self', 'filename_skip_take', 'in_memory']","[None, None, None]","[None, None, 'False']",187,"['        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n']","['ArrowReader.read_table', 'len', 'table.slice']",3
repos/datasets/src/datasets/arrow_reader.py:ArrowReader:read_table,ArrowReader:read_table,method,6,8,8,91,11.38,0,1,"['filename', 'in_memory']","[None, None]","[None, 'False']",360,"['        """"""\n', '        Read table from file.\n', '\n', '        Args:\n', '            filename (str): File name of the table.\n', '            in_memory (bool, default=False): Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            pyarrow.Table\n', '        """"""\n']",['table_cls.from_file'],1
repos/datasets/src/datasets/arrow_reader.py:BaseReader:__init__,BaseReader:__init__,method,7,9,9,100,11.11,0,0,"['self', 'path', 'info']","[None, ' str', ' Optional[""DatasetInfo""]']","[None, None, None]",176,"['        """"""Initializes ArrowReader.\n', '\n', '        Args:\n', '            path (str): path where tfrecords are stored.\n', '            info (DatasetInfo): info about the dataset.\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_reader.py:BaseReader:_get_table_from_filename,BaseReader:_get_table_from_filename,method,1,2,2,24,12.0,0,0,"['self', 'filename_skip_take', 'in_memory']","[None, None, None]","[None, None, 'False']",187,"['        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n']",[],0
repos/datasets/src/datasets/arrow_reader.py:BaseReader:_read_files,BaseReader:_read_files,method,18,98,74,772,7.88,2,3,"['self', 'files', 'in_memory']","[None, None, None]","[None, None, 'False']",191,"['        """"""Returns Dataset for given file instructions.\n', '\n', '        Args:\n', '            files: List[dict(filename, skip, take)], the files information.\n', '                The filenames contain the absolute path, not relative.\n', '                skip/take indicates which example read in the file: `ds.slice(skip, take)`\n', '            in_memory (bool, default False): Whether to copy the data in-memory.\n', '        """"""\n']","['len', 'all', 'ValueError', 'copy.deepcopy', 'thread_map', 'partial', 'disable=len', 'concat_tables']",8
repos/datasets/src/datasets/arrow_reader.py:BaseReader:download_from_hf_gcs,BaseReader:download_from_hf_gcs,method,23,68,53,1124,16.53,2,1,"['self', 'download_config', 'relative_data_dir']","[None, ' DownloadConfig', None]","[None, None, None]",289,"['        """"""\n', '        Download the dataset files from the Hf GCS\n', '\n', '        Args:\n', '            dl_cache_dir: `str`, the local cache directory used to download files\n', '            relative_data_dir: `str`, the relative directory of the remote files from\n', '                the `datasets` directory on GCS.\n', '\n', '        """"""\n']","['relative_data_dir.replace', 'cached_path', 'remote_dataset_info.replace', 'shutil.move', 'DatasetNotOnHfGcsError', 'self.get_file_instructions', 'str', 'remote_prepared_filename.replace', 'MissingFilesOnHfGcsError']",9
repos/datasets/src/datasets/arrow_reader.py:BaseReader:get_file_instructions,BaseReader:get_file_instructions,method,5,12,11,187,15.58,0,0,"['self', 'name', 'instruction', 'split_infos']","[None, None, None, None]","[None, None, None, None]",223,"['        """"""Return list of dict {\'filename\': str, \'skip\': int, \'take\': int}""""""\n']",['make_file_instructions'],1
repos/datasets/src/datasets/arrow_reader.py:BaseReader:read,BaseReader:read,method,7,20,20,241,12.05,0,1,"['self', 'name', 'instructions', 'split_infos', 'in_memory', '']","[None, None, None, None, None, None]","[None, None, None, None, 'False', None]",231,"['        """"""Returns Dataset instance(s).\n', '\n', '        Args:\n', '            name (str): name of the dataset.\n', '            instructions (ReadInstruction): instructions to read.\n', '                Instruction can be string and will then be passed to the Instruction\n', '                constructor as it.\n', '            split_infos (list of SplitInfo proto): the available splits for dataset.\n', '            in_memory (bool, default False): Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '             kwargs to build a single Dataset instance.\n', '        """"""\n']","['self.get_file_instructions', 'ValueError', 'self.read_files']",3
repos/datasets/src/datasets/arrow_reader.py:BaseReader:read_files,BaseReader:read_files,method,10,28,26,264,9.43,0,1,"['self', 'files', 'original_instructions', '""ReadInstruction""', '""Split""] ', 'in_memory', '']","[None, ' List[dict]', ' Union[None', None, None, None, None]","[None, None, None, None, ' None', 'False', None]",258,"['        """"""Returns single Dataset instance for the set of file instructions.\n', '\n', '        Args:\n', '            files: List[dict(filename, skip, take)], the files information.\n', '                The filenames contains the relative path, not absolute.\n', '                skip/take indicates which example read in the file: `ds.skip().take()`\n', '            original_instructions: store the original instructions used to build the dataset split in the dataset.\n', '            in_memory (bool, default False): Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            kwargs to build a Dataset instance.\n', '        """"""\n']","['self._read_files', 'Split']",2
repos/datasets/src/datasets/arrow_reader.py:ParquetReader:__init__,ParquetReader:__init__,method,2,4,4,59,14.75,0,0,"['self', 'path', 'info']","[None, ' str', ' Optional[""DatasetInfo""]']","[None, None, None]",176,"['        """"""Initializes ArrowReader.\n', '\n', '        Args:\n', '            path (str): path where tfrecords are stored.\n', '            info (DatasetInfo): info about the dataset.\n', '        """"""\n']",['super'],1
repos/datasets/src/datasets/arrow_reader.py:ParquetReader:_get_table_from_filename,ParquetReader:_get_table_from_filename,method,9,45,29,351,7.8,0,1,"['self', 'filename_skip_take', '**kwargs']","[None, None, None]","[None, None, None]",391,"['        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n']","['pq.read_table', 'len', 'pa_table.slice']",3
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction:__add__,ReadInstruction:__add__,method,12,49,43,522,10.65,0,2,"['self', 'other']","[None, None]","[None, None]",630,"['        """"""Returns a new ReadInstruction obj, result of appending other to self.""""""\n']","['isinstance', 'TypeError', 'ValueError', 'self._read_instruction_from_relative_instructions']",4
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction:__init__,ReadInstruction:__init__,method,1,5,5,69,13.8,0,0,"['self', 'split_name', 'rounding', 'from_', 'to', 'unit']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",550,"['        """"""Initialize ReadInstruction.\n', '\n', '        Args:\n', ""            split_name (str): name of the split to read. Eg: 'train'.\n"", '            rounding (str, optional): The rounding behaviour to use when percent slicing is\n', '                used. Ignored when slicing with absolute indices.\n', '                Possible values:\n', ""                 - 'closest' (default): The specified percentages are rounded to the\n"", '                     closest value. Use this if you want specified percents to be as\n', '                     much exact as possible.\n', ""                 - 'pct1_dropremainder': the specified percentages are treated as\n"", '                     multiple of 1%. Use this option if you want consistency. Eg:\n', '                         len(5%) == 5 * len(1%).\n', '                     Using this option, one might not be able to use the full set of\n', '                     examples, if the number of those is not a multiple of 100.\n', '            from_ (int):\n', '            to (int): alternative way of specifying slicing boundaries. If any of\n', '                {from_, to, unit} argument is used, slicing cannot be specified as\n', '                string.\n', '            unit (str): optional, one of:\n', ""                '%': to set the slicing unit as percents of the split size.\n"", ""                'abs': to set the slicing unit as absolute numbers.\n"", '        """"""\n']",['self._init'],1
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction:__repr__,ReadInstruction:__repr__,method,2,2,2,55,27.5,0,0,['self'],[None],[None],648,[],"['f""ReadInstruction']",1
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction:__str__,ReadInstruction:__str__,method,2,2,2,20,10.0,0,0,['self'],[None],[None],645,[],['self.to_spec'],1
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction:_init,ReadInstruction:_init,method,2,2,2,49,24.5,0,0,"['self', 'relative_instructions']","[None, None]","[None, None]",538,[],[],0
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction:_read_instruction_from_relative_instructions,ReadInstruction:_read_instruction_from_relative_instructions,method,4,8,7,104,13.0,0,0,"['cls', 'relative_instructions']","[None, None]","[None, None]",543,"['        """"""Returns ReadInstruction obj initialized with relative_instructions.""""""\n']","['cls.__new__', 'result._init']",2
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction:from_spec,ReadInstruction:from_spec,method,9,36,33,280,7.78,0,1,"['cls', 'spec']","[None, None]","[None, None]",580,"['        """"""Creates a `ReadInstruction` instance out of a string spec.\n', '\n', '        Args:\n', '            spec (`str`):\n', '                Split(s) + optional slice(s) to read + optional rounding\n', '                if percents are used as the slicing unit. A slice can be specified,\n', '                using absolute numbers (`int`) or percentages (`int`).\n', '\n', '        Examples:\n', '\n', '            ```\n', '            test: test split.\n', '            test + validation: test split + validation split.\n', '            test[10:]: test split, minus its first 10 records.\n', '            test[:10%]: first 10% records of test split.\n', '            test[:20%](pct1_dropremainder): first 10% records, rounded with the pct1_dropremainder rounding.\n', '            test[:-5%]+train[40%:60%]: first 95% of test + middle 20% of train.\n', '            ```\n', '\n', '        Returns:\n', '            ReadInstruction instance.\n', '        """"""\n']","['str', '_ADDITION_SEP_RE.split', 'ValueError', '_str_to_read_instruction', 'sum']",5
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction:to_absolute,ReadInstruction:to_absolute,method,1,7,7,86,12.29,0,0,"['self', 'name2len']","[None, None]","[None, None]",651,"['        """"""Translate instruction into a list of absolute instructions.\n', '\n', '        Those absolute instructions are then to be added together.\n', '\n', '        Args:\n', '            name2len (`dict`):\n', '                Associating split names to number of examples.\n', '\n', '        Returns:\n', '            list of _AbsoluteInstruction instances (corresponds to the + in spec).\n', '        """"""\n']",[],0
repos/datasets/src/datasets/arrow_reader.py:ReadInstruction:to_spec,ReadInstruction:to_spec,method,19,79,42,569,7.2,1,2,['self'],[None],[None],610,[],"['str', 'rel_instr_specs.append']",2
repos/datasets/src/datasets/arrow_reader.py:_RelativeInstruction:__post_init__,_RelativeInstruction:__post_init__,method,8,115,48,686,5.97,0,5,['self'],[None],[None],425,[],"['ValueError', 'abs']",2
repos/datasets/src/datasets/arrow_writer.py:get_parquet_lengths,get_parquet_lengths,function,7,13,12,182,14.0,1,0,['sources'],[None],[None],728,[],"['hf_tqdm', 'shard_lengths.append']",2
repos/datasets/src/datasets/arrow_writer.py:parquet_to_arrow,parquet_to_arrow,function,15,28,26,381,13.61,1,1,"['source', 'destination']","[None, None]","[None, None]",736,"['    """"""Convert parquet file to arrow file. Inputs can be str paths or file-like objects""""""\n']","['isinstance', 'ArrowWriter', 'parquet_file.iter_batches', 'writer.write_table', 'writer.finalize']",5
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter,ArrowWriter,class,178,862,412,8689,10.08,11,48,[],[],[],291,[],[],0
repos/datasets/src/datasets/arrow_writer.py:BeamWriter,BeamWriter,class,90,270,192,2829,10.48,3,6,[],[],[],622,[],[],0
repos/datasets/src/datasets/arrow_writer.py:OptimizedTypedSequence,OptimizedTypedSequence,class,9,71,62,626,8.82,0,1,[],[],[],269,[],[],0
repos/datasets/src/datasets/arrow_writer.py:ParquetWriter,ParquetWriter,class,2,2,2,30,15.0,0,0,[],[],[],618,[],[],0
repos/datasets/src/datasets/arrow_writer.py:SchemaInferenceError,SchemaInferenceError,class,0,1,1,4,4.0,0,0,[],[],[],56,[],[],0
repos/datasets/src/datasets/arrow_writer.py:TypedSequence,TypedSequence,class,63,476,205,4601,9.67,0,20,[],[],[],60,[],[],0
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:__enter__,ArrowWriter:__enter__,method,1,2,2,10,5.0,0,0,['self'],[None],[None],365,[],[],0
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:__exit__,ArrowWriter:__exit__,method,1,1,1,12,12.0,0,0,"['self', 'exc_type', 'exc_val', 'exc_tb']","[None, None, None, None]","[None, None, None, None]",368,[],['self.close'],1
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:__init__,ArrowWriter:__init__,method,53,124,84,1280,10.32,0,5,"['self', 'schema', 'features', 'path', 'stream', 'fingerprint', 'writer_batch_size', 'hash_salt', 'check_duplicates', 'disable_nullable', 'update_features', 'with_metadata', 'unit', 'embed_local_files', 'storage_options', '']","[None, ' Optional[pa.Schema] ', ' Optional[Features] ', ' Optional[str] ', ' Optional[pa.NativeFile] ', ' Optional[str] ', ' Optional[int] ', ' Optional[str] ', ' Optional[bool] ', ' bool ', ' bool ', ' bool ', ' str ', ' bool ', ' Optional[dict] ', None]","[None, ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' False', ' False', ' False', ' True', ' ""examples""', ' False', ' None', None]",296,[],"['ValueError', 'Features.from_arrow_schema', 'KeyHasher', 'url_to_fs', 'is_remote_filesystem']",5
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:__len__,ArrowWriter:__len__,method,2,4,4,74,18.5,0,0,['self'],[None],[None],361,"['        """"""Return the number of writed and staged examples""""""\n']",['len'],1
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:_build_metadata,ArrowWriter:_build_metadata,method,8,35,33,270,7.71,0,1,"['info', 'fingerprint']","[' DatasetInfo', ' Optional[str] ']","[None, ' None']",418,[],"['asdict', 'json.dumps']",2
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:_build_writer,ArrowWriter:_build_writer,method,35,77,54,890,11.56,2,6,"['self', 'inferred_schema']","[None, ' pa.Schema']","[None, None]",381,[],"['Features.from_arrow_schema', 'pa.schema', 'schema.with_metadata', 'self._WRITER_CLASS']",4
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:check_duplicate_keys,ArrowWriter:check_duplicate_keys,method,8,30,26,282,9.4,1,1,['self'],[None],[None],502,"['        """"""Raises error if duplicates found in a batch""""""\n']","['set', 'str', 'enumerate', 'DuplicatedKeysError', 'tmp_record.add']",5
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:close,ArrowWriter:close,method,11,30,25,224,7.47,0,2,['self'],[None],[None],371,[],[],0
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:finalize,ArrowWriter:finalize,method,17,58,47,589,10.16,0,5,"['self', 'close_stream']","[None, None]","[None, 'True']",592,[],"['self.write_rows_on_file', 'self.check_duplicate_keys', 'self.write_examples_on_file', 'self._build_writer', 'SchemaInferenceError', 'logger.debug']",6
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:schema,ArrowWriter:schema,method,5,42,26,284,6.76,0,2,['self'],[None],[None],407,[],['pa.schema'],1
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:write,ArrowWriter:write,method,11,31,23,433,13.97,0,4,"['self', 'example', 'Any]', 'key', 'int', 'bytes]] ', 'writer_batch_size', '']","[None, ' Dict[str', None, ' Optional[Union[str', None, None, ' Optional[int] ', None]","[None, None, None, None, None, ' None', ' None', None]",469,"['        """"""Add a given (Example,Key) pair to the write-pool of examples which is written to file.\n', '\n', '        Args:\n', '            example: the Example to add.\n', '            key: Optional, a unique identifier(str, int or bytes) associated with each example\n', '        """"""\n']","['len', 'self.check_duplicate_keys', 'self.write_examples_on_file']",3
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:write_batch,ArrowWriter:write_batch,method,42,134,78,1321,9.86,3,10,"['self', 'batch_examples', 'List]', 'writer_batch_size', '']","[None, ' Dict[str', None, ' Optional[int] ', None]","[None, None, None, ' None', None]",531,"['        """"""Write a batch of Example to file.\n', '        Ignores the batch if it appears to be empty,\n', '        preventing a potential schema update of unknown types.\n', '\n', '        Args:\n', '            batch_examples: the batch of examples to add.\n', '        """"""\n']","['len', 'Features', 'set', 'batch_examples.keys', 'list', 'isinstance', 'cast_array_to_feature', 'arrays.append', 'generate_from_arrow_type', 'OptimizedTypedSequence', 'typed_sequence.get_inferred_type', 'self.write_table']",12
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:write_examples_on_file,ArrowWriter:write_examples_on_file,method,22,101,60,911,9.02,5,5,['self'],[None],[None],427,"['        """"""Write stored examples from the write-pool of examples. It makes a table out of the examples and write it.""""""\n']","['set', 'list', 'all', 'isinstance', 'pa.concat_arrays', 'self.write_batch']",6
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:write_row,ArrowWriter:write_row,method,6,32,27,300,9.38,0,3,"['self', 'row', 'writer_batch_size']","[None, ' pa.Table', ' Optional[int] ']","[None, None, ' None']",517,"['        """"""Add a given single-row Table to the write-pool of rows which is written to file.\n', '\n', '        Args:\n', '            row: the row to add.\n', '        """"""\n']","['len', 'ValueError', 'self.write_rows_on_file']",3
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:write_rows_on_file,ArrowWriter:write_rows_on_file,method,5,9,9,117,13.0,0,1,['self'],[None],[None],461,"['        """"""Write stored rows from the write-pool of rows. It concatenates the single-row tables and it writes the resulting table.""""""\n']","['pa.concat_tables', 'self.write_table']",2
repos/datasets/src/datasets/arrow_writer.py:ArrowWriter:write_table,ArrowWriter:write_table,method,14,28,20,412,14.71,0,3,"['self', 'pa_table', 'writer_batch_size']","[None, ' pa.Table', ' Optional[int] ']","[None, None, ' None']",574,"['        """"""Write a Table to file.\n', '\n', '        Args:\n', '            example: the Table to add.\n', '        """"""\n']","['self._build_writer', 'pa_table.combine_chunks', 'table_cast', 'embed_table_storage']",4
repos/datasets/src/datasets/arrow_writer.py:BeamWriter:__init__,BeamWriter:__init__,method,21,63,43,533,8.46,0,3,"['self', 'features', 'schema', 'path', 'namespace', 'cache_dir', '']","[None, ' Optional[Features] ', ' Optional[pa.Schema] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', None]","[None, ' None', ' None', ' None', ' None', ' None', None]",100,"['        """"""Return the inferred feature type.\n', '        This is done by converting the sequence to an Arrow array, and getting the corresponding\n', '        feature type.\n', '\n', '        Since building the Arrow array can be expensive, the value of the inferred type is cached\n', '        as soon as pa.array is called on the typed sequence.\n', '\n', '        Returns:\n', '            FeatureType: inferred feature type of the sequence.\n', '        """"""\n']","['ValueError', 'Features.from_arrow_schema']",2
repos/datasets/src/datasets/arrow_writer.py:BeamWriter:finalize,BeamWriter:finalize,method,57,146,112,1658,11.36,3,3,"['self', 'metrics_query_result']","[None, ' dict']","[None, None]",674,"['        """"""\n', '        Run after the pipeline has finished.\n', '        It converts the resulting parquet files to arrow and it completes the info from the pipeline metrics.\n', '\n', '        Args:\n', '            metrics_query_result: `dict` obtained from pipeline_results.metrics().query(m_filter). Make sure\n', '                that the filter keeps only the metrics for the considered split, under the namespace `split_name`.\n', '        """"""\n']","['url_to_fs', 'str', 'is_remote_filesystem', 'fs.unstrip_protocol', 'fs.glob', 'sum', 'get_parquet_lengths', 'logger.info', 'hf_tqdm', 'fs.open', 'parquet_to_arrow', 'logger.warning', 'os.makedirs', 'hash_url_to_filename', 'fs.download', 'local_parquet_path.replace', 'shard.replace', 'fs.upload']",18
repos/datasets/src/datasets/arrow_writer.py:BeamWriter:write_from_pcollection,BeamWriter:write_from_pcollection,method,12,35,31,369,10.54,0,0,"['self', 'pcoll_examples']","[None, None]","[None, None]",654,"['        """"""Add the final steps of the beam pipeline: write to parquet files.""""""\n']","['inc_num_examples', 'beam.Map', 'beam.Values']",3
repos/datasets/src/datasets/arrow_writer.py:OptimizedTypedSequence:__init__,OptimizedTypedSequence:__init__,method,8,54,50,457,8.46,0,1,"['self', 'data', 'type', 'try_type', 'col', 'optimized_int_type', '']","[None, None, ' Optional[FeatureType] ', ' Optional[FeatureType] ', ' Optional[str] ', ' Optional[FeatureType] ', None]","[None, None, ' None', ' None', ' None', ' None', None]",100,"['        """"""Return the inferred feature type.\n', '        This is done by converting the sequence to an Arrow array, and getting the corresponding\n', '        feature type.\n', '\n', '        Since building the Arrow array can be expensive, the value of the inferred type is cached\n', '        as soon as pa.array is called on the typed sequence.\n', '\n', '        Returns:\n', '            FeatureType: inferred feature type of the sequence.\n', '        """"""\n']","['Value', 'optimized_int_type_by_col.get', 'super']",3
repos/datasets/src/datasets/arrow_writer.py:TypedSequence:__arrow_array__,TypedSequence:__arrow_array__,method,46,351,153,3490,9.94,0,15,"['self', 'type']","[None, ' Optional[pa.DataType] ']","[None, ' None']",161,"['        """"""This function is called when calling pa.array(typed_sequence)""""""\n']","['ValueError', 'pa.array', 'self._infer_custom_type_and_encode', 'get_nested_type', 'isinstance', 'to_pyarrow_listarray', 'numpy_to_pyarrow_listarray', 'list_of_np_array_to_pyarrow_listarray', 'out.cast', 'array_cast', 'pa.list_', 'cast_array_to_feature', 'any', 'str', 'OverflowError', 'np.dtype', 'logger.info', 'cast_to_python_objects']",18
repos/datasets/src/datasets/arrow_writer.py:TypedSequence:__init__,TypedSequence:__init__,method,16,55,32,372,6.76,0,2,"['self', 'data', 'type', 'try_type', 'optimized_int_type', '']","[None, ' Iterable', ' Optional[FeatureType] ', ' Optional[FeatureType] ', ' Optional[FeatureType] ', None]","[None, None, ' None', ' None', ' None', None]",100,[],['ValueError'],1
repos/datasets/src/datasets/arrow_writer.py:TypedSequence:_infer_custom_type_and_encode,TypedSequence:_infer_custom_type_and_encode,method,10,31,24,261,8.42,0,2,['data'],[' Iterable'],[None],137,"['        """"""Implement type inference for custom objects like PIL.Image.Image -> Image type.\n', '\n', ""        This function is only used for custom python objects that can't be direclty passed to build\n"", '        an Arrow array. In such cases is infers the feature type to use, and it encodes the data so\n', '        that they can be passed to an Arrow array.\n', '\n', '        Args:\n', '            data (Iterable): array of data to infer the type, e.g. a list of PIL images.\n', '\n', '        Returns:\n', '            Tuple[Iterable, Optional[FeatureType]]: a tuple with:\n', '                - the (possibly encoded) array, if the inferred feature type requires encoding\n', '                - the inferred feature type if the array is made of supported custom objects like\n', '                    PIL images, else None.\n', '        """"""\n']","['first_non_null_value', 'isinstance', 'Image']",3
repos/datasets/src/datasets/arrow_writer.py:TypedSequence:get_inferred_type,TypedSequence:get_inferred_type,method,3,8,6,120,15.0,0,1,['self'],[None],[None],121,"['        """"""Return the inferred feature type.\n', '        This is done by converting the sequence to an Arrow array, and getting the corresponding\n', '        feature type.\n', '\n', '        Since building the Arrow array can be expensive, the value of the inferred type is cached\n', '        as soon as pa.array is called on the typed sequence.\n', '\n', '        Returns:\n', '            FeatureType: inferred feature type of the sequence.\n', '        """"""\n']",['generate_from_arrow_type'],1
repos/datasets/src/datasets/builder.py:ArrowBasedBuilder,ArrowBasedBuilder,class,106,544,318,6115,11.24,3,12,[],[],[],1803,[],[],0
repos/datasets/src/datasets/builder.py:BeamBasedBuilder,BeamBasedBuilder,class,137,560,374,6986,12.47,5,10,[],[],[],2053,[],[],0
repos/datasets/src/datasets/builder.py:BuilderConfig,BuilderConfig,class,45,228,157,2470,10.83,2,13,[],[],[],103,[],[],0
repos/datasets/src/datasets/builder.py:DatasetBuilder,DatasetBuilder,class,380,2429,1144,29199,12.02,17,102,[],[],[],216,[],[],0
repos/datasets/src/datasets/builder.py:GeneratorBasedBuilder,GeneratorBasedBuilder,class,115,555,324,6453,11.63,3,12,[],[],[],1538,[],[],0
repos/datasets/src/datasets/builder.py:InvalidConfigName,InvalidConfigName,class,0,1,1,4,4.0,0,0,[],[],[],98,[],[],0
repos/datasets/src/datasets/builder.py:MissingBeamOptions,MissingBeamOptions,class,0,1,1,4,4.0,0,0,[],[],[],2048,[],[],0
repos/datasets/src/datasets/builder.py:ArrowBasedBuilder:_generate_tables,ArrowBasedBuilder:_generate_tables,method,1,2,2,26,13.0,0,0,"['self', '**kwargs']","[None, None]","[None, None]",1807,"['        """"""Default function generating examples for each `SplitGenerator`.\n', '\n', '        This function preprocess the examples from the raw data to the preprocessed\n', '        dataset files.\n', '        This function is called once for each `SplitGenerator` defined in\n', '        `_split_generators`. The examples yielded here will be written on\n', '        disk.\n', '\n', '        Args:\n', '            **kwargs (additional keyword arguments):\n', '                Arguments forwarded from the SplitGenerator.gen_kwargs\n', '\n', '        Yields:\n', '            key: `str` or `int`, a unique deterministic example identification key.\n', '                * Unique: An error will be raised if two examples are yield with the\n', '                    same key.\n', '                * Deterministic: When generating the dataset twice, the same example\n', '                    should have the same key.\n', '                Good keys can be the image id, or line number if examples are extracted\n', '                from a text file.\n', '                The key will be hashed and sorted to shuffle examples deterministically,\n', '                such as generating the dataset multiple times keep examples in the\n', '                same order.\n', '            example: `pyarrow.Table`, a feature table\n', '                ready to be encoded and written to disk.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/builder.py:ArrowBasedBuilder:_get_examples_iterable_for_split,ArrowBasedBuilder:_get_examples_iterable_for_split,method,2,3,3,84,28.0,0,0,"['self', 'split_generator']","[None, ' SplitGenerator']","[None, None]",1528,"['        """"""Generate the examples on the fly.\n', '\n', '        Args:\n', '            split_generator (`SplitGenerator`):\n', '                Split generator to process\n', '        """"""\n']",['ArrowExamplesIterable'],1
repos/datasets/src/datasets/builder.py:ArrowBasedBuilder:_prepare_split,ArrowBasedBuilder:_prepare_split,method,62,323,200,3479,10.77,2,7,"['self', 'split_generator', 'file_format', 'num_proc', 'max_shard_size', 'int]] ', '']","[None, ' SplitGenerator', ' str ', ' Optional[int] ', ' Optional[Union[str', None, None]","[None, None, ' ""arrow""', ' None', None, ' None', None]",1498,"['        """"""Generate the examples and record them on disk.\n', '\n', '        Args:\n', '            split_generator (`SplitGenerator`):\n', '                Split generator to process\n', '            file_format (`str`, *optional*):\n', '                format of the data files in which the dataset will be written.\n', '                Supported formats: ""arrow"", ""parquet"". Default to ""arrow"" format.\n', '            max_shard_size (`Union[str, int]`, *optional*):\n', '                Maximum number of bytes written per shard, default is ""500MB"".\n', '                The size is based on uncompressed data size, so in practice your shard files may be smaller than\n', '                `max_shard_size` thanks to Parquet compression for example.\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.7.0""/>\n', '            **kwargs: Additional kwargs forwarded from _download_and_prepare (ex:\n', '                beam pipeline)\n', '        """"""\n']","['convert_file_size_to_int', 'posixpath.join', '_number_of_shards_in_gen_kwargs', 'logger.warning', 'hf_tqdm', 'self._prepare_split_single', 'pbar.update', 'enumerate', '_split_gen_kwargs', 'len', 'Pool', 'iflatmap_unordered', 'sum', 'logger.debug', '_rename_shard', 'self._rename', 'fpath.replace', 'range', 'thread_map']",19
repos/datasets/src/datasets/builder.py:ArrowBasedBuilder:_prepare_split_single,ArrowBasedBuilder:_prepare_split_single,method,43,171,109,2065,12.08,1,5,"['self', 'gen_kwargs', 'fpath', 'file_format', 'max_shard_size', 'job_id']","[None, ' dict', ' str', ' str', ' int', ' int']","[None, None, None, None, None, None]",1720,[],"['tracked_list', 'isinstance', 'gen_kwargs.items', 'self._generate_tables', 'writer_class', 'time.time', 'writer.finalize', 'writer.close', 'shard_lengths.append', 'writer.write_table', 'DatasetGenerationCastError.from_cast_error', 'len', 'DatasetGenerationError']",13
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:__init__,BeamBasedBuilder:__init__,method,8,12,12,144,12.0,0,0,"['self', '*args', 'beam_runner', 'beam_options', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",2056,[],['super'],1
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:_build_pcollection,BeamBasedBuilder:_build_pcollection,method,0,1,1,1,1.0,0,0,"['self', 'pipeline', '**kwargs']","[None, None, None]","[None, None, None]",2073,"['        """"""Build the beam pipeline examples for each `SplitGenerator`.\n', '\n', '        This function extracts examples from the raw data with parallel transforms\n', '        in a Beam pipeline. It is called once for each `SplitGenerator` defined in\n', '        `_split_generators`. The examples from the PCollection will be\n', '        encoded and written to disk.\n', '\n', '        <Tip warning={true}>\n', '        Warning: When running in a distributed setup, make sure that the data\n', '        which will be read (download_dir, manual_dir,...) and written (cache_dir)\n', '        can be accessed by the workers jobs. The data should be located in a\n', '        shared filesystem, like GCS.\n', '        </Tip>\n', '\n', '        Args:\n', '            pipeline ([`utils.beam_utils.BeamPipeline`]):\n', '                Apache Beam pipeline.\n', '            **kwargs (additional keyword arguments):\n', '                Arguments forwarded from the SplitGenerator.gen_kwargs.\n', '\n', '        Returns:\n', '            `beam.PCollection`: Apache Beam PCollection containing the\n', '                example to send to `self.info.features.encode_example(...)`.\n', '\n', '        Example:\n', '\n', '        ```\n', '        def _build_pcollection(pipeline, extracted_dir=None):\n', '            return (\n', '                    pipeline\n', '                    | beam.Create(gfile.io.listdir(extracted_dir))\n', '                    | beam.Map(_process_file)\n', '            )\n', '        ```\n', '        """"""\n']",[],0
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:_download_and_prepare,BeamBasedBuilder:_download_and_prepare,method,63,238,183,2622,11.02,1,4,"['self', 'dl_manager', 'verification_mode', '**prepare_splits_kwargs']","[None, None, None, None]","[None, None, None, None]",2111,[],"['f""load_dataset', 'MissingBeamOptions', 'PipelineOptions', 'logger.warning', 'prepare_splits_kwargs.pop', 'NotImplementedError', 'beam_utils.BeamPipeline', 'super', 'pipeline.run', 'pipeline_results.wait_until_finish', 'pipeline_results.metrics', 'beam_writer.finalize', 'hasattr', 'len', 'prepare_splits_kwargs.get', 'posixpath.join', 'self._rename']",17
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:_generate_examples_from_hf_gcs,BeamBasedBuilder:_generate_examples_from_hf_gcs,method,14,54,42,712,13.19,3,1,"['self', 'split']","[None, ' SplitInfo']","[None, None]",2251,[],"['len', 'range', 'DownloadConfig', 'xopen', 'record_batch.to_pylist']",5
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:_get_examples_iterable_for_split,BeamBasedBuilder:_get_examples_iterable_for_split,method,2,4,4,75,18.75,0,0,"['self', 'split']","[None, ' SplitInfo']","[None, None]",2248,[],['ExamplesIterable'],1
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:_make_split_generators_kwargs,BeamBasedBuilder:_make_split_generators_kwargs,method,5,12,11,254,21.17,0,1,"['self', 'prepare_split_kwargs']","[None, None]","[None, None]",2062,[],['inspect.signature'],1
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:_prepare_split,BeamBasedBuilder:_prepare_split,method,23,80,71,928,11.6,0,1,"['self', 'split_generator', 'pipeline', 'file_format', 'max_shard_size', 'int]] ']","[None, None, None, None, ' Optional[Union[str', None]","[None, None, None, '""arrow""', None, ' None']",2195,[],"['NotImplementedError', 'posixpath.join', 'BeamWriter', '_build_pcollection', 'self._build_pcollection', 'beam.Map', 'encode_example', 'beam_writer.write_from_pcollection']",8
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:_remote_cache_dir_from_hf_gcs,BeamBasedBuilder:_remote_cache_dir_from_hf_gcs,method,5,6,6,119,19.83,0,0,['self'],[None],[None],2293,[],"['self._relative_data_dir', 'Path']",2
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:_request_info_from_hf_gcs,BeamBasedBuilder:_request_info_from_hf_gcs,method,13,35,31,526,15.03,0,0,['self'],[None],[None],2274,[],"['DownloadConfig', 'xopen', 'json.load', 'DatasetNotOnHfGcsError']",4
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:_save_info,BeamBasedBuilder:_save_info,method,6,25,19,424,16.96,0,1,['self'],[None],[None],2183,[],"['DownloadConfig', 'xopen']",2
repos/datasets/src/datasets/builder.py:BeamBasedBuilder:as_streaming_dataset,BeamBasedBuilder:as_streaming_dataset,method,12,33,29,394,11.94,1,2,"['self', 'split', '']","[None, ' Optional[str] ', None]","[None, ' None', None]",2230,[],"['self._request_info_from_hf_gcs', 'IterableDataset', 'ValueError', 'isinstance', 'IterableDatasetDict']",5
repos/datasets/src/datasets/builder.py:BuilderConfig:__eq__,BuilderConfig:__eq__,method,2,16,14,139,8.69,0,1,"['self', 'o']","[None, None]","[None, None]",139,[],"['set', 'all', 'getattr']",3
repos/datasets/src/datasets/builder.py:BuilderConfig:__post_init__,BuilderConfig:__post_init__,method,8,54,45,451,8.35,1,2,['self'],[None],[None],128,[],"['InvalidConfigName', 'isinstance', 'ValueError']",3
repos/datasets/src/datasets/builder.py:BuilderConfig:_resolve_data_files,BuilderConfig:_resolve_data_files,method,5,13,12,188,14.46,0,1,"['self', 'base_path', 'download_config']","[None, ' str', ' DownloadConfig']","[None, None, None]",210,[],"['isinstance', 'xjoin']",2
repos/datasets/src/datasets/builder.py:BuilderConfig:create_config_id,BuilderConfig:create_config_id,method,21,104,68,1256,12.08,1,9,"['self', 'config_kwargs', 'custom_features', '']","[None, ' dict', ' Optional[Features] ', None]","[None, None, ' None', None]",146,"['        """"""\n', '        The config id is used to build the cache directory.\n', '        By default it is equal to the config name.\n', '        However the name of a config is not sufficient to have a unique identifier for the dataset being generated\n', ""        since it doesn't take into account:\n"", '        - the config kwargs that can be used to overwrite attributes\n', '        - the custom features used to write the dataset\n', '        - the data_files for json/text/csv/pandas datasets\n', '\n', '        Therefore the config id is just the config name with an optional suffix based on these.\n', '        """"""\n']","['config_kwargs.copy', 'config_kwargs_to_add_to_suffix.pop', 'sorted', 'all', 'config_kwargs_to_add_to_suffix.values', 'str', 'config_kwargs_to_add_to_suffix.items', 'len', 'Hasher.hash', 'Hasher', 'm.update', 'm.hexdigest']",12
repos/datasets/src/datasets/builder.py:DatasetBuilder:__getstate__,DatasetBuilder:__getstate__,method,2,2,2,19,9.5,0,0,['self'],[None],[None],449,[],[],0
repos/datasets/src/datasets/builder.py:DatasetBuilder:__init__,DatasetBuilder:__init__,method,87,276,190,3215,11.65,0,12,"['self', 'cache_dir', 'dataset_name', 'config_name', 'hash', 'base_path', 'info', 'features', 'token', 'str]] ', 'use_auth_token', 'repo_id', 'data_files', 'list', 'dict', 'DataFilesDict]] ', 'data_dir', 'storage_options', 'writer_batch_size', 'name', '**config_kwargs', '']","[None, ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[DatasetInfo] ', ' Optional[Features] ', ' Optional[Union[bool', None, None, ' Optional[str] ', ' Optional[Union[str', None, None, None, ' Optional[str] ', ' Optional[dict] ', ' Optional[int] ', None, None, None]","[None, ' None', ' None', ' None', ' None', ' None', ' None', ' None', None, ' None', '""deprecated""', ' None', None, None, None, ' None', ' None', ' None', ' None', '""deprecated""', None, None]",315,[],"['warnings.warn', 'camelcase_to_snakecase', 'isinstance', 'DataFilesDict.from_patterns', 'sanitize_patterns', 'download_config=DownloadConfig', 'inspect.signature', 'self._create_builder_config', 'self.get_exported_dataset_info', 'info.update', 'str', 'is_remote_url', 'posixpath.join', 'self._build_cache_dir', 'os.makedirs', 'Path', 'FileLock', 'len', 'logger.info', 'DatasetInfo.from_directory', 'logger.warning', 'os.rmdir', 'fsspec.filesystem', 'extend_dataset_builder_for_streaming']",24
repos/datasets/src/datasets/builder.py:DatasetBuilder:__setstate__,DatasetBuilder:__setstate__,method,3,3,3,58,19.33,0,0,"['self', 'd']","[None, None]","[None, None]",452,[],['extend_dataset_builder_for_streaming'],1
repos/datasets/src/datasets/builder.py:DatasetBuilder:_as_dataset,DatasetBuilder:_as_dataset,method,16,21,20,396,18.86,0,1,"['self', 'split', 'Split] ', 'in_memory']","[None, ' Union[ReadInstruction', None, ' bool ']","[None, None, ' Split.TRAIN', ' False']",1352,"['        """"""Constructs a `Dataset`.\n', '\n', '        This is the internal implementation to overwrite called when user calls\n', '        `as_dataset`. It should read the pre-processed datasets files and generate\n', '        the `Dataset` object.\n', '\n', '        Args:\n', '            split (`datasets.Split`):\n', '                which subset of the data to read.\n', '            in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            `Dataset`\n', '        """"""\n']","['self._check_legacy_cache', 'ArrowReader', 'self._get_dataset_fingerprint', 'Dataset']",4
repos/datasets/src/datasets/builder.py:DatasetBuilder:_as_streaming_dataset_single,DatasetBuilder:_as_streaming_dataset_single,method,7,16,16,247,15.44,0,1,"['self', 'splits_generator', '']","[None, None, None]","[None, None, None]",1425,[],"['self._get_examples_iterable_for_split', 'IterableDataset']",2
repos/datasets/src/datasets/builder.py:DatasetBuilder:_build_cache_dir,DatasetBuilder:_build_cache_dir,method,22,71,57,915,12.89,1,4,['self'],[None],[None],699,"['        """"""Return the data directory for the current version.""""""\n']","['posixpath.join', 'self._relative_data_dir', '_other_versions_on_disk', 'os.listdir', 'version_dirnames.append', 'version_dirnames.sort', 'is_remote_url', 'logger.warning']",8
repos/datasets/src/datasets/builder.py:DatasetBuilder:_build_single_dataset,DatasetBuilder:_build_single_dataset,method,51,168,107,2328,13.86,4,12,"['self', 'split', 'ReadInstruction', 'Split]', 'run_post_process', 'verification_mode', 'in_memory', '']","[None, ' Union[str', None, None, ' bool', ' VerificationMode', ' bool ', None]","[None, None, None, None, None, None, ' False', None]",1283,"['        """"""as_dataset for a single split.""""""\n']","['isinstance', 'str', 'Split', 'self._as_dataset', 'self._post_processing_resources', 'ValueError', 'self._post_process', 'resources_paths.items', 'get_size_checksum_dict', 'verify_checksums', 'PostProcessedInfo', 'sum', 'split_checksums_dicts.values', 'self._save_info']",14
repos/datasets/src/datasets/builder.py:DatasetBuilder:_check_legacy_cache,DatasetBuilder:_check_legacy_cache,method,19,61,46,758,12.43,0,2,['self'],[None],[None],467,"['        """"""Check for the old cache directory template {cache_dir}/{namespace}___{builder_name} from 2.13""""""\n']","['is_remote_url', '_PACKAGED_DATASETS_MODULES.get', 'posixpath.join']",3
repos/datasets/src/datasets/builder.py:DatasetBuilder:_check_legacy_cache2,DatasetBuilder:_check_legacy_cache2,method,31,107,83,1414,13.21,1,4,"['self', 'dataset_module']","[None, ' ""DatasetModule""']","[None, None]",490,"['        """"""Check for the old cache directory template {cache_dir}/{namespace}___{dataset_name}/{config_name}-xxx from 2.14 and 2.15""""""\n']","['is_remote_url', 'update_hash_with_config_parameters', 'sorted', 'Hasher', 'm.update', 'm.hexdigest', 'patch.object', 'Hasher.hash', '_PACKAGED_DATASETS_MODULES.get', 'posixpath.join']",10
repos/datasets/src/datasets/builder.py:DatasetBuilder:_check_manual_download,DatasetBuilder:_check_manual_download,method,4,41,39,406,9.9,0,1,"['self', 'dl_manager']","[None, None]","[None, None]",1050,[],"['ManualDownloadError', 'textwrap.dedent', 'datasets.load_dataset']",3
repos/datasets/src/datasets/builder.py:DatasetBuilder:_create_builder_config,DatasetBuilder:_create_builder_config,method,28,231,135,2487,10.77,1,15,"['self', 'config_name', 'custom_features', '**config_kwargs']","[None, None, None, None]","[None, 'None', 'None', None]",566,"['        """"""Create and validate BuilderConfig object as well as a unique config id for this config.\n', '        Raises ValueError if there are multiple builder configs and config_name and DEFAULT_CONFIG_NAME are None.\n', '        config_kwargs override the defaults kwargs in config\n', '        """"""\n']","['logger.info', 'len', 'f""load_dataset', 'ValueError', 'isinstance', 'hasattr', 'self.BUILDER_CONFIG_CLASS', 'copy.deepcopy', 'config_kwargs.items', 'setattr', 'builder_config._resolve_data_files', 'download_config=DownloadConfig', 'builder_config.create_config_id']",13
repos/datasets/src/datasets/builder.py:DatasetBuilder:_download_and_prepare,DatasetBuilder:_download_and_prepare,method,27,113,94,1364,12.07,1,3,"['self', 'dl_manager', 'verification_mode', '**prepare_split_kwargs']","[None, None, None, None]","[None, None, None, None]",1083,"['        """"""Downloads and prepares dataset for reading.\n', '\n', '        This is the internal implementation to overwrite called when user calls\n', '        `download_and_prepare`. It should download all required data and generate\n', '        the pre-processed datasets files.\n', '\n', '        Args:\n', '            dl_manager ([`DownloadManager`]):\n', '                `DownloadManager` used to download and cache data.\n', '            verification_mode ([`VerificationMode`]):\n', '                if `ALL_CHECKS`, perform all the verifications including checksums.\n', '                if `BASIC_CHECKS`, do not perform checksums, only perform split tests.\n', '                if `NO_CHECKS`, do not perform any verification.\n', '            prepare_split_kwargs: Additional options, such as `file_format`, `max_shard_size`\n', '        """"""\n']","['SplitDict', 'self._make_split_generators_kwargs', 'self._split_generators', 'verify_checksums', 'dl_manager.get_recorded_sizes_checksums', 'str', 'ValueError', 'logger.info', 'split_dict.add', 'self._prepare_split', 'OSError', 'DuplicatedKeysError', 'dl_manager.manage_extracted_files', 'verify_splits']",14
repos/datasets/src/datasets/builder.py:DatasetBuilder:_download_post_processing_resources,DatasetBuilder:_download_post_processing_resources,method,1,2,2,10,5.0,0,0,"['self', 'split', 'resource_name', 'dl_manager']","[None, ' str', ' str', ' DownloadManager']","[None, None, None, None]",1444,"['        """"""Download the resource using the download manager and return the downloaded path.""""""\n']",[],0
repos/datasets/src/datasets/builder.py:DatasetBuilder:_download_prepared_from_hf_gcs,DatasetBuilder:_download_prepared_from_hf_gcs,method,22,61,53,874,14.33,2,1,"['self', 'download_config']","[None, ' DownloadConfig']","[None, None]",1063,[],"['self._relative_data_dir', 'ArrowReader', 'reader.download_from_hf_gcs', 'DatasetInfo.from_directory', 'relative_data_dir.replace', 'self._post_processing_resources', 'ValueError', 'cached_path', 'shutil.move', 'logger.info']",10
repos/datasets/src/datasets/builder.py:DatasetBuilder:_get_dataset_fingerprint,DatasetBuilder:_get_dataset_fingerprint,method,6,15,14,219,14.6,0,0,"['self', 'split', 'Split]']","[None, ' Union[ReadInstruction', None]","[None, None, None]",1381,"['        """"""The dataset fingerprint is the hash of the relative directory dataset_name/config_name/version/hash, as well as the split specs.""""""\n']","['Hasher', 'hasher.update', 'hasher.hexdigest']",3
repos/datasets/src/datasets/builder.py:DatasetBuilder:_get_examples_iterable_for_split,DatasetBuilder:_get_examples_iterable_for_split,method,1,2,2,26,13.0,0,0,"['self', 'split_generator']","[None, ' SplitGenerator']","[None, None]",1528,"['        """"""Generate the examples on the fly.\n', '\n', '        Args:\n', '            split_generator (`SplitGenerator`):\n', '                Split generator to process\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/builder.py:DatasetBuilder:_info,DatasetBuilder:_info,method,1,2,2,24,12.0,0,0,['self'],[None],[None],734,"['        """"""Construct the DatasetInfo object. See `DatasetInfo` for details.\n', '\n', '        Warning: This function is only called once and the result is cached for all\n', '        following .info() calls.\n', '\n', '        Returns:\n', '            info: (DatasetInfo) The dataset information\n', '        """"""\n']",[],0
repos/datasets/src/datasets/builder.py:DatasetBuilder:_load_info,DatasetBuilder:_load_info,method,2,3,3,91,30.33,0,0,['self'],[None],[None],1164,[],['DatasetInfo.from_directory'],1
repos/datasets/src/datasets/builder.py:DatasetBuilder:_make_split_generators_kwargs,DatasetBuilder:_make_split_generators_kwargs,method,3,4,4,32,8.0,0,0,"['self', 'prepare_split_kwargs']","[None, None]","[None, None]",1185,"['        """"""Get kwargs for `self._split_generators()` from `prepare_split_kwargs`.""""""\n']",[],0
repos/datasets/src/datasets/builder.py:DatasetBuilder:_post_process,DatasetBuilder:_post_process,method,1,2,2,10,5.0,0,0,"['self', 'dataset', 'resources_paths', 'str]']","[None, ' Dataset', ' Mapping[str', None]","[None, None, None, None]",1436,"['        """"""Run dataset transforms or add indexes""""""\n']",[],0
repos/datasets/src/datasets/builder.py:DatasetBuilder:_post_processing_resources,DatasetBuilder:_post_processing_resources,method,1,2,2,8,4.0,0,0,"['self', 'split']","[None, ' str']","[None, None]",1440,"['        """"""Mapping resource_name -> resource_file_name""""""\n']",[],0
repos/datasets/src/datasets/builder.py:DatasetBuilder:_prepare_split,DatasetBuilder:_prepare_split,method,1,2,2,26,13.0,0,0,"['self', 'split_generator', 'file_format', 'max_shard_size', 'int]] ', 'num_proc', '**kwargs', '']","[None, ' SplitGenerator', ' str ', ' Optional[Union[str', None, ' Optional[int] ', None, None]","[None, None, ' ""arrow""', None, ' None', ' None', None, None]",1498,"['        """"""Generate the examples and record them on disk.\n', '\n', '        Args:\n', '            split_generator (`SplitGenerator`):\n', '                Split generator to process\n', '            file_format (`str`, *optional*):\n', '                format of the data files in which the dataset will be written.\n', '                Supported formats: ""arrow"", ""parquet"". Default to ""arrow"" format.\n', '            max_shard_size (`Union[str, int]`, *optional*):\n', '                Maximum number of bytes written per shard, default is ""500MB"".\n', '                The size is based on uncompressed data size, so in practice your shard files may be smaller than\n', '                `max_shard_size` thanks to Parquet compression for example.\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.7.0""/>\n', '            **kwargs: Additional kwargs forwarded from _download_and_prepare (ex:\n', '                beam pipeline)\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/builder.py:DatasetBuilder:_relative_data_dir,DatasetBuilder:_relative_data_dir,method,12,49,30,577,11.78,0,4,"['self', 'with_version', 'with_hash']","[None, None, None]","[None, 'True', 'True']",679,"['        """"""Relative path of this dataset in cache_dir:\n', '        Will be:\n', '            self.dataset_name/self.config.version/self.hash/\n', '        or if a repo_id with a namespace has been specified:\n', '            self.namespace___self.dataset_name/self.config.version/self.hash/\n', '        If any of these element is missing or if ``with_version=False`` the corresponding subfolders are dropped.\n', '        """"""\n']","['posixpath.join', 'str', 'isinstance']",3
repos/datasets/src/datasets/builder.py:DatasetBuilder:_rename,DatasetBuilder:_rename,method,1,3,3,24,8.0,0,0,"['self', 'src', 'dst']","[None, ' str', ' str']","[None, None, None]",750,[],['rename'],1
repos/datasets/src/datasets/builder.py:DatasetBuilder:_save_info,DatasetBuilder:_save_info,method,5,14,14,221,15.79,0,1,['self'],[None],[None],1167,[],"['FileLock', 'is_remote_filesystem', 'contextlib.nullcontext']",3
repos/datasets/src/datasets/builder.py:DatasetBuilder:_save_infos,DatasetBuilder:_save_infos,method,5,14,14,234,16.71,0,1,['self'],[None],[None],1176,[],"['FileLock', 'is_remote_filesystem', 'contextlib.nullcontext', 'DatasetInfosDict']",4
repos/datasets/src/datasets/builder.py:DatasetBuilder:_split_generators,DatasetBuilder:_split_generators,method,1,2,2,26,13.0,0,0,"['self', 'dl_manager', 'StreamingDownloadManager]']","[None, ' Union[DownloadManager', None]","[None, None, None]",1451,"['        """"""Specify feature dictionary generators and dataset splits.\n', '\n', '        This function returns a list of `SplitGenerator`s defining how to generate\n', '        data and what splits to use.\n', '\n', '        Example:\n', '\n', '            return [\n', '                    datasets.SplitGenerator(\n', '                            name=datasets.Split.TRAIN,\n', ""                            gen_kwargs={'file': 'train_data.zip'},\n"", '                    ),\n', '                    datasets.SplitGenerator(\n', '                            name=datasets.Split.TEST,\n', ""                            gen_kwargs={'file': 'test_data.zip'},\n"", '                    ),\n', '            ]\n', '\n', ""        The above code will first call `_generate_examples(file='train_data.zip')`\n"", ""        to write the train data, then `_generate_examples(file='test_data.zip')` to\n"", '        write the test data.\n', '\n', '        Datasets are typically split into different subsets to be used at various\n', '        stages of training and evaluation.\n', '\n', '        Note that for datasets without a `VALIDATION` split, you can use a\n', '        fraction of the `TRAIN` data for evaluation as you iterate on your model\n', '        so as not to overfit to the `TEST` data.\n', '\n', '        For downloads and extractions, use the given `download_manager`.\n', '        Note that the `DownloadManager` caches downloads, so it is fine to have each\n', '        generator attempt to download the source data.\n', '\n', '        A good practice is to download all data in this function, and then\n', '        distribute the relevant parts to each split with the `gen_kwargs` argument\n', '\n', '        Args:\n', '            dl_manager (`Union[DownloadManager, StreamingDownloadManager]`):\n', '                Download manager to download the data\n', '\n', '        Returns:\n', '            `list<SplitGenerator>`.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/builder.py:DatasetBuilder:_use_legacy_cache_dir_if_possible,DatasetBuilder:_use_legacy_cache_dir_if_possible,method,4,12,10,183,15.25,0,0,"['self', 'dataset_module']","[None, ' ""DatasetModule""']","[None, None]",671,[],"['self._check_legacy_cache2', 'self._check_legacy_cache', 'self._build_cache_dir']",3
repos/datasets/src/datasets/builder.py:DatasetBuilder:as_dataset,DatasetBuilder:as_dataset,method,25,147,104,1394,9.48,1,7,"['self', 'split', 'run_post_process', 'verification_mode', 'str]] ', 'ignore_verifications', 'in_memory', '']","[None, ' Optional[Split] ', None, ' Optional[Union[VerificationMode', None, None, None, None]","[None, ' None', 'True', None, ' None', '""deprecated""', 'False', None]",1190,"['        """"""Return a Dataset for the specified split.\n', '\n', '        Args:\n', '            split (`datasets.Split`):\n', '                Which subset of the data to return.\n', '            run_post_process (`bool`, defaults to `True`):\n', '                Whether to run post-processing dataset transforms and/or add\n', '                indexes.\n', '            verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n', '                Verification mode determining the checks to run on the\n', '                downloaded/processed dataset information (checksums/size/splits/...).\n', '\n', '                <Added version=""2.9.1""/>\n', '            ignore_verifications (`bool`, defaults to `False`):\n', '                Whether to ignore the verifications of the\n', '                downloaded/processed dataset information (checksums/size/splits/...).\n', '\n', '                <Deprecated version=""2.9.1"">\n', '\n', '                `ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\n', '                Please use `verification_mode` instead.\n', '\n', '                </Deprecated>\n', '            in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '\n', '        Returns:\n', '            datasets.Dataset\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset_builder\n', ""        >>> builder = load_dataset_builder('rotten_tomatoes')\n"", '        >>> builder.download_and_prepare()\n', ""        >>> ds = builder.as_dataset(split='train')\n"", '        >>> ds\n', '        Dataset({\n', ""            features: ['text', 'label'],\n"", '            num_rows: 8530\n', '        })\n', '        ```\n', '        """"""\n']","['warnings.warn', 'FileFormatError', 'is_remote_filesystem', 'NotImplementedError', 'FileNotFoundError', 'logger.debug', 'VerificationMode', 'map_nested', 'partial', 'isinstance', 'DatasetDict']",11
repos/datasets/src/datasets/builder.py:DatasetBuilder:as_streaming_dataset,DatasetBuilder:as_streaming_dataset,method,19,68,54,861,12.66,1,3,"['self', 'split', 'base_path', '']","[None, ' Optional[str] ', ' Optional[str] ', None]","[None, ' None', ' None', None]",1389,[],"['is_remote_filesystem', 'NotImplementedError', 'StreamingDownloadManager', 'download_config=DownloadConfig', 'self._check_manual_download', 'self._split_generators', 'ValueError', 'map_nested', 'isinstance', 'IterableDatasetDict']",10
repos/datasets/src/datasets/builder.py:DatasetBuilder:builder_configs,DatasetBuilder:builder_configs,method,7,28,22,237,8.46,2,1,['cls'],[None],[None],659,"['        """"""Dictionary of pre-defined configurations for this builder class.""""""\n']","['len', 'ValueError']",2
repos/datasets/src/datasets/builder.py:DatasetBuilder:cache_dir,DatasetBuilder:cache_dir,method,2,2,2,21,10.5,0,0,['self'],[None],[None],668,[],[],0
repos/datasets/src/datasets/builder.py:DatasetBuilder:download_and_prepare,DatasetBuilder:download_and_prepare,method,86,480,286,5567,11.6,1,25,"['self', 'output_dir', 'download_config', 'download_mode', 'str]] ', 'verification_mode', 'str]] ', 'ignore_verifications', 'try_from_hf_gcs', 'dl_manager', 'base_path', 'use_auth_token', 'file_format', 'max_shard_size', 'str]] ', 'num_proc', 'storage_options', '**download_and_prepare_kwargs', '']","[None, ' Optional[str] ', ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[Union[VerificationMode', None, None, None, ' Optional[DownloadManager] ', ' Optional[str] ', None, ' str ', ' Optional[Union[int', None, ' Optional[int] ', ' Optional[dict] ', None, None]","[None, ' None', ' None', None, ' None', None, ' None', '""deprecated""', '""deprecated""', ' None', ' None', '""deprecated""', ' ""arrow""', None, ' None', ' None', ' None', None, None]",753,"['        """"""Downloads and prepares dataset for reading.\n', '\n', '        Args:\n', '            output_dir (`str`, *optional*):\n', '                Output directory for the dataset.\n', ""                Default to this builder's `cache_dir`, which is inside `~/.cache/huggingface/datasets` by default.\n"", '\n', '                <Added version=""2.5.0""/>\n', '            download_config (`DownloadConfig`, *optional*):\n', '                Specific download configuration parameters.\n', '            download_mode ([`DownloadMode`] or `str`, *optional*):\n', '                Select the download/generate mode, default to `REUSE_DATASET_IF_EXISTS`.\n', '            verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n', '                Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n', '\n', '                <Added version=""2.9.1""/>\n', '            ignore_verifications (`bool`, defaults to `False`):\n', '                Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).\n', '\n', '                <Deprecated version=""2.9.1"">\n', '\n', '                `ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\n', '                Please use `verification_mode` instead.\n', '\n', '                </Deprecated>\n', '            try_from_hf_gcs (`bool`):\n', '                If `True`, it will try to download the already prepared dataset from the HF Google cloud storage.\n', '\n', '                <Deprecated version=""2.16.0"">\n', '\n', '                `try_from_hf_gcs` was deprecated in version 2.16.0 and will be removed in 3.0.0.\n', '                Host the processed files on the Hugging Face Hub instead.\n', '\n', '                </Deprecated>\n', '            dl_manager (`DownloadManager`, *optional*):\n', '                Specific `DownloadManger` to use.\n', '            base_path (`str`, *optional*):\n', '                Base path for relative paths that are used to download files. This can be a remote url.\n', '                If not specified, the value of the `base_path` attribute (`self.base_path`) will be used instead.\n', '            use_auth_token (`Union[str, bool]`, *optional*):\n', '                Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '                If True, or not specified, will get token from ~/.huggingface.\n', '\n', '                <Deprecated version=""2.7.1"">\n', '\n', '                Pass `use_auth_token` to `load_dataset_builder` instead.\n', '\n', '                </Deprecated>\n', '            file_format (`str`, *optional*):\n', '                Format of the data files in which the dataset will be written.\n', '                Supported formats: ""arrow"", ""parquet"". Default to ""arrow"" format.\n', '                If the format is ""parquet"", then image and audio data are embedded into the Parquet files instead of pointing to local files.\n', '\n', '                <Added version=""2.5.0""/>\n', '            max_shard_size (`Union[str, int]`, *optional*):\n', '                Maximum number of bytes written per shard, default is ""500MB"".\n', '                The size is based on uncompressed data size, so in practice your shard files may be smaller than\n', '                `max_shard_size` thanks to Parquet compression for example.\n', '\n', '                <Added version=""2.5.0""/>\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.7.0""/>\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the caching file-system backend, if any.\n', '\n', '                <Added version=""2.5.0""/>\n', '            **download_and_prepare_kwargs (additional keyword arguments): Keyword arguments.\n', '\n', '        Example:\n', '\n', '        Download and prepare the dataset as Arrow files that can be loaded as a Dataset using `builder.as_dataset()`:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset_builder\n', '        >>> builder = load_dataset_builder(""rotten_tomatoes"")\n', '        >>> builder.download_and_prepare()\n', '        ```\n', '\n', '        Download and prepare the dataset as sharded Parquet files locally:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset_builder\n', '        >>> builder = load_dataset_builder(""rotten_tomatoes"")\n', '        >>> builder.download_and_prepare(""./output_dir"", file_format=""parquet"")\n', '        ```\n', '\n', '        Download and prepare the dataset as sharded Parquet files in a cloud storage:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset_builder\n', '        >>> storage_options = {""key"": aws_access_key_id, ""secret"": aws_secret_access_key}\n', '        >>> builder = load_dataset_builder(""rotten_tomatoes"")\n', '        >>> builder.download_and_prepare(""s3://my-bucket/my_rotten_tomatoes"", storage_options=storage_options, file_format=""parquet"")\n', '        ```\n', '        """"""\n']","['warnings.warn', 'url_to_fs', 'is_remote_filesystem', 'DownloadMode', 'VerificationMode', 'ValueError', 'RuntimeError', 'DownloadConfig', 'DownloadManager', 'isinstance', 'Path', 'FileLock', 'contextlib.nullcontext', 'logger.info', 'self._load_info', 'self.download_post_processing_resources', 'has_sufficient_disk_space', 'directory=Path', 'OSError', 'incomplete_dir', 'os.makedirs', 'shutil.rmtree', 'shutil.move', 'self._check_manual_download', 'temporary_assignment', 'self._download_prepared_from_hf_gcs', 'logger.warning', 'self._download_and_prepare', 'sum', 'dl_manager.get_recorded_sizes_checksums', 'self._save_info']",31
repos/datasets/src/datasets/builder.py:DatasetBuilder:download_post_processing_resources,DatasetBuilder:download_post_processing_resources,method,17,58,45,711,12.26,2,4,"['self', 'dl_manager']","[None, None]","[None, None]",1148,[],"['self._post_processing_resources', 'is_remote_filesystem', 'NotImplementedError', 'ValueError', 'self._download_post_processing_resources', 'logger.info', 'shutil.move']",7
repos/datasets/src/datasets/builder.py:DatasetBuilder:get_all_exported_dataset_infos,DatasetBuilder:get_all_exported_dataset_infos,method,2,2,2,68,34.0,0,0,['cls'],[None],[None],538,"['        """"""Empty dict if doesn\'t exist\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset_builder\n', ""        >>> ds_builder = load_dataset_builder('rotten_tomatoes')\n"", '        >>> ds_builder.get_all_exported_dataset_infos()\n', '        {\'default\': DatasetInfo(description=""Movie Review Dataset.\\nThis is a dataset of containing 5,331 positive and 5,331 negative processed\\nsentences from Rotten Tomatoes movie reviews. This data was first used in Bo\\nPang and Lillian Lee, ``Seeing stars: Exploiting class relationships for\\nsentiment categorization with respect to rating scales.\'\', Proceedings of the\\nACL, 2005.\\n"", citation=\'@InProceedings{Pang+Lee:05a,\\n  author =       {Bo Pang and Lillian Lee},\\n  title =        {Seeing stars: Exploiting class relationships for sentiment\\n                  categorization with respect to rating scales},\\n  booktitle =    {Proceedings of the ACL},\\n  year =         2005\\n}\\n\', homepage=\'http://www.cs.cornell.edu/people/pabo/movie-review-data/\', license=\'\', features={\'text\': Value(dtype=\'string\', id=None), \'label\': ClassLabel(num_classes=2, names=[\'neg\', \'pos\'], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=\'\', output=\'\'), task_templates=[TextClassification(task=\'text-classification\', text_column=\'text\', label_column=\'label\')], builder_name=\'rotten_tomatoes_movie_review\', config_name=\'default\', version=1.0.0, splits={\'train\': SplitInfo(name=\'train\', num_bytes=1074810, num_examples=8530, dataset_name=\'rotten_tomatoes_movie_review\'), \'validation\': SplitInfo(name=\'validation\', num_bytes=134679, num_examples=1066, dataset_name=\'rotten_tomatoes_movie_review\'), \'test\': SplitInfo(name=\'test\', num_bytes=135972, num_examples=1066, dataset_name=\'rotten_tomatoes_movie_review\')}, download_checksums={\'https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz\': {\'num_bytes\': 487770, \'checksum\': \'a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9\'}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}\n', '        ```\n', '        """"""\n']",['DatasetInfosDict.from_directory'],1
repos/datasets/src/datasets/builder.py:DatasetBuilder:get_exported_dataset_info,DatasetBuilder:get_exported_dataset_info,method,2,3,3,79,26.33,0,0,['self'],[None],[None],552,"['        """"""Empty `DatasetInfo` if doesn\'t exist\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset_builder\n', ""        >>> ds_builder = load_dataset_builder('rotten_tomatoes')\n"", '        >>> ds_builder.get_exported_dataset_info()\n', '        DatasetInfo(description=""Movie Review Dataset.\\nThis is a dataset of containing 5,331 positive and 5,331 negative processed\\nsentences from Rotten Tomatoes movie reviews. This data was first used in Bo\\nPang and Lillian Lee, ``Seeing stars: Exploiting class relationships for\\nsentiment categorization with respect to rating scales.\'\', Proceedings of the\\nACL, 2005.\\n"", citation=\'@InProceedings{Pang+Lee:05a,\\n  author =       {Bo Pang and Lillian Lee},\\n  title =        {Seeing stars: Exploiting class relationships for sentiment\\n                  categorization with respect to rating scales},\\n  booktitle =    {Proceedings of the ACL},\\n  year =         2005\\n}\\n\', homepage=\'http://www.cs.cornell.edu/people/pabo/movie-review-data/\', license=\'\', features={\'text\': Value(dtype=\'string\', id=None), \'label\': ClassLabel(num_classes=2, names=[\'neg\', \'pos\'], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=\'\', output=\'\'), task_templates=[TextClassification(task=\'text-classification\', text_column=\'text\', label_column=\'label\')], builder_name=\'rotten_tomatoes_movie_review\', config_name=\'default\', version=1.0.0, splits={\'train\': SplitInfo(name=\'train\', num_bytes=1074810, num_examples=8530, dataset_name=\'rotten_tomatoes_movie_review\'), \'validation\': SplitInfo(name=\'validation\', num_bytes=134679, num_examples=1066, dataset_name=\'rotten_tomatoes_movie_review\'), \'test\': SplitInfo(name=\'test\', num_bytes=135972, num_examples=1066, dataset_name=\'rotten_tomatoes_movie_review\')}, download_checksums={\'https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz\': {\'num_bytes\': 487770, \'checksum\': \'a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9\'}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)\n', '        ```\n', '        """"""\n']","['self.get_all_exported_dataset_infos', 'DatasetInfo']",2
repos/datasets/src/datasets/builder.py:DatasetBuilder:get_imported_module_dir,DatasetBuilder:get_imported_module_dir,method,2,2,2,62,31.0,0,0,['cls'],[None],[None],746,"['        """"""Return the path of the module of this class or subclass.""""""\n']",[],0
repos/datasets/src/datasets/builder.py:DatasetBuilder:manual_download_instructions,DatasetBuilder:manual_download_instructions,method,1,2,2,10,5.0,0,0,['self'],[None],[None],464,[],[],0
repos/datasets/src/datasets/builder.py:GeneratorBasedBuilder:_download_and_prepare,GeneratorBasedBuilder:_download_and_prepare,method,8,10,10,208,20.8,0,0,"['self', 'dl_manager', 'verification_mode', '**prepare_splits_kwargs']","[None, None, None, None]","[None, None, None, None]",1790,[],['super'],1
repos/datasets/src/datasets/builder.py:GeneratorBasedBuilder:_generate_examples,GeneratorBasedBuilder:_generate_examples,method,1,2,2,26,13.0,0,0,"['self', '**kwargs']","[None, None]","[None, None]",1548,"['        """"""Default function generating examples for each `SplitGenerator`.\n', '\n', '        This function preprocess the examples from the raw data to the preprocessed\n', '        dataset files.\n', '        This function is called once for each `SplitGenerator` defined in\n', '        `_split_generators`. The examples yielded here will be written on\n', '        disk.\n', '\n', '        Args:\n', '            **kwargs (additional keyword arguments):\n', '                Arguments forwarded from the SplitGenerator.gen_kwargs\n', '\n', '        Yields:\n', '            key: `str` or `int`, a unique deterministic example identification key.\n', '                * Unique: An error will be raised if two examples are yield with the\n', '                    same key.\n', '                * Deterministic: When generating the dataset twice, the same example\n', '                    should have the same key.\n', '                Good keys can be the image id, or line number if examples are extracted\n', '                from a text file.\n', '                The key will be hashed and sorted to shuffle examples deterministically,\n', '                such as generating the dataset multiple times keep examples in the\n', '                same order.\n', '            example: `dict<str feature_name, feature_value>`, a feature dictionary\n', '                ready to be encoded and written to disk. The example will be\n', '                encoded with `self.info.features.encode_example({...})`.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/builder.py:GeneratorBasedBuilder:_get_examples_iterable_for_split,GeneratorBasedBuilder:_get_examples_iterable_for_split,method,2,3,3,74,24.67,0,0,"['self', 'split_generator']","[None, ' SplitGenerator']","[None, None]",1528,"['        """"""Generate the examples on the fly.\n', '\n', '        Args:\n', '            split_generator (`SplitGenerator`):\n', '                Split generator to process\n', '        """"""\n']",['ExamplesIterable'],1
repos/datasets/src/datasets/builder.py:GeneratorBasedBuilder:_prepare_split,GeneratorBasedBuilder:_prepare_split,method,63,330,202,3550,10.76,2,8,"['self', 'split_generator', 'check_duplicate_keys', 'file_format', 'num_proc', 'max_shard_size', 'str]] ', '']","[None, ' SplitGenerator', ' bool', None, ' Optional[int] ', ' Optional[Union[int', None, None]","[None, None, None, '""arrow""', ' None', None, ' None', None]",1498,"['        """"""Generate the examples and record them on disk.\n', '\n', '        Args:\n', '            split_generator (`SplitGenerator`):\n', '                Split generator to process\n', '            file_format (`str`, *optional*):\n', '                format of the data files in which the dataset will be written.\n', '                Supported formats: ""arrow"", ""parquet"". Default to ""arrow"" format.\n', '            max_shard_size (`Union[str, int]`, *optional*):\n', '                Maximum number of bytes written per shard, default is ""500MB"".\n', '                The size is based on uncompressed data size, so in practice your shard files may be smaller than\n', '                `max_shard_size` thanks to Parquet compression for example.\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.7.0""/>\n', '            **kwargs: Additional kwargs forwarded from _download_and_prepare (ex:\n', '                beam pipeline)\n', '        """"""\n']","['convert_file_size_to_int', 'posixpath.join', '_number_of_shards_in_gen_kwargs', 'logger.warning', 'hf_tqdm', 'self._prepare_split_single', 'pbar.update', 'enumerate', '_split_gen_kwargs', 'len', 'Pool', 'iflatmap_unordered', 'sum', 'logger.debug', '_rename_shard', 'self._rename', 'fpath.replace', 'range', 'thread_map']",19
repos/datasets/src/datasets/builder.py:GeneratorBasedBuilder:_prepare_split_single,GeneratorBasedBuilder:_prepare_split_single,method,42,156,98,1968,12.62,1,4,"['self', 'gen_kwargs', 'fpath', 'file_format', 'max_shard_size', 'split_info', 'check_duplicate_keys', 'job_id', '']","[None, ' dict', ' str', ' str', ' int', ' SplitInfo', ' bool', ' int', None]","[None, None, None, None, None, None, None, None, None]",1720,[],"['self._generate_examples', 'writer_class', 'time.time', 'writer.finalize', 'writer.close', 'shard_lengths.append', 'writer.write', 'isinstance', 'DatasetGenerationError']",9
repos/datasets/src/datasets/combine.py:concatenate_datasets,concatenate_datasets,function,25,166,87,1197,7.21,2,6,"['dsets', 'info', 'split', 'axis', '']","[' List[DatasetType]', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' int ', None]","[None, ' None', ' None', ' 0', None]",158,"['    """"""\n', '    Converts a list of [`Dataset`] with the same schema into a single [`Dataset`].\n', '\n', '    Args:\n', '        dsets (`List[datasets.Dataset]`):\n', '            List of Datasets to concatenate.\n', '        info (`DatasetInfo`, *optional*):\n', '            Dataset information, like description, citation, etc.\n', '        split (`NamedSplit`, *optional*):\n', '            Name of the dataset split.\n', '        axis (`{0, 1}`, defaults to `0`):\n', '            Axis to concatenate over, where `0` means over rows (vertically) and `1` means over columns\n', '            (horizontally).\n', '\n', '            <Added version=""1.6.0""/>\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> ds3 = concatenate_datasets([ds1, ds2])\n', '    ```\n', '    """"""\n']","['ValueError', 'enumerate', 'isinstance', '_concatenate_map_style_datasets', '_concatenate_iterable_datasets']",5
repos/datasets/src/datasets/combine.py:interleave_datasets,interleave_datasets,function,29,198,103,1526,7.71,2,7,"['datasets', 'probabilities', 'seed', 'info', 'split', 'stopping_strategy', '""all_exhausted""] ', '']","[' List[DatasetType]', ' Optional[List[float]] ', ' Optional[int] ', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Literal[""first_exhausted""', None, None]","[None, ' None', ' None', ' None', ' None', None, ' ""first_exhausted""', None]",18,"['    """"""\n', '    Interleave several datasets (sources) into a single dataset.\n', '    The new dataset is constructed by alternating between the sources to get the examples.\n', '\n', '    You can use this function on a list of [`Dataset`] objects, or on a list of [`IterableDataset`] objects.\n', '\n', '        - If `probabilities` is `None` (default) the new dataset is constructed by cycling between each source to get the examples.\n', '        - If `probabilities` is not `None`, the new dataset is constructed by getting examples from a random source at a time according to the provided probabilities.\n', '\n', '    The resulting dataset ends when one of the source datasets runs out of examples except when `oversampling` is `True`,\n', '    in which case, the resulting dataset ends when all datasets have ran out of examples at least one time.\n', '\n', '    Note for iterable datasets:\n', '\n', '    In a distributed setup or in PyTorch DataLoader workers, the stopping strategy is applied per process.\n', '    Therefore the ""first_exhausted"" strategy on an sharded iterable dataset can generate less samples in total (up to 1 missing sample per subdataset per worker).\n', '\n', '    Args:\n', '        datasets (`List[Dataset]` or `List[IterableDataset]`):\n', '            List of datasets to interleave.\n', '        probabilities (`List[float]`, *optional*, defaults to `None`):\n', '            If specified, the new dataset is constructed by sampling\n', '            examples from one source at a time according to these probabilities.\n', '        seed (`int`, *optional*, defaults to `None`):\n', '            The random seed used to choose a source for each example.\n', '        info ([`DatasetInfo`], *optional*):\n', '            Dataset information, like description, citation, etc.\n', '            <Added version=""2.4.0""/>\n', '        split ([`NamedSplit`], *optional*):\n', '            Name of the dataset split.\n', '            <Added version=""2.4.0""/>\n', '        stopping_strategy (`str`, defaults to `first_exhausted`):\n', '            Two strategies are proposed right now, `first_exhausted` and `all_exhausted`.\n', '            By default, `first_exhausted` is an undersampling strategy, i.e the dataset construction is stopped as soon as one dataset has ran out of samples.\n', '            If the strategy is `all_exhausted`,  we use an oversampling strategy, i.e the dataset construction is stopped as soon as every samples of every dataset has been added at least once.\n', '            Note that if the strategy is `all_exhausted`, the interleaved dataset size can get enormous:\n', '            - with no probabilities, the resulting dataset will have `max_length_datasets*nb_dataset` samples.\n', '            - with given probabilities, the resulting dataset will have more samples if some datasets have really low probability of visiting.\n', '    Returns:\n', '        [`Dataset`] or [`IterableDataset`]: Return type depends on the input `datasets`\n', '        parameter. `Dataset` if the input is a list of `Dataset`, `IterableDataset` if the input is a list of\n', '        `IterableDataset`.\n', '\n', '    Example:\n', '\n', '        For regular datasets (map-style):\n', '\n', '        ```python\n', '        >>> from datasets import Dataset, interleave_datasets\n', '        >>> d1 = Dataset.from_dict({""a"": [0, 1, 2]})\n', '        >>> d2 = Dataset.from_dict({""a"": [10, 11, 12]})\n', '        >>> d3 = Dataset.from_dict({""a"": [20, 21, 22]})\n', '        >>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42, stopping_strategy=""all_exhausted"")\n', '        >>> dataset[""a""]\n', '        [10, 0, 11, 1, 2, 20, 12, 10, 0, 1, 2, 21, 0, 11, 1, 2, 0, 1, 12, 2, 10, 0, 22]\n', '        >>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42)\n', '        >>> dataset[""a""]\n', '        [10, 0, 11, 1, 2]\n', '        >>> dataset = interleave_datasets([d1, d2, d3])\n', '        >>> dataset[""a""]\n', '        [0, 10, 20, 1, 11, 21, 2, 12, 22]\n', '        >>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy=""all_exhausted"")\n', '        >>> dataset[""a""]\n', '        [0, 10, 20, 1, 11, 21, 2, 12, 22]\n', '        >>> d1 = Dataset.from_dict({""a"": [0, 1, 2]})\n', '        >>> d2 = Dataset.from_dict({""a"": [10, 11, 12, 13]})\n', '        >>> d3 = Dataset.from_dict({""a"": [20, 21, 22, 23, 24]})\n', '        >>> dataset = interleave_datasets([d1, d2, d3])\n', '        >>> dataset[""a""]\n', '        [0, 10, 20, 1, 11, 21, 2, 12, 22]\n', '        >>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy=""all_exhausted"")\n', '        >>> dataset[""a""]\n', '        [0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 23, 1, 10, 24]\n', '        >>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42)\n', '        >>> dataset[""a""]\n', '        [10, 0, 11, 1, 2]\n', '        >>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42, stopping_strategy=""all_exhausted"")\n', '        >>> dataset[""a""]\n', '        [10, 0, 11, 1, 2, 20, 12, 13, ..., 0, 1, 2, 0, 24]\n', '        For datasets in streaming mode (iterable):\n', '\n', '        >>> from datasets import load_dataset, interleave_datasets\n', '        >>> d1 = load_dataset(""oscar"", ""unshuffled_deduplicated_en"", split=""train"", streaming=True)\n', '        >>> d2 = load_dataset(""oscar"", ""unshuffled_deduplicated_fr"", split=""train"", streaming=True)\n', '        >>> dataset = interleave_datasets([d1, d2])\n', '        >>> iterator = iter(dataset)\n', '        >>> next(iterator)\n', ""        {'text': 'Mtendere Village was inspired by the vision...}\n"", '        >>> next(iterator)\n', '        {\'text\': ""Média de débat d\'idées, de culture...}\n', '        ```\n', '    """"""\n']","['ValueError', 'enumerate', 'isinstance', '_interleave_map_style_datasets', '_interleave_iterable_datasets']",5
repos/datasets/src/datasets/commands/__init__.py:BaseDatasetsCLICommand,BaseDatasetsCLICommand,class,3,12,9,160,13.33,0,0,[],[],[],5,[],[],0
repos/datasets/src/datasets/commands/__init__.py:BaseDatasetsCLICommand:register_subcommand,BaseDatasetsCLICommand:register_subcommand,method,1,2,2,26,13.0,0,0,['parser'],[' ArgumentParser'],[None],8,[],['NotImplementedError'],1
repos/datasets/src/datasets/commands/__init__.py:BaseDatasetsCLICommand:run,BaseDatasetsCLICommand:run,method,1,2,2,26,13.0,0,0,['self'],[None],[None],12,[],['NotImplementedError'],1
repos/datasets/src/datasets/commands/convert_to_parquet.py:_command_factory,_command_factory,function,2,7,7,100,14.29,0,0,['args'],[None],[None],8,[],['ConvertToParquetCommand'],1
repos/datasets/src/datasets/commands/convert_to_parquet.py:ConvertToParquetCommand,ConvertToParquetCommand,class,18,82,70,930,11.34,0,0,[],[],[],17,[],[],0
repos/datasets/src/datasets/commands/convert_to_parquet.py:ConvertToParquetCommand:__init__,ConvertToParquetCommand:__init__,method,8,8,8,111,13.88,0,0,"['self', 'dataset_id', 'token', 'revision', 'trust_remote_code', '']","[None, ' str', ' Optional[str]', ' Optional[str]', ' bool', None]","[None, None, None, None, None, None]",31,[],[],0
repos/datasets/src/datasets/commands/convert_to_parquet.py:ConvertToParquetCommand:register_subcommand,ConvertToParquetCommand:register_subcommand,method,5,48,39,520,10.83,0,0,['parser'],[None],[None],19,[],"['parser.add_parser', 'parser.add_argument', 'parser.set_defaults']",3
repos/datasets/src/datasets/commands/convert_to_parquet.py:ConvertToParquetCommand:run,ConvertToParquetCommand:run,method,2,7,7,124,17.71,0,0,['self'],[None],[None],43,[],['convert_to_parquet'],1
repos/datasets/src/datasets/commands/datasets_cli.py:main,main,function,23,38,38,772,20.32,0,1,[],[],[],18,[],"['ArgumentParser', 'parser.add_subparsers', 'set_verbosity_info', 'ConvertCommand.register_subcommand', 'EnvironmentCommand.register_subcommand', 'TestCommand.register_subcommand', 'RunBeamCommand.register_subcommand', 'DummyDataCommand.register_subcommand', 'ConvertToParquetCommand.register_subcommand', 'DeleteFromHubCommand.register_subcommand', 'parser.parse_known_args', 'hasattr', 'parser.print_help', 'exit', 'parse_unknown_args', 'args.func', 'service.run']",17
repos/datasets/src/datasets/commands/datasets_cli.py:parse_unknown_args,parse_unknown_args,function,1,9,8,84,9.33,0,0,['unknown_args'],[None],[None],14,[],['zip'],1
repos/datasets/src/datasets/commands/delete_from_hub.py:_command_factory,_command_factory,function,2,7,7,91,13.0,0,0,['args'],[None],[None],8,[],['DeleteFromHubCommand'],1
repos/datasets/src/datasets/commands/delete_from_hub.py:DeleteFromHubCommand,DeleteFromHubCommand,class,18,68,59,798,11.74,0,0,[],[],[],17,[],[],0
repos/datasets/src/datasets/commands/delete_from_hub.py:DeleteFromHubCommand:__init__,DeleteFromHubCommand:__init__,method,8,8,8,99,12.38,0,0,"['self', 'dataset_id', 'config_name', 'token', 'revision', '']","[None, ' str', ' str', ' Optional[str]', ' Optional[str]', None]","[None, None, None, None, None, None]",29,[],[],0
repos/datasets/src/datasets/commands/delete_from_hub.py:DeleteFromHubCommand:register_subcommand,DeleteFromHubCommand:register_subcommand,method,5,36,31,436,12.11,0,0,['parser'],[None],[None],19,[],"['parser.add_parser', 'parser.add_argument', 'parser.set_defaults']",3
repos/datasets/src/datasets/commands/delete_from_hub.py:DeleteFromHubCommand:run,DeleteFromHubCommand:run,method,2,5,5,95,19.0,0,0,['self'],[None],[None],41,[],['delete_from_hub'],1
repos/datasets/src/datasets/commands/dummy_data.py:dummy_data_command_factory,dummy_data_command_factory,function,2,12,12,190,15.83,0,0,['args'],[None],[None],27,[],['DummyDataCommand'],1
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataCommand,DummyDataCommand,class,120,996,481,9694,9.73,4,12,[],[],[],219,[],[],0
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataGeneratorDownloadManager,DummyDataGeneratorDownloadManager,class,100,478,278,5273,11.03,7,15,[],[],[],41,[],[],0
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataCommand:__init__,DummyDataCommand:__init__,method,22,31,29,521,16.81,0,1,"['self', 'path_to_dataset', 'auto_generate', 'n_lines', 'json_field', 'xml_tag', 'match_text_files', 'keep_uncompressed', 'cache_dir', 'encoding', '']","[None, ' str', ' bool', ' int', ' Optional[str]', ' Optional[str]', ' Optional[str]', ' bool', ' Optional[str]', ' Optional[str]', None]","[None, None, None, None, None, None, None, None, None, None, None]",265,[],['path_to_dataset.replace'],1
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataCommand:_autogenerate_dummy_data,DummyDataCommand:_autogenerate_dummy_data,method,34,170,116,2092,12.31,1,3,"['self', 'dataset_builder', 'mock_dl_manager', 'keep_uncompressed']","[None, None, None, None]","[None, None, None, None]",331,[],"['DownloadConfig', 'DummyDataGeneratorDownloadManager', 'dataset_builder._split_generators', 'dl_manager.auto_generate_dummy_data_folder', 'dl_manager.compress_autogenerated_dummy_data', 'os.makedirs', 'dataset_builder._prepare_split', 'logger.error', 'str', 'all', 'n_examples_per_split.values', 'logger.warning', 'logger.info']",13
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataCommand:_print_dummy_data_instructions,DummyDataCommand:_print_dummy_data_instructions,method,40,490,211,3761,7.68,2,3,"['self', 'dataset_builder', 'mock_dl_manager']","[None, None, None]","[None, None, None]",389,[],"['logger.info', 'os.makedirs', 'dataset_builder._split_generators', 'print', 'set', 'split_names.append', 'dataset_builder._generate_examples', 'files_to_create.add', 'len', 'next', 'split']",11
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataCommand:register_subcommand,DummyDataCommand:register_subcommand,method,4,181,105,1566,8.65,0,0,['parser'],[' ArgumentParser'],[None],221,[],"['parser.add_parser', 'test_parser.add_argument', 'test_parser.set_defaults']",3
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataCommand:run,DummyDataCommand:run,method,27,85,66,1271,14.95,1,5,['self'],[None],[None],292,[],"['set_verbosity_warning', 'dataset_module_factory', 'import_main_class', 'tempfile.TemporaryDirectory', 'builder_cls', 'MockDownloadManager', 'auto_generate_results.append', 'self._autogenerate_dummy_data', 'self._print_dummy_data_instructions', 'all', 'print']",11
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataGeneratorDownloadManager:__init__,DummyDataGeneratorDownloadManager:__init__,method,5,8,8,141,17.62,0,0,"['self', 'mock_download_manager', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",42,[],['super'],1
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataGeneratorDownloadManager:_create_dummy_data,DummyDataGeneratorDownloadManager:_create_dummy_data,method,58,253,159,2368,9.36,5,10,"['self', 'src_path', 'dst_path', 'n_lines', 'json_field', 'xml_tag', 'match_text_files', 'encoding', '']","[None, ' str', ' str', ' int', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', None]","[None, None, None, None, ' None', ' None', ' None', ' None', None]",104,[],"['logger.debug', 'Path', 'any', 'match_text_files.split', 'fnmatch.fnmatch', 'open', 'enumerate', 'first_lines.append', 'dst_file.write', 'json.load', 'isinstance', 'all', 'json_data.values', 'ValueError', 'json_data.items', 'json.dump', 'logger.warning', 'self._create_xml_dummy_data', 'os.walk', 'name.startswith', 'self._create_dummy_data']",21
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataGeneratorDownloadManager:_create_xml_dummy_data,DummyDataGeneratorDownloadManager:_create_xml_dummy_data,method,17,41,34,400,9.76,1,4,"['src_path', 'dst_path', 'xml_tag', 'n_lines', 'encoding']","[None, None, None, None, None]","[None, None, None, '5', 'DEFAULT_ENCODING']",189,[],"['Path', 'open', 'ET.iterparse', 'parents.append', 'parents.pop', 'ET.ElementTree']",6
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataGeneratorDownloadManager:auto_generate_dummy_data_folder,DummyDataGeneratorDownloadManager:auto_generate_dummy_data_folder,method,12,67,54,854,12.75,1,1,"['self', 'n_lines', 'json_field', 'xml_tag', 'match_text_files', 'encoding', '']","[None, ' int ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', None]","[None, ' 5', ' None', ' None', ' None', ' None', None]",62,[],"['os.makedirs', 'zip', 'self._create_dummy_data', 'logger.error']",4
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataGeneratorDownloadManager:compress_autogenerated_dummy_data,DummyDataGeneratorDownloadManager:compress_autogenerated_dummy_data,method,7,19,19,293,15.42,0,0,"['self', 'path_to_dataset']","[None, None]","[None, None]",207,[],"['logger.info', 'shutil.make_archive', 'shutil.rmtree']",3
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataGeneratorDownloadManager:download,DummyDataGeneratorDownloadManager:download,method,6,12,10,253,21.08,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",48,[],"['super', 'map_nested']",2
repos/datasets/src/datasets/commands/dummy_data.py:DummyDataGeneratorDownloadManager:download_and_extract,DummyDataGeneratorDownloadManager:download_and_extract,method,6,12,10,270,22.5,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",55,[],"['super', 'map_nested']",2
repos/datasets/src/datasets/commands/env.py:info_command_factory,info_command_factory,function,2,2,2,26,13.0,0,0,['_'],[None],[None],13,[],['EnvironmentCommand'],1
repos/datasets/src/datasets/commands/env.py:EnvironmentCommand,EnvironmentCommand,class,12,61,51,690,11.31,0,0,[],[],[],17,[],[],0
repos/datasets/src/datasets/commands/env.py:EnvironmentCommand:format_dict,EnvironmentCommand:format_dict,method,1,10,10,62,6.2,0,0,['d'],[None],[None],40,[],['d.items'],1
repos/datasets/src/datasets/commands/env.py:EnvironmentCommand:register_subcommand,EnvironmentCommand:register_subcommand,method,3,8,8,139,17.38,0,0,['parser'],[' ArgumentParser'],[None],19,[],"['parser.add_parser', 'download_parser.set_defaults']",2
repos/datasets/src/datasets/commands/env.py:EnvironmentCommand:run,EnvironmentCommand:run,method,6,34,28,379,11.15,0,0,['self'],[None],[None],23,[],"['platform.platform', 'platform.python_version', 'print']",3
repos/datasets/src/datasets/commands/run_beam.py:run_beam_command_factory,run_beam_command_factory,function,2,15,15,218,14.53,0,0,"['args', '**kwargs']","[None, None]","[None, None]",17,[],['RunBeamCommand'],1
repos/datasets/src/datasets/commands/run_beam.py:RunBeamCommand,RunBeamCommand,class,66,299,213,3954,13.22,2,7,[],[],[],35,[],[],0
repos/datasets/src/datasets/commands/run_beam.py:RunBeamCommand:__init__,RunBeamCommand:__init__,method,20,20,20,317,15.85,0,0,"['self', 'dataset', 'name', 'cache_dir', 'beam_pipeline_options', 'data_dir', 'all_configs', 'save_infos', 'ignore_verifications', 'force_redownload', '**config_kwargs', '']","[None, ' str', ' str', ' str', ' str', ' str', ' bool', ' bool', ' bool', ' bool', None, None]","[None, None, None, None, None, None, None, None, None, None, None, None]",69,[],[],0
repos/datasets/src/datasets/commands/run_beam.py:RunBeamCommand:register_subcommand,RunBeamCommand:register_subcommand,method,4,101,74,1275,12.62,0,0,['parser'],[' ArgumentParser'],[None],37,[],"['parser.add_parser', 'run_beam_parser.add_argument', 'run_beam_parser.set_defaults']",3
repos/datasets/src/datasets/commands/run_beam.py:RunBeamCommand:run,RunBeamCommand:run,method,46,149,109,2082,13.97,2,7,['self'],[None],[None],93,[],"['print', 'exit', 'dataset_module_factory', 'import_main_class', 'len', 'builders.append', 'builder_cls', 'builder.download_and_prepare', 'download_config=DownloadConfig', 'builder._save_infos', 'Path', 'copyfile']",12
repos/datasets/src/datasets/commands/test.py:_test_command_factory,_test_command_factory,function,2,15,15,210,14.0,0,0,['args'],[None],[None],20,[],['TestCommand'],1
repos/datasets/src/datasets/commands/test.py:TestCommand,TestCommand,class,80,441,276,5004,11.35,2,14,[],[],[],35,[],[],0
repos/datasets/src/datasets/commands/test.py:TestCommand:__init__,TestCommand:__init__,method,23,74,61,619,8.36,0,2,"['self', 'dataset', 'name', 'cache_dir', 'data_dir', 'all_configs', 'save_infos', 'ignore_verifications', 'force_redownload', 'clear_cache', 'num_proc', '']","[None, ' str', ' str', ' str', ' str', ' bool', ' bool', ' bool', ' bool', ' bool', ' int', None]","[None, None, None, None, None, None, None, None, None, None, None, None]",75,[],"['print', 'exit']",2
repos/datasets/src/datasets/commands/test.py:TestCommand:register_subcommand,TestCommand:register_subcommand,method,4,111,76,1272,11.46,0,0,['parser'],[' ArgumentParser'],[None],39,[],"['parser.add_parser', 'test_parser.add_argument', 'test_parser.set_defaults']",3
repos/datasets/src/datasets/commands/test.py:TestCommand:run,TestCommand:run,method,55,215,146,2800,13.02,2,12,['self'],[None],[None],108,[],"['logging.getLogger', 'print', 'exit', 'dataset_module_factory', 'import_main_class', 'len', 'get_builders', 'enumerate', 'builder_cls', 'builder.download_and_prepare', 'builder.as_dataset', 'builder._save_infos', 'builder_cls.get_imported_module_dir', 'Path', 'copyfile', 'logger.warning', 'rmtree']",17
repos/datasets/src/datasets/data_files.py:_get_data_files_patterns,_get_data_files_patterns,function,27,109,70,1031,9.46,6,4,"['pattern_resolver', 'List[str]]']","[' Callable[[str]', None]","[None, None]",261,"['    """"""\n', '    Get the default pattern from a directory or repository by testing all the supported patterns.\n', '    The first patterns to return a non-empty list of data files is returned.\n', '\n', '    In order, it first tests if SPLIT_PATTERN_SHARDED works, otherwise it tests the patterns in ALL_DEFAULT_PATTERNS.\n', '    """"""\n']","['split_pattern.replace', 'pattern_resolver', 'len', 'string_to_dict', 'glob_pattern_to_regex', 'any', 're.match', 'ValueError', 'sorted', 'set', 'patterns_dict.items', 'non_empty_splits.append', 'FileNotFoundError']",13
repos/datasets/src/datasets/data_files.py:_get_metadata_files_patterns,_get_metadata_files_patterns,function,8,29,27,324,11.17,1,2,"['pattern_resolver', 'List[str]]']","[' Callable[[str]', None]","[None, None]",303,"['    """"""\n', '    Get the supported metadata patterns from a directory or repository.\n', '    """"""\n']","['pattern_resolver', 'len', 'non_empty_patterns.append', 'FileNotFoundError']",4
repos/datasets/src/datasets/data_files.py:_get_origin_metadata,_get_origin_metadata,function,9,24,22,303,12.62,0,1,"['data_files', 'download_config', 'max_workers', '']","[' List[str]', ' Optional[DownloadConfig] ', ' Optional[int] ', None]","[None, ' None', ' None', None]",538,[],"['thread_map', 'partial', 'disable=len']",3
repos/datasets/src/datasets/data_files.py:_get_single_origin_metadata,_get_single_origin_metadata,function,17,52,42,707,13.6,1,2,"['data_file', 'download_config', '']","[' str', ' Optional[DownloadConfig] ', None]","[None, ' None', None]",516,[],"['_prepare_path_and_storage_options', 'url_to_fs', 'isinstance', 'fs.resolve_path', 'data_file.startswith', 'HfFileSystem', 'data_file[len', 'hffs.resolve_path', 'fs.info']",9
repos/datasets/src/datasets/data_files.py:_is_inside_unrequested_special_dir,_is_inside_unrequested_special_dir,function,5,19,13,281,14.79,2,0,"['matched_rel_path', 'pattern']","[' str', ' str']","[None, None]",166,"['    """"""\n', ""    When a path matches a pattern, we additionnally check if it's inside a special directory\n"", '    we ignore by default (if it starts with a double underscore).\n', '\n', '    Users can still explicitly request a filepath inside such a directory if ""__pycache__"" is\n', '    mentioned explicitly in the requested pattern.\n', '\n', '    Some examples:\n', '\n', '    base directory:\n', '\n', '        ./\n', '        └── __pycache__\n', '            └── b.txt\n', '\n', '    >>> _is_inside_unrequested_special_dir(""__pycache__/b.txt"", ""**"")\n', '    True\n', '    >>> _is_inside_unrequested_special_dir(""__pycache__/b.txt"", ""*/b.txt"")\n', '    True\n', '    >>> _is_inside_unrequested_special_dir(""__pycache__/b.txt"", ""__pycache__/*"")\n', '    False\n', '    >>> _is_inside_unrequested_special_dir(""__pycache__/b.txt"", ""__*/*"")\n', '    False\n', '    """"""\n']","['PurePath', 'part.startswith', 'len']",3
repos/datasets/src/datasets/data_files.py:_is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir,_is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir,function,3,31,18,309,9.97,0,0,"['matched_rel_path', 'pattern']","[' str', ' str']","[None, None]",199,"['    """"""\n', ""    When a path matches a pattern, we additionnally check if it's a hidden file or if it's inside\n"", '    a hidden directory we ignore by default, i.e. if the file name or a parent directory name starts with a dot.\n', '\n', '    Users can still explicitly request a filepath that is hidden or is inside a hidden directory\n', '    if the hidden part is mentioned explicitly in the requested pattern.\n', '\n', '    Some examples:\n', '\n', '    base directory:\n', '\n', '        ./\n', '        └── .hidden_file.txt\n', '\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_file.txt"", ""**"")\n', '    True\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_file.txt"", "".*"")\n', '    False\n', '\n', '    base directory:\n', '\n', '        ./\n', '        └── .hidden_dir\n', '            └── a.txt\n', '\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_dir/a.txt"", ""**"")\n', '    True\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_dir/a.txt"", "".*/*"")\n', '    False\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_dir/a.txt"", "".hidden_dir/*"")\n', '    False\n', '\n', '    base directory:\n', '\n', '        ./\n', '        └── .hidden_dir\n', '            └── .hidden_file.txt\n', '\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_dir/.hidden_file.txt"", ""**"")\n', '    True\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_dir/.hidden_file.txt"", "".*/*"")\n', '    True\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_dir/.hidden_file.txt"", "".*/.*"")\n', '    False\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_dir/.hidden_file.txt"", "".hidden_dir/*"")\n', '    True\n', '    >>> _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir("".hidden_dir/.hidden_file.txt"", "".hidden_dir/.*"")\n', '    False\n', '    """"""\n']","['PurePath', 'part.startswith', 'set', 'len']",4
repos/datasets/src/datasets/data_files.py:contains_wildcards,contains_wildcards,function,2,8,7,78,9.75,0,0,['pattern'],[' str'],[None],124,[],['any'],1
repos/datasets/src/datasets/data_files.py:get_data_patterns,get_data_patterns,function,6,21,21,243,11.57,0,0,"['base_path', 'download_config']","[' str', ' Optional[DownloadConfig] ']","[None, ' None']",411,"['    """"""\n', '    Get the default pattern from a directory testing all the supported patterns.\n', '    The first patterns to return a non-empty list of data files is returned.\n', '\n', '    Some examples of supported patterns:\n', '\n', '    Input:\n', '\n', '        my_dataset_repository/\n', '        ├── README.md\n', '        └── dataset.csv\n', '\n', '    Output:\n', '\n', ""        {'train': ['**']}\n"", '\n', '    Input:\n', '\n', '        my_dataset_repository/\n', '        ├── README.md\n', '        ├── train.csv\n', '        └── test.csv\n', '\n', '        my_dataset_repository/\n', '        ├── README.md\n', '        └── data/\n', '            ├── train.csv\n', '            └── test.csv\n', '\n', '        my_dataset_repository/\n', '        ├── README.md\n', '        ├── train_0.csv\n', '        ├── train_1.csv\n', '        ├── train_2.csv\n', '        ├── train_3.csv\n', '        ├── test_0.csv\n', '        └── test_1.csv\n', '\n', '    Output:\n', '\n', ""        {'train': ['**/train[-._ 0-9]*', '**/*[-._ 0-9]train[-._ 0-9]*', '**/training[-._ 0-9]*', '**/*[-._ 0-9]training[-._ 0-9]*'],\n"", ""         'test': ['**/test[-._ 0-9]*', '**/*[-._ 0-9]test[-._ 0-9]*', '**/testing[-._ 0-9]*', '**/*[-._ 0-9]testing[-._ 0-9]*', ...]}\n"", '\n', '    Input:\n', '\n', '        my_dataset_repository/\n', '        ├── README.md\n', '        └── data/\n', '            ├── train/\n', '            │   ├── shard_0.csv\n', '            │   ├── shard_1.csv\n', '            │   ├── shard_2.csv\n', '            │   └── shard_3.csv\n', '            └── test/\n', '                ├── shard_0.csv\n', '                └── shard_1.csv\n', '\n', '    Output:\n', '\n', ""        {'train': ['**/train/**', '**/train[-._ 0-9]*/**', '**/*[-._ 0-9]train/**', '**/*[-._ 0-9]train[-._ 0-9]*/**', ...],\n"", ""         'test': ['**/test/**', '**/test[-._ 0-9]*/**', '**/*[-._ 0-9]test/**', '**/*[-._ 0-9]test[-._ 0-9]*/**', ...]}\n"", '\n', '    Input:\n', '\n', '        my_dataset_repository/\n', '        ├── README.md\n', '        └── data/\n', '            ├── train-00000-of-00003.csv\n', '            ├── train-00001-of-00003.csv\n', '            ├── train-00002-of-00003.csv\n', '            ├── test-00000-of-00001.csv\n', '            ├── random-00000-of-00003.csv\n', '            ├── random-00001-of-00003.csv\n', '            └── random-00002-of-00003.csv\n', '\n', '    Output:\n', '\n', ""        {'train': ['data/train-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*'],\n"", ""         'test': ['data/test-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*'],\n"", ""         'random': ['data/random-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*']}\n"", '\n', '    In order, it first tests if SPLIT_PATTERN_SHARDED works, otherwise it tests the patterns in ALL_DEFAULT_PATTERNS.\n', '    """"""\n']","['partial', '_get_data_files_patterns', 'EmptyDatasetError']",3
repos/datasets/src/datasets/data_files.py:get_metadata_patterns,get_metadata_patterns,function,5,21,21,250,11.9,0,0,"['base_path', 'download_config', '']","[' str', ' Optional[DownloadConfig] ', None]","[None, ' None', None]",502,"['    """"""\n', '    Get the supported metadata patterns from a local directory.\n', '    """"""\n']","['partial', '_get_metadata_files_patterns', 'FileNotFoundError']",3
repos/datasets/src/datasets/data_files.py:resolve_pattern,resolve_pattern,function,41,162,113,1596,9.85,0,7,"['pattern', 'base_path', 'allowed_extensions', 'download_config', '']","[' str', ' str', ' Optional[List[str]] ', ' Optional[DownloadConfig] ', None]","[None, None, ' None', ' None', None]",320,"['    """"""\n', '    Resolve the paths and URLs of the data files from the pattern passed by the user.\n', '\n', '    You can use patterns to resolve multiple local files. Here are a few examples:\n', '    - *.csv to match all the CSV files at the first level\n', '    - **.csv to match all the CSV files at any level\n', '    - data/* to match all the files inside ""data""\n', '    - data/** to match all the files inside ""data"" and its subdirectories\n', '\n', '    The patterns are resolved using the fsspec glob. In fsspec>=2023.12.0 this is equivalent to\n', ""    Python's glob.glob, Path.glob, Path.match and fnmatch where ** is unsupported with a prefix/suffix\n"", '    other than a forward slash /.\n', '\n', '    More generally:\n', ""    - '*' matches any character except a forward-slash (to match just the file or directory name)\n"", ""    - '**' matches any character including a forward-slash /\n"", '\n', '    Hidden files and directories (i.e. whose names start with a dot) are ignored, unless they are explicitly requested.\n', '    The same applies to special directories that start with a double underscore like ""__pycache__"".\n', '    You can still include one if the pattern explicilty mentions it:\n', '    - to include a hidden file: ""*/.hidden.txt"" or ""*/.*""\n', '    - to include a hidden directory: "".hidden/*"" or "".*/*""\n', '    - to include a special directory: ""__special__/*"" or ""__*/*""\n', '\n', '    Example::\n', '\n', '        >>> from datasets.data_files import resolve_pattern\n', '        >>> base_path = "".""\n', '        >>> resolve_pattern(""docs/**/*.py"", base_path)\n', ""        [/Users/mariosasko/Desktop/projects/datasets/docs/source/_config.py']\n"", '\n', '    Args:\n', '        pattern (str): Unix pattern or paths or URLs of the data files to resolve.\n', '            The paths can be absolute or relative to base_path.\n', '            Remote filesystems using fsspec are supported, e.g. with the hf:// protocol.\n', '        base_path (str): Base path to use when resolving relative paths.\n', '        allowed_extensions (Optional[list], optional): White-list of file extensions to use. Defaults to None (all extensions).\n', '            For example: allowed_extensions=["".csv"", "".json"", "".txt"", "".parquet""]\n', '    Returns:\n', '        List[str]: List of paths or URLs to the local or remote files that match the patterns.\n', '    """"""\n']","['is_relative_path', 'xjoin', 'is_local_path', '_prepare_path_and_storage_options', 'url_to_fs', 'set', 'isinstance', 'version.parse', 'filepath.startswith', 'fs.glob', '_is_inside_unrequested_special_dir', '_is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir', 'any', 'xbasename', 'len', 'list', 'logger.info', 'FileNotFoundError']",18
repos/datasets/src/datasets/data_files.py:sanitize_patterns,sanitize_patterns,function,11,115,79,881,7.66,2,4,"['patterns', 'List', 'str]']","[' Union[Dict', None, None]","[None, None, None]",128,"['    """"""\n', '    Take the data_files patterns from the user, and format them into a dictionary.\n', '    Each key is the name of the split, and each value is a list of data files patterns (paths or urls).\n', '    The default split is ""train"".\n', '\n', '    Returns:\n', '        patterns: dictionary of split_name -> list of patterns\n', '    """"""\n']","['isinstance', 'patterns.items', 'any', 'len', 'ValueError', 'str', 'sanitize_patterns']",7
repos/datasets/src/datasets/data_files.py:DataFilesDict,DataFilesDict,class,16,152,55,1821,11.98,4,3,[],[],[],642,[],[],0
repos/datasets/src/datasets/data_files.py:DataFilesList,DataFilesList,class,26,162,85,2009,12.4,1,3,[],[],[],555,[],[],0
repos/datasets/src/datasets/data_files.py:DataFilesPatternsDict,DataFilesPatternsDict,class,17,75,52,842,11.23,3,1,[],[],[],786,[],[],0
repos/datasets/src/datasets/data_files.py:DataFilesPatternsList,DataFilesPatternsList,class,22,100,76,1210,12.1,1,2,[],[],[],733,[],[],0
repos/datasets/src/datasets/data_files.py:EmptyDatasetError,EmptyDatasetError,class,0,1,1,4,4.0,0,0,[],[],[],35,[],[],0
repos/datasets/src/datasets/data_files.py:Url,Url,class,0,1,1,4,4.0,0,0,[],[],[],31,[],[],0
repos/datasets/src/datasets/data_files.py:DataFilesDict:filter_extensions,DataFilesDict:filter_extensions,method,8,11,10,119,10.82,1,0,"['self', 'extensions']","[None, ' List[str]']","[None, None]",726,[],"['type', 'self.items', 'data_files_list.filter_extensions']",3
repos/datasets/src/datasets/data_files.py:DataFilesDict:from_hf_repo,DataFilesDict:from_hf_repo,method,8,25,23,312,12.48,1,1,"['cls', 'patterns', 'Union[List[str]', 'DataFilesList]]', 'dataset_info', 'base_path', 'allowed_extensions', 'download_config', '']","[None, ' Dict[str', None, None, ' huggingface_hub.hf_api.DatasetInfo', ' Optional[str] ', ' Optional[List[str]] ', ' Optional[DownloadConfig] ', None]","[None, None, None, None, None, ' None', ' None', ' None', None]",581,[],"['cls', 'patterns.items', 'DataFilesList.from_hf_repo', 'isinstance']",4
repos/datasets/src/datasets/data_files.py:DataFilesDict:from_local_or_remote,DataFilesDict:from_local_or_remote,method,8,24,22,293,12.21,1,1,"['cls', 'patterns', 'Union[List[str]', 'DataFilesList]]', 'base_path', 'allowed_extensions', 'download_config', '']","[None, ' Dict[str', None, None, ' Optional[str] ', ' Optional[List[str]] ', ' Optional[DownloadConfig] ', None]","[None, None, None, None, ' None', ' None', ' None', None]",595,[],"['cls', 'patterns.items', 'DataFilesList.from_local_or_remote', 'isinstance']",4
repos/datasets/src/datasets/data_files.py:DataFilesDict:from_patterns,DataFilesDict:from_patterns,method,8,24,22,286,11.92,1,1,"['cls', 'patterns', 'Union[List[str]', 'DataFilesList]]', 'base_path', 'allowed_extensions', 'download_config', '']","[None, ' Dict[str', None, None, ' Optional[str] ', ' Optional[List[str]] ', ' Optional[DownloadConfig] ', None]","[None, None, None, None, ' None', ' None', ' None', None]",608,[],"['cls', 'patterns.items', 'DataFilesList.from_patterns', 'isinstance']",4
repos/datasets/src/datasets/data_files.py:DataFilesList:__add__,DataFilesList:__add__,method,2,5,5,78,15.6,0,0,"['self', 'other']","[None, None]","[None, None]",577,[],['DataFilesList'],1
repos/datasets/src/datasets/data_files.py:DataFilesList:__init__,DataFilesList:__init__,method,3,3,3,65,21.67,0,0,"['self', 'data_files', 'origin_metadata']","[None, ' List[str]', ' List[Tuple[str]]']","[None, None, None]",573,[],['super'],1
repos/datasets/src/datasets/data_files.py:DataFilesList:filter_extensions,DataFilesList:filter_extensions,method,5,20,16,208,10.4,0,0,"['self', 'extensions']","[None, ' List[str]']","[None, None]",633,[],"['re.compile', 'DataFilesList', 'pattern.match']",3
repos/datasets/src/datasets/data_files.py:DataFilesList:from_hf_repo,DataFilesList:from_hf_repo,method,5,11,11,217,19.73,0,0,"['cls', 'patterns', 'dataset_info', 'base_path', 'allowed_extensions', 'download_config', '']","[None, ' List[str]', ' huggingface_hub.hf_api.DatasetInfo', ' Optional[str] ', ' Optional[List[str]] ', ' Optional[DownloadConfig] ', None]","[None, None, None, ' None', ' None', ' None', None]",581,[],['cls.from_patterns'],1
repos/datasets/src/datasets/data_files.py:DataFilesList:from_local_or_remote,DataFilesList:from_local_or_remote,method,4,16,14,196,12.25,0,1,"['cls', 'patterns', 'base_path', 'allowed_extensions', 'download_config', '']","[None, ' List[str]', ' Optional[str] ', ' Optional[List[str]] ', ' Optional[DownloadConfig] ', None]","[None, None, ' None', ' None', ' None', None]",595,[],"['Path', 'cls.from_patterns']",2
repos/datasets/src/datasets/data_files.py:DataFilesList:from_patterns,DataFilesList:from_patterns,method,12,36,31,428,11.89,1,2,"['cls', 'patterns', 'base_path', 'allowed_extensions', 'download_config', '']","[None, ' List[str]', ' Optional[str] ', ' Optional[List[str]] ', ' Optional[DownloadConfig] ', None]","[None, None, ' None', ' None', ' None', None]",608,[],"['Path', 'data_files.extend', 'resolve_pattern', 'has_magic', '_get_origin_metadata', 'cls']",6
repos/datasets/src/datasets/data_files.py:DataFilesPatternsDict:filter_extensions,DataFilesPatternsDict:filter_extensions,method,8,11,10,137,12.45,1,0,"['self', 'extensions']","[None, ' List[str]']","[None, None]",817,[],"['type', 'self.items', 'data_files_patterns_list.filter_extensions']",3
repos/datasets/src/datasets/data_files.py:DataFilesPatternsDict:from_patterns,DataFilesPatternsDict:from_patterns,method,8,22,20,248,11.27,1,1,"['cls', 'patterns', 'List[str]]', 'allowed_extensions']","[None, ' Dict[str', None, ' Optional[List[str]] ']","[None, None, None, ' None']",608,[],"['cls', 'patterns.items', 'DataFilesPatternsList.from_patterns', 'isinstance']",4
repos/datasets/src/datasets/data_files.py:DataFilesPatternsDict:resolve,DataFilesPatternsDict:resolve,method,8,12,11,145,12.08,1,0,"['self', 'base_path', 'download_config', '']","[None, ' str', ' Optional[DownloadConfig] ', None]","[None, None, ' None', None]",757,"['    """"""\n', '    Dict of split_name -> list of data files patterns (absolute local paths or URLs).\n', '    """"""\n']","['DataFilesDict', 'self.items', 'data_files_patterns_list.resolve']",3
repos/datasets/src/datasets/data_files.py:DataFilesPatternsList:__add__,DataFilesPatternsList:__add__,method,2,5,5,84,16.8,0,0,"['self', 'other']","[None, None]","[None, None]",748,[],['DataFilesList'],1
repos/datasets/src/datasets/data_files.py:DataFilesPatternsList:__init__,DataFilesPatternsList:__init__,method,3,3,3,69,23.0,0,0,"['self', 'patterns', 'allowed_extensions', '']","[None, ' List[str]', ' List[Optional[List[str]]]', None]","[None, None, None, None]",740,[],['super'],1
repos/datasets/src/datasets/data_files.py:DataFilesPatternsList:filter_extensions,DataFilesPatternsList:filter_extensions,method,2,10,10,113,11.3,0,0,"['self', 'extensions']","[None, ' List[str]']","[None, None]",780,[],['DataFilesPatternsList'],1
repos/datasets/src/datasets/data_files.py:DataFilesPatternsList:from_patterns,DataFilesPatternsList:from_patterns,method,2,4,4,54,13.5,0,0,"['cls', 'patterns', 'allowed_extensions']","[None, ' List[str]', ' Optional[List[str]] ']","[None, None, ' None']",752,[],"['cls', 'len']",2
repos/datasets/src/datasets/data_files.py:DataFilesPatternsList:resolve,DataFilesPatternsList:resolve,method,13,38,33,482,12.68,1,2,"['self', 'base_path', 'download_config', '']","[None, ' str', ' Optional[DownloadConfig] ', None]","[None, None, ' None', None]",757,[],"['Path', 'zip', 'data_files.extend', 'resolve_pattern', 'has_magic', '_get_origin_metadata', 'DataFilesList']",7
repos/datasets/src/datasets/dataset_dict.py:DatasetDict,DatasetDict,class,245,1994,759,21767,10.92,42,37,[],[],[],46,[],[],0
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict,IterableDatasetDict,class,19,262,109,2713,10.35,4,0,[],[],[],1869,[],[],0
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:__enter__,DatasetDict:__enter__,method,1,2,2,10,5.0,0,0,['self'],[None],[None],62,[],[],0
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:__exit__,DatasetDict:__exit__,method,6,14,11,122,8.71,1,2,"['self', 'exc_type', 'exc_val', 'exc_tb']","[None, None, None, None]","[None, None, None, None]",65,[],"['self.values', 'hasattr']",2
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:__getitem__,DatasetDict:__getitem__,method,7,49,42,433,8.84,0,2,"['self', 'k']","[None, None]","[None, None]",73,[],"['isinstance', 'len', 'super', 'list', 'KeyError']",5
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:__repr__,DatasetDict:__repr__,method,4,18,16,120,6.67,0,0,['self'],[None],[None],261,[],"['self.items', 're.sub', 'f""DatasetDict']",3
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:_check_values_features,DatasetDict:_check_values_features,method,7,33,30,281,8.52,1,1,['self'],[None],[None],54,[],"['list', 'zip', 'ValueError']",3
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:_check_values_type,DatasetDict:_check_values_type,method,4,21,19,151,7.19,1,1,['self'],[None],[None],49,[],"['self.values', 'isinstance', 'TypeError']",3
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:align_labels_with_mapping,DatasetDict:align_labels_with_mapping,method,6,14,14,158,11.29,1,0,"['self', 'label2id', 'label_column']","[None, ' Dict', ' str']","[None, None, None]",1556,[],"['self._check_values_type', 'DatasetDict', 'dataset.align_labels_with_mapping', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:cache_files,DatasetDict:cache_files,method,6,9,9,81,9.0,1,0,['self'],[None],[None],103,"['        """"""The cache files containing the Apache Arrow table backing each split.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.cache_files\n', ""        {'test': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-test.arrow'}],\n"", ""         'train': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-train.arrow'}],\n"", ""         'validation': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'self.items']",2
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:cast,DatasetDict:cast,method,3,9,9,106,11.78,0,0,"['self', 'features']","[None, ' Features']","[None, None]",266,"['        """"""\n', '        Cast the dataset to a new set of features.\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            features ([`Features`]):\n', '                New features to cast the dataset to.\n', '                The name and order of the fields in the features must match the current column names.\n', '                The type of the data must also be convertible from one type to the other.\n', '                For non-trivial conversion, e.g. `string` <-> `ClassLabel` you should use [`~DatasetDict.map`] to update the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds[""train""].features\n', ""        {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", '        >>> new_features = ds[""train""].features.copy()\n', ""        >>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n"", ""        >>> new_features['text'] = Value('large_string')\n"", '        >>> ds = ds.cast(new_features)\n', '        >>> ds[""train""].features\n', ""        {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n"", ""         'text': Value(dtype='large_string', id=None)}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.cast', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:cast_column,DatasetDict:cast_column,method,3,10,10,125,12.5,0,0,"['self', 'column', 'feature']","[None, ' str', None]","[None, None, None]",298,"['        """"""Cast column to feature for decoding.\n', '\n', '        Args:\n', '            column (`str`):\n', '                Column name.\n', '            feature ([`Feature`]):\n', '                Target feature.\n', '\n', '        Returns:\n', '            [`DatasetDict`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds[""train""].features\n', ""        {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", ""        >>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n"", '        >>> ds[""train""].features\n', ""        {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.cast_column', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:class_encode_column,DatasetDict:class_encode_column,method,3,12,12,147,12.25,0,0,"['self', 'column', 'include_nulls']","[None, ' str', ' bool ']","[None, None, ' False']",488,"['        """"""Casts the given column as [`~datasets.features.ClassLabel`] and updates the tables.\n', '\n', '        Args:\n', '            column (`str`):\n', '                The name of the column to cast.\n', '            include_nulls (`bool`, defaults to `False`):\n', '                Whether to include null values in the class labels. If `True`, the null values will be encoded as the `""None""` class label.\n', '\n', '                <Added version=""1.14.2""/>\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""boolq"")\n', '        >>> ds[""train""].features\n', ""        {'answer': Value(dtype='bool', id=None),\n"", ""         'passage': Value(dtype='string', id=None),\n"", ""         'question': Value(dtype='string', id=None)}\n"", '        >>> ds = ds.class_encode_column(""answer"")\n', '        >>> ds[""train""].features\n', ""        {'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n"", ""         'passage': Value(dtype='string', id=None),\n"", ""         'question': Value(dtype='string', id=None)}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.class_encode_column', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:cleanup_cache_files,DatasetDict:cleanup_cache_files,method,3,9,9,91,10.11,0,0,['self'],[None],[None],242,"['        """"""Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.\n', '        Be careful when running this command that no other process is currently using other cache files.\n', '\n', '        Return:\n', '            `Dict` with the number of removed files for each split\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.cleanup_cache_files()\n', ""        {'test': 0, 'train': 0, 'validation': 0}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'dataset.cleanup_cache_files', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:column_names,DatasetDict:column_names,method,6,9,9,82,9.11,1,0,['self'],[None],[None],153,"['        """"""Names of the columns in each split of the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.column_names\n', ""        {'test': ['text', 'label'],\n"", ""         'train': ['text', 'label'],\n"", ""         'validation': ['text', 'label']}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'self.items']",2
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:data,DatasetDict:data,method,6,9,9,74,8.22,1,0,['self'],[None],[None],88,"['        """"""The Apache Arrow tables backing each split.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.data\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'self.items']",2
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:filter,DatasetDict:filter,method,9,38,35,494,13.0,2,1,"['self', 'function', 'with_indices', 'with_rank', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'keep_in_memory', 'load_from_cache_file', 'cache_file_names', 'Optional[str]]] ', 'writer_batch_size', 'fn_kwargs', 'num_proc', 'desc', '']","[None, ' Optional[Callable] ', ' bool ', ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' bool ', ' Optional[bool] ', ' Optional[Dict[str', None, ' Optional[int] ', ' Optional[dict] ', ' Optional[int] ', ' Optional[str] ', None]","[None, ' None', ' False', ' False', None, ' None', ' False', ' 1000', ' False', ' None', None, ' None', ' 1000', ' None', ' None', ' None', None]",893,"['        """"""Apply a filter function to all the elements in the table in batches\n', '        and update the table so that the dataset only includes examples according to the filter function.\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            function (`Callable`): Callable with one of the following signatures:\n', '\n', '                - `function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False` and `with_rank=False`\n', '                - `function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '                - `function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False` and `with_rank=False`\n', '                - `function(batch: Dict[str, List], *extra_args) -> List[bool]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '\n', '                If no function is provided, defaults to an always `True` function: `lambda x: True`.\n', '            with_indices (`bool`, defaults to `False`):\n', '                Provide example indices to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example, idx[, rank]): ...`.\n', '            with_rank (`bool`, defaults to `False`):\n', '                Provide process rank to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example[, idx], rank): ...`.\n', '            input_columns (`[Union[str, List[str]]]`, *optional*, defaults to `None`):\n', '                The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`):\n', '                Provide batch of examples to `function`.\n', '            batch_size (`int`, *optional*, defaults to `1000`):\n', '                Number of examples per batch provided to `function` if `batched=True`\n', '                `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to `function`.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '                You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            fn_kwargs (`Dict`, *optional*, defaults to `None`):\n', '                Keyword arguments to be passed to `function`\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', ""                Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '            desc (`str`, *optional*, defaults to `None`):\n', '                Meaningful description to be displayed alongside with the progress bar while filtering examples.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.filter(lambda x: x[""label""] == 1)\n', '        DatasetDict({\n', '            train: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 4265\n', '            })\n', '            validation: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 533\n', '            })\n', '            test: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 533\n', '            })\n', '        })\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.filter', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:flatten,DatasetDict:flatten,method,3,9,9,111,12.33,0,0,"['self', 'max_depth']","[None, None]","[None, '16']",186,"['        """"""Flatten the Apache Arrow Table of each split (nested features are flatten).\n', '        Each column with a struct type is flattened into one column per struct field.\n', '        Other columns are left unchanged.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""squad"")\n', '        >>> ds[""train""].features\n', ""        {'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n"", ""         'context': Value(dtype='string', id=None),\n"", ""         'id': Value(dtype='string', id=None),\n"", ""         'question': Value(dtype='string', id=None),\n"", ""         'title': Value(dtype='string', id=None)}\n"", '        >>> ds.flatten()\n', '        DatasetDict({\n', '            train: Dataset({\n', ""                features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n"", '                num_rows: 87599\n', '            })\n', '            validation: Dataset({\n', ""                features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n"", '                num_rows: 10570\n', '            })\n', '        })\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.flatten', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:flatten_indices,DatasetDict:flatten_indices,method,9,32,29,379,11.84,2,1,"['self', 'keep_in_memory', 'cache_file_names', 'Optional[str]]] ', 'writer_batch_size', 'features', 'disable_nullable', 'num_proc', 'new_fingerprint', '']","[None, ' bool ', ' Optional[Dict[str', None, ' Optional[int] ', ' Optional[Features] ', ' bool ', ' Optional[int] ', ' Optional[str] ', None]","[None, ' False', None, ' None', ' 1000', ' None', ' False', ' None', ' None', None]",1003,"['        """"""Create and cache a new Dataset by flattening the indices mapping.\n', '\n', '        Args:\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            cache_file_names (`Dict[str, str]`, *optional*, default `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '                You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            features (`Optional[datasets.Features]`, defaults to `None`):\n', '                Use a specific [`Features`] to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (`bool`, defaults to `False`):\n', '                Allow null values in the table.\n', '            num_proc (`int`, optional, default `None`):\n', '                Max number of processes when generating cache. Already cached shards are loaded sequentially\n', '            new_fingerprint (`str`, *optional*, defaults to `None`):\n', '                The new fingerprint of the dataset after transform.\n', '                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.flatten_indices', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:formatted_as,DatasetDict:formatted_as,method,15,51,29,542,10.63,5,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",521,"['        """"""To be used in a `with` statement. Set `__getitem__` return format (type and columns).\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            type (`str`, *optional*):\n', ""                Output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n"", '                `None` means `__getitem__` returns python objects (default).\n', '            columns (`List[str]`, *optional*):\n', '                Columns to format in the output.\n', '                `None` means `__getitem__` returns all columns (default).\n', '            output_all_columns (`bool`, defaults to False):\n', '                Keep un-formatted columns as well in the output (as python objects).\n', '            **format_kwargs (additional keyword arguments):\n', '                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '        """"""\n']","['self._check_values_type', 'self.items', 'self.set_format', 'dataset.set_format']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:from_csv,DatasetDict:from_csv,method,4,12,12,157,13.08,0,0,"['path_or_paths', 'PathLike]', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[' Dict[str', None, ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' False', None, None]",1384,"['        """"""Create [`DatasetDict`] from CSV file(s).\n', '\n', '        Args:\n', '            path_or_paths (`dict` of path-like):\n', '                Path(s) of the CSV file(s).\n', '            features ([`Features`], *optional*):\n', '                Dataset features.\n', '            cache_dir (str, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to [`pandas.read_csv`].\n', '\n', '        Returns:\n', '            [`DatasetDict`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import DatasetDict\n', ""        >>> ds = DatasetDict.from_csv({'train': 'path/to/dataset.csv'})\n"", '        ```\n', '        """"""\n']",['CsvDatasetReader'],1
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:from_json,DatasetDict:from_json,method,4,12,12,160,13.33,0,0,"['path_or_paths', 'PathLike]', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[' Dict[str', None, ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' False', None, None]",1423,"['        """"""Create [`DatasetDict`] from JSON Lines file(s).\n', '\n', '        Args:\n', '            path_or_paths (`path-like` or list of `path-like`):\n', '                Path(s) of the JSON Lines file(s).\n', '            features ([`Features`], *optional*):\n', '                Dataset features.\n', '            cache_dir (str, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to [`JsonConfig`].\n', '\n', '        Returns:\n', '            [`DatasetDict`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import DatasetDict\n', ""        >>> ds = DatasetDict.from_json({'train': 'path/to/dataset.json'})\n"", '        ```\n', '        """"""\n']",['JsonDatasetReader'],1
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:from_parquet,DatasetDict:from_parquet,method,4,13,13,191,14.69,0,0,"['path_or_paths', 'PathLike]', 'features', 'cache_dir', 'keep_in_memory', 'columns', '**kwargs', '']","[' Dict[str', None, ' Optional[Features] ', ' str ', ' bool ', ' Optional[List[str]] ', None, None]","[None, None, ' None', ' None', ' False', ' None', None, None]",1462,"['        """"""Create [`DatasetDict`] from Parquet file(s).\n', '\n', '        Args:\n', '            path_or_paths (`dict` of path-like):\n', '                Path(s) of the CSV file(s).\n', '            features ([`Features`], *optional*):\n', '                Dataset features.\n', '            cache_dir (`str`, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            columns (`List[str]`, *optional*):\n', '                If not `None`, only these columns will be read from the file.\n', ""                A column name may be a prefix of a nested field, e.g. 'a' will select\n"", ""                'a.b', 'a.c', and 'a.d.e'.\n"", '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to [`ParquetConfig`].\n', '\n', '        Returns:\n', '            [`DatasetDict`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import DatasetDict\n', ""        >>> ds = DatasetDict.from_parquet({'train': 'path/to/dataset/parquet'})\n"", '        ```\n', '        """"""\n']",['ParquetDatasetReader'],1
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:from_text,DatasetDict:from_text,method,4,12,12,160,13.33,0,0,"['path_or_paths', 'PathLike]', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[' Dict[str', None, ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, ' None', ' None', ' False', None, None]",1511,"['        """"""Create [`DatasetDict`] from text file(s).\n', '\n', '        Args:\n', '            path_or_paths (`dict` of path-like):\n', '                Path(s) of the text file(s).\n', '            features ([`Features`], *optional*):\n', '                Dataset features.\n', '            cache_dir (`str`, *optional*, defaults to `""~/.cache/huggingface/datasets""`):\n', '                Directory to cache data.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Whether to copy the data in-memory.\n', '            **kwargs (additional keyword arguments):\n', '                Keyword arguments to be passed to [`TextConfig`].\n', '\n', '        Returns:\n', '            [`DatasetDict`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import DatasetDict\n', ""        >>> ds = DatasetDict.from_text({'train': 'path/to/dataset.txt'})\n"", '        ```\n', '        """"""\n']",['TextDatasetReader'],1
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:load_from_disk,DatasetDict:load_from_disk,method,25,126,97,1420,11.27,1,3,"['dataset_dict_path', 'fs', 'keep_in_memory', 'storage_options', '']","[' PathLike', None, ' Optional[bool] ', ' Optional[dict] ', None]","[None, '""deprecated""', ' None', ' None', None]",1307,"['        """"""\n', '        Load a dataset that was previously saved using [`save_to_disk`] from a filesystem using `fsspec.spec.AbstractFileSystem`.\n', '\n', '        Args:\n', '            dataset_dict_path (`str`):\n', '                Path (e.g. `""dataset/train""`) or remote URI (e.g. `""s3//my-bucket/dataset/train""`)\n', '                of the dataset dict directory where the dataset dict will be loaded from.\n', '            fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n', '                Instance of the remote filesystem where the dataset will be saved to.\n', '\n', '                <Deprecated version=""2.8.0"">\n', '\n', '                `fs` was deprecated in version 2.8.0 and will be removed in 3.0.0.\n', '                Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`\n', '\n', '                </Deprecated>\n', '\n', '            keep_in_memory (`bool`, defaults to `None`):\n', '                Whether to copy the dataset in-memory. If `None`, the\n', '                dataset will not be copied in-memory unless explicitly enabled by setting\n', '                `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n', '                [improve performance](../cache#improve-performance) section.\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.8.0""/>\n', '\n', '        Returns:\n', '            [`DatasetDict`]\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> ds = load_from_disk('path/to/dataset/directory')\n"", '        ```\n', '        """"""\n']","['warnings.warn', 'url_to_fs', 'posixpath.join', 'fs.isfile', 'FileNotFoundError', 'fs.open', 'json.load', 'DatasetDict', 'Dataset.load_from_disk']",9
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:map,DatasetDict:map,method,9,42,39,609,14.5,2,1,"['self', 'function', 'with_indices', 'with_rank', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'drop_last_batch', 'remove_columns', 'List[str]]] ', 'keep_in_memory', 'load_from_cache_file', 'cache_file_names', 'Optional[str]]] ', 'writer_batch_size', 'features', 'disable_nullable', 'fn_kwargs', 'num_proc', 'desc', '']","[None, ' Optional[Callable] ', ' bool ', ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[bool] ', ' Optional[Dict[str', None, ' Optional[int] ', ' Optional[Features] ', ' bool ', ' Optional[dict] ', ' Optional[int] ', ' Optional[str] ', None]","[None, ' None', ' False', ' False', None, ' None', ' False', ' 1000', ' False', None, ' None', ' False', ' None', None, ' None', ' 1000', ' None', ' False', ' None', ' None', ' None', None]",765,"['        """"""Apply a function to all the elements in the table (individually or in batches)\n', '        and update the table (if function does updated examples).\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            function (`callable`): with one of the following signature:\n', '                - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False`\n', '                - `function(example: Dict[str, Any], indices: int) -> Dict[str, Any]` if `batched=False` and `with_indices=True`\n', '                - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False`\n', '                - `function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if `batched=True` and `with_indices=True`\n', '\n', '                For advanced usage, the function can also return a `pyarrow.Table`.\n', '                Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n', '\n', '            with_indices (`bool`, defaults to `False`):\n', '                Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n', '            with_rank (`bool`, defaults to `False`):\n', '                Provide process rank to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example[, idx], rank): ...`.\n', '            input_columns (`[Union[str, List[str]]]`, *optional*, defaults to `None`):\n', '                The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`):\n', '                Provide batch of examples to `function`.\n', '            batch_size (`int`, *optional*, defaults to `1000`):\n', '                Number of examples per batch provided to `function` if `batched=True`,\n', '                `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to `function`.\n', '            drop_last_batch (`bool`, defaults to `False`):\n', '                Whether a last batch smaller than the batch_size should be\n', '                dropped instead of being processed by the function.\n', '            remove_columns (`[Union[str, List[str]]]`, *optional*, defaults to `None`):\n', '                Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '                You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (`int`, default `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            features (`[datasets.Features]`, *optional*, defaults to `None`):\n', '                Use a specific [`Features`] to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (`bool`, defaults to `False`):\n', '                Disallow null values in the table.\n', '            fn_kwargs (`Dict`, *optional*, defaults to `None`):\n', '                Keyword arguments to be passed to `function`\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', ""                Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '            desc (`str`, *optional*, defaults to `None`):\n', '                Meaningful description to be displayed alongside with the progress bar while mapping examples.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> def add_prefix(example):\n', '        ...     example[""text""] = ""Review: "" + example[""text""]\n', '        ...     return example\n', '        >>> ds = ds.map(add_prefix)\n', '        >>> ds[""train""][0:3][""text""]\n', '        [\'Review: the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\',\n', '         \'Review: the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\',\n', ""         'Review: effective but too-tepid biopic']\n"", '\n', '        # process a batch of examples\n', '        >>> ds = ds.map(lambda example: tokenizer(example[""text""]), batched=True)\n', '        # set number of processors\n', '        >>> ds = ds.map(add_prefix, num_proc=4)\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.map', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:num_columns,DatasetDict:num_columns,method,6,9,9,81,9.0,1,0,['self'],[None],[None],121,"['        """"""Number of columns in each split of the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.num_columns\n', ""        {'test': 2, 'train': 2, 'validation': 2}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'self.items']",2
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:num_rows,DatasetDict:num_rows,method,6,9,9,78,8.67,1,0,['self'],[None],[None],137,"['        """"""Number of rows in each split of the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.num_rows\n', ""        {'test': 1066, 'train': 8530, 'validation': 1066}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'self.items']",2
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:prepare_for_task,DatasetDict:prepare_for_task,method,3,10,10,116,11.6,0,0,"['self', 'task', 'TaskTemplate]', 'id']","[None, ' Union[str', None, ' int ']","[None, None, None, ' 0']",1551,[],"['self._check_values_type', 'DatasetDict', 'dataset.prepare_for_task', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:push_to_hub,DatasetDict:push_to_hub,method,122,506,322,6202,12.26,7,17,"['self', 'repo_id', 'config_name', 'set_default', 'data_dir', 'commit_message', 'commit_description', 'private', 'token', 'revision', 'branch', 'create_pr', 'max_shard_size', 'str]] ', 'num_shards', 'int]] ', 'embed_external_files', '']","[None, None, ' str ', ' Optional[bool] ', ' Optional[str] ', ' Optional[str] ', ' Optional[str] ', ' Optional[bool] ', ' Optional[str] ', ' Optional[str] ', None, ' Optional[bool] ', ' Optional[Union[int', None, ' Optional[Dict[str', None, ' bool ', None]","[None, None, ' ""default""', ' None', ' None', ' None', ' None', ' False', ' None', ' None', '""deprecated""', ' False', None, ' None', None, ' None', ' True', None]",1565,"['        """"""Pushes the [`DatasetDict`] to the hub as a Parquet dataset.\n', '        The [`DatasetDict`] is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n', '\n', '        Each dataset split will be pushed independently. The pushed dataset will keep the original split names.\n', '\n', '        The resulting Parquet files are self-contained by default: if your dataset contains [`Image`] or [`Audio`]\n', '        data, the Parquet files will store the bytes of your images or audio files.\n', '        You can disable this by setting `embed_external_files` to False.\n', '\n', '        Args:\n', '            repo_id (`str`):\n', '                The ID of the repository to push to in the following format: `<user>/<dataset_name>` or\n', '                `<org>/<dataset_name>`. Also accepts `<dataset_name>`, which will default to the namespace\n', '                of the logged-in user.\n', '            config_name (`str`):\n', '                Configuration name of a dataset. Defaults to ""default"".\n', '            set_default (`bool`, *optional*):\n', '                Whether to set this configuration as the default one. Otherwise, the default configuration is the one\n', '                named ""default"".\n', '            data_dir (`str`, *optional*):\n', '                Directory name that will contain the uploaded data files. Defaults to the `config_name` if different\n', '                from ""default"", else ""data"".\n', '\n', '                <Added version=""2.17.0""/>\n', '            commit_message (`str`, *optional*):\n', '                Message to commit while pushing. Will default to `""Upload dataset""`.\n', '            commit_description (`str`, *optional*):\n', '                Description of the commit that will be created.\n', '                Additionally, description of the PR if a PR is created (`create_pr` is True).\n', '\n', '                <Added version=""2.16.0""/>\n', '            private (`bool`, *optional*):\n', '                Whether the dataset repository should be set to private or not. Only affects repository creation:\n', '                a repository that already exists will not be affected by that parameter.\n', '            token (`str`, *optional*):\n', '                An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n', '                to the token saved locally when logging in with `huggingface-cli login`. Will raise an error\n', '                if no token is passed and the user is not logged-in.\n', '            revision (`str`, *optional*):\n', '                Branch to push the uploaded files to. Defaults to the `""main""` branch.\n', '\n', '                <Added version=""2.15.0""/>\n', '            branch (`str`, *optional*):\n', '                The git branch on which to push the dataset. This defaults to the default branch as specified\n', '                in your repository, which defaults to `""main""`.\n', '\n', '                <Deprecated version=""2.15.0"">\n', '\n', '                `branch` was deprecated in favor of `revision` in version 2.15.0 and will be removed in 3.0.0.\n', '\n', '                </Deprecated>\n', '            create_pr (`bool`, *optional*, defaults to `False`):\n', '                Whether to create a PR with the uploaded files or directly commit.\n', '\n', '                <Added version=""2.15.0""/>\n', '            max_shard_size (`int` or `str`, *optional*, defaults to `""500MB""`):\n', '                The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n', '                (like `""500MB""` or `""1GB""`).\n', '            num_shards (`Dict[str, int]`, *optional*):\n', '                Number of shards to write. By default, the number of shards depends on `max_shard_size`.\n', '                Use a dictionary to define a different num_shards for each split.\n', '\n', '                <Added version=""2.8.0""/>\n', '            embed_external_files (`bool`, defaults to `True`):\n', '                Whether to embed file bytes in the shards.\n', '                In particular, this will do the following before the push for the fields of type:\n', '\n', '                - [`Audio`] and [`Image`] removes local path information and embed file content in the Parquet files.\n', '\n', '        Return:\n', '            huggingface_hub.CommitInfo\n', '\n', '        Example:\n', '\n', '        ```python\n', '        >>> dataset_dict.push_to_hub(""<organization>/<dataset_id>"")\n', '        >>> dataset_dict.push_to_hub(""<organization>/<dataset_id>"", private=True)\n', '        >>> dataset_dict.push_to_hub(""<organization>/<dataset_id>"", max_shard_size=""1GB"")\n', '        >>> dataset_dict.push_to_hub(""<organization>/<dataset_id>"", num_shards={""train"": 1024, ""test"": 8})\n', '        ```\n', '\n', '        If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):\n', '\n', '        ```python\n', '        >>> english_dataset.push_to_hub(""<organization>/<dataset_id>"", ""en"")\n', '        >>> french_dataset.push_to_hub(""<organization>/<dataset_id>"", ""fr"")\n', '        >>> # later\n', '        >>> english_dataset = load_dataset(""<organization>/<dataset_id>"", ""en"")\n', '        >>> french_dataset = load_dataset(""<organization>/<dataset_id>"", ""fr"")\n', '        ```\n', '        """"""\n']","['isinstance', 'ValueError', 'warnings.warn', 'self._check_values_type', 'self._check_values_features', 'next', 'SplitDict', 'self.keys', 're.match', 'HfApi', 'api.create_repo', 'api.create_branch', 'logger.info', 'SplitInfo', 'num_examples=len', 'api.list_repo_tree', 'deletions.append', 'fnmatch.fnmatch', 'PUSH_TO_HUB_WITHOUT_METADATA_CONFIGS_SPLIT_PATTERN_SHARDED.replace', 'string_to_dict', 'glob_pattern_to_regex', 'repo_splits.append', 'api.hf_hub_download', 'DatasetCard.load', 'MetadataConfigs.from_dataset_card_data', 'DatasetCardData', 'MetadataConfigs', 'metadata_configs.get_default_config_name', 'open', 'json.load', 'asdict', 'BytesIO', 'buffer.write', 'additions.append', 'CommitOperationAdd', 'DatasetInfosDict', 'DatasetCard', 'path_or_fileobj=str', 'len', 'api.create_commit', 'math.ceil', 'range']",42
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:remove_columns,DatasetDict:remove_columns,method,3,9,9,124,13.78,0,0,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",327,"['        """"""\n', '        Remove one or several column(s) from each split in the dataset\n', '        and the features associated to the column(s).\n', '\n', '        The transformation is applied to all the splits of the dataset dictionary.\n', '\n', '        You can also remove a column using [`~DatasetDict.map`] with `remove_columns` but the present method\n', ""        doesn't copy the data of the remaining columns and is thus faster.\n"", '\n', '        Args:\n', '            column_names (`Union[str, List[str]]`):\n', '                Name of the column(s) to remove.\n', '\n', '        Returns:\n', '            [`DatasetDict`]: A copy of the dataset object without the columns to remove.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds = ds.remove_columns(""label"")\n', '        DatasetDict({\n', '            train: Dataset({\n', ""                features: ['text'],\n"", '                num_rows: 8530\n', '            })\n', '            validation: Dataset({\n', ""                features: ['text'],\n"", '                num_rows: 1066\n', '            })\n', '            test: Dataset({\n', ""                features: ['text'],\n"", '                num_rows: 1066\n', '            })\n', '        })\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.remove_columns', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:rename_column,DatasetDict:rename_column,method,6,14,14,176,12.57,1,0,"['self', 'original_column_name', 'new_column_name']","[None, ' str', ' str']","[None, None, None]",369,"['        """"""\n', '        Rename a column in the dataset and move the features associated to the original column under the new column name.\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        You can also rename a column using [`~DatasetDict.map`] with `remove_columns` but the present method:\n', '            - takes care of moving the original features under the new column name.\n', ""            - doesn't copy the data to a new dataset and is thus much faster.\n"", '\n', '        Args:\n', '            original_column_name (`str`):\n', '                Name of the column to rename.\n', '            new_column_name (`str`):\n', '                New name for the column.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds = ds.rename_column(""label"", ""label_new"")\n', '        DatasetDict({\n', '            train: Dataset({\n', ""                features: ['text', 'label_new'],\n"", '                num_rows: 8530\n', '            })\n', '            validation: Dataset({\n', ""                features: ['text', 'label_new'],\n"", '                num_rows: 1066\n', '            })\n', '            test: Dataset({\n', ""                features: ['text', 'label_new'],\n"", '                num_rows: 1066\n', '            })\n', '        })\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.rename_column', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:rename_columns,DatasetDict:rename_columns,method,3,9,9,128,14.22,0,0,"['self', 'column_mapping', 'str]']","[None, ' Dict[str', None]","[None, None, None]",414,"['        """"""\n', '        Rename several columns in the dataset, and move the features associated to the original columns under\n', '        the new column names.\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            column_mapping (`Dict[str, str]`):\n', '                A mapping of columns to rename to their new names.\n', '\n', '        Returns:\n', '            [`DatasetDict`]: A copy of the dataset with renamed columns.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', ""        >>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n"", '        DatasetDict({\n', '            train: Dataset({\n', ""                features: ['text_new', 'label_new'],\n"", '                num_rows: 8530\n', '            })\n', '            validation: Dataset({\n', ""                features: ['text_new', 'label_new'],\n"", '                num_rows: 1066\n', '            })\n', '            test: Dataset({\n', ""                features: ['text_new', 'label_new'],\n"", '                num_rows: 1066\n', '            })\n', '        })\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.rename_columns', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:reset_format,DatasetDict:reset_format,method,4,6,6,73,12.17,1,0,['self'],[None],[None],603,"['        """"""Reset `__getitem__` return format to python objects and all columns.\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Same as `self.set_format()`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n', '        >>> ds = ds.map(lambda x: tokenizer(x[""text""], truncation=True, padding=True), batched=True)\n', '        >>> ds.set_format(type=""numpy"", columns=[\'input_ids\', \'token_type_ids\', \'attention_mask\', \'label\'])\n', '        >>> ds[""train""].format\n', ""        {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': 'numpy'}\n"", '        >>> ds.reset_format()\n', '        >>> ds[""train""].format\n', ""        {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': None}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'self.values', 'dataset.set_format']",3
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:save_to_disk,DatasetDict:save_to_disk,method,25,99,88,923,9.32,2,2,"['self', 'dataset_dict_path', 'fs', 'max_shard_size', 'int]] ', 'num_shards', 'int]] ', 'num_proc', 'storage_options', '']","[None, ' PathLike', None, ' Optional[Union[str', None, ' Optional[Dict[str', None, ' Optional[int] ', ' Optional[dict] ', None]","[None, None, '""deprecated""', None, ' None', None, ' None', ' None', ' None', None]",1216,"['        """"""\n', '        Saves a dataset dict to a filesystem using `fsspec.spec.AbstractFileSystem`.\n', '\n', '        For [`Image`] and [`Audio`] data:\n', '\n', '        All the Image() and Audio() data are stored in the arrow files.\n', '        If you want to store paths or urls, please use the Value(""string"") type.\n', '\n', '        Args:\n', '            dataset_dict_path (`str`):\n', '                Path (e.g. `dataset/train`) or remote URI\n', '                (e.g. `s3://my-bucket/dataset/train`) of the dataset dict directory where the dataset dict will be\n', '                saved to.\n', '            fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n', '                Instance of the remote filesystem where the dataset will be saved to.\n', '\n', '                <Deprecated version=""2.8.0"">\n', '\n', '                `fs` was deprecated in version 2.8.0 and will be removed in 3.0.0.\n', '                Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`\n', '\n', '                </Deprecated>\n', '\n', '            max_shard_size (`int` or `str`, *optional*, defaults to `""500MB""`):\n', '                The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n', '                (like `""50MB""`).\n', '            num_shards (`Dict[str, int]`, *optional*):\n', '                Number of shards to write. By default the number of shards depends on `max_shard_size` and `num_proc`.\n', '                You need to provide the number of shards for each dataset in the dataset dictionary.\n', '                Use a dictionary to define a different num_shards for each split.\n', '\n', '                <Added version=""2.8.0""/>\n', '            num_proc (`int`, *optional*, default `None`):\n', '                Number of processes when downloading and generating the dataset locally.\n', '                Multiprocessing is disabled by default.\n', '\n', '                <Added version=""2.8.0""/>\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.8.0""/>\n', '\n', '        Example:\n', '\n', '        ```python\n', '        >>> dataset_dict.save_to_disk(""path/to/dataset/directory"")\n', '        >>> dataset_dict.save_to_disk(""path/to/dataset/directory"", max_shard_size=""1GB"")\n', '        >>> dataset_dict.save_to_disk(""path/to/dataset/directory"", num_shards={""train"": 1024, ""test"": 8})\n', '        ```\n', '        """"""\n']","['warnings.warn', 'url_to_fs', 'isinstance', 'ValueError', 'fs.makedirs', 'fs.open', 'json.dump', 'list', 'self.items', 'dataset.save_to_disk', 'posixpath.join']",11
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:select_columns,DatasetDict:select_columns,method,3,9,9,124,13.78,0,0,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",452,"['        """"""Select one or several column(s) from each split in the dataset and\n', '        the features associated to the column(s).\n', '\n', '        The transformation is applied to all the splits of the dataset\n', '        dictionary.\n', '\n', '        Args:\n', '            column_names (`Union[str, List[str]]`):\n', '                Name of the column(s) to keep.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.select_columns(""text"")\n', '        DatasetDict({\n', '            train: Dataset({\n', ""                features: ['text'],\n"", '                num_rows: 8530\n', '            })\n', '            validation: Dataset({\n', ""                features: ['text'],\n"", '                num_rows: 1066\n', '            })\n', '            test: Dataset({\n', ""                features: ['text'],\n"", '                num_rows: 1066\n', '            })\n', '        })\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.select_columns', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:set_format,DatasetDict:set_format,method,4,9,9,152,16.89,1,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",557,"['        """"""Set `__getitem__` return format (type and columns).\n', '        The format is set for every dataset in the dataset dictionary.\n', '\n', '        Args:\n', '            type (`str`, *optional*):\n', ""                Output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n"", '                `None` means `__getitem__` returns python objects (default).\n', '            columns (`List[str]`, *optional*):\n', '                Columns to format in the output.\n', '                `None` means `__getitem__` returns all columns (default).\n', '            output_all_columns (`bool`, defaults to False):\n', '                Keep un-formatted columns as well in the output (as python objects),\n', '            **format_kwargs (additional keyword arguments):\n', '                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '\n', '        It is possible to call `map` after calling `set_format`. Since `map` may add new columns, then the list of formatted columns\n', '        gets updated. In this case, if you apply `map` on a dataset to add a new column, then this column will be formatted:\n', '\n', '            `new formatted columns = (all columns - previously unformatted columns)`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n', '        >>> ds = ds.map(lambda x: tokenizer(x[""text""], truncation=True, padding=True), batched=True)\n', '        >>> ds.set_format(type=""numpy"", columns=[\'input_ids\', \'token_type_ids\', \'attention_mask\', \'label\'])\n', '        >>> ds[""train""].format\n', ""        {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': 'numpy'}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'self.values', 'dataset.set_format']",3
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:set_transform,DatasetDict:set_transform,method,4,9,9,155,17.22,1,0,"['self', 'transform', 'columns', 'output_all_columns', '']","[None, ' Optional[Callable]', ' Optional[List] ', ' bool ', None]","[None, None, ' None', ' False', None]",635,"['        """"""Set ``__getitem__`` return format using this transform. The transform is applied on-the-fly on batches when ``__getitem__`` is called.\n', '        The transform is set for every dataset in the dataset dictionary\n', '        As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`\n', '\n', '        Args:\n', '            transform (`Callable`, optional): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n', '                A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n', '                This function is applied right before returning the objects in ``__getitem__``.\n', '            columns (`List[str]`, optional): columns to format in the output\n', '                If specified, then the input batch of the transform only contains those columns.\n', '            output_all_columns (`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n', '                If set to True, then the other un-formatted columns are kept with the output of the transform.\n', '\n', '        """"""\n']","['self._check_values_type', 'self.values', 'dataset.set_format']",3
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:shape,DatasetDict:shape,method,6,9,9,75,8.33,1,0,['self'],[None],[None],171,"['        """"""Shape of each split of the dataset (number of columns, number of rows).\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.shape\n', ""        {'test': (1066, 2), 'train': (8530, 2), 'validation': (1066, 2)}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'self.items']",2
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:shuffle,DatasetDict:shuffle,method,15,92,45,651,7.08,5,5,"['self', 'seeds', 'Dict[str', 'Optional[int]]]] ', 'seed', 'generators', 'np.random.Generator]] ', 'keep_in_memory', 'load_from_cache_file', 'indices_cache_file_names', 'Optional[str]]] ', 'writer_batch_size', '']","[None, ' Optional[Union[int', None, None, ' Optional[int] ', ' Optional[Dict[str', None, ' bool ', ' Optional[bool] ', ' Optional[Dict[str', None, ' Optional[int] ', None]","[None, None, None, ' None', ' None', None, ' None', ' False', ' None', None, ' None', ' 1000', None]",1133,"['        """"""Create a new Dataset where the rows are shuffled.\n', '\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Currently shuffling uses numpy random generators.\n', ""        You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n"", '\n', '        Args:\n', '            seeds (`Dict[str, int]` or `int`, *optional*):\n', '                A seed to initialize the default BitGenerator if `generator=None`.\n', '                If `None`, then fresh, unpredictable entropy will be pulled from the OS.\n', '                If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n', '                You can provide one `seed` per dataset in the dataset dictionary.\n', '            seed (`int`, *optional*):\n', '                A seed to initialize the default BitGenerator if `generator=None`. Alias for seeds (a `ValueError` is raised if both are provided).\n', '            generators (`Dict[str, *optional*, np.random.Generator]`):\n', '                Numpy random Generator to use to compute the permutation of the dataset rows.\n', '                If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n', '                You have to provide one `generator` per dataset in the dataset dictionary.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_names (`Dict[str, str]`, *optional*):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                indices mappings instead of the automatically generated cache file name.\n', '                You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds[""train""][""label""][:10]\n', '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n', '\n', '        # set a seed\n', '        >>> shuffled_ds = ds.shuffle(seed=42)\n', '        >>> shuffled_ds[""train""][""label""][:10]\n', '        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'ValueError', 'isinstance', 'DatasetDict', 'dataset.shuffle', 'self.items']",6
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:sort,DatasetDict:sort,method,9,33,30,423,12.82,2,1,"['self', 'column_names', 'Sequence[str]]', 'reverse', 'Sequence[bool]] ', 'kind', 'null_placement', 'keep_in_memory', 'load_from_cache_file', 'indices_cache_file_names', 'Optional[str]]] ', 'writer_batch_size', '']","[None, ' Union[str', None, ' Union[bool', None, None, ' str ', ' bool ', ' Optional[bool] ', ' Optional[Dict[str', None, ' Optional[int] ', None]","[None, None, None, None, ' False', '""deprecated""', ' ""at_end""', ' False', ' None', None, ' None', ' 1000', None]",1055,"['        """"""Create a new dataset sorted according to a single or multiple columns.\n', '\n', '        Args:\n', '            column_names (`Union[str, Sequence[str]]`):\n', '                Column name(s) to sort by.\n', '            reverse (`Union[bool, Sequence[bool]]`, defaults to `False`):\n', '                If `True`, sort by descending order rather than ascending. If a single bool is provided,\n', '                the value is applied to the sorting of all column names. Otherwise a list of bools with the\n', '                same length and order as column_names must be provided.\n', '            kind (`str`, *optional*):\n', '                Pandas algorithm for sorting selected in `{quicksort, mergesort, heapsort, stable}`,\n', '                The default is `quicksort`. Note that both `stable` and `mergesort` use timsort under the covers and, in general,\n', '                the actual implementation will vary with data type. The `mergesort` option is retained for backwards compatibility.\n', '                <Deprecated version=""2.8.0"">\n', '\n', '                `kind` was deprecated in version 2.10.0 and will be removed in 3.0.0.\n', '\n', '                </Deprecated>\n', '            null_placement (`str`, defaults to `at_end`):\n', '                Put `None` values at the beginning if `at_start` or `first` or at the end if `at_end` or `last`\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the sorted indices in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the sorted indices\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                indices mapping instead of the automatically generated cache file name.\n', '                You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                Higher value gives smaller cache files, lower value consume less temporary memory.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', ""        >>> ds = load_dataset('rotten_tomatoes')\n"", ""        >>> ds['train']['label'][:10]\n"", '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n', ""        >>> sorted_ds = ds.sort('label')\n"", ""        >>> sorted_ds['train']['label'][:10]\n"", '        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n', ""        >>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])\n"", ""        >>> another_sorted_ds['train']['label'][:10]\n"", '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n', '        ```\n', '        """"""\n']","['self._check_values_type', 'DatasetDict', 'dataset.sort', 'self.items']",4
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:unique,DatasetDict:unique,method,3,9,9,84,9.33,0,0,"['self', 'column']","[None, ' str']","[None, None]",218,"['        """"""Return a list of the unique elements in a column for each split.\n', '\n', '        This is implemented in the low-level backend and as such, very fast.\n', '\n', '        Args:\n', '            column (`str`):\n', '                column name (list all the column names with [`~datasets.DatasetDict.column_names`])\n', '\n', '        Returns:\n', '            Dict[`str`, `list`]: Dictionary of unique elements in the given column.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.unique(""label"")\n', ""        {'test': [1, 0], 'train': [1, 0], 'validation': [1, 0]}\n"", '        ```\n', '        """"""\n']","['self._check_values_type', 'dataset.unique', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:with_format,DatasetDict:with_format,method,4,8,7,141,17.62,0,0,"['self', 'type', 'columns', 'output_all_columns', '**format_kwargs', '']","[None, ' Optional[str] ', ' Optional[List] ', ' bool ', None, None]","[None, ' None', ' None', ' False', None, None]",659,"['        """"""Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n', '        The format `type` (for example ""numpy"") is used to format batches when using `__getitem__`.\n', '        The format is set for every dataset in the dataset dictionary.\n', '\n', ""        It's also possible to use custom transforms for formatting using [`~datasets.Dataset.with_transform`].\n"", '\n', '        Contrary to [`~datasets.DatasetDict.set_format`], `with_format` returns a new [`DatasetDict`] object with new [`Dataset`] objects.\n', '\n', '        Args:\n', '            type (`str`, *optional*):\n', ""                Output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n"", '                `None` means `__getitem__` returns python objects (default).\n', '            columns (`List[str]`, *optional*):\n', '                Columns to format in the output.\n', '                `None` means `__getitem__` returns all columns (default).\n', '            output_all_columns (`bool`, defaults to `False`):\n', '                Keep un-formatted columns as well in the output (as python objects).\n', '            **format_kwargs (additional keyword arguments):\n', '                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n', ""        >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n"", '        >>> ds[""train""].format\n', ""        {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': None}\n"", ""        >>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n"", '        >>> ds[""train""].format\n', ""        {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': 'tensorflow'}\n"", '        ```\n', '        """"""\n']","['copy.deepcopy', 'dataset.set_format']",2
repos/datasets/src/datasets/dataset_dict.py:DatasetDict:with_transform,DatasetDict:with_transform,method,4,7,6,138,19.71,0,0,"['self', 'transform', 'columns', 'output_all_columns', '']","[None, ' Optional[Callable]', ' Optional[List] ', ' bool ', None]","[None, None, ' None', ' False', None]",711,"['        """"""Set `__getitem__` return format using this transform. The transform is applied on-the-fly on batches when `__getitem__` is called.\n', '        The transform is set for every dataset in the dataset dictionary\n', '\n', '        As [`~datasets.Dataset.set_format`], this can be reset using [`~datasets.Dataset.reset_format`].\n', '\n', '        Contrary to [`~datasets.DatasetDict.set_transform`], `with_transform` returns a new [`DatasetDict`] object with new [`Dataset`] objects.\n', '\n', '        Args:\n', '            transform (`Callable`, *optional*):\n', '                User-defined formatting transform, replaces the format defined by [`~datasets.Dataset.set_format`].\n', '                A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n', '                This function is applied right before returning the objects in `__getitem__`.\n', '            columns (`List[str]`, *optional*):\n', '                Columns to format in the output.\n', '                If specified, then the input batch of the transform only contains those columns.\n', '            output_all_columns (`bool`, defaults to False):\n', '                Keep un-formatted columns as well in the output (as python objects).\n', '                If set to `True`, then the other un-formatted columns are kept with the output of the transform.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n', '        >>> def encode(example):\n', '        ...     return tokenizer(example[\'text\'], truncation=True, padding=True, return_tensors=""pt"")\n', '        >>> ds = ds.with_transform(encode)\n', '        >>> ds[""train""][0]\n', ""        {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n"", '         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n', '         1, 1, 1, 1, 1, 1, 1, 1, 1]),\n', ""         'input_ids': tensor([  101,  1103,  2067,  1110, 17348,  1106,  1129,  1103,  6880,  1432,\n"", '                112,   188,  1207,   107, 14255,  1389,   107,  1105,  1115,  1119,\n', '                112,   188,  1280,  1106,  1294,   170, 24194,  1256,  3407,  1190,\n', '                170, 11791,  5253,   188,  1732,  7200, 10947, 12606,  2895,   117,\n', '                179,  7766,   118,   172, 15554,  1181,  3498,  6961,  3263,  1137,\n', '                188,  1566,  7912, 14516,  6997,   119,   102]),\n', ""         'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n"", '                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n', '                0, 0, 0, 0, 0, 0, 0, 0, 0])}\n', '        ```\n', '        """"""\n']","['copy.deepcopy', 'dataset.set_transform']",2
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:__repr__,IterableDatasetDict:__repr__,method,4,18,16,128,7.11,0,0,['self'],[None],[None],261,[],"['self.items', 're.sub', 'f""IterableDatasetDict']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:cast,IterableDatasetDict:cast,method,2,8,8,88,11.0,0,0,"['self', 'features', '']","[None, ' Features', None]","[None, None, None]",2258,"['        """"""\n', '        Cast the dataset to a new set of features.\n', '        The type casting is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            features (`Features`):\n', '                New features to cast the dataset to.\n', '                The name of the fields in the features must match the current column names.\n', '                The type of the data must also be convertible from one type to the other.\n', '                For non-trivial conversion, e.g. `string` <-> `ClassLabel` you should use [`map`] to update the Dataset.\n', '\n', '        Returns:\n', '            [`IterableDatasetDict`]: A copy of the dataset with casted features.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", streaming=True)\n', '        >>> ds[""train""].features\n', ""        {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", '        >>> new_features = ds[""train""].features.copy()\n', ""        >>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n"", ""        >>> new_features['text'] = Value('large_string')\n"", '        >>> ds = ds.cast(new_features)\n', '        >>> ds[""train""].features\n', ""        {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n"", ""         'text': Value(dtype='large_string', id=None)}\n"", '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.cast', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:cast_column,IterableDatasetDict:cast_column,method,2,11,11,109,9.91,0,0,"['self', 'column', 'feature']","[None, ' str', ' FeatureType']","[None, None, None]",2227,"['        """"""Cast column to feature for decoding.\n', '        The type casting is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            column (`str`):\n', '                Column name.\n', '            feature ([`Feature`]):\n', '                Target feature.\n', '\n', '        Returns:\n', '            [`IterableDatasetDict`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", streaming=True)\n', '        >>> ds[""train""].features\n', ""        {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", ""        >>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n"", '        >>> ds[""train""].features\n', ""        {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.cast_column', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:filter,IterableDatasetDict:filter,method,5,19,19,215,11.32,1,0,"['self', 'function', 'with_indices', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'fn_kwargs', '']","[None, ' Optional[Callable] ', None, ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' Optional[dict] ', None]","[None, ' None', 'False', None, ' None', ' False', ' 1000', ' None', None]",893,"['        """"""Apply a filter function to all the elements in the table in batches\n', '        and update the table so that the dataset only includes examples according to the filter function.\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            function (`Callable`): Callable with one of the following signatures:\n', '\n', '                - `function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False` and `with_rank=False`\n', '                - `function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '                - `function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False` and `with_rank=False`\n', '                - `function(batch: Dict[str, List], *extra_args) -> List[bool]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n', '\n', '                If no function is provided, defaults to an always `True` function: `lambda x: True`.\n', '            with_indices (`bool`, defaults to `False`):\n', '                Provide example indices to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example, idx[, rank]): ...`.\n', '            with_rank (`bool`, defaults to `False`):\n', '                Provide process rank to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example[, idx], rank): ...`.\n', '            input_columns (`[Union[str, List[str]]]`, *optional*, defaults to `None`):\n', '                The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`):\n', '                Provide batch of examples to `function`.\n', '            batch_size (`int`, *optional*, defaults to `1000`):\n', '                Number of examples per batch provided to `function` if `batched=True`\n', '                `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to `function`.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '                You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            fn_kwargs (`Dict`, *optional*, defaults to `None`):\n', '                Keyword arguments to be passed to `function`\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', ""                Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '            desc (`str`, *optional*, defaults to `None`):\n', '                Meaningful description to be displayed alongside with the progress bar while filtering examples.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds.filter(lambda x: x[""label""] == 1)\n', '        DatasetDict({\n', '            train: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 4265\n', '            })\n', '            validation: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 533\n', '            })\n', '            test: Dataset({\n', ""                features: ['text', 'label'],\n"", '                num_rows: 533\n', '            })\n', '        })\n', '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.filter', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:map,IterableDatasetDict:map,method,5,21,21,276,13.14,1,0,"['self', 'function', 'with_indices', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'drop_last_batch', 'remove_columns', 'List[str]]] ', 'fn_kwargs', '']","[None, ' Optional[Callable] ', ' bool ', ' Optional[Union[str', None, ' bool ', ' int ', ' bool ', ' Optional[Union[str', None, ' Optional[dict] ', None]","[None, ' None', ' False', None, ' None', ' False', ' 1000', ' False', None, ' None', ' None', None]",765,"['        """"""Apply a function to all the elements in the table (individually or in batches)\n', '        and update the table (if function does updated examples).\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            function (`callable`): with one of the following signature:\n', '                - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False`\n', '                - `function(example: Dict[str, Any], indices: int) -> Dict[str, Any]` if `batched=False` and `with_indices=True`\n', '                - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False`\n', '                - `function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if `batched=True` and `with_indices=True`\n', '\n', '                For advanced usage, the function can also return a `pyarrow.Table`.\n', '                Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n', '\n', '            with_indices (`bool`, defaults to `False`):\n', '                Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n', '            with_rank (`bool`, defaults to `False`):\n', '                Provide process rank to `function`. Note that in this case the\n', '                signature of `function` should be `def function(example[, idx], rank): ...`.\n', '            input_columns (`[Union[str, List[str]]]`, *optional*, defaults to `None`):\n', '                The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`):\n', '                Provide batch of examples to `function`.\n', '            batch_size (`int`, *optional*, defaults to `1000`):\n', '                Number of examples per batch provided to `function` if `batched=True`,\n', '                `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to `function`.\n', '            drop_last_batch (`bool`, defaults to `False`):\n', '                Whether a last batch smaller than the batch_size should be\n', '                dropped instead of being processed by the function.\n', '            remove_columns (`[Union[str, List[str]]]`, *optional*, defaults to `None`):\n', '                Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                results of the computation instead of the automatically generated cache file name.\n', '                You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (`int`, default `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '            features (`[datasets.Features]`, *optional*, defaults to `None`):\n', '                Use a specific [`Features`] to store the cache file\n', '                instead of the automatically generated one.\n', '            disable_nullable (`bool`, defaults to `False`):\n', '                Disallow null values in the table.\n', '            fn_kwargs (`Dict`, *optional*, defaults to `None`):\n', '                Keyword arguments to be passed to `function`\n', '            num_proc (`int`, *optional*, defaults to `None`):\n', ""                Number of processes for multiprocessing. By default it doesn't\n"", '                use multiprocessing.\n', '            desc (`str`, *optional*, defaults to `None`):\n', '                Meaningful description to be displayed alongside with the progress bar while mapping examples.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> def add_prefix(example):\n', '        ...     example[""text""] = ""Review: "" + example[""text""]\n', '        ...     return example\n', '        >>> ds = ds.map(add_prefix)\n', '        >>> ds[""train""][0:3][""text""]\n', '        [\'Review: the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\',\n', '         \'Review: the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\',\n', ""         'Review: effective but too-tepid biopic']\n"", '\n', '        # process a batch of examples\n', '        >>> ds = ds.map(lambda example: tokenizer(example[""text""]), batched=True)\n', '        # set number of processors\n', '        >>> ds = ds.map(add_prefix, num_proc=4)\n', '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.map', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:remove_columns,IterableDatasetDict:remove_columns,method,2,8,8,93,11.62,0,0,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",2175,"['        """"""\n', '        Remove one or several column(s) in the dataset and the features associated to them.\n', '        The removal is done on-the-fly on the examples when iterating over the dataset.\n', '        The removal is applied to all the datasets of the dataset dictionary.\n', '\n', '\n', '        Args:\n', '            column_names (`Union[str, List[str]]`):\n', '                Name of the column(s) to remove.\n', '\n', '        Returns:\n', '            [`IterableDatasetDict`]: A copy of the dataset object without the columns to remove.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", streaming=True)\n', '        >>> ds = ds.remove_columns(""label"")\n', '        >>> next(iter(ds[""train""]))\n', '        {\'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'}\n', '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.remove_columns', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:rename_column,IterableDatasetDict:rename_column,method,5,13,13,158,12.15,1,0,"['self', 'original_column_name', 'new_column_name']","[None, ' str', ' str']","[None, None, None]",2114,"['        """"""\n', '        Rename a column in the dataset, and move the features associated to the original column under the new column\n', '        name.\n', '        The renaming is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            original_column_name (`str`):\n', '                Name of the column to rename.\n', '            new_column_name (`str`):\n', '                New name for the column.\n', '\n', '        Returns:\n', '            [`IterableDatasetDict`]: A copy of the dataset with a renamed column.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", streaming=True)\n', '        >>> ds = ds.rename_column(""text"", ""movie_review"")\n', '        >>> next(iter(ds[""train""]))\n', ""        {'label': 1,\n"", '         \'movie_review\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'}\n', '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.rename_column', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:rename_columns,IterableDatasetDict:rename_columns,method,2,10,10,112,11.2,0,0,"['self', 'column_mapping', 'str]']","[None, ' Dict[str', None]","[None, None, None]",2147,"['        """"""\n', '        Rename several columns in the dataset, and move the features associated to the original columns under\n', '        the new column names.\n', '        The renaming is applied to all the datasets of the dataset dictionary.\n', '\n', '        Args:\n', '            column_mapping (`Dict[str, str]`):\n', '                A mapping of columns to rename to their new names.\n', '\n', '        Returns:\n', '            [`IterableDatasetDict`]: A copy of the dataset with renamed columns\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", streaming=True)\n', '        >>> ds = ds.rename_columns({""text"": ""movie_review"", ""label"": ""rating""})\n', '        >>> next(iter(ds[""train""]))\n', '        {\'movie_review\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\',\n', ""         'rating': 1}\n"", '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.rename_columns', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:select_columns,IterableDatasetDict:select_columns,method,2,8,8,93,11.62,0,0,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",2201,"['        """"""Select one or several column(s) in the dataset and the features\n', '        associated to them. The selection is done on-the-fly on the examples\n', '        when iterating over the dataset. The selection is applied to all the\n', '        datasets of the dataset dictionary.\n', '\n', '\n', '        Args:\n', '            column_names (`Union[str, List[str]]`):\n', '                Name of the column(s) to keep.\n', '\n', '        Returns:\n', '            [`IterableDatasetDict`]: A copy of the dataset object with only selected columns.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", streaming=True)\n', '        >>> ds = ds.select(""text"")\n', '        >>> next(iter(ds[""train""]))\n', '        {\'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'}\n', '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.select_columns', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:shuffle,IterableDatasetDict:shuffle,method,5,14,14,132,9.43,1,0,"['self', 'seed', 'generator', 'buffer_size']","[None, None, ' Optional[np.random.Generator] ', ' int ']","[None, 'None', ' None', ' 1000']",1133,"['        """"""Create a new Dataset where the rows are shuffled.\n', '\n', '        The transformation is applied to all the datasets of the dataset dictionary.\n', '\n', '        Currently shuffling uses numpy random generators.\n', ""        You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n"", '\n', '        Args:\n', '            seeds (`Dict[str, int]` or `int`, *optional*):\n', '                A seed to initialize the default BitGenerator if `generator=None`.\n', '                If `None`, then fresh, unpredictable entropy will be pulled from the OS.\n', '                If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n', '                You can provide one `seed` per dataset in the dataset dictionary.\n', '            seed (`int`, *optional*):\n', '                A seed to initialize the default BitGenerator if `generator=None`. Alias for seeds (a `ValueError` is raised if both are provided).\n', '            generators (`Dict[str, *optional*, np.random.Generator]`):\n', '                Numpy random Generator to use to compute the permutation of the dataset rows.\n', '                If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n', '                You have to provide one `generator` per dataset in the dataset dictionary.\n', '            keep_in_memory (`bool`, defaults to `False`):\n', '                Keep the dataset in memory instead of writing it to a cache file.\n', '            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n', '                If a cache file storing the current computation from `function`\n', '                can be identified, use it instead of recomputing.\n', '            indices_cache_file_names (`Dict[str, str]`, *optional*):\n', '                Provide the name of a path for the cache file. It is used to store the\n', '                indices mappings instead of the automatically generated cache file name.\n', '                You have to provide one `cache_file_name` per dataset in the dataset dictionary.\n', '            writer_batch_size (`int`, defaults to `1000`):\n', '                Number of rows per write operation for the cache file writer.\n', '                This value is a good trade-off between memory usage during the processing, and processing speed.\n', '                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> ds[""train""][""label""][:10]\n', '        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n', '\n', '        # set a seed\n', '        >>> shuffled_ds = ds.shuffle(seed=42)\n', '        >>> shuffled_ds[""train""][""label""][:10]\n', '        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n', '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.shuffle', 'self.items']",3
repos/datasets/src/datasets/dataset_dict.py:IterableDatasetDict:with_format,IterableDatasetDict:with_format,method,2,8,8,87,10.88,0,0,"['self', 'type', '']","[None, ' Optional[str] ', None]","[None, ' None', None]",659,"['        """"""Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n', '        The format `type` (for example ""numpy"") is used to format batches when using `__getitem__`.\n', '        The format is set for every dataset in the dataset dictionary.\n', '\n', ""        It's also possible to use custom transforms for formatting using [`~datasets.Dataset.with_transform`].\n"", '\n', '        Contrary to [`~datasets.DatasetDict.set_format`], `with_format` returns a new [`DatasetDict`] object with new [`Dataset`] objects.\n', '\n', '        Args:\n', '            type (`str`, *optional*):\n', ""                Output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n"", '                `None` means `__getitem__` returns python objects (default).\n', '            columns (`List[str]`, *optional*):\n', '                Columns to format in the output.\n', '                `None` means `__getitem__` returns all columns (default).\n', '            output_all_columns (`bool`, defaults to `False`):\n', '                Keep un-formatted columns as well in the output (as python objects).\n', '            **format_kwargs (additional keyword arguments):\n', '                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> from transformers import AutoTokenizer\n', '        >>> ds = load_dataset(""rotten_tomatoes"")\n', '        >>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n', ""        >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n"", '        >>> ds[""train""].format\n', ""        {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': None}\n"", ""        >>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n"", '        >>> ds[""train""].format\n', ""        {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n"", ""         'format_kwargs': {},\n"", ""         'output_all_columns': False,\n"", ""         'type': 'tensorflow'}\n"", '        ```\n', '        """"""\n']","['IterableDatasetDict', 'dataset.with_format', 'self.items']",3
repos/datasets/src/datasets/distributed.py:split_dataset_by_node,split_dataset_by_node,function,4,12,10,195,16.25,0,1,"['dataset', 'rank', 'world_size']","[' DatasetType', ' int', ' int']","[None, None, None]",10,"['    """"""\n', '    Split a dataset for the node at rank `rank` in a pool of nodes of size `world_size`.\n', '\n', '    For map-style datasets:\n', '\n', '    Each node is assigned a chunk of data, e.g. rank 0 is given the first chunk of the dataset.\n', '    To maximize data loading throughput, chunks are made of contiguous data on disk if possible.\n', '\n', '    For iterable datasets:\n', '\n', '    If the dataset has a number of shards that is a factor of `world_size` (i.e. if `dataset.n_shards % world_size == 0`),\n', '    then the shards are evenly assigned across the nodes, which is the most optimized.\n', '    Otherwise, each node keeps 1 example out of `world_size`, skipping the other examples.\n', '\n', '    Args:\n', '        dataset ([`Dataset`] or [`IterableDataset`]):\n', '            The dataset to split by node.\n', '        rank (`int`):\n', '            Rank of the current node.\n', '        world_size (`int`):\n', '            Total number of nodes.\n', '\n', '    Returns:\n', '        [`Dataset`] or [`IterableDataset`]: The dataset to be used on the node at rank `rank`.\n', '    """"""\n']","['isinstance', '_split_by_node_map_style_dataset', '_split_by_node_iterable_dataset']",3
repos/datasets/src/datasets/download/download_config.py:DownloadConfig,DownloadConfig,class,43,153,107,1449,9.47,0,4,[],[],[],11,[],[],0
repos/datasets/src/datasets/download/download_config.py:DownloadConfig:__post_init__,DownloadConfig:__post_init__,method,4,43,39,358,8.33,0,2,"['self', 'use_auth_token']","[None, None]","[None, None]",88,[],['warnings.warn'],1
repos/datasets/src/datasets/download/download_config.py:DownloadConfig:__setattr__,DownloadConfig:__setattr__,method,5,30,25,304,10.13,0,2,"['self', 'name', 'value']","[None, None, None]","[None, None, None]",102,[],"['getattr', 'super']",2
repos/datasets/src/datasets/download/download_config.py:DownloadConfig:copy,DownloadConfig:copy,method,2,8,8,73,9.12,0,0,['self'],[None],[None],99,[],"['self.__class__', 'copy.deepcopy']",2
repos/datasets/src/datasets/download/download_manager.py:DownloadManager,DownloadManager,class,140,481,318,6977,14.51,3,11,[],[],[],86,[],[],0
repos/datasets/src/datasets/download/download_manager.py:DownloadMode,DownloadMode,class,3,6,6,131,21.83,0,0,[],[],[],55,[],[],0
repos/datasets/src/datasets/download/download_manager.py:GenerateMode,GenerateMode,class,5,13,13,198,15.23,0,0,[],[],[],76,[],[],0
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:__init__,DownloadManager:__init__,method,19,24,21,325,13.54,0,0,"['self', 'dataset_name', 'data_dir', 'download_config', 'base_path', 'record_checksums', '']","[None, ' Optional[str] ', ' Optional[str] ', ' Optional[DownloadConfig] ', ' Optional[str] ', None, None]","[None, ' None', ' None', ' None', ' None', 'True', None]",89,"['        """"""Download manager constructor.\n', '\n', '        Args:\n', '            data_dir:\n', '                can be used to specify a manual directory to get the files from.\n', '            dataset_name (`str`):\n', '                name of dataset this instance will be used for. If\n', '                provided, downloads will contain which datasets they were used for.\n', '            download_config (`DownloadConfig`):\n', '                to specify the cache directory and other\n', '                download options\n', '            base_path (`str`):\n', '                base path that is used when relative paths are used to\n', '                download files. This can be a remote url.\n', '            record_checksums (`bool`, defaults to `True`):\n', '                Whether to record the checksums of the downloaded files. If None, the value is inferred from the builder.\n', '        """"""\n']",['DownloadConfig'],1
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:_download_batched,DownloadManager:_download_batched,method,26,79,70,976,12.35,0,3,"['self', 'url_or_filenames', 'download_config', '']","[None, ' List[str]', ' DownloadConfig', None]","[None, None, None, None]",279,[],"['len', 'download_config.copy', 'partial', 'url_to_fs', 'fs.info', 'thread_map', 'multiprocessing.current_process', 'self._download_single']",8
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:_download_single,DownloadManager:_download_single,method,9,15,12,267,17.8,0,1,"['self', 'url_or_filename', 'download_config']","[None, ' str', ' DownloadConfig']","[None, None, None]",318,[],"['str', 'is_relative_path', 'url_or_path_join', 'cached_path', 'tracked_str', 'out.set_origin']",6
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:_record_sizes_checksums,DownloadManager:_record_sizes_checksums,method,9,18,18,251,13.94,1,0,"['self', 'url_or_urls', 'downloaded_path_or_paths']","[None, ' NestedDataStructure', ' NestedDataStructure']","[None, None, None]",169,"['        """"""Record size/checksum of downloaded files.""""""\n']","['hf_tqdm', 'list', 'downloaded_path_or_paths.flatten', 'get_size_checksum_dict']",4
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:delete_extracted_files,DownloadManager:delete_extracted_files,method,8,17,14,228,13.41,1,1,['self'],[None],[None],439,[],"['set', 'list', 'os.remove']",3
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:download,DownloadManager:download,method,20,61,49,1059,17.36,0,1,"['self', 'url_or_urls']","[None, None]","[None, None]",229,"['        """"""Download given URL(s).\n', '\n', '        By default, only one process is used for download. Pass customized `download_config.num_proc` to change this behavior.\n', '\n', '        Args:\n', '            url_or_urls (`str` or `list` or `dict`):\n', '                URL or `list` or `dict` of URLs to download. Each URL is a `str`.\n', '\n', '        Returns:\n', '            `str` or `list` or `dict`:\n', '                The downloaded paths matching the given input `url_or_urls`.\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n"", '        ```\n', '        """"""\n']","['partial', 'datetime.now', 'stack_multiprocessing_download_progress_bars', 'map_nested', 'logger.info', 'NestedDataStructure', 'downloaded_path_or_paths.flatten', 'self._record_sizes_checksums']",8
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:download_and_extract,DownloadManager:download_and_extract,method,2,2,2,46,23.0,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",418,"['        """"""Download and extract given `url_or_urls`.\n', '\n', '        Is roughly equivalent to:\n', '\n', '        ```\n', '        extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))\n', '        ```\n', '\n', '        Args:\n', '            url_or_urls (`str` or `list` or `dict`):\n', '                URL or `list` or `dict` of URLs to download and extract. Each URL is a `str`.\n', '\n', '        Returns:\n', '            extracted_path(s): `str`, extracted paths of given URL(s).\n', '        """"""\n']",['self.extract'],1
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:download_custom,DownloadManager:download_custom,method,23,56,43,910,16.25,1,1,"['self', 'url_or_urls', 'custom_download']","[None, None, None]","[None, None, None]",183,"['        """"""\n', '        Download given urls(s) by calling `custom_download`.\n', '\n', '        Args:\n', '            url_or_urls (`str` or `list` or `dict`):\n', '                URL or `list` or `dict` of URLs to download and extract. Each URL is a `str`.\n', '            custom_download (`Callable[src_url, dst_path]`):\n', '                The source URL and destination path. For example\n', '                `tf.io.gfile.copy`, that lets you download from  Google storage.\n', '\n', '        Returns:\n', '            downloaded_path(s): `str`, The downloaded paths matching the given input\n', '                `url_or_urls`.\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)\n"", '        ```\n', '        """"""\n']","['url_to_downloaded_path', 'hash_url_to_filename', 'map_nested', 'NestedDataStructure', 'zip', 'downloaded_path_or_paths.flatten', 'get_from_cache', 'custom_download', 'self._record_sizes_checksums']",9
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:downloaded_size,DownloadManager:downloaded_size,method,2,6,6,96,16.0,0,0,['self'],[None],[None],129,"['        """"""Returns the total size of downloaded files.""""""\n']",['sum'],1
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:extract,DownloadManager:extract,method,15,48,45,693,14.44,0,1,"['self', 'path_or_paths', 'num_proc']","[None, None, None]","[None, None, '""deprecated""']",372,"['        """"""Extract given path(s).\n', '\n', '        Args:\n', '            path_or_paths (path or `list` or `dict`):\n', '                Path of file to extract. Each path is a `str`.\n', '            num_proc (`int`):\n', '                Use multi-processing if `num_proc` > 1 and the length of\n', '                `path_or_paths` is larger than `num_proc`.\n', '\n', '                <Deprecated version=""2.6.2"">\n', '\n', '                Pass `DownloadConfig(num_proc=<num_proc>)` to the initializer instead.\n', '\n', '                </Deprecated>\n', '\n', '        Returns:\n', '            extracted_path(s): `str`, The extracted paths matching the given input\n', '            path_or_paths.\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n"", '        >>> extracted_files = dl_manager.extract(downloaded_files)\n', '        ```\n', '        """"""\n']","['warnings.warn', 'partial', 'map_nested', 'NestedDataStructure', 'extracted_paths.flatten']",5
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:get_recorded_sizes_checksums,DownloadManager:get_recorded_sizes_checksums,method,2,2,2,43,21.5,0,0,['self'],[None],[None],436,[],[],0
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:iter_archive,DownloadManager:iter_archive,method,4,8,7,128,16.0,0,1,"['self', 'path_or_buf', 'io.BufferedReader]']","[None, ' Union[str', None]","[None, None, None]",328,"['        """"""Iterate over files within an archive.\n', '\n', '        Args:\n', '            path_or_buf (`str` or `io.BufferedReader`):\n', '                Archive path or archive binary file object.\n', '\n', '        Yields:\n', '            `tuple[str, io.BufferedReader]`:\n', '                2-tuple (path_within_archive, file_object).\n', '                File object is opened in binary mode.\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n"", '        >>> files = dl_manager.iter_archive(archive)\n', '        ```\n', '        """"""\n']","['hasattr', 'ArchiveIterable.from_buf', 'ArchiveIterable.from_urlpath']",3
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:iter_files,DownloadManager:iter_files,method,2,2,2,40,20.0,0,0,"['self', 'paths', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",353,"['        """"""Iterate over file paths.\n', '\n', '        Args:\n', '            paths (`str` or `list` of `str`):\n', '                Root paths.\n', '\n', '        Yields:\n', '            `str`: File path.\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')\n"", '        >>> files = dl_manager.iter_files(files)\n', '        ```\n', '        """"""\n']",['FilesIterable.from_urlpaths'],1
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:manage_extracted_files,DownloadManager:manage_extracted_files,method,2,3,3,70,23.33,0,1,['self'],[None],[None],446,[],['self.delete_extracted_files'],1
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:manual_dir,DownloadManager:manual_dir,method,2,2,2,20,10.0,0,0,['self'],[None],[None],125,[],[],0
repos/datasets/src/datasets/download/download_manager.py:DownloadManager:ship_files_with_pipeline,DownloadManager:ship_files_with_pipeline,method,14,49,42,689,14.06,0,1,"['downloaded_path_or_paths', 'pipeline']","[None, None]","[None, None]",134,"['        """"""Ship the files using Beam FileSystems to the pipeline temp dir.\n', '\n', '        Args:\n', '            downloaded_path_or_paths (`str` or `list[str]` or `dict[str, str]`):\n', '                Nested structure containing the\n', '                downloaded path(s).\n', '            pipeline ([`utils.beam_utils.BeamPipeline`]):\n', '                Apache Beam Pipeline.\n', '\n', '        Returns:\n', '            `str` or `list[str]` or `dict[str, str]`\n', '        """"""\n']","['ValueError', 'upload', 'posixpath.join', 'logger.info', 'upload_local_to_remote', 'map_nested']",6
repos/datasets/src/datasets/download/download_manager.py:GenerateMode:help_message,GenerateMode:help_message,method,1,4,4,33,8.25,0,0,['self'],[None],[None],82,[],[],0
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager,MockDownloadManager,class,106,413,236,5255,12.72,12,17,[],[],[],33,[],[],0
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:__init__,MockDownloadManager:__init__,method,19,23,23,336,14.61,0,0,"['self', 'dataset_name', 'config', 'version', 'str]', 'cache_dir', 'use_local_dummy_data', 'load_existing_dummy_data', 'download_callbacks', '']","[None, ' str', ' str', ' Union[Version', None, ' Optional[str] ', ' bool ', ' bool ', ' Optional[List[Callable]] ', None]","[None, None, None, None, None, ' None', ' False', ' True', ' None', None]",38,[],['str'],1
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:create_dummy_data_dict,MockDownloadManager:create_dummy_data_dict,method,16,65,40,738,11.35,4,3,"['self', 'path_to_dummy_data', 'data_url']","[None, None, None]","[None, None, None]",145,[],"['data_url.items', 'isinstance', 'download_callback', 'all', 'dummy_data_dict.values', 'len', 'dummy_data_dict.items']",7
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:create_dummy_data_list,MockDownloadManager:create_dummy_data_list,method,13,41,32,541,13.2,2,1,"['self', 'path_to_dummy_data', 'data_url']","[None, None, None]","[None, None, None]",173,[],"['all', 'url.startswith', 'len', 'download_callback', 'dummy_data_list.append']",5
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:create_dummy_data_single,MockDownloadManager:create_dummy_data_single,method,7,18,17,264,14.67,1,1,"['self', 'path_to_dummy_data', 'data_url']","[None, None, None]","[None, None, None]",191,[],['download_callback'],1
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:delete_extracted_files,MockDownloadManager:delete_extracted_files,method,0,1,1,4,4.0,0,0,['self'],[None],[None],207,[],[],0
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:download,MockDownloadManager:download,method,2,2,2,41,20.5,0,0,"['self', 'data_url', '*args']","[None, None, None]","[None, None, None]",130,[],['self.download_and_extract'],1
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:download_and_extract,MockDownloadManager:download_and_extract,method,9,24,17,338,14.08,0,2,"['self', 'data_url', '*args']","[None, None, None]","[None, None, None]",113,[],"['isinstance', 'self.create_dummy_data_dict', 'self.create_dummy_data_list', 'self.create_dummy_data_single']",4
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:download_custom,MockDownloadManager:download_custom,method,2,2,2,41,20.5,0,0,"['self', 'data_url', 'custom_download']","[None, None, None]","[None, None, None]",134,[],['self.download_and_extract'],1
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:download_dummy_data,MockDownloadManager:download_dummy_data,method,5,20,19,297,14.85,0,0,['self'],[None],[None],83,[],['cached_path'],1
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:dummy_data_folder,MockDownloadManager:dummy_data_folder,method,3,12,10,132,11.0,0,1,['self'],[None],[None],72,[],[],0
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:dummy_file,MockDownloadManager:dummy_file,method,3,8,6,92,11.5,0,1,['self'],[None],[None],66,[],['self.download_dummy_data'],1
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:dummy_zip_file,MockDownloadManager:dummy_zip_file,method,2,3,3,59,19.67,0,0,['self'],[None],[None],80,[],[],0
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:extract,MockDownloadManager:extract,method,2,2,2,10,5.0,0,0,"['self', 'path', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",138,[],[],0
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:get_recorded_sizes_checksums,MockDownloadManager:get_recorded_sizes_checksums,method,1,2,2,8,4.0,0,0,['self'],[None],[None],142,[],[],0
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:github_path_to_dummy_data,MockDownloadManager:github_path_to_dummy_data,method,3,10,8,138,13.8,0,1,['self'],[None],[None],99,[],['hf_github_url'],1
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:iter_archive,MockDownloadManager:iter_archive,method,17,41,36,575,14.02,2,2,"['self', 'path']","[None, None]","[None, None]",213,[],"['_iter_archive_members', 'Path', 'path.relative_to', 'ZipFile', 'zip_file.namelist', 'member.startswith', 'dummy_parent_path.joinpath', 'path.rglob', 'file_path.is_file', 'file_path.relative_to', 'file_path.open']",11
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:iter_files,MockDownloadManager:iter_files,method,15,37,27,333,9.0,3,4,"['self', 'paths']","[None, None]","[None, None]",230,[],"['isinstance', 'os.walk', 'dirnames.sort', 'sorted', 'filename.startswith']",5
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:local_path_to_dummy_data,MockDownloadManager:local_path_to_dummy_data,method,2,4,4,83,20.75,0,0,['self'],[None],[None],95,[],[],0
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:manage_extracted_files,MockDownloadManager:manage_extracted_files,method,0,1,1,4,4.0,0,0,['self'],[None],[None],210,[],[],0
repos/datasets/src/datasets/download/mock_download_manager.py:MockDownloadManager:manual_dir,MockDownloadManager:manual_dir,method,3,7,6,123,17.57,0,1,['self'],[None],[None],105,[],[],0
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager,StreamingDownloadManager,class,49,182,125,2182,11.99,0,4,[],[],[],46,[],[],0
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager:__init__,StreamingDownloadManager:__init__,method,11,12,11,158,13.17,0,0,"['self', 'dataset_name', 'data_dir', 'download_config', 'base_path', '']","[None, ' Optional[str] ', ' Optional[str] ', ' Optional[DownloadConfig] ', ' Optional[str] ', None]","[None, ' None', ' None', ' None', ' None', None]",56,[],['DownloadConfig'],1
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager:_download_single,StreamingDownloadManager:_download_single,method,5,9,7,113,12.56,0,1,"['self', 'urlpath']","[None, ' str']","[None, None]",92,[],"['str', 'is_relative_path', 'url_or_path_join']",3
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager:_extract,StreamingDownloadManager:_extract,method,19,77,62,873,11.34,0,2,"['self', 'urlpath']","[None, ' str']","[None, None]",121,[],"['str', '_get_extraction_protocol', 'urlpath.split', '_get_path_extension', 'path.endswith', 'NotImplementedError', 'dl_manager.download', 'dl_manager.iter_archive', 'inner_file.rindex']",9
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager:download,StreamingDownloadManager:download,method,3,6,5,90,15.0,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",72,"['        """"""Normalize URL(s) of files to stream data from.\n', '        This is the lazy version of `DownloadManager.download` for streaming.\n', '\n', '        Args:\n', '            url_or_urls (`str` or `list` or `dict`):\n', '                URL(s) of files to stream data from. Each url is a `str`.\n', '\n', '        Returns:\n', '            url(s): (`str` or `list` or `dict`), URL(s) to stream data from matching the given input url_or_urls.\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n"", '        ```\n', '        """"""\n']",['map_nested'],1
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager:download_and_extract,StreamingDownloadManager:download_and_extract,method,2,2,2,46,23.0,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",148,"['        """"""Prepare given `url_or_urls` for streaming (add extraction protocol).\n', '\n', '        This is the lazy version of `DownloadManager.download_and_extract` for streaming.\n', '\n', '        Is equivalent to:\n', '\n', '        ```\n', '        urls = dl_manager.extract(dl_manager.download(url_or_urls))\n', '        ```\n', '\n', '        Args:\n', '            url_or_urls (`str` or `list` or `dict`):\n', '                URL(s) to stream from data from. Each url is a `str`.\n', '\n', '        Returns:\n', '            url(s): (`str` or `list` or `dict`), URL(s) to stream data from matching the given input `url_or_urls`.\n', '        """"""\n']",['self.extract'],1
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager:extract,StreamingDownloadManager:extract,method,3,6,5,76,12.67,0,0,"['self', 'url_or_urls']","[None, None]","[None, None]",99,"['        """"""Add extraction protocol for given url(s) for streaming.\n', '\n', '        This is the lazy version of `DownloadManager.extract` for streaming.\n', '\n', '        Args:\n', '            url_or_urls (`str` or `list` or `dict`):\n', '                URL(s) of files to stream data from. Each url is a `str`.\n', '\n', '        Returns:\n', '            url(s): (`str` or `list` or `dict`), URL(s) to stream data from matching the given input `url_or_urls`.\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n"", '        >>> extracted_files = dl_manager.extract(downloaded_files)\n', '        ```\n', '        """"""\n']",['map_nested'],1
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager:iter_archive,StreamingDownloadManager:iter_archive,method,4,9,8,174,19.33,0,1,"['self', 'urlpath_or_buf', 'io.BufferedReader]']","[None, ' Union[str', None]","[None, None, None]",168,"['        """"""Iterate over files within an archive.\n', '\n', '        Args:\n', '            urlpath_or_buf (`str` or `io.BufferedReader`):\n', '                Archive path or archive binary file object.\n', '\n', '        Yields:\n', '            `tuple[str, io.BufferedReader]`:\n', '                2-tuple (path_within_archive, file_object).\n', '                File object is opened in binary mode.\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n"", '        >>> files = dl_manager.iter_archive(archive)\n', '        ```\n', '        """"""\n']","['hasattr', 'ArchiveIterable.from_buf', 'ArchiveIterable.from_urlpath']",3
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager:iter_files,StreamingDownloadManager:iter_files,method,2,3,3,80,26.67,0,0,"['self', 'urlpaths', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",193,"['        """"""Iterate over files.\n', '\n', '        Args:\n', '            urlpaths (`str` or `list` of `str`):\n', '                Root paths.\n', '\n', '        Yields:\n', '            str: File URL path.\n', '\n', '        Example:\n', '\n', '        ```py\n', ""        >>> files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')\n"", '        >>> files = dl_manager.iter_files(files)\n', '        ```\n', '        """"""\n']",['FilesIterable.from_urlpaths'],1
repos/datasets/src/datasets/download/streaming_download_manager.py:StreamingDownloadManager:manual_dir,StreamingDownloadManager:manual_dir,method,2,2,2,20,10.0,0,0,['self'],[None],[None],69,[],[],0
repos/datasets/src/datasets/exceptions.py:DataFilesNotFoundError,DataFilesNotFoundError,class,0,0,0,0,0.0,0,0,[],[],[],24,[],[],0
repos/datasets/src/datasets/exceptions.py:DatasetBuildError,DatasetBuildError,class,0,1,1,4,4.0,0,0,[],[],[],37,[],[],0
repos/datasets/src/datasets/exceptions.py:DatasetGenerationCastError,DatasetGenerationCastError,class,40,139,108,1478,10.63,3,5,[],[],[],53,[],[],0
repos/datasets/src/datasets/exceptions.py:DatasetGenerationError,DatasetGenerationError,class,0,1,1,4,4.0,0,0,[],[],[],49,[],[],0
repos/datasets/src/datasets/exceptions.py:DatasetNotFoundError,DatasetNotFoundError,class,0,0,0,0,0.0,0,0,[],[],[],28,[],[],0
repos/datasets/src/datasets/exceptions.py:DatasetsError,DatasetsError,class,0,0,0,0,0.0,0,0,[],[],[],12,[],[],0
repos/datasets/src/datasets/exceptions.py:DefunctDatasetError,DefunctDatasetError,class,0,0,0,0,0.0,0,0,[],[],[],16,[],[],0
repos/datasets/src/datasets/exceptions.py:FileFormatError,FileFormatError,class,0,1,1,4,4.0,0,0,[],[],[],45,[],[],0
repos/datasets/src/datasets/exceptions.py:FileNotFoundDatasetsError,FileNotFoundDatasetsError,class,0,0,0,0,0.0,0,0,[],[],[],20,[],[],0
repos/datasets/src/datasets/exceptions.py:ManualDownloadError,ManualDownloadError,class,0,1,1,4,4.0,0,0,[],[],[],41,[],[],0
repos/datasets/src/datasets/exceptions.py:DatasetGenerationCastError:from_cast_error,DatasetGenerationCastError:from_cast_error,method,39,122,91,1308,10.72,3,5,"['cls', 'cast_error', 'builder_name', 'gen_kwargs', 'Any]', 'token', 'str]]', '']","[None, ' CastError', ' str', ' Dict[str', None, ' Optional[Union[bool', None, None]","[None, None, None, None, None, None, None, None]",55,[],"['gen_kwargs.values', 'isinstance', 'gen_kwarg.get_origin', 'gen_kwarg.startswith', 'HfFileSystem', 'resolved_path.unresolve', 'gen_kwarg.replace', 'formatted_tracked_gen_kwargs.append', 'cls']",9
repos/datasets/src/datasets/features/audio.py:Audio,Audio,class,101,588,298,5403,9.19,1,14,[],[],[],21,[],[],0
repos/datasets/src/datasets/features/audio.py:Audio:__call__,Audio:__call__,method,2,2,2,18,9.0,0,0,['self'],[None],[None],74,[],[],0
repos/datasets/src/datasets/features/audio.py:Audio:cast_storage,Audio:cast_storage,method,12,72,40,1022,14.19,0,3,"['self', 'storage', 'pa.StructArray]']","[None, ' Union[pa.StringArray', None]","[None, None, None]",209,"['        """"""Cast an Arrow array to the Audio arrow storage type.\n', '        The Arrow types that can be converted to the Audio pyarrow storage type are:\n', '\n', '        - `pa.string()` - it must contain the ""path"" data\n', '        - `pa.binary()` - it must contain the audio bytes\n', '        - `pa.struct({""bytes"": pa.binary()})`\n', '        - `pa.struct({""path"": pa.string()})`\n', '        - `pa.struct({""bytes"": pa.binary(), ""path"": pa.string()})`  - order doesn\'t matter\n', '\n', '        Args:\n', '            storage (`Union[pa.StringArray, pa.StructArray]`):\n', '                PyArrow array to cast.\n', '\n', '        Returns:\n', '            `pa.StructArray`: Array in the Audio arrow storage type, that is\n', '                `pa.struct({""bytes"": pa.binary(), ""path"": pa.string()})`\n', '        """"""\n']","['pa.array', 'len', 'storage.to_pylist', 'storage.field', 'array_cast']",5
repos/datasets/src/datasets/features/audio.py:Audio:decode_example,Audio:decode_example,method,40,210,133,1767,8.41,0,6,"['self', 'value', 'token_per_repo_id', 'Union[str', 'bool', 'None]]] ']","[None, ' dict', ' Optional[Dict[str', None, None, None]","[None, None, None, None, None, ' None']",126,"['        """"""Decode example audio file into audio data.\n', '\n', '        Args:\n', '            value (`dict`):\n', '                A dictionary with keys:\n', '\n', '                - `path`: String with relative audio file path.\n', '                - `bytes`: Bytes of the audio file.\n', '            token_per_repo_id (`dict`, *optional*):\n', '                To access and decode\n', '                audio files from private repositories on the Hub, you can pass\n', '                a dictionary repo_id (`str`) -> token (`bool` or `str`)\n', '\n', '        Returns:\n', '            `dict`\n', '        """"""\n']","['RuntimeError', 'Audio', 'BytesIO', 'ValueError', 'ImportError', 'xsplitext', 'path.split', 'source_url.startswith', 'string_to_dict', 'DownloadConfig', 'xopen', 'sf.read', 'librosa.to_mono', 'librosa.resample']",14
repos/datasets/src/datasets/features/audio.py:Audio:embed_storage,Audio:embed_storage,method,16,61,44,529,8.67,1,0,"['self', 'storage']","[None, ' pa.StructArray']","[None, None]",247,"['        """"""Embed audio files into the Arrow array.\n', '\n', '        Args:\n', '            storage (`pa.StructArray`):\n', '                PyArrow array to embed.\n', '\n', '        Returns:\n', '            `pa.StructArray`: Array in the Audio arrow storage type, that is\n', '                `pa.struct({""bytes"": pa.binary(), ""path"": pa.string()})`.\n', '        """"""\n']","['path_to_bytes', 'xopen', 'f.read', 'pa.array', 'storage.to_pylist', 'storage.field', 'array_cast']",7
repos/datasets/src/datasets/features/audio.py:Audio:encode_example,Audio:encode_example,method,28,154,96,1270,8.25,0,4,"['self', 'value', 'bytes', 'dict]']","[None, ' Union[str', None, None]","[None, None, None, None]",77,"['        """"""Encode example into a format for Arrow.\n', '\n', '        Args:\n', '            value (`str` or `dict`):\n', '                Data passed as input to Audio feature.\n', '\n', '        Returns:\n', '            `dict`\n', '        """"""\n']","['ImportError', 'isinstance', 'BytesIO', 'sf.write', 'buffer.getvalue', 'value.get', 'KeyError', 'np.frombuffer', 'np.memmap', 'ValueError']",10
repos/datasets/src/datasets/features/audio.py:Audio:flatten,Audio:flatten,method,6,20,20,152,7.6,0,1,['self'],[None],[None],198,"['        """"""If in the decodable state, raise an error, otherwise flatten the feature into a dictionary.""""""\n']","['ValueError', 'Value']",2
repos/datasets/src/datasets/features/features.py:_align_features,_align_features,function,13,45,29,361,8.02,2,1,['features_list'],[' List[Features]'],[None],2169,"['    """"""Align dictionaries of features so that the keys that are found in multiple dictionaries share the same feature.""""""\n']","['features.items', 'isinstance', '_align_features', 'features.keys']",4
repos/datasets/src/datasets/features/features.py:_arrow_to_datasets_dtype,_arrow_to_datasets_dtype,function,32,154,83,2026,13.16,0,2,['arrow_type'],[' pa.DataType'],[None],52,"['    """"""\n', '    _arrow_to_datasets_dtype takes a pyarrow.DataType and converts it to a datasets string dtype.\n', '    In effect, `dt == string_to_arrow(_arrow_to_datasets_dtype(dt))`\n', '    """"""\n']","['ValueError', 'f""decimal128', 'f""decimal256']",3
repos/datasets/src/datasets/features/features.py:_cast_to_python_objects,_cast_to_python_objects,function,50,353,114,3344,9.47,8,14,"['obj', 'only_1d_for_numpy', 'optimize_list_casting']","[' Any', ' bool', ' bool']","[None, None, None]",265,"['    """"""\n', '    Cast pytorch/tensorflow/pandas objects to python numpy array/lists.\n', '    It works recursively.\n', '\n', '    If `optimize_list_casting` is True, to avoid iterating over possibly long lists, it first checks (recursively) if the first element that is not None or empty (if it is a sequence) has to be casted.\n', ""    If the first element needs to be casted, then all the elements of the list will be casted, otherwise they'll stay the same.\n"", '    This trick allows to cast objects that contain tokenizers outputs without iterating over every single token for example.\n', '\n', '    Args:\n', '        obj: the object (nested struct) to cast.\n', '        only_1d_for_numpy (bool): whether to keep the full multi-dim tensors as multi-dim numpy arrays, or convert them to\n', '            nested lists of 1-dimensional numpy arrays. This can be useful to keep only 1-d arrays to instantiate Arrow arrays.\n', '            Indeed Arrow only support converting 1-dimensional array values.\n', '        optimize_list_casting (bool): whether to optimize list casting by checking the first non-null element to see if it needs to be casted\n', ""            and if it doesn't, not checking the rest of the list elements.\n"", '\n', '    Returns:\n', '        casted_obj: the casted object\n', '        has_changed (bool): True if the object has been changed, False if it is identical\n', '    """"""\n']","['isinstance', '_cast_to_python_objects', 'obj.detach', 'obj.numpy', 'np.asarray', 'encode_pil_image', 'obj.tolist', 'obj.to_dict', 'obj.to_pydatetime', 'obj.to_pytimedelta', 'obj.items', 'hasattr', 'obj.__array__', 'len', '_check_non_null_non_empty_recursive', 'list']",16
repos/datasets/src/datasets/features/features.py:_check_if_features_can_be_aligned,_check_if_features_can_be_aligned,function,12,75,52,567,7.56,4,2,['features_list'],[' List[Features]'],[None],2183,"['    """"""Check if the dictionaries of features can be aligned.\n', '\n', '    Two dictonaries of features can be aligned if the keys they share have the same type or some of them is of type `Value(""null"")`.\n', '    """"""\n']","['features.items', 'isinstance', '_check_if_features_can_be_aligned', 'ValueError', 'Value']",5
repos/datasets/src/datasets/features/features.py:_check_non_null_non_empty_recursive,_check_non_null_non_empty_recursive,function,7,46,29,326,7.09,0,3,"['obj', 'schema']","[None, ' Optional[FeatureType] ']","[None, ' None']",1177,"['    """"""\n', '    Check if the object is not None.\n', '    If the object is a list or a tuple, recursively check the first element of the sequence and stop if at any point the first element is not a sequence or is an empty sequence.\n', '    """"""\n']","['isinstance', 'len', '_check_non_null_non_empty_recursive']",3
repos/datasets/src/datasets/features/features.py:_is_zero_copy_only,_is_zero_copy_only,function,6,22,19,285,12.95,0,2,"['pa_type', 'unnest']","[' pa.DataType', ' bool ']","[None, ' False']",705,"['    """"""\n', '    When converting a pyarrow array to a numpy array, we must know whether this could be done in zero-copy or not.\n', '    This function returns the value of the ``zero_copy_only`` parameter to pass to ``.to_numpy()``, given the type of the pyarrow array.\n', '\n', '    # zero copy is available for all primitive types except booleans and temporal types (date, time, timestamp or duration)\n', '    # primitive types are types for which the physical representation in arrow and in numpy\n', '    # https://github.com/wesm/arrow/blob/c07b9b48cf3e0bbbab493992a492ae47e5b04cad/python/pyarrow/types.pxi#L821\n', '    # see https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.to_numpy\n', '    # and https://issues.apache.org/jira/browse/ARROW-2871?jql=text%20~%20%22boolean%20to_numpy%22\n', '    """"""\n']",['_unnest_pa_type'],1
repos/datasets/src/datasets/features/features.py:_visit,_visit,function,5,37,28,314,8.49,0,2,"['feature', 'func', 'Optional[FeatureType]]']","[' FeatureType', ' Callable[[FeatureType]', None]","[None, None, None]",1521,"['    """"""Visit a (possibly nested) feature.\n', '\n', '    Args:\n', '        feature (FeatureType): the feature type to be checked\n', '    Returns:\n', '        visited feature (FeatureType)\n', '    """"""\n']","['isinstance', 'func', '_visit', 'feature.items']",4
repos/datasets/src/datasets/features/features.py:any_np_array_to_pyarrow_listarray,any_np_array_to_pyarrow_listarray,function,4,16,14,211,13.19,0,1,"['data', 'List]', 'type']","[' Union[np.ndarray', None, ' pa.DataType ']","[None, None, ' None']",1489,"['    """"""Convert to PyArrow ListArray either a NumPy ndarray or (recursively) a list that may contain any NumPy ndarray.\n', '\n', '    Args:\n', '        data (Union[np.ndarray, List]): Data.\n', '        type (pa.DataType): Explicit PyArrow DataType passed to coerce the ListArray data type.\n', '\n', '    Returns:\n', '        pa.ListArray\n', '    """"""\n']","['isinstance', 'numpy_to_pyarrow_listarray', 'list_of_pa_arrays_to_pyarrow_listarray']",3
repos/datasets/src/datasets/features/features.py:cast_to_python_objects,cast_to_python_objects,function,2,6,6,119,19.83,0,0,"['obj', 'only_1d_for_numpy', 'optimize_list_casting']","[' Any', None, None]","[None, 'False', 'True']",428,"['    """"""\n', '    Cast numpy/pytorch/tensorflow/pandas objects to python lists.\n', '    It works recursively.\n', '\n', '    If `optimize_list_casting` is True, To avoid iterating over possibly long lists, it first checks (recursively) if the first element that is not None or empty (if it is a sequence) has to be casted.\n', ""    If the first element needs to be casted, then all the elements of the list will be casted, otherwise they'll stay the same.\n"", '    This trick allows to cast objects that contain tokenizers outputs without iterating over every single token for example.\n', '\n', '    Args:\n', '        obj: the object (nested struct) to cast\n', '        only_1d_for_numpy (bool, default ``False``): whether to keep the full multi-dim tensors as multi-dim numpy arrays, or convert them to\n', '            nested lists of 1-dimensional numpy arrays. This can be useful to keep only 1-d arrays to instantiate Arrow arrays.\n', '            Indeed Arrow only support converting 1-dimensional array values.\n', '        optimize_list_casting (bool, default ``True``): whether to optimize list casting by checking the first non-null element to see if it needs to be casted\n', ""            and if it doesn't, not checking the rest of the list elements.\n"", '\n', '    Returns:\n', '        casted_obj: the casted object\n', '    """"""\n']",['_cast_to_python_objects'],1
repos/datasets/src/datasets/features/features.py:contains_any_np_array,contains_any_np_array,function,3,13,10,145,11.15,0,1,['data'],[' Any'],[None],1472,"['    """"""Return `True` if data is a NumPy ndarray or (recursively) if first non-null value in list is a NumPy ndarray.\n', '\n', '    Args:\n', '        data (Any): Data.\n', '\n', '    Returns:\n', '        bool\n', '    """"""\n']","['isinstance', 'contains_any_np_array']",2
repos/datasets/src/datasets/features/features.py:decode_nested_example,decode_nested_example,function,12,95,57,825,8.68,1,8,"['schema', 'obj', 'token_per_repo_id', 'Union[str', 'bool', 'None]]] ']","[None, None, ' Optional[Dict[str', None, None, None]","[None, None, None, None, None, ' None']",1305,"['    """"""Decode a nested example.\n', '    This is used since some features (in particular Audio and Image) have some logic during decoding.\n', '\n', '    To avoid iterating over possibly long lists, it first checks (recursively) if the first element that is not None or empty (if it is a sequence) has to be decoded.\n', ""    If the first element needs to be decoded, then all the elements of the list will be decoded, otherwise they'll stay the same.\n"", '    """"""\n']","['isinstance', 'decode_nested_example', 'zip_dict', 'len', '_check_non_null_non_empty_recursive', 'list', 'schema.decode_example']",7
repos/datasets/src/datasets/features/features.py:encode_nested_example,encode_nested_example,function,18,212,92,1560,7.36,4,15,"['schema', 'obj', 'level']","[None, None, None]","[None, None, '0']",1232,"['    """"""Encode a nested example.\n', '    This is used since some features (in particular ClassLabel) have some logic during encoding.\n', '\n', '    To avoid iterating over possibly long lists, it first checks (recursively) if the first element that is not None or empty (if it is a sequence) has to be encoded.\n', ""    If the first element needs to be encoded, then all the elements of the list will be encoded, otherwise they'll stay the same.\n"", '    """"""\n']","['isinstance', 'ValueError', 'encode_nested_example', 'obj.get', 'len', '_check_non_null_non_empty_recursive', 'list', 'o.get', 'schema.encode_example']",9
repos/datasets/src/datasets/features/features.py:generate_from_arrow_type,generate_from_arrow_type,function,15,79,60,915,11.58,0,2,['pa_type'],[' pa.DataType'],[None],1407,"['    """"""\n', '    generate_from_arrow_type accepts an arrow DataType and returns a datasets FeatureType to be used as the type for\n', '        a single field.\n', '\n', '    This is the high-level arrow->datasets type conversion and is inverted by get_nested_type().\n', '\n', '    This operates at the individual *field* level, whereas Features.from_arrow_schema() operates at the\n', '        full schema level and holds the methods that represent the bijection from Features<->pyarrow.Schema\n', '    """"""\n']","['isinstance', 'generate_from_arrow_type', 'Sequence', 'array_feature', 'TODO', 'Value', 'ValueError']",7
repos/datasets/src/datasets/features/features.py:generate_from_dict,generate_from_dict,function,14,74,52,627,8.47,1,4,['obj'],[' Any'],[None],1376,"['    """"""Regenerate the nested feature object from a deserialized dict.\n', ""    We use the '_type' fields to get the dataclass name to load.\n"", '\n', '    generate_from_dict is the recursive helper for Features.from_dict, and allows for a convenient constructor syntax\n', '    to define features from deserialized JSON dictionaries. This function is used in particular when deserializing\n', '    a :class:`DatasetInfo` that was dumped to a JSON object. This acts as an analogue to\n', ""    :meth:`Features.from_arrow_schema` and handles the recursive field-by-field instantiation, but doesn't require any\n"", '    mapping to/from pyarrow, except for the fact that it takes advantage of the mapping of pyarrow primitive dtypes\n', '    that :class:`Value` automatically performs.\n', '    """"""\n']","['isinstance', 'generate_from_dict', 'obj.items', 'dict', 'obj.pop', '_FEATURE_TYPES.get', 'globals', 'ValueError', 'Sequence', 'fields', 'class_type']",11
repos/datasets/src/datasets/features/features.py:get_nested_type,get_nested_type,function,19,96,66,760,7.92,0,3,['schema'],[' FeatureType'],[None],1199,"['    """"""\n', '    get_nested_type() converts a datasets.FeatureType into a pyarrow.DataType, and acts as the inverse of\n', '        generate_from_arrow_type().\n', '\n', '    It performs double-duty as the implementation of Features.type and handles the conversion of\n', '        datasets.Feature->pa.struct\n', '    """"""\n']","['isinstance', 'pa.struct', 'get_nested_type', 'len', 'ValueError', 'pa.list_', 'schema']",7
repos/datasets/src/datasets/features/features.py:keep_features_dicts_synced,keep_features_dicts_synced,function,11,36,33,366,10.17,0,1,['func'],[None],[None],1596,"['    """"""\n', '    Wrapper to keep the secondary dictionary, which tracks whether keys are decodable, of the :class:`datasets.Features` object\n', '    in sync with the main dictionary.\n', '    """"""\n']","['wrapper', 'kwargs.pop', 'func', 'hasattr', 'require_decoding', 'self.items']",6
repos/datasets/src/datasets/features/features.py:list_of_np_array_to_pyarrow_listarray,list_of_np_array_to_pyarrow_listarray,function,3,24,19,177,7.38,0,1,"['l_arr', 'type']","[' List[np.ndarray]', ' pa.DataType ']","[None, ' None']",1462,"['    """"""Build a PyArrow ListArray from a possibly nested list of NumPy arrays""""""\n']","['len', 'list_of_pa_arrays_to_pyarrow_listarray', 'pa.array']",3
repos/datasets/src/datasets/features/features.py:list_of_pa_arrays_to_pyarrow_listarray,list_of_pa_arrays_to_pyarrow_listarray,function,21,53,40,430,8.11,1,1,['l_arr'],[' List[Optional[pa.Array]]'],[None],1449,[],"['np.array', 'np.arange', 'np.cumsum', 'np.insert', 'pa.array', 'pa.concat_arrays']",6
repos/datasets/src/datasets/features/features.py:numpy_to_pyarrow_listarray,numpy_to_pyarrow_listarray,function,13,31,27,300,9.68,1,0,"['arr', 'type']","[' np.ndarray', ' pa.DataType ']","[None, ' None']",1437,"['    """"""Build a PyArrow ListArray from a multidimensional NumPy array""""""\n']","['np.array', 'pa.array', 'range', 'reduce']",4
repos/datasets/src/datasets/features/features.py:pandas_types_mapper,pandas_types_mapper,function,3,5,5,92,18.4,0,1,['dtype'],[None],[None],926,[],"['isinstance', 'PandasArrayExtensionDtype']",2
repos/datasets/src/datasets/features/features.py:register_feature,register_feature,function,5,15,15,197,13.13,0,1,"['feature_cls', 'feature_type', '']","[' type', ' str', None]","[None, None, None]",1361,"['    """"""\n', '    Register a Feature object using a name and class.\n', '    This function must be used on a Feature class.\n', '    """"""\n']",['logger.warning'],1
repos/datasets/src/datasets/features/features.py:require_decoding,require_decoding,function,5,31,24,328,10.58,0,1,"['feature', 'ignore_decode_attribute']","[' FeatureType', ' bool ']","[None, ' False']",1540,"['    """"""Check if a (possibly nested) feature requires decoding.\n', '\n', '    Args:\n', '        feature (FeatureType): the feature type to be checked\n', '        ignore_decode_attribute (:obj:`bool`, default ``False``): Whether to ignore the current value\n', '            of the `decode` attribute of the decodable feature types.\n', '    Returns:\n', '        :obj:`bool`\n', '    """"""\n']","['isinstance', 'any', 'feature.values', 'require_decoding', 'hasattr']",5
repos/datasets/src/datasets/features/features.py:require_storage_cast,require_storage_cast,function,5,24,18,283,11.79,0,1,['feature'],[' FeatureType'],[None],1560,"['    """"""Check if a (possibly nested) feature requires storage casting.\n', '\n', '    Args:\n', '        feature (FeatureType): the feature type to be checked\n', '    Returns:\n', '        :obj:`bool`\n', '    """"""\n']","['isinstance', 'any', 'feature.values', 'require_storage_cast', 'hasattr']",5
repos/datasets/src/datasets/features/features.py:require_storage_embed,require_storage_embed,function,5,24,18,284,11.83,0,1,['feature'],[' FeatureType'],[None],1578,"['    """"""Check if a (possibly nested) feature requires embedding data into storage.\n', '\n', '    Args:\n', '        feature (FeatureType): the feature type to be checked\n', '    Returns:\n', '        :obj:`bool`\n', '    """"""\n']","['isinstance', 'any', 'feature.values', 'require_storage_cast', 'hasattr']",5
repos/datasets/src/datasets/features/features.py:string_to_arrow,string_to_arrow,function,36,337,188,4215,12.51,0,16,['datasets_dtype'],[' str'],[None],116,"['    """"""\n', '    string_to_arrow takes a datasets string dtype and converts it to a pyarrow.DataType.\n', '\n', '    In effect, `dt == string_to_arrow(_arrow_to_datasets_dtype(dt))`\n', '\n', '    This is necessary because the datasets.Value() primitive type is constructed using a string dtype\n', '\n', '    Value(dtype=str)\n', '\n', '    But Features.type (via `get_nested_type()` expects to resolve Features into a pyarrow Schema,\n', '        which means that each Value() must be able to resolve into a corresponding pyarrow.DataType, which is the\n', '        purpose of this function.\n', '    """"""\n']","['_dtype_error_msg', 'len', 're.search', 'timestamp_matches.group', 'pa.timestamp', 'internals_matches.group', 'ValueError', 'duration_matches.group', 'pa.duration', 'time_matches.group', 'pa.time32', 'pa.time64', 'decimal_matches.group', 'decimal_internals_precision_and_scale.group', 'pa.decimal128', 'int', 'pa.decimal256']",17
repos/datasets/src/datasets/features/features.py:to_pyarrow_listarray,to_pyarrow_listarray,function,4,9,8,149,16.56,0,1,"['data', 'pa_type']","[' Any', ' _ArrayXDExtensionType']","[None, None]",1505,"['    """"""Convert to PyArrow ListArray.\n', '\n', '    Args:\n', '        data (Any): Sequence, iterable, np.ndarray or pd.Series.\n', '        pa_type (_ArrayXDExtensionType): Any of the ArrayNDExtensionType.\n', '\n', '    Returns:\n', '        pyarrow.Array\n', '    """"""\n']","['contains_any_np_array', 'any_np_array_to_pyarrow_listarray', 'pa.array']",3
repos/datasets/src/datasets/features/features.py:Array2D,Array2D,class,8,12,12,100,8.33,0,0,[],[],[],537,[],[],0
repos/datasets/src/datasets/features/features.py:Array2DExtensionType,Array2DExtensionType,class,1,2,2,7,3.5,0,0,[],[],[],682,[],[],0
repos/datasets/src/datasets/features/features.py:Array3D,Array3D,class,8,12,12,100,8.33,0,0,[],[],[],562,[],[],0
repos/datasets/src/datasets/features/features.py:Array3DExtensionType,Array3DExtensionType,class,1,2,2,7,3.5,0,0,[],[],[],686,[],[],0
repos/datasets/src/datasets/features/features.py:Array4D,Array4D,class,8,12,12,100,8.33,0,0,[],[],[],587,[],[],0
repos/datasets/src/datasets/features/features.py:Array4DExtensionType,Array4DExtensionType,class,1,2,2,7,3.5,0,0,[],[],[],690,[],[],0
repos/datasets/src/datasets/features/features.py:Array5D,Array5D,class,8,12,12,100,8.33,0,0,[],[],[],612,[],[],0
repos/datasets/src/datasets/features/features.py:Array5DExtensionType,Array5DExtensionType,class,1,2,2,7,3.5,0,0,[],[],[],694,[],[],0
repos/datasets/src/datasets/features/features.py:ArrayExtensionArray,ArrayExtensionArray,class,51,130,93,1621,12.47,3,5,[],[],[],727,[],[],0
repos/datasets/src/datasets/features/features.py:ClassLabel,ClassLabel,class,72,482,233,3921,8.13,2,21,[],[],[],932,[],[],0
repos/datasets/src/datasets/features/features.py:Features,Features,class,130,935,429,9340,9.99,11,38,[],[],[],1618,[],[],0
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray,PandasArrayExtensionArray,class,58,314,189,2850,9.08,3,10,[],[],[],822,[],[],0
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionDtype,PandasArrayExtensionDtype,class,19,63,44,765,12.14,0,1,[],[],[],788,[],[],0
repos/datasets/src/datasets/features/features.py:Sequence,Sequence,class,12,19,18,159,8.37,0,0,[],[],[],1130,[],[],0
repos/datasets/src/datasets/features/features.py:Value,Value,class,21,61,42,611,10.02,0,3,[],[],[],454,[],[],0
repos/datasets/src/datasets/features/features.py:_ArrayXD,_ArrayXD,class,9,17,14,210,12.35,0,0,[],[],[],524,[],[],0
repos/datasets/src/datasets/features/features.py:_ArrayXDExtensionType,_ArrayXDExtensionType,class,36,116,90,1240,10.69,2,3,[],[],[],636,[],[],0
repos/datasets/src/datasets/features/features.py:ArrayExtensionArray:__array__,ArrayExtensionArray:__array__,method,4,5,5,115,23.0,0,0,['self'],[None],[None],728,[],"['_is_zero_copy_only', 'self.to_numpy']",2
repos/datasets/src/datasets/features/features.py:ArrayExtensionArray:__getitem__,ArrayExtensionArray:__getitem__,method,2,2,2,21,10.5,0,0,"['self', 'i']","[None, None]","[None, None]",732,[],[],0
repos/datasets/src/datasets/features/features.py:ArrayExtensionArray:to_numpy,ArrayExtensionArray:to_numpy,method,39,92,72,1128,12.26,3,4,"['self', 'zero_copy_only']","[None, None]","[None, 'True']",735,[],"['storage.is_null', 'np.arange', 'range', 'storage.flatten', 'storage.to_numpy', 'numpy_arr.reshape', 'len', 'np.insert', 'np.array', 'enumerate', 'arrays.append', 'storage_el.flatten', 'storage_el.to_numpy', 'np.empty']",14
repos/datasets/src/datasets/features/features.py:ArrayExtensionArray:to_pylist,ArrayExtensionArray:to_pylist,method,10,21,20,251,11.95,0,1,['self'],[None],[None],779,[],"['_is_zero_copy_only', 'self.to_numpy', 'numpy_arr.tolist']",3
repos/datasets/src/datasets/features/features.py:ClassLabel:__call__,ClassLabel:__call__,method,2,2,2,18,9.0,0,0,['self'],[None],[None],1003,[],[],0
repos/datasets/src/datasets/features/features.py:ClassLabel:__post_init__,ClassLabel:__post_init__,method,16,126,73,995,7.9,1,5,"['self', 'num_classes', 'names_file']","[None, None, None]","[None, None, None]",974,[],"['ValueError', 'self._load_names_from_file', 'range', 'isinstance', 'TypeError', 'len', 'enumerate']",7
repos/datasets/src/datasets/features/features.py:ClassLabel:_load_names_from_file,ClassLabel:_load_names_from_file,method,3,17,17,126,7.41,0,0,['names_filepath'],[None],[None],1124,[],"['open', 'f.read', 'name.strip']",3
repos/datasets/src/datasets/features/features.py:ClassLabel:_strval2int,ClassLabel:_strval2int,method,9,43,28,365,8.49,0,4,"['self', 'value']","[None, ' str']","[None, None]",1030,[],"['str', 'int', 'ValueError']",3
repos/datasets/src/datasets/features/features.py:ClassLabel:cast_storage,ClassLabel:cast_storage,method,9,49,37,430,8.78,0,2,"['self', 'storage', 'pa.IntegerArray]']","[None, ' Union[pa.StringArray', None]","[None, None, None]",1097,"['        """"""Cast an Arrow array to the `ClassLabel` arrow storage type.\n', '        The Arrow types that can be converted to the `ClassLabel` pyarrow storage type are:\n', '\n', '        - `pa.string()`\n', '        - `pa.int()`\n', '\n', '        Args:\n', '            storage (`Union[pa.StringArray, pa.IntegerArray]`):\n', '                PyArrow array to cast.\n', '\n', '        Returns:\n', '            `pa.Int64Array`: Array in the `ClassLabel` arrow storage type.\n', '        """"""\n']","['isinstance', 'len', 'pc.min_max', 'ValueError', 'pa.array', 'storage.to_pylist', 'array_cast']",7
repos/datasets/src/datasets/features/features.py:ClassLabel:encode_example,ClassLabel:encode_example,method,6,45,40,372,8.27,0,3,"['self', 'example_data']","[None, None]","[None, None]",1081,[],"['ValueError', 'isinstance', 'self.str2int']",3
repos/datasets/src/datasets/features/features.py:ClassLabel:int2str,ClassLabel:int2str,method,9,63,46,421,6.68,1,4,"['self', 'values', 'Iterable]']","[None, ' Union[int', None]","[None, None, None]",1051,"['        """"""Conversion `integer` => class name `string`.\n', '\n', '        Regarding unknown/missing labels: passing negative integers raises `ValueError`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"")\n', '        >>> ds.features[""label""].int2str(0)\n', ""        'neg'\n"", '        ```\n', '        """"""\n']","['isinstance', 'ValueError']",2
repos/datasets/src/datasets/features/features.py:ClassLabel:str2int,ClassLabel:str2int,method,7,47,39,333,7.09,0,3,"['self', 'values', 'Iterable]']","[None, ' Union[str', None]","[None, None, None]",1006,"['        """"""Conversion class name `string` => `integer`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"")\n', '        >>> ds.features[""label""].str2int(\'neg\')\n', '        0\n', '        ```\n', '        """"""\n']","['isinstance', 'ValueError']",2
repos/datasets/src/datasets/features/features.py:Features:__init__,Features:__init__,method,9,30,30,252,8.4,0,1,"['*args', '**kwargs']","[None, None]","[None, None]",1651,[],"['TypeError', 'super', 'require_decoding', 'self.items']",4
repos/datasets/src/datasets/features/features.py:Features:__reduce__,Features:__reduce__,method,2,3,3,28,9.33,0,0,['self'],[None],[None],1669,[],[],0
repos/datasets/src/datasets/features/features.py:Features:_from_yaml_list,Features:_from_yaml_list,method,21,172,108,1735,10.09,0,11,"['cls', 'yaml_data']","[None, ' list']","[None, None]",1843,[],"['copy.deepcopy', 'unsimplify', 'isinstance', 'TypeError', 'sorted', 'list', 'ValueError', 'from_yaml_inner', 'next', 'Value', 'snakecase_to_camelcase', 'zip', 'cls.from_dict']",13
repos/datasets/src/datasets/features/features.py:Features:_to_yaml_list,Features:_to_yaml_list,method,18,179,92,1869,10.44,0,9,['self'],[None],[None],1756,[],"['self.to_dict', 'simplify', 'isinstance', 'TypeError', 'list', 'str', 'enumerate', 'to_yaml_inner', 'obj.pop', 'camelcase_to_snakecase', 'obj.items', 'to_yaml_types']",12
repos/datasets/src/datasets/features/features.py:Features:arrow_schema,Features:arrow_schema,method,4,7,7,130,18.57,0,0,['self'],[None],[None],1683,"['        """"""\n', '        Features schema.\n', '\n', '        Returns:\n', '            :obj:`pyarrow.Schema`\n', '        """"""\n']","['self.to_dict', 'pa.schema', 'json.dumps']",3
repos/datasets/src/datasets/features/features.py:Features:copy,Features:copy,method,2,2,2,25,12.5,0,0,['self'],[None],[None],2029,"['        """"""\n', '        Make a deep copy of [`Features`].\n', '\n', '        Returns:\n', '            [`Features`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"")\n', '        >>> copy_of_features = ds.features.copy()\n', '        >>> copy_of_features\n', ""        {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", '        ```\n', '        """"""\n']",['copy.deepcopy'],1
repos/datasets/src/datasets/features/features.py:Features:decode_batch,Features:decode_batch,method,8,32,24,292,9.12,2,2,"['self', 'batch', 'token_per_repo_id', 'Union[str', 'bool', 'None]]] ']","[None, ' dict', ' Optional[Dict[str', None, None, None]","[None, None, None, None, None, ' None']",2002,"['        """"""Decode batch with custom feature decoding.\n', '\n', '        Args:\n', '            batch (`dict[str, list[Any]]`):\n', '                Dataset batch data.\n', '            token_per_repo_id (`dict`, *optional*):\n', '                To access and decode audio or image files from private repositories on the Hub, you can pass\n', '                a dictionary repo_id (str) -> token (bool or str)\n', '\n', '        Returns:\n', '            `dict[str, list[Any]]`\n', '        """"""\n']","['batch.items', 'decode_nested_example']",2
repos/datasets/src/datasets/features/features.py:Features:decode_column,Features:decode_column,method,3,20,16,155,7.75,0,1,"['self', 'column', 'column_name']","[None, ' list', ' str']","[None, None, None]",1984,"['        """"""Decode column with custom feature decoding.\n', '\n', '        Args:\n', '            column (`list[Any]`):\n', '                Dataset column data.\n', '            column_name (`str`):\n', '                Dataset column name.\n', '\n', '        Returns:\n', '            `list[Any]`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/features/features.py:Features:decode_example,Features:decode_example,method,8,30,25,254,8.47,2,1,"['self', 'example', 'token_per_repo_id', 'Union[str', 'bool', 'None]]] ']","[None, ' dict', ' Optional[Dict[str', None, None, None]","[None, None, None, None, None, ' None']",1961,"['        """"""Decode example with custom feature decoding.\n', '\n', '        Args:\n', '            example (`dict[str, Any]`):\n', '                Dataset row data.\n', '            token_per_repo_id (`dict`, *optional*):\n', '                To access and decode audio or image files from private repositories on the Hub, you can pass\n', '                a dictionary `repo_id (str) -> token (bool or str)`.\n', '\n', '        Returns:\n', '            `dict[str, Any]`\n', '        """"""\n']","['decode_nested_example', 'zip_dict', 'self.items']",3
repos/datasets/src/datasets/features/features.py:Features:encode_batch,Features:encode_batch,method,8,31,27,290,9.35,1,1,"['self', 'batch']","[None, None]","[None, None]",1942,"['        """"""\n', '        Encode batch into a format for Arrow.\n', '\n', '        Args:\n', '            batch (`dict[str, list[Any]]`):\n', '                Data in a Dataset batch.\n', '\n', '        Returns:\n', '            `dict[str, list[Any]]`\n', '        """"""\n']","['set', 'ValueError', 'batch.items', 'cast_to_python_objects']",4
repos/datasets/src/datasets/features/features.py:Features:encode_column,Features:encode_column,method,3,10,10,112,11.2,0,0,"['self', 'column', 'column_name']","[None, None, ' str']","[None, None, None]",1926,"['        """"""\n', '        Encode column into a format for Arrow.\n', '\n', '        Args:\n', '            column (`list[Any]`):\n', '                Data in a Dataset column.\n', '            column_name (`str`):\n', '                Dataset column name.\n', '\n', '        Returns:\n', '            `list[Any]`\n', '        """"""\n']",['cast_to_python_objects'],1
repos/datasets/src/datasets/features/features.py:Features:encode_example,Features:encode_example,method,4,5,5,81,16.2,0,0,"['self', 'example']","[None, None]","[None, None]",1912,"['        """"""\n', '        Encode example into a format for Arrow.\n', '\n', '        Args:\n', '            example (`dict[str, Any]`):\n', '                Data in a Dataset row.\n', '\n', '        Returns:\n', '            `dict[str, Any]`\n', '        """"""\n']","['cast_to_python_objects', 'encode_nested_example']",2
repos/datasets/src/datasets/features/features.py:Features:flatten,Features:flatten,method,18,79,48,724,9.16,3,2,"['self', 'max_depth']","[None, None]","[None, '16']",2116,"['        """"""Flatten the features. Every dictionary column is removed and is replaced by\n', '        all the subfields it contains. The new fields are named by concatenating the\n', '        name of the original column and the subfield name like this: `<original>.<subfield>`.\n', '\n', '        If a column contains nested dictionaries, then all the lower-level subfields names are\n', '        also concatenated to form new columns: `<original>.<subfield>.<subsubfield>`, etc.\n', '\n', '        Returns:\n', '            [`Features`]:\n', '                The flattened features.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""squad"", split=""train"")\n', '        >>> ds.features.flatten()\n', ""        {'answers.answer_start': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n"", ""         'answers.text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n"", ""         'context': Value(dtype='string', id=None),\n"", ""         'id': Value(dtype='string', id=None),\n"", ""         'question': Value(dtype='string', id=None),\n"", ""         'title': Value(dtype='string', id=None)}\n"", '        ```\n', '        """"""\n']","['range', 'self.copy', 'self.items', 'isinstance', 'flattened.update', 'subfeature.items', 'Sequence', 'hasattr', 'subfeature.flatten']",9
repos/datasets/src/datasets/features/features.py:Features:from_arrow_schema,Features:from_arrow_schema,method,16,52,38,620,11.92,1,2,"['cls', 'pa_schema']","[None, ' pa.Schema']","[None, None]",1694,"['        """"""\n', '        Construct [`Features`] from Arrow Schema.\n', '        It also checks the schema metadata for Hugging Face Datasets features.\n', '        Non-nullable fields are not supported and set to nullable.\n', '\n', '        Args:\n', '            pa_schema (`pyarrow.Schema`):\n', '                Arrow Schema.\n', '\n', '        Returns:\n', '            [`Features`]\n', '        """"""\n']","['Features', 'json.loads', 'Features.from_dict', 'metadata_features_schema.field', 'generate_from_arrow_type', 'cls']",6
repos/datasets/src/datasets/features/features.py:Features:from_dict,Features:from_dict,method,4,4,4,44,11.0,0,0,"['cls', 'dic']","[None, None]","[None, None]",1725,"['        """"""\n', '        Construct [`Features`] from dict.\n', '\n', '        Regenerate the nested feature object from a deserialized dict.\n', '        We use the `_type` key to infer the dataclass name of the feature `FieldType`.\n', '\n', '        It allows for a convenient constructor syntax\n', '        to define features from deserialized JSON dictionaries. This function is used in particular when deserializing\n', '        a [`DatasetInfo`] that was dumped to a JSON object. This acts as an analogue to\n', ""        [`Features.from_arrow_schema`] and handles the recursive field-by-field instantiation, but doesn't require\n"", '        any mapping to/from pyarrow, except for the fact that it takes advantage of the mapping of pyarrow primitive\n', '        dtypes that [`Value`] automatically performs.\n', '\n', '        Args:\n', '            dic (`dict[str, Any]`):\n', '                Python dictionary.\n', '\n', '        Returns:\n', '            `Features`\n', '\n', '        Example::\n', ""            >>> Features.from_dict({'_type': {'dtype': 'string', 'id': None, '_type': 'Value'}})\n"", ""            {'_type': Value(dtype='string', id=None)}\n"", '        """"""\n']","['generate_from_dict', 'cls']",2
repos/datasets/src/datasets/features/features.py:Features:reorder_fields_as,Features:reorder_fields_as,method,25,172,92,1487,8.65,2,8,"['self', 'other']","[None, ' ""Features""']","[None, None]",2049,"['        """"""\n', '        Reorder Features fields to match the field order of other [`Features`].\n', '\n', '        The order of the fields is important since it matters for the underlying arrow data.\n', '        Re-ordering the fields allows to make the underlying arrow data type match.\n', '\n', '        Args:\n', '            other ([`Features`]):\n', '                The other [`Features`] to align with.\n', '\n', '        Returns:\n', '            [`Features`]\n', '\n', '        Example::\n', '\n', '            >>> from datasets import Features, Sequence, Value\n', ""            >>> # let's say we have to features with a different order of nested fields (for a and b for example)\n"", '            >>> f1 = Features({""root"": Sequence({""a"": Value(""string""), ""b"": Value(""string"")})})\n', '            >>> f2 = Features({""root"": {""b"": Sequence(Value(""string"")), ""a"": Sequence(Value(""string""))}})\n', '            >>> assert f1.type != f2.type\n', '            >>> # re-ordering keeps the base structure (here Sequence is defined at the root level), but make the fields order match\n', '            >>> f1.reorder_fields_as(f2)\n', ""            {'root': Sequence(feature={'b': Value(dtype='string', id=None), 'a': Value(dtype='string', id=None)}, length=-1, id=None)}\n"", '            >>> assert f1.reorder_fields_as(f2).type == f2.type\n', '        """"""\n']","['recursive_reorder', 'isinstance', 'target.items', 'source.items', 'Sequence', 'reordered.items', 'ValueError', 'sorted', 'len', 'range', 'Features']",11
repos/datasets/src/datasets/features/features.py:Features:to_dict,Features:to_dict,method,2,2,2,18,9.0,0,0,['self'],[None],[None],1753,[],['asdict'],1
repos/datasets/src/datasets/features/features.py:Features:type,Features:type,method,2,2,2,27,13.5,0,0,['self'],[None],[None],1673,"['        """"""\n', '        Features field types.\n', '\n', '        Returns:\n', '            :obj:`pyarrow.DataType`\n', '        """"""\n']",['get_nested_type'],1
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:__array__,PandasArrayExtensionArray:__array__,method,11,23,19,189,8.22,1,2,"['self', 'dtype']","[None, None]","[None, 'None']",827,"['        """"""\n', '        Convert to NumPy Array.\n', '        Note that Pandas expects a 1D array when dtype is set to object.\n', '        But for other dtypes, the returned shape is the same as the one of ``data``.\n', '\n', '        More info about pandas 1D requirement for PandasExtensionArray here:\n', '        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.api.extensions.ExtensionArray.html#pandas.api.extensions.ExtensionArray\n', '\n', '        """"""\n']","['np.empty', 'range']",2
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:__eq__,PandasArrayExtensionArray:__eq__,method,3,14,14,153,10.93,0,1,"['self', 'other']","[None, None]","[None, None]",920,[],"['isinstance', 'NotImplementedError']",2
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:__getitem__,PandasArrayExtensionArray:__getitem__,method,4,8,7,107,13.38,0,1,"['self', 'item', 'slice', 'np.ndarray]']","[None, ' Union[int', None, None]","[None, None, None, None]",889,[],"['isinstance', 'PandasArrayExtensionArray']",2
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:__init__,PandasArrayExtensionArray:__init__,method,6,9,9,92,10.22,0,1,"['self', 'data', 'copy']","[None, ' np.ndarray', ' bool ']","[None, None, ' False']",823,[],"['np.array', 'PandasArrayExtensionDtype']",2
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:__len__,PandasArrayExtensionArray:__len__,method,1,2,2,21,10.5,0,0,['self'],[None],[None],917,[],['len'],1
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:__setitem__,PandasArrayExtensionArray:__setitem__,method,1,2,2,26,13.0,0,0,"['self', 'key', 'slice', 'np.ndarray]', 'value']","[None, ' Union[int', None, None, ' Any']","[None, None, None, None, None]",886,[],['NotImplementedError'],1
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:_concat_same_type,PandasArrayExtensionArray:_concat_same_type,method,14,35,27,283,8.09,2,1,"['cls', 'to_concat']","[None, ' Sequence_[""PandasArrayExtensionArray""]']","[None, None]",864,[],"['len', 'all', 'np.vstack', 'np.empty', 'cls']",5
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:_from_sequence,PandasArrayExtensionArray:_from_sequence,method,8,38,33,287,7.55,0,1,"['cls', 'scalars', 'dtype', 'copy']","[None, None, ' Optional[PandasArrayExtensionDtype] ', ' bool ']","[None, None, ' None', ' False']",851,[],"['len', 'all', 'isinstance', 'np.array', 'np.empty', 'cls']",6
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:copy,PandasArrayExtensionArray:copy,method,2,3,3,53,17.67,0,0,"['self', 'deep']","[None, ' bool ']","[None, ' False']",847,[],['PandasArrayExtensionArray'],1
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:dtype,PandasArrayExtensionArray:dtype,method,2,2,2,17,8.5,0,0,['self'],[None],[None],876,[],[],0
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:isna,PandasArrayExtensionArray:isna,method,2,6,6,54,9.0,0,0,['self'],[None],[None],883,[],['np.array'],1
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:nbytes,PandasArrayExtensionArray:nbytes,method,2,2,2,23,11.5,0,0,['self'],[None],[None],880,[],[],0
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionArray:take,PandasArrayExtensionArray:take,method,19,76,63,680,8.95,0,3,"['self', 'indices', 'allow_fill', 'fill_value']","[None, ' Sequence_[int]', ' bool ', ' bool ']","[None, None, ' False', ' None']",894,[],"['np.asarray', 'ValueError', 'len', 'np.all', 'IndexError', 'np.array', 'PandasArrayExtensionArray', 'mask.any', 'np.sum']",9
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionDtype:__from_arrow__,PandasArrayExtensionDtype:__from_arrow__,method,9,16,16,286,17.88,0,1,"['self', 'array', 'pa.ChunkedArray]']","[None, ' Union[pa.Array', None]","[None, None, None]",794,[],"['isinstance', '_is_zero_copy_only', 'array.to_numpy', 'PandasArrayExtensionArray']",4
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionDtype:__init__,PandasArrayExtensionDtype:__init__,method,2,2,2,27,13.5,0,0,"['self', 'value_type', 'np.dtype]']","[None, ' Union[""PandasArrayExtensionDtype""', None]","[None, None, None]",791,[],[],0
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionDtype:construct_array_type,PandasArrayExtensionDtype:construct_array_type,method,2,2,2,31,15.5,0,0,['cls'],[None],[None],802,[],[],0
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionDtype:kind,PandasArrayExtensionDtype:kind,method,1,2,2,9,4.5,0,0,['self'],[None],[None],810,[],[],0
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionDtype:name,PandasArrayExtensionDtype:name,method,1,2,2,33,16.5,0,0,['self'],[None],[None],814,[],[],0
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionDtype:type,PandasArrayExtensionDtype:type,method,2,2,2,16,8.0,0,0,['self'],[None],[None],806,[],[],0
repos/datasets/src/datasets/features/features.py:PandasArrayExtensionDtype:value_type,PandasArrayExtensionDtype:value_type,method,2,2,2,22,11.0,0,0,['self'],[None],[None],818,[],[],0
repos/datasets/src/datasets/features/features.py:Value:__call__,Value:__call__,method,2,2,2,18,9.0,0,0,['self'],[None],[None],508,[],[],0
repos/datasets/src/datasets/features/features.py:Value:__post_init__,Value:__post_init__,method,3,20,12,161,8.05,0,2,['self'],[None],[None],501,[],['string_to_arrow'],1
repos/datasets/src/datasets/features/features.py:Value:encode_example,Value:encode_example,method,8,19,13,242,12.74,0,1,"['self', 'value']","[None, None]","[None, None]",511,[],"['bool', 'int', 'float', 'str']",4
repos/datasets/src/datasets/features/features.py:_ArrayXD:__call__,_ArrayXD:__call__,method,3,6,5,95,15.83,0,0,['self'],[None],[None],508,[],['globals'],1
repos/datasets/src/datasets/features/features.py:_ArrayXD:__post_init__,_ArrayXD:__post_init__,method,2,2,2,28,14.0,0,0,['self'],[None],[None],501,[],['tuple'],1
repos/datasets/src/datasets/features/features.py:_ArrayXD:encode_example,_ArrayXD:encode_example,method,2,2,2,11,5.5,0,0,"['self', 'value']","[None, None]","[None, None]",511,[],[],0
repos/datasets/src/datasets/features/features.py:_ArrayXDExtensionType:__arrow_ext_class__,_ArrayXDExtensionType:__arrow_ext_class__,method,2,2,2,25,12.5,0,0,['self'],[None],[None],667,[],[],0
repos/datasets/src/datasets/features/features.py:_ArrayXDExtensionType:__arrow_ext_deserialize__,_ArrayXDExtensionType:__arrow_ext_deserialize__,method,4,4,4,44,11.0,0,0,"['cls', 'storage_type', 'serialized']","[None, None, None]","[None, None, None]",656,[],"['json.loads', 'cls']",2
repos/datasets/src/datasets/features/features.py:_ArrayXDExtensionType:__arrow_ext_serialize__,_ArrayXDExtensionType:__arrow_ext_serialize__,method,2,3,3,55,18.33,0,0,['self'],[None],[None],652,[],['json.dumps'],1
repos/datasets/src/datasets/features/features.py:_ArrayXDExtensionType:__hash__,_ArrayXDExtensionType:__hash__,method,2,4,4,55,13.75,0,0,['self'],[None],[None],664,[],['hash'],1
repos/datasets/src/datasets/features/features.py:_ArrayXDExtensionType:__init__,_ArrayXDExtensionType:__init__,method,13,60,51,529,8.82,1,3,"['self', 'shape', 'dtype']","[None, ' tuple', ' str']","[None, None, None]",639,[],"['ValueError', 'len', 'range', 'tuple', 'self._generate_dtype']",5
repos/datasets/src/datasets/features/features.py:_ArrayXDExtensionType:__reduce__,_ArrayXDExtensionType:__reduce__,method,2,4,4,87,21.75,0,0,['self'],[None],[None],661,[],['self.__arrow_ext_serialize__'],1
repos/datasets/src/datasets/features/features.py:_ArrayXDExtensionType:_generate_dtype,_ArrayXDExtensionType:_generate_dtype,method,6,10,8,90,9.0,1,0,"['self', 'dtype']","[None, None]","[None, None]",670,[],"['string_to_arrow', 'reversed', 'pa.list_']",3
repos/datasets/src/datasets/features/features.py:_ArrayXDExtensionType:to_pandas_dtype,_ArrayXDExtensionType:to_pandas_dtype,method,2,2,2,48,24.0,0,0,['self'],[None],[None],678,[],['PandasArrayExtensionDtype'],1
repos/datasets/src/datasets/features/image.py:encode_np_array,encode_np_array,function,31,150,98,1265,8.43,1,7,['array'],[' np.ndarray'],[None],318,[],"['ImportError', 'TypeError', 'np.dtype', 'warnings.warn', 'str', 'image_to_bytes']",6
repos/datasets/src/datasets/features/image.py:encode_pil_image,encode_pil_image,function,4,17,14,148,8.71,0,1,['image'],"[' ""PIL.Image.Image""']",[None],311,[],"['hasattr', 'image_to_bytes']",2
repos/datasets/src/datasets/features/image.py:image_to_bytes,image_to_bytes,function,8,25,22,213,8.52,0,1,['image'],"[' ""PIL.Image.Image""']",[None],300,"['    """"""Convert a PIL Image object to bytes using native compression if possible, otherwise use PNG/TIFF compression.""""""\n']","['BytesIO', 'list_image_compression_formats', 'image.save', 'buffer.getvalue']",4
repos/datasets/src/datasets/features/image.py:list_image_compression_formats,list_image_compression_formats,function,7,26,23,318,12.23,0,2,[],[],[],287,[],"['ImportError', 'list', 'set']",3
repos/datasets/src/datasets/features/image.py:objects_to_list_of_image_dicts,objects_to_list_of_image_dicts,function,12,65,40,568,8.74,0,4,"['objs', 'List[dict]', 'List[np.ndarray]', 'List[""PIL.Image.Image""]]', '']","[' Union[List[str]', None, None, None, None]","[None, None, None, None, None]",361,"['    """"""Encode a list of objects into a format suitable for creating an extension array of type `ImageExtensionType`.""""""\n']","['ImportError', 'first_non_null_value', 'isinstance', 'no_op_if_value_is_null']",4
repos/datasets/src/datasets/features/image.py:Image,Image,class,84,449,233,4439,9.89,1,14,[],[],[],46,[],[],0
repos/datasets/src/datasets/features/image.py:Image:__call__,Image:__call__,method,2,2,2,18,9.0,0,0,['self'],[None],[None],91,[],[],0
repos/datasets/src/datasets/features/image.py:Image:cast_storage,Image:cast_storage,method,14,85,44,1182,13.91,0,3,"['self', 'storage', 'pa.StructArray', 'pa.ListArray]']","[None, ' Union[pa.StringArray', None, None]","[None, None, None, None]",208,"['        """"""Cast an Arrow array to the Image arrow storage type.\n', '        The Arrow types that can be converted to the Image pyarrow storage type are:\n', '\n', '        - `pa.string()` - it must contain the ""path"" data\n', '        - `pa.binary()` - it must contain the image bytes\n', '        - `pa.struct({""bytes"": pa.binary()})`\n', '        - `pa.struct({""path"": pa.string()})`\n', '        - `pa.struct({""bytes"": pa.binary(), ""path"": pa.string()})`  - order doesn\'t matter\n', '        - `pa.list(*)` - it must contain the image array data\n', '\n', '        Args:\n', '            storage (`Union[pa.StringArray, pa.StructArray, pa.ListArray]`):\n', '                PyArrow array to cast.\n', '\n', '        Returns:\n', '            `pa.StructArray`: Array in the Image arrow storage type, that is\n', '                `pa.struct({""bytes"": pa.binary(), ""path"": pa.string()})`.\n', '        """"""\n']","['pa.array', 'len', 'storage.field', 'storage.to_pylist', 'array_cast']",5
repos/datasets/src/datasets/features/image.py:Image:decode_example,Image:decode_example,method,37,129,96,1206,9.35,0,8,"['self', 'value', 'token_per_repo_id']","[None, ' dict', None]","[None, None, 'None']",133,"['        """"""Decode example image file into image data.\n', '\n', '        Args:\n', '            value (`str` or `dict`):\n', '                A string with the absolute image file path, a dictionary with\n', '                keys:\n', '\n', '                - `path`: String with absolute or relative image file path.\n', '                - `bytes`: The bytes of the image file.\n', '            token_per_repo_id (`dict`, *optional*):\n', '                To access and decode\n', '                image files from private repositories on the Hub, you can pass\n', '                a dictionary repo_id (`str`) -> token (`bool` or `str`).\n', '\n', '        Returns:\n', '            `PIL.Image.Image`\n', '        """"""\n']","['RuntimeError', 'Image', 'ImportError', 'ValueError', 'is_local_path', 'path.split', 'source_url.startswith', 'string_to_dict', 'token_per_repo_id.get', 'DownloadConfig', 'xopen', 'BytesIO', 'image.load', 'image.getexif', 'image.convert']",15
repos/datasets/src/datasets/features/image.py:Image:embed_storage,Image:embed_storage,method,16,61,44,529,8.67,1,0,"['self', 'storage']","[None, ' pa.StructArray']","[None, None]",254,"['        """"""Embed image files into the Arrow array.\n', '\n', '        Args:\n', '            storage (`pa.StructArray`):\n', '                PyArrow array to embed.\n', '\n', '        Returns:\n', '            `pa.StructArray`: Array in the Image arrow storage type, that is\n', '                `pa.struct({""bytes"": pa.binary(), ""path"": pa.string()})`.\n', '        """"""\n']","['path_to_bytes', 'xopen', 'f.read', 'pa.array', 'storage.to_pylist', 'storage.field', 'array_cast']",7
repos/datasets/src/datasets/features/image.py:Image:encode_example,Image:encode_example,method,13,93,61,750,8.06,0,3,"['self', 'value', 'bytes', 'dict', 'np.ndarray', '""PIL.Image.Image""]']","[None, ' Union[str', None, None, None, None]","[None, None, None, None, None, None]",94,"['        """"""Encode example into a format for Arrow.\n', '\n', '        Args:\n', '            value (`str`, `np.ndarray`, `PIL.Image.Image` or `dict`):\n', '                Data passed as input to Image feature.\n', '\n', '        Returns:\n', '            `dict` with ""path"" and ""bytes"" fields\n', '        """"""\n']","['ImportError', 'isinstance', 'np.array', 'encode_np_array', 'encode_pil_image', 'value.get', 'ValueError']",7
repos/datasets/src/datasets/features/image.py:Image:flatten,Image:flatten,method,4,17,17,110,6.47,0,0,['self'],[None],[None],195,"['        """"""If in the decodable state, return the feature itself, otherwise flatten the feature into a dictionary.""""""\n']",['Value'],1
repos/datasets/src/datasets/features/translation.py:Translation,Translation,class,18,42,37,385,9.17,0,0,[],[],[],12,[],[],0
repos/datasets/src/datasets/features/translation.py:TranslationVariableLanguages,TranslationVariableLanguages,class,41,119,91,1293,10.87,1,2,[],[],[],52,[],[],0
repos/datasets/src/datasets/features/translation.py:Translation:__call__,Translation:__call__,method,2,7,7,66,9.43,0,0,['self'],[None],[None],41,[],"['pa.struct', 'pa.string', 'sorted']",3
repos/datasets/src/datasets/features/translation.py:Translation:flatten,Translation:flatten,method,4,11,11,78,7.09,0,0,['self'],[None],[None],44,"['        """"""Flatten the Translation feature into a dictionary.""""""\n']","['Value', 'sorted']",2
repos/datasets/src/datasets/features/translation.py:TranslationVariableLanguages:__call__,TranslationVariableLanguages:__call__,method,2,5,5,87,17.4,0,0,['self'],[None],[None],96,[],"['pa.struct', 'pa.list_']",2
repos/datasets/src/datasets/features/translation.py:TranslationVariableLanguages:__post_init__,TranslationVariableLanguages:__post_init__,method,3,12,8,129,10.75,0,0,['self'],[None],[None],92,[],"['sorted', 'len']",2
repos/datasets/src/datasets/features/translation.py:TranslationVariableLanguages:encode_example,TranslationVariableLanguages:encode_example,method,16,57,48,586,10.28,1,2,"['self', 'translation_dict']","[None, None]","[None, None]",99,[],"['set', 'ValueError', 'translation_dict.items', 'isinstance', 'translation_tuples.append', 'translation_tuples.extend', 'zip']",7
repos/datasets/src/datasets/features/translation.py:TranslationVariableLanguages:flatten,TranslationVariableLanguages:flatten,method,6,12,11,122,10.17,0,0,['self'],[None],[None],122,"['        """"""Flatten the TranslationVariableLanguages feature into a dictionary.""""""\n']",['Sequence'],1
repos/datasets/src/datasets/filesystems/__init__.py:extract_path_from_uri,extract_path_from_uri,function,3,8,7,83,10.38,0,1,['dataset_path'],[' str'],[None],37,"['    """"""\n', '    Preprocesses `dataset_path` and removes remote filesystem (e.g. removing `s3://`).\n', '\n', '    Args:\n', '        dataset_path (`str`):\n', '            Path (e.g. `dataset/train`) or remote uri (e.g. `s3://my-bucket/dataset/train`) of the dataset directory.\n', '    """"""\n']",['dataset_path.split'],1
repos/datasets/src/datasets/filesystems/__init__.py:is_remote_filesystem,is_remote_filesystem,function,2,4,4,39,9.75,0,0,['fs'],[' fsspec.AbstractFileSystem'],[None],50,"['    """"""\n', '    Checks if `fs` is a remote filesystem.\n', '\n', '    Args:\n', '        fs (`fsspec.spec.AbstractFileSystem`):\n', ""            An abstract super-class for pythonic file-systems, e.g. `fsspec.filesystem(\\'file\\')` or [`datasets.filesystems.S3FileSystem`].\n"", '    """"""\n']",['isinstance'],1
repos/datasets/src/datasets/filesystems/__init__.py:rename,rename,function,3,9,9,127,14.11,0,1,"['fs', 'src', 'dst']","[' fsspec.AbstractFileSystem', ' str', ' str']","[None, None, None]",61,"['    """"""\n', '    Renames the file `src` in `fs` to `dst`.\n', '    """"""\n']","['is_remote_filesystem', 'shutil.move', 'fs._strip_protocol', 'fs.mv']",4
repos/datasets/src/datasets/filesystems/compression.py:BaseCompressedFileFileSystem,BaseCompressedFileFileSystem,class,35,186,140,1655,8.9,0,3,[],[],[],9,[],[],0
repos/datasets/src/datasets/filesystems/compression.py:Bz2FileSystem,Bz2FileSystem,class,3,6,5,49,8.17,0,0,[],[],[],88,[],[],0
repos/datasets/src/datasets/filesystems/compression.py:GzipFileSystem,GzipFileSystem,class,3,6,5,50,8.33,0,0,[],[],[],96,[],[],0
repos/datasets/src/datasets/filesystems/compression.py:Lz4FileSystem,Lz4FileSystem,class,3,6,5,49,8.17,0,0,[],[],[],104,[],[],0
repos/datasets/src/datasets/filesystems/compression.py:XzFileSystem,XzFileSystem,class,3,6,5,46,7.67,0,0,[],[],[],112,[],[],0
repos/datasets/src/datasets/filesystems/compression.py:ZstdFileSystem,ZstdFileSystem,class,3,6,5,51,8.5,0,0,[],[],[],120,[],[],0
repos/datasets/src/datasets/filesystems/compression.py:BaseCompressedFileFileSystem:__init__,BaseCompressedFileFileSystem:__init__,method,9,63,54,687,10.9,0,1,"['self', 'fo', 'target_protocol', 'target_options', '**kwargs']","[None, ' str ', ' Optional[str] ', ' Optional[dict] ', None]","[None, ' """"', ' None', ' None', None]",19,"['        """"""\n', '        The compressed file system can be instantiated from any compressed file.\n', '        It reads the contents of compressed file as a filesystem with one file inside, as if it was an archive.\n', '\n', '        The single file inside the filesystem is named after the compresssed file,\n', '        without the compression extension at the end of the filename.\n', '\n', '        Args:\n', '            fo (:obj:``str``): Path to compressed file. Will fetch file using ``fsspec.open()``\n', ""            mode (:obj:``str``): Currently, only 'rb' accepted\n"", '            target_protocol(:obj:``str``, optional): To override the FS protocol inferred from a URL.\n', '            target_options (:obj:``dict``, optional): Kwargs passed when instantiating the target FS.\n', '        """"""\n']","['super', 'fo.__fspath__', 'hasattr', 'partial']",4
repos/datasets/src/datasets/filesystems/compression.py:BaseCompressedFileFileSystem:_get_dirs,BaseCompressedFileFileSystem:_get_dirs,method,2,11,10,130,11.82,0,1,['self'],[None],[None],64,[],[],0
repos/datasets/src/datasets/filesystems/compression.py:BaseCompressedFileFileSystem:_open,BaseCompressedFileFileSystem:_open,method,6,21,18,162,7.71,0,1,"['self', 'path', 'mode', 'block_size', 'autocommit', 'cache_options', '**kwargs', '']","[None, ' str', ' str ', None, None, None, None, None]","[None, None, ' ""rb""', 'None', 'True', 'None', None, None]",73,[],"['self._strip_protocol', 'ValueError', 'self._open_with_fsspec']",3
repos/datasets/src/datasets/filesystems/compression.py:BaseCompressedFileFileSystem:_strip_protocol,BaseCompressedFileFileSystem:_strip_protocol,method,2,2,2,47,23.5,0,0,"['cls', 'path']","[None, None]","[None, None]",60,[],['super'],1
repos/datasets/src/datasets/filesystems/compression.py:BaseCompressedFileFileSystem:cat,BaseCompressedFileFileSystem:cat,method,4,6,6,54,9.0,0,0,"['self', 'path']","[None, ' str']","[None, None]",69,[],"['self._open_with_fsspec', 'f.read']",2
repos/datasets/src/datasets/filesystems/s3filesystem.py:S3FileSystem,S3FileSystem,class,0,0,0,0,0,0,0,[],[],[],7,[],[],0
repos/datasets/src/datasets/fingerprint.py:disable_caching,disable_caching,function,1,4,4,45,11.25,0,0,[],[],[],116,"['    """"""\n', '    When applying transforms on a dataset, the data are stored in cache files.\n', ""    The caching mechanism allows to reload an existing cache file if it's already been computed.\n"", '\n', '    Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\n', '    after each transform.\n', '\n', '    If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\n', '    More precisely, if the caching is disabled:\n', '    - cache files are always recreated\n', '    - cache files are written to a temporary directory that is deleted when session closes\n', '    - cache files are named using a random hash instead of the dataset fingerprint\n', '    - use [`~datasets.Dataset.save_to_disk`] to save a transformed dataset or it will be deleted when session closes\n', ""    - caching doesn't affect [`~datasets.load_dataset`]. If you want to regenerate a dataset from scratch you should use\n"", '    the `download_mode` parameter in [`~datasets.load_dataset`].\n', '    """"""\n']",[],0
repos/datasets/src/datasets/fingerprint.py:enable_caching,enable_caching,function,1,4,4,44,11.0,0,0,[],[],[],95,"['    """"""\n', '    When applying transforms on a dataset, the data are stored in cache files.\n', ""    The caching mechanism allows to reload an existing cache file if it's already been computed.\n"", '\n', '    Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\n', '    after each transform.\n', '\n', '    If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\n', '    More precisely, if the caching is disabled:\n', '    - cache files are always recreated\n', '    - cache files are written to a temporary directory that is deleted when session closes\n', '    - cache files are named using a random hash instead of the dataset fingerprint\n', '    - use [`~datasets.Dataset.save_to_disk`] to save a transformed dataset or it will be deleted when session closes\n', ""    - caching doesn't affect [`~datasets.load_dataset`]. If you want to regenerate a dataset from scratch you should use\n"", '    the `download_mode` parameter in [`~datasets.load_dataset`].\n', '    """"""\n']",[],0
repos/datasets/src/datasets/fingerprint.py:fingerprint_transform,fingerprint_transform,function,51,207,127,1991,9.62,1,12,"['inplace', 'use_kwargs', 'ignore_kwargs', 'fingerprint_names', 'randomized_function', 'version', '']","[' bool', ' Optional[List[str]] ', ' Optional[List[str]] ', ' Optional[List[str]] ', ' bool ', ' Optional[str] ', None]","[None, ' None', ' None', ' None', ' False', ' None', None]",392,"['    """"""\n', '    Wrapper for dataset transforms to update the dataset fingerprint using ``update_fingerprint``\n', '    Args:\n', '        inplace (:obj:`bool`):  If inplace is True, the fingerprint of the dataset is updated inplace.\n', '            Otherwise, a parameter ""new_fingerprint"" is passed to the wrapped method that should take care of\n', '            setting the fingerprint of the returned Dataset.\n', '        use_kwargs (:obj:`List[str]`, optional): optional white list of argument names to take into account\n', '            to update the fingerprint to the wrapped method that should take care of\n', '            setting the fingerprint of the returned Dataset. By default all the arguments are used.\n', '        ignore_kwargs (:obj:`List[str]`, optional): optional black list of argument names to take into account\n', '            to update the fingerprint. Note that ignore_kwargs prevails on use_kwargs.\n', '        fingerprint_names (:obj:`List[str]`, optional, defaults to [""new_fingerprint""]):\n', '            If the dataset transforms is not inplace and returns a DatasetDict, then it can require\n', '            several fingerprints (one per dataset in the DatasetDict). By specifying fingerprint_names,\n', '            one fingerprint named after each element of fingerprint_names is going to be passed.\n', '        randomized_function (:obj:`bool`, defaults to False): If the dataset transform is random and has\n', '            optional parameters ""seed"" and ""generator"", then you can set randomized_function to True.\n', '            This way, even if users set ""seed"" and ""generator"" to None, then the fingerprint is\n', ""            going to be randomly generated depending on numpy's current state. In this case, the\n"", '            generator is set to np.random.default_rng(np.random.get_state()[1][0]).\n', '        version (:obj:`str`, optional): version of the transform. The version is taken into account when\n', '            computing the fingerprint. If a datase transform changes (or at least if the output data\n', '            that are cached changes), then one should increase the version. If the version stays the\n', '            same, then old cached data could be reused that are not compatible with the new transform.\n', '            It should be in the format ""MAJOR.MINOR.PATCH"".\n', '    """"""\n']","['isinstance', 'ValueError', '_fingerprint', 'all', 'format_transform_for_fingerprint', 'wrapper', 'format_kwargs_for_fingerprint', 'kwargs.pop', 'update_fingerprint', 'kwargs.get', 'validate_fingerprint', 'func']",12
repos/datasets/src/datasets/fingerprint.py:format_kwargs_for_fingerprint,format_kwargs_for_fingerprint,function,48,129,79,1202,9.32,5,7,"['func', 'args', 'kwargs', 'Any]', 'use_kwargs', 'ignore_kwargs', 'randomized_function', '']","[' Callable', ' Tuple', ' Dict[str', None, ' Optional[List[str]] ', ' Optional[List[str]] ', ' bool ', None]","[None, None, None, None, ' None', ' None', ' False', None]",347,"['    """"""\n', '    Format the kwargs of a transform to the format that will be used to update the fingerprint.\n', '    """"""\n']","['kwargs.copy', 'inspect.signature', 'kwargs_for_fingerprint.update', 'next', 'kwargs_for_fingerprint.items', 'kwargs_for_fingerprint.get', 'default_values.items', 'kwargs_for_fingerprint.pop']",8
repos/datasets/src/datasets/fingerprint.py:format_transform_for_fingerprint,format_transform_for_fingerprint,function,3,12,10,111,9.25,0,1,"['func', 'version']","[' Callable', ' Optional[str] ']","[None, ' None']",337,"['    """"""\n', '    Format a transform to the format that will be used to update the fingerprint.\n', '    """"""\n']",[],0
repos/datasets/src/datasets/fingerprint.py:generate_fingerprint,generate_fingerprint,function,12,21,18,255,12.14,2,1,['dataset'],"[' ""Dataset""']",[None],249,[],"['Hasher', 'sorted', 'hasher.update', 'hasher.hexdigest']",4
repos/datasets/src/datasets/fingerprint.py:generate_random_fingerprint,generate_random_fingerprint,function,2,2,2,58,29.0,0,0,['nbits'],[' int '],[' 64'],263,[],[],0
repos/datasets/src/datasets/fingerprint.py:get_datasets_with_cache_file_in_temp_dir,get_datasets_with_cache_file_in_temp_dir,function,1,9,9,93,10.33,0,0,[],[],[],91,[],['list'],1
repos/datasets/src/datasets/fingerprint.py:get_temporary_cache_files_directory,get_temporary_cache_files_directory,function,4,10,9,165,16.5,0,1,[],[],[],182,"['    """"""Return a directory that is deleted when session closes.""""""\n']",['_TempCacheDir'],1
repos/datasets/src/datasets/fingerprint.py:hashregister,hashregister,function,6,12,10,75,6.25,1,0,['*types'],[None],[None],196,[],['proxy'],1
repos/datasets/src/datasets/fingerprint.py:is_caching_enabled,is_caching_enabled,function,3,4,4,51,12.75,0,0,[],[],[],161,"['    """"""\n', '    When applying transforms on a dataset, the data are stored in cache files.\n', ""    The caching mechanism allows to reload an existing cache file if it's already been computed.\n"", '\n', '    Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\n', '    after each transform.\n', '\n', '    If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\n', '    More precisely, if the caching is disabled:\n', '    - cache files are always recreated\n', '    - cache files are written to a temporary directory that is deleted when session closes\n', '    - cache files are named using a random hash instead of the dataset fingerprint\n', '    - use [`~datasets.Dataset.save_to_disk`]] to save a transformed dataset or it will be deleted when session closes\n', ""    - caching doesn't affect [`~datasets.load_dataset`]. If you want to regenerate a dataset from scratch you should use\n"", '    the `download_mode` parameter in [`~datasets.load_dataset`].\n', '    """"""\n']",['bool'],1
repos/datasets/src/datasets/fingerprint.py:maybe_register_dataset_for_temp_dir_deletion,maybe_register_dataset_for_temp_dir_deletion,function,8,24,18,349,14.54,1,3,['dataset'],[None],[None],72,"['    """"""\n', '    This function registers the datasets that have cache files in _TEMP_DIR_FOR_TEMP_CACHE_FILES in order\n', '    to properly delete them before deleting the temporary directory.\n', '    The temporary directory _TEMP_DIR_FOR_TEMP_CACHE_FILES is used when caching is disabled.\n', '    """"""\n']","['weakref.WeakSet', 'any', 'Path', '_DATASETS_WITH_TABLE_IN_TEMP_DIR.add']",4
repos/datasets/src/datasets/fingerprint.py:set_caching_enabled,set_caching_enabled,function,2,4,4,53,13.25,0,0,['boolean'],[' bool'],[None],140,"['    """"""\n', '    When applying transforms on a dataset, the data are stored in cache files.\n', ""    The caching mechanism allows to reload an existing cache file if it's already been computed.\n"", '\n', '    Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated\n', '    after each transform.\n', '\n', '    If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets.\n', '    More precisely, if the caching is disabled:\n', '    - cache files are always recreated\n', '    - cache files are written to a temporary directory that is deleted when session closes\n', '    - cache files are named using a random hash instead of the dataset fingerprint\n', '    - use :func:`datasets.Dataset.save_to_disk` to save a transformed dataset or it will be deleted when session closes\n', ""    - caching doesn't affect :func:`datasets.load_dataset`. If you want to regenerate a dataset from scratch you should use\n"", '    the ``download_mode`` parameter in :func:`datasets.load_dataset`.\n', '    """"""\n']",['bool'],1
repos/datasets/src/datasets/fingerprint.py:update_fingerprint,update_fingerprint,function,23,284,106,2120,7.46,1,4,"['fingerprint', 'transform', 'transform_args']","[None, None, None]","[None, None, None]",267,[],"['Hasher', 'hasher.update', 'fingerprint_warnings.get', 'logger.warning', 'logger.info', 'generate_random_fingerprint', 'sorted', 'hasher.hexdigest']",8
repos/datasets/src/datasets/fingerprint.py:validate_fingerprint,validate_fingerprint,function,5,74,55,580,7.84,1,3,"['fingerprint', 'max_length']","[' str', None]","[None, '64']",317,"['    """"""\n', '    Make sure the fingerprint is a non-empty string that is not longer that max_length=64 by default,\n', '    so that the fingerprint can be used to name cache files without issues.\n', '    """"""\n']","['isinstance', 'ValueError', 'len']",3
repos/datasets/src/datasets/fingerprint.py:Hasher,Hasher,class,25,70,47,677,9.67,1,1,[],[],[],205,[],[],0
repos/datasets/src/datasets/fingerprint.py:_TempCacheDir,_TempCacheDir,class,15,47,43,479,10.19,1,2,[],[],[],46,[],[],0
repos/datasets/src/datasets/fingerprint.py:Hasher:__init__,Hasher:__init__,method,2,2,2,21,10.5,0,0,['self'],[None],[None],210,[],['xxhash.xxh64'],1
repos/datasets/src/datasets/fingerprint.py:Hasher:hash,Hasher:hash,method,2,2,2,34,17.0,0,0,"['cls', 'value']","[None, ' Any']","[None, None]",227,[],['cls.hash_bytes'],1
repos/datasets/src/datasets/fingerprint.py:Hasher:hash_bytes,Hasher:hash_bytes,method,8,16,16,109,6.81,1,1,"['cls', 'value', 'List[bytes]]']","[None, ' Union[bytes', None]","[None, None, None]",214,[],"['isinstance', 'xxhash.xxh64', 'm.update', 'm.hexdigest']",4
repos/datasets/src/datasets/fingerprint.py:Hasher:hash_default,Hasher:hash_default,method,2,2,2,21,10.5,0,0,"['cls', 'value']","[None, ' Any']","[None, None]",223,[],['cls.hash'],1
repos/datasets/src/datasets/fingerprint.py:Hasher:hexdigest,Hasher:hexdigest,method,2,2,2,24,12.0,0,0,['self'],[None],[None],236,[],[],0
repos/datasets/src/datasets/fingerprint.py:Hasher:update,Hasher:update,method,5,6,6,168,28.0,0,0,"['self', 'value']","[None, ' Any']","[None, None]",230,[],['self.hash'],1
repos/datasets/src/datasets/fingerprint.py:_TempCacheDir:__init__,_TempCacheDir:__init__,method,4,5,5,116,23.2,0,0,['self'],[None],[None],52,[],"['tempfile.mkdtemp', 'weakref.finalize']",2
repos/datasets/src/datasets/fingerprint.py:_TempCacheDir:_cleanup,_TempCacheDir:_cleanup,method,6,33,32,262,7.94,1,1,['self'],[None],[None],56,[],"['get_datasets_with_cache_file_in_temp_dir', 'dset.__del__', 'shutil.rmtree', 'OSError']",4
repos/datasets/src/datasets/fingerprint.py:_TempCacheDir:cleanup,_TempCacheDir:cleanup,method,2,3,3,43,14.33,0,1,['self'],[None],[None],67,[],['self._cleanup'],1
repos/datasets/src/datasets/formatting/__init__.py:_register_formatter,_register_formatter,function,9,45,30,448,9.96,1,3,"['formatter_cls', 'format_type', 'aliases', '']","[' type', ' Optional[str]', ' Optional[List[str]] ', None]","[None, None, ' None', None]",41,"['    """"""\n', '    Register a Formatter object using a name and optional aliases.\n', '    This function must be used on a Formatter class.\n', '    """"""\n']","['logger.warning', 'set']",2
repos/datasets/src/datasets/formatting/__init__.py:_register_unavailable_formatter,_register_unavailable_formatter,function,4,16,14,136,8.5,1,1,"['unavailable_error', 'format_type', 'aliases']","[' Exception', ' Optional[str]', ' Optional[List[str]] ']","[None, None, ' None']",64,"['    """"""\n', '    Register an unavailable Formatter object using a name and optional aliases.\n', '    This function must be used on an Exception object that is raised when trying to get the unavailable formatter.\n', '    """"""\n']",['set'],1
repos/datasets/src/datasets/formatting/__init__.py:get_format_type_from_alias,get_format_type_from_alias,function,4,9,7,102,11.33,0,1,['format_type'],[' Optional[str]'],[None],116,"['    """"""If the given format type is a known alias, then return its main type name. Otherwise return the type with no change.""""""\n']",[],0
repos/datasets/src/datasets/formatting/__init__.py:get_formatter,get_formatter,function,8,37,27,371,10.03,0,2,"['format_type', '**format_kwargs']","[' Optional[str]', None]","[None, None]",124,"['    """"""\n', '    Factory function to get a Formatter given its type name and keyword arguments.\n', '    A formatter is an object that extracts and formats data from pyarrow table.\n', '    It defines the formatting for rows, colums and batches.\n', ""    If the formatter for a given type name doesn't exist or is not available, an error is raised.\n"", '    """"""\n']","['get_format_type_from_alias', 'ValueError', '_FORMAT_TYPES.keys']",3
repos/datasets/src/datasets/formatting/formatting.py:_check_valid_column_key,_check_valid_column_key,function,3,18,14,100,5.56,0,1,"['key', 'columns']","[' str', ' List[str]']","[None, None]",519,[],['KeyError'],1
repos/datasets/src/datasets/formatting/formatting.py:_check_valid_index_key,_check_valid_index_key,function,5,55,38,459,8.35,0,4,"['key', 'slice', 'range', 'Iterable]', 'size']","[' Union[int', None, None, None, ' int']","[None, None, None, None, None]",524,[],"['isinstance', 'IndexError', 'len', '_check_valid_index_key', '_raise_bad_key_type']",5
repos/datasets/src/datasets/formatting/formatting.py:_is_array_with_nulls,_is_array_with_nulls,function,2,4,4,27,6.75,0,0,['pa_array'],[' pa.Array'],[None],106,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:_is_range_contiguous,_is_range_contiguous,function,5,6,6,39,6.5,0,0,['key'],[' range'],[None],41,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:_query_table,_query_table,function,14,61,42,552,9.05,0,7,"['table', 'key', 'slice', 'range', 'str', 'Iterable]']","[' Table', ' Union[int', None, None, None, None]","[None, None, None, None, None, None]",81,"['    """"""\n', '    Query a pyarrow Table to extract the subtable that correspond to the given key.\n', '    """"""\n']","['isinstance', 'table.fast_slice', 'range', '_is_range_contiguous', 'np.fromiter', 'len', 'table.fast_gather', '_raise_bad_key_type']",8
repos/datasets/src/datasets/formatting/formatting.py:_query_table_with_indices_mapping,_query_table_with_indices_mapping,function,13,62,44,629,10.15,0,6,"['table', 'key', 'slice', 'range', 'str', 'Iterable]', 'indices']","[' Table', ' Union[int', None, None, None, None, ' Table']","[None, None, None, None, None, None, None]",51,"['    """"""\n', '    Query a pyarrow Table to extract the subtable that correspond to the given key.\n', '    The :obj:`indices` parameter corresponds to the indices mapping in case we cant to take into\n', '    account a shuffling or an indices selection for example.\n', '    The indices table must contain one column named ""indices"" of type uint64.\n', '    """"""\n']","['isinstance', 'indices.fast_slice', '_query_table', 'range', '_is_range_contiguous', 'table.select', 'indices.column', '_raise_bad_key_type']",8
repos/datasets/src/datasets/formatting/formatting.py:_raise_bad_key_type,_raise_bad_key_type,function,1,19,18,104,5.47,0,0,['key'],[' Any'],[None],45,[],['TypeError'],1
repos/datasets/src/datasets/formatting/formatting.py:_unnest,_unnest,function,5,8,8,49,6.12,1,0,"['py_dict', 'List[T]]']","[' Dict[str', None]","[None, None]",127,"['    """"""Return the first element of a batch (dict) as a row (dict)""""""\n']",['py_dict.items'],1
repos/datasets/src/datasets/formatting/formatting.py:format_table,format_table,function,20,89,58,1012,11.37,0,5,"['table', 'key', 'slice', 'range', 'str', 'Iterable]', 'formatter', 'format_columns', 'output_all_columns', '']","[' Table', ' Union[int', None, None, None, None, ' Formatter', ' Optional[list] ', None, None]","[None, None, None, None, None, None, None, ' None', 'False', None]",596,"['    """"""\n', '    Format a Table depending on the key that was used and a Formatter object.\n', '\n', '    Args:\n', '        table (``datasets.table.Table``): The input Table to format\n', '        key (``Union[int, slice, range, str, Iterable]``): Depending on the key that was used, the formatter formats\n', '            the table as either a row, a column or a batch.\n', '        formatter (``datasets.formatting.formatting.Formatter``): Any subclass of a Formatter such as\n', '            PythonFormatter, NumpyFormatter, etc.\n', '        format_columns (:obj:`List[str]`, optional): if not None, it defines the columns that will be formatted using the\n', '            given formatter. Other columns are discarded (unless ``output_all_columns`` is True)\n', '        output_all_columns (:obj:`bool`, defaults to False). If True, the formatted output is completed using the columns\n', '            that are not in the ``format_columns`` list. For these columns, the PythonFormatter is used.\n', '\n', '\n', '    Returns:\n', '        A row, column or batch formatted object defined by the Formatter:\n', '        - the PythonFormatter returns a dictionary for a row or a batch, and a list for a column.\n', '        - the NumpyFormatter returns a dictionary for a row or a batch, and a np.array for a column.\n', '        - the PandasFormatter returns a pd.DataFrame for a row or a batch, and a pd.Series for a column.\n', '        - the TorchFormatter returns a dictionary for a row or a batch, and a torch.Tensor for a column.\n', '        - the TFFormatter returns a dictionary for a row or a batch, and a tf.Tensor for a column.\n', '    """"""\n']","['isinstance', 'key_to_query_type', 'PythonFormatter', 'formatter', 'python_formatter', 'pa_table.drop', 'formatted_output.update', 'TypeError']",8
repos/datasets/src/datasets/formatting/formatting.py:key_to_query_type,key_to_query_type,function,3,18,13,157,8.72,0,1,"['key', 'slice', 'range', 'str', 'Iterable]']","[' Union[int', None, None, None, None]","[None, None, None, None, None]",543,[],"['isinstance', '_raise_bad_key_type']",2
repos/datasets/src/datasets/formatting/formatting.py:query_table,query_table,function,15,45,35,441,9.8,0,4,"['table', 'key', 'slice', 'range', 'str', 'Iterable]', 'indices', '']","[' Table', ' Union[int', None, None, None, None, ' Optional[Table] ', None]","[None, None, None, None, None, None, ' None', None]",553,"['    """"""\n', '    Query a Table to extract the subtable that correspond to the given key.\n', '\n', '    Args:\n', '        table (``datasets.table.Table``): The input Table to query from\n', '        key (``Union[int, slice, range, str, Iterable]``): The key can be of different types:\n', '            - an integer i: the subtable containing only the i-th row\n', '            - a slice [i:j:k]: the subtable containing the rows that correspond to this slice\n', '            - a range(i, j, k): the subtable containing the rows that correspond to this range\n', '            - a string c: the subtable containing all the rows but only the column c\n', '            - an iterable l: the subtable that is the concatenation of all the i-th rows for all i in the iterable\n', '        indices (Optional ``datasets.table.Table``): If not None, it is used to re-map the given key to the table rows.\n', '            The indices table must contain one column named ""indices"" of type uint64.\n', '            This is used in case of shuffling or rows selection.\n', '\n', '\n', '    Returns:\n', '        ``pyarrow.Table``: the result of the query on the input table\n', '    """"""\n']","['isinstance', 'operator.index', '_raise_bad_key_type', '_check_valid_column_key', '_check_valid_index_key', '_query_table', '_query_table_with_indices_mapping']",7
repos/datasets/src/datasets/formatting/formatting.py:ArrowFormatter,ArrowFormatter,class,5,24,13,330,13.75,0,0,[],[],[],418,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:BaseArrowExtractor,BaseArrowExtractor,class,4,24,13,237,9.88,0,0,[],[],[],110,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:CustomFormatter,CustomFormatter,class,18,153,77,1218,7.96,0,2,[],[],[],471,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:Formatter,Formatter,class,25,68,48,883,12.99,0,1,[],[],[],379,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyBatch,LazyBatch,class,3,5,5,84,16.8,0,0,[],[],[],374,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyDict,LazyDict,class,57,215,96,2201,10.24,2,9,[],[],[],257,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyRow,LazyRow,class,3,5,5,87,17.4,0,0,[],[],[],369,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:NumpyArrowExtractor,NumpyArrowExtractor,class,21,137,72,1539,11.23,1,5,[],[],[],154,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PandasArrowExtractor,PandasArrowExtractor,class,7,24,13,394,16.42,0,0,[],[],[],200,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PandasFeaturesDecoder,PandasFeaturesDecoder,class,19,74,49,812,10.97,1,5,[],[],[],225,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PandasFormatter,PandasFormatter,class,10,37,21,552,14.92,0,0,[],[],[],454,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PythonArrowExtractor,PythonArrowExtractor,class,7,24,13,242,10.08,0,0,[],[],[],143,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PythonFeaturesDecoder,PythonFeaturesDecoder,class,10,45,29,408,9.07,0,0,[],[],[],211,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PythonFormatter,PythonFormatter,class,17,54,33,717,13.28,0,2,[],[],[],429,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:SimpleArrowExtractor,SimpleArrowExtractor,class,6,24,13,209,8.71,0,0,[],[],[],132,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:TensorFormatter,TensorFormatter,class,2,6,6,71,11.83,0,0,[],[],[],413,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:ArrowFormatter:format_batch,ArrowFormatter:format_batch,method,2,2,2,59,29.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",425,[],['self.simple_arrow_extractor'],1
repos/datasets/src/datasets/formatting/formatting.py:ArrowFormatter:format_column,ArrowFormatter:format_column,method,2,2,2,60,30.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",422,[],['self.simple_arrow_extractor'],1
repos/datasets/src/datasets/formatting/formatting.py:ArrowFormatter:format_row,ArrowFormatter:format_row,method,2,2,2,57,28.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",419,[],['self.simple_arrow_extractor'],1
repos/datasets/src/datasets/formatting/formatting.py:BaseArrowExtractor:extract_batch,BaseArrowExtractor:extract_batch,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",123,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:BaseArrowExtractor:extract_column,BaseArrowExtractor:extract_column,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",120,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:BaseArrowExtractor:extract_row,BaseArrowExtractor:extract_row,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",117,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:CustomFormatter:__init__,CustomFormatter:__init__,method,3,3,3,60,20.0,0,0,"['self', 'transform', 'dict]', 'features', '**kwargs']","[None, ' Callable[[dict]', None, None, None]","[None, None, None, 'None', None]",481,[],['super'],1
repos/datasets/src/datasets/formatting/formatting.py:CustomFormatter:format_batch,CustomFormatter:format_batch,method,5,6,5,142,23.67,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",513,[],"['self.python_arrow_extractor', 'self.transform']",2
repos/datasets/src/datasets/formatting/formatting.py:CustomFormatter:format_column,CustomFormatter:format_column,method,7,86,52,569,6.62,0,2,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",494,[],"['self.format_batch', 'hasattr', 'len', 'TypeError']",4
repos/datasets/src/datasets/formatting/formatting.py:CustomFormatter:format_row,CustomFormatter:format_row,method,6,33,30,222,6.73,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",485,[],"['self.format_batch', '_unnest', 'TypeError']",3
repos/datasets/src/datasets/formatting/formatting.py:Formatter:__call__,Formatter:__call__,method,5,15,10,172,11.47,0,1,"['self', 'pa_table', 'query_type']","[None, ' pa.Table', ' str']","[None, None, None]",395,[],"['self.format_row', 'self.format_column', 'self.format_batch']",3
repos/datasets/src/datasets/formatting/formatting.py:Formatter:__init__,Formatter:__init__,method,6,6,6,154,25.67,0,0,"['self', 'features']","[None, ' Optional[Features] ']","[None, ' None']",390,[],"['PythonFeaturesDecoder', 'PandasFeaturesDecoder']",2
repos/datasets/src/datasets/formatting/formatting.py:Formatter:format_batch,Formatter:format_batch,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",409,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:Formatter:format_column,Formatter:format_column,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",406,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:Formatter:format_row,Formatter:format_row,method,1,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",403,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyBatch:format,LazyBatch:format,method,2,2,2,63,31.5,0,0,"['self', 'key']","[None, None]","[None, None]",360,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:__contains__,LazyDict:__contains__,method,3,4,4,20,5.0,0,0,"['self', 'key']","[None, None]","[None, None]",291,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:__copy__,LazyDict:__copy__,method,8,9,8,212,23.56,0,0,['self'],[None],[None],342,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:__delitem__,LazyDict:__delitem__,method,5,7,7,77,11.0,0,1,"['self', 'key']","[None, None]","[None, None]",283,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:__getitem__,LazyDict:__getitem__,method,7,13,11,136,10.46,0,1,"['self', 'key']","[None, None]","[None, None]",270,[],['self.format'],1
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:__init__,LazyDict:__init__,method,8,13,13,139,10.69,1,0,"['self', 'pa_table', 'formatter']","[None, ' pa.Table', ' ""Formatter""']","[None, None, None]",260,[],['set'],1
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:__iter__,LazyDict:__iter__,method,2,2,2,21,10.5,0,0,['self'],[None],[None],288,[],['iter'],1
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:__len__,LazyDict:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],267,[],['len'],1
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:__repr__,LazyDict:__repr__,method,3,3,3,40,13.33,0,0,['self'],[None],[None],294,[],"['self._format_all', 'repr']",2
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:__setitem__,LazyDict:__setitem__,method,5,7,7,80,11.43,0,1,"['self', 'key', 'value']","[None, None, None]","[None, None, None]",278,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:_format_all,LazyDict:_format_all,method,5,7,7,88,12.57,1,0,['self'],[None],[None],363,[],['self.format'],1
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:copy,LazyDict:copy,method,4,4,4,32,8.0,0,0,['self'],[None],[None],351,[],['copy.copy'],1
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:format,LazyDict:format,method,1,2,2,24,12.0,0,0,"['self', 'key']","[None, None]","[None, None]",360,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyDict:fromkeys,LazyDict:fromkeys,method,1,2,2,24,12.0,0,0,"['cls', 'iterable', 'value']","[None, None, None]","[None, None, 'None']",357,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:LazyRow:format,LazyRow:format,method,2,2,2,66,33.0,0,0,"['self', 'key']","[None, None]","[None, None]",360,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:NumpyArrowExtractor:__init__,NumpyArrowExtractor:__init__,method,2,2,2,36,18.0,0,0,"['self', '**np_array_kwargs']","[None, None]","[None, None]",155,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:NumpyArrowExtractor:_arrow_array_to_numpy,NumpyArrowExtractor:_arrow_array_to_numpy,method,12,97,50,1058,10.91,1,5,"['self', 'pa_array']","[None, ' pa.Array']","[None, None]",167,[],"['isinstance', '_is_zero_copy_only', 'chunk.to_numpy', 'all', '_is_array_with_nulls', 'pa_array.to_numpy', 'len', 'any', 'np.isnan', 'np.array']",10
repos/datasets/src/datasets/formatting/formatting.py:NumpyArrowExtractor:extract_batch,NumpyArrowExtractor:extract_batch,method,2,7,7,82,11.71,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",164,[],['self._arrow_array_to_numpy'],1
repos/datasets/src/datasets/formatting/formatting.py:NumpyArrowExtractor:extract_column,NumpyArrowExtractor:extract_column,method,2,2,2,68,34.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",161,[],['self._arrow_array_to_numpy'],1
repos/datasets/src/datasets/formatting/formatting.py:NumpyArrowExtractor:extract_row,NumpyArrowExtractor:extract_row,method,2,2,2,43,21.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",158,[],['_unnest'],1
repos/datasets/src/datasets/formatting/formatting.py:PandasArrowExtractor:extract_batch,PandasArrowExtractor:extract_batch,method,2,2,2,58,29.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",207,[],['pa_table.to_pandas'],1
repos/datasets/src/datasets/formatting/formatting.py:PandasArrowExtractor:extract_column,PandasArrowExtractor:extract_column,method,2,2,2,96,48.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",204,[],['pa_table.select'],1
repos/datasets/src/datasets/formatting/formatting.py:PandasArrowExtractor:extract_row,PandasArrowExtractor:extract_row,method,2,2,2,74,37.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",201,[],['pa_table.slice'],1
repos/datasets/src/datasets/formatting/formatting.py:PandasFeaturesDecoder:__init__,PandasFeaturesDecoder:__init__,method,2,2,2,22,11.0,0,0,"['self', 'features']","[None, ' Optional[Features]']","[None, None]",212,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PandasFeaturesDecoder:decode_batch,PandasFeaturesDecoder:decode_batch,method,2,2,2,28,14.0,0,0,"['self', 'batch']","[None, ' pd.DataFrame']","[None, None]",253,[],['self.decode_row'],1
repos/datasets/src/datasets/formatting/formatting.py:PandasFeaturesDecoder:decode_column,PandasFeaturesDecoder:decode_column,method,8,21,17,256,12.19,0,2,"['self', 'column', 'column_name']","[None, ' pd.Series', ' str']","[None, None, None]",243,[],"['no_op_if_value_is_null', 'column.transform']",2
repos/datasets/src/datasets/formatting/formatting.py:PandasFeaturesDecoder:decode_row,PandasFeaturesDecoder:decode_row,method,9,25,23,280,11.2,1,3,"['self', 'row']","[None, ' pd.DataFrame']","[None, None]",229,[],"['no_op_if_value_is_null', 'row[list', 'row.transform']",3
repos/datasets/src/datasets/formatting/formatting.py:PandasFormatter:format_batch,PandasFormatter:format_batch,method,4,6,4,118,19.67,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",465,[],['self.pandas_arrow_extractor'],1
repos/datasets/src/datasets/formatting/formatting.py:PandasFormatter:format_column,PandasFormatter:format_column,method,4,7,5,157,22.43,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",460,[],['self.pandas_arrow_extractor'],1
repos/datasets/src/datasets/formatting/formatting.py:PandasFormatter:format_row,PandasFormatter:format_row,method,4,6,4,114,19.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",455,[],['self.pandas_arrow_extractor'],1
repos/datasets/src/datasets/formatting/formatting.py:PythonArrowExtractor:extract_batch,PythonArrowExtractor:extract_batch,method,2,2,2,26,13.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",150,[],['pa_table.to_pydict'],1
repos/datasets/src/datasets/formatting/formatting.py:PythonArrowExtractor:extract_column,PythonArrowExtractor:extract_column,method,2,2,2,36,18.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",147,[],['pa_table.column'],1
repos/datasets/src/datasets/formatting/formatting.py:PythonArrowExtractor:extract_row,PythonArrowExtractor:extract_row,method,2,2,2,35,17.5,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",144,[],['_unnest'],1
repos/datasets/src/datasets/formatting/formatting.py:PythonFeaturesDecoder:__init__,PythonFeaturesDecoder:__init__,method,2,2,2,22,11.0,0,0,"['self', 'features']","[None, ' Optional[Features]']","[None, None]",212,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PythonFeaturesDecoder:decode_batch,PythonFeaturesDecoder:decode_batch,method,2,6,6,63,10.5,0,0,"['self', 'batch']","[None, ' dict']","[None, None]",221,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PythonFeaturesDecoder:decode_column,PythonFeaturesDecoder:decode_column,method,2,7,7,78,11.14,0,0,"['self', 'column', 'column_name']","[None, ' list', ' str']","[None, None, None]",218,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PythonFeaturesDecoder:decode_row,PythonFeaturesDecoder:decode_row,method,2,6,6,61,10.17,0,0,"['self', 'row']","[None, ' dict']","[None, None]",215,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:PythonFormatter:__init__,PythonFormatter:__init__,method,3,3,3,41,13.67,0,0,"['self', 'features', 'lazy']","[None, None, None]","[None, 'None', 'False']",430,[],['super'],1
repos/datasets/src/datasets/formatting/formatting.py:PythonFormatter:format_batch,PythonFormatter:format_batch,method,6,11,8,170,15.45,0,1,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",446,[],"['LazyBatch', 'self.python_arrow_extractor']",2
repos/datasets/src/datasets/formatting/formatting.py:PythonFormatter:format_column,PythonFormatter:format_column,method,4,7,5,157,22.43,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",441,[],['self.python_arrow_extractor'],1
repos/datasets/src/datasets/formatting/formatting.py:PythonFormatter:format_row,PythonFormatter:format_row,method,6,11,8,156,14.18,0,1,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",434,[],"['LazyRow', 'self.python_arrow_extractor']",2
repos/datasets/src/datasets/formatting/formatting.py:SimpleArrowExtractor:extract_batch,SimpleArrowExtractor:extract_batch,method,2,2,2,14,7.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",139,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:SimpleArrowExtractor:extract_column,SimpleArrowExtractor:extract_column,method,2,2,2,24,12.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",136,[],['pa_table.column'],1
repos/datasets/src/datasets/formatting/formatting.py:SimpleArrowExtractor:extract_row,SimpleArrowExtractor:extract_row,method,2,2,2,14,7.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",133,[],[],0
repos/datasets/src/datasets/formatting/formatting.py:TensorFormatter:recursive_tensorize,TensorFormatter:recursive_tensorize,method,1,2,2,24,12.0,0,0,"['self', 'data_struct']","[None, ' dict']","[None, None]",414,[],[],0
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter,JaxFormatter,class,70,358,208,3772,10.54,1,16,[],[],[],38,[],[],0
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter:__init__,JaxFormatter:__init__,method,18,105,77,861,8.2,0,4,"['self', 'features', 'device', '**jnp_array_kwargs']","[None, None, None, None]","[None, 'None', 'None', None]",39,[],"['super', 'isinstance', 'ValueError', 'str', 'self._map_devices_to_str', 'list', 'logger.warning']",7
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter:_consolidate,JaxFormatter:_consolidate,method,9,31,26,210,6.77,0,2,"['self', 'column']","[None, None]","[None, None]",72,[],"['isinstance', 'all', 'jnp.stack']",3
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter:_map_devices_to_str,JaxFormatter:_map_devices_to_str,method,3,9,8,60,6.67,0,0,[],[],[],67,[],['jax.devices'],1
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter:_recursive_tensorize,JaxFormatter:_recursive_tensorize,method,23,59,41,651,11.03,0,5,"['self', 'data_struct']","[None, None]","[None, None]",120,[],"['isinstance', 'self._tensorize', 'hasattr', 'data_struct.__array__', 'self._consolidate']",5
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter:_tensorize,JaxFormatter:_tensorize,method,19,77,49,856,11.12,0,5,"['self', 'value']","[None, None]","[None, None]",83,[],"['isinstance', 'type', 'np.issubdtype', 'value.tolist', 'np.asarray', 'self._map_devices_to_str', 'jax.default_device', 'jnp.array']",8
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter:format_batch,JaxFormatter:format_batch,method,8,14,11,243,17.36,1,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",154,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize', 'self._consolidate']",3
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter:format_column,JaxFormatter:format_column,method,6,11,7,229,20.82,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",147,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize', 'self._consolidate']",3
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter:format_row,JaxFormatter:format_row,method,5,6,5,139,23.17,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",142,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize']",2
repos/datasets/src/datasets/formatting/jax_formatter.py:JaxFormatter:recursive_tensorize,JaxFormatter:recursive_tensorize,method,2,4,4,70,17.5,0,0,"['self', 'data_struct']","[None, ' dict']","[None, None]",139,[],['map_nested'],1
repos/datasets/src/datasets/formatting/np_formatter.py:NumpyFormatter,NumpyFormatter,class,45,212,120,2617,12.34,1,11,[],[],[],26,[],[],0
repos/datasets/src/datasets/formatting/np_formatter.py:NumpyFormatter:__init__,NumpyFormatter:__init__,method,3,3,3,72,24.0,0,0,"['self', 'features', '**np_array_kwargs']","[None, None, None]","[None, 'None', None]",27,[],['super'],1
repos/datasets/src/datasets/formatting/np_formatter.py:NumpyFormatter:_consolidate,NumpyFormatter:_consolidate,method,9,32,25,241,7.53,0,2,"['self', 'column']","[None, None]","[None, None]",31,[],"['isinstance', 'all', 'np.stack', 'np.empty']",4
repos/datasets/src/datasets/formatting/np_formatter.py:NumpyFormatter:_recursive_tensorize,NumpyFormatter:_recursive_tensorize,method,13,48,30,617,12.85,0,6,"['self', 'data_struct']","[None, None]","[None, None]",68,[],"['isinstance', 'self._tensorize', 'hasattr', 'data_struct.__array__', 'self._consolidate']",5
repos/datasets/src/datasets/formatting/np_formatter.py:NumpyFormatter:_tensorize,NumpyFormatter:_tensorize,method,9,59,33,649,11.0,0,3,"['self', 'value']","[None, None]","[None, None]",46,[],"['isinstance', 'type', 'np.issubdtype', 'np.asarray']",4
repos/datasets/src/datasets/formatting/np_formatter.py:NumpyFormatter:format_batch,NumpyFormatter:format_batch,method,8,14,11,243,17.36,1,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",100,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize', 'self._consolidate']",3
repos/datasets/src/datasets/formatting/np_formatter.py:NumpyFormatter:format_column,NumpyFormatter:format_column,method,6,11,7,229,20.82,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",93,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize', 'self._consolidate']",3
repos/datasets/src/datasets/formatting/np_formatter.py:NumpyFormatter:format_row,NumpyFormatter:format_row,method,5,6,5,139,23.17,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",88,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize']",2
repos/datasets/src/datasets/formatting/np_formatter.py:NumpyFormatter:recursive_tensorize,NumpyFormatter:recursive_tensorize,method,2,4,4,70,17.5,0,0,"['self', 'data_struct']","[None, ' dict']","[None, None]",85,[],['map_nested'],1
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsArrowExtractor,PolarsArrowExtractor,class,10,102,34,883,8.66,0,6,[],[],[],33,[],[],0
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFeaturesDecoder,PolarsFeaturesDecoder,class,26,85,58,878,10.33,1,5,[],[],[],68,[],[],0
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFormatter,PolarsFormatter,class,24,59,40,844,14.31,0,0,[],[],[],101,[],[],0
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsArrowExtractor:extract_batch,PolarsArrowExtractor:extract_batch,method,7,28,22,219,7.82,0,2,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",56,[],"['polars.from_arrow', 'ValueError']",2
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsArrowExtractor:extract_column,PolarsArrowExtractor:extract_column,method,7,28,22,257,9.18,0,2,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",45,[],"['polars.from_arrow', 'ValueError']",2
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsArrowExtractor:extract_row,PolarsArrowExtractor:extract_row,method,7,28,22,235,8.39,0,2,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",34,[],"['polars.from_arrow', 'ValueError']",2
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFeaturesDecoder:__init__,PolarsFeaturesDecoder:__init__,method,9,13,11,74,5.69,0,0,"['self', 'features']","[None, ' Optional[Features]']","[None, None]",69,[],[],0
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFeaturesDecoder:decode_batch,PolarsFeaturesDecoder:decode_batch,method,2,2,2,28,14.0,0,0,"['self', 'batch']","[None, ' ""pl.DataFrame""']","[None, None]",97,[],['self.decode_row'],1
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFeaturesDecoder:decode_column,PolarsFeaturesDecoder:decode_column,method,8,21,17,259,12.33,0,2,"['self', 'column', 'column_name']","[None, ' ""pl.Series""', ' str']","[None, None, None]",87,[],"['no_op_if_value_is_null', 'column.map_elements']",2
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFeaturesDecoder:decode_row,PolarsFeaturesDecoder:decode_row,method,9,25,23,279,11.16,1,3,"['self', 'row']","[None, ' ""pl.DataFrame""']","[None, None]",73,[],"['no_op_if_value_is_null', 'row[list', 'row.map_rows']",3
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFormatter:__init__,PolarsFormatter:__init__,method,14,18,16,234,13.0,0,0,"['self', 'features', '**np_array_kwargs']","[None, None, None]","[None, 'None', None]",102,[],"['super', 'PolarsFeaturesDecoder']",2
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFormatter:format_batch,PolarsFormatter:format_batch,method,4,6,4,118,19.67,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",119,[],['self.polars_arrow_extractor'],1
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFormatter:format_column,PolarsFormatter:format_column,method,4,7,5,157,22.43,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",114,[],['self.polars_arrow_extractor'],1
repos/datasets/src/datasets/formatting/polars_formatter.py:PolarsFormatter:format_row,PolarsFormatter:format_row,method,4,6,4,114,19.0,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",109,[],['self.polars_arrow_extractor'],1
repos/datasets/src/datasets/formatting/tf_formatter.py:TFFormatter,TFFormatter,class,60,240,135,2665,11.1,2,10,[],[],[],32,[],[],0
repos/datasets/src/datasets/formatting/tf_formatter.py:TFFormatter:__init__,TFFormatter:__init__,method,10,14,12,130,9.29,0,0,"['self', 'features', '**tf_tensor_kwargs']","[None, None, None]","[None, 'None', None]",33,[],['super'],1
repos/datasets/src/datasets/formatting/tf_formatter.py:TFFormatter:_consolidate,TFFormatter:_consolidate,method,10,46,31,328,7.13,1,2,"['self', 'column']","[None, None]","[None, None]",38,[],"['isinstance', 'all', 'tf.stack']",3
repos/datasets/src/datasets/formatting/tf_formatter.py:TFFormatter:_recursive_tensorize,TFFormatter:_recursive_tensorize,method,24,61,43,662,10.85,0,5,"['self', 'data_struct']","[None, None]","[None, None]",75,[],"['isinstance', 'self._tensorize', 'hasattr', 'data_struct.__array__', 'self._consolidate']",5
repos/datasets/src/datasets/formatting/tf_formatter.py:TFFormatter:_tensorize,TFFormatter:_tensorize,method,12,49,33,505,10.31,0,3,"['self', 'value']","[None, None]","[None, None]",55,[],"['isinstance', 'np.issubdtype', 'np.asarray', 'tf.convert_to_tensor']",4
repos/datasets/src/datasets/formatting/tf_formatter.py:TFFormatter:format_batch,TFFormatter:format_batch,method,8,14,11,243,17.36,1,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",109,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize', 'self._consolidate']",3
repos/datasets/src/datasets/formatting/tf_formatter.py:TFFormatter:format_column,TFFormatter:format_column,method,6,11,7,229,20.82,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",102,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize', 'self._consolidate']",3
repos/datasets/src/datasets/formatting/tf_formatter.py:TFFormatter:format_row,TFFormatter:format_row,method,5,6,5,139,23.17,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",97,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize']",2
repos/datasets/src/datasets/formatting/tf_formatter.py:TFFormatter:recursive_tensorize,TFFormatter:recursive_tensorize,method,2,4,4,70,17.5,0,0,"['self', 'data_struct']","[None, ' dict']","[None, None]",94,[],['map_nested'],1
repos/datasets/src/datasets/formatting/torch_formatter.py:TorchFormatter,TorchFormatter,class,63,228,137,2637,11.57,2,10,[],[],[],32,[],[],0
repos/datasets/src/datasets/formatting/torch_formatter.py:TorchFormatter:__init__,TorchFormatter:__init__,method,8,11,9,124,11.27,0,0,"['self', 'features', '**torch_tensor_kwargs']","[None, None, None]","[None, 'None', None]",33,[],['super'],1
repos/datasets/src/datasets/formatting/torch_formatter.py:TorchFormatter:_consolidate,TorchFormatter:_consolidate,method,8,26,22,190,7.31,1,2,"['self', 'column']","[None, None]","[None, None]",38,[],"['isinstance', 'all', 'torch.stack']",3
repos/datasets/src/datasets/formatting/torch_formatter.py:TorchFormatter:_recursive_tensorize,TorchFormatter:_recursive_tensorize,method,20,46,35,496,10.78,0,3,"['self', 'data_struct']","[None, None]","[None, None]",80,[],"['hasattr', 'isinstance', 'data_struct.__array__', 'self._consolidate', 'self._tensorize']",5
repos/datasets/src/datasets/formatting/torch_formatter.py:TorchFormatter:_tensorize,TorchFormatter:_tensorize,method,19,75,47,781,10.41,0,5,"['self', 'value']","[None, None]","[None, None]",49,[],"['isinstance', 'type', 'np.issubdtype', 'value.tolist', 'value.astype', 'np.asarray', 'value.transpose', 'torch.tensor']",8
repos/datasets/src/datasets/formatting/torch_formatter.py:TorchFormatter:format_batch,TorchFormatter:format_batch,method,8,14,11,243,17.36,1,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",109,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize', 'self._consolidate']",3
repos/datasets/src/datasets/formatting/torch_formatter.py:TorchFormatter:format_column,TorchFormatter:format_column,method,6,11,7,229,20.82,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",102,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize', 'self._consolidate']",3
repos/datasets/src/datasets/formatting/torch_formatter.py:TorchFormatter:format_row,TorchFormatter:format_row,method,5,6,5,139,23.17,0,0,"['self', 'pa_table']","[None, ' pa.Table']","[None, None]",97,[],"['self.numpy_arrow_extractor', 'self.recursive_tensorize']",2
repos/datasets/src/datasets/formatting/torch_formatter.py:TorchFormatter:recursive_tensorize,TorchFormatter:recursive_tensorize,method,2,4,4,70,17.5,0,0,"['self', 'data_struct']","[None, ' dict']","[None, None]",94,[],['map_nested'],1
repos/datasets/src/datasets/hub.py:_delete_files,_delete_files,function,16,92,47,1134,12.33,3,5,"['dataset_id', 'revision', 'token']","[None, None, None]","[None, 'None', 'None']",177,[],"['dataset_id.split', 'HfApi', 'hf_api.list_repo_files', 'hf_api.delete_file', 'legacy_json_file.append', 'filename.endswith', 'python_files.append', 'data_files.append']",8
repos/datasets/src/datasets/hub.py:convert_to_parquet,convert_to_parquet,function,24,112,78,1421,12.69,1,2,"['repo_id', 'revision', 'token', 'str]] ', 'trust_remote_code', '']","[' str', ' Optional[str] ', ' Optional[Union[bool', None, ' Optional[bool] ', None]","[None, ' None', None, ' None', ' None', None]",23,"['    """"""Convert Hub [script-based dataset](dataset_script) to Parquet [data-only dataset](repository_structure), so that\n', '    the dataset viewer will be supported.\n', '\n', '    This function:\n', '    - makes a copy of the script on the ""main"" branch into a dedicated branch called ""script"" (if it does not already exist)\n', '    - creates a pull request to the Hub dataset to convert it to Parquet files (and deletes the script from the main branch)\n', '\n', '    If in the future you need to recreate the Parquet files from the ""script"" branch, pass the `revision=""script""` argument.\n', '\n', '    Note that you should pass the `trust_remote_code=True` argument only if you trust the remote code to be executed locally on your machine.\n', '\n', '    Args:\n', '        repo_id (`str`): ID of the source Hub dataset repository, in the following format: `<user>/<dataset_name>` or\n', '            `<org>/<dataset_name>`.\n', '        revision (`str`, *optional*): Branch of the source Hub dataset repository. Defaults to the `""main""` branch.\n', '        token (`bool` or `str`, *optional*): Authentication token for the Hugging Face Hub.\n', '        trust_remote_code (`bool`, defaults to `True`): Whether you trust the remote code of the Hub script-based\n', '            dataset to be executed locally on your machine. This option should only be set to `True` for repositories\n', '            where you have read the code and which you trust.\n', '\n', '            <Tip warning={true}>\n', '\n', '            `trust_remote_code` will default to False in the next major release.\n', '\n', '            </Tip>\n', '\n', '    Returns:\n', '        `huggingface_hub.CommitInfo`\n', '    """"""\n']","['print', 'get_dataset_config_names', 'get_dataset_default_config_name', 'configs.remove', 'configs.pop', 'load_dataset', 'dataset.push_to_hub', 'time.sleep', '_delete_files', 'HfApi', 'api.create_branch']",11
repos/datasets/src/datasets/hub.py:delete_from_hub,delete_from_hub,function,39,110,88,2006,18.24,1,6,"['repo_id', 'config_name', 'revision', 'token', 'str]] ', '']","[' str', ' str', ' Optional[str] ', ' Optional[Union[bool', None, None]","[None, None, ' None', None, ' None', None]",105,"['    """"""Delete a dataset configuration from a [data-only dataset](repository_structure) on the Hub.\n', '\n', '    Args:\n', '        repo_id (`str`): ID of the Hub dataset repository, in the following format: `<user>/<dataset_name>` or\n', '            `<org>/<dataset_name>`.\n', '        config_name (`str`): Name of the dataset configuration.\n', '        revision (`str`, *optional*): Branch to delete the configuration from. Defaults to the `""main""` branch.\n', '        token (`bool` or `str`, *optional*): Authentication token for the Hugging Face Hub.\n', '\n', '    Returns:\n', '        `huggingface_hub.CommitInfo`\n', '    """"""\n']","['HfFileSystem', 'load_dataset_builder', 'chain', 'fs.resolve_path', 'operations.append', 'DatasetCard.load', 'MetadataConfigs.from_dataset_card_data', 'metadata_configs.pop', 'DatasetCardData', 'metadata_configs.to_dataset_card_data', 'DatasetInfosDict.from_dataset_card_data', 'dataset_infos.pop', 'dataset_infos.to_dataset_card_data', 'CommitOperationAdd', 'path_or_fileobj=str', 'HfApi', 'api.create_commit', 'print']",18
repos/datasets/src/datasets/info.py:DatasetInfo,DatasetInfo,class,119,613,297,6618,10.8,7,26,[],[],[],94,[],[],0
repos/datasets/src/datasets/info.py:DatasetInfosDict,DatasetInfosDict,class,53,250,140,3816,15.26,5,13,[],[],[],402,[],[],0
repos/datasets/src/datasets/info.py:DownloadChecksumsEntryData,DownloadChecksumsEntryData,class,3,6,5,23,3.83,0,0,[],[],[],64,[],[],0
repos/datasets/src/datasets/info.py:MetricInfo,MetricInfo,class,44,157,109,1651,10.52,2,4,[],[],[],512,[],[],0
repos/datasets/src/datasets/info.py:MissingCachedSizesConfigError,MissingCachedSizesConfigError,class,0,0,0,0,0.0,0,0,[],[],[],69,[],[],0
repos/datasets/src/datasets/info.py:NonMatchingCachedSizesError,NonMatchingCachedSizesError,class,0,0,0,0,0.0,0,0,[],[],[],73,[],[],0
repos/datasets/src/datasets/info.py:PostProcessedInfo,PostProcessedInfo,class,15,44,35,418,9.5,1,1,[],[],[],78,[],[],0
repos/datasets/src/datasets/info.py:SupervisedKeysData,SupervisedKeysData,class,3,6,5,26,4.33,0,0,[],[],[],58,[],[],0
repos/datasets/src/datasets/info.py:DatasetInfo:__post_init__,DatasetInfo:__post_init__,method,18,138,59,1530,11.09,1,13,['self'],[None],[None],172,[],"['isinstance', 'Features.from_dict', 'PostProcessedInfo.from_dict', 'Version', 'Version.from_dict', 'SplitDict.from_split_dict', 'SupervisedKeysData', 'task_template_from_dict', 'list', 'template.align_with_features']",10
repos/datasets/src/datasets/info.py:DatasetInfo:_dump_info,DatasetInfo:_dump_info,method,1,6,6,83,13.83,0,0,"['self', 'file', 'pretty_print']","[None, None, None]","[None, None, 'False']",262,"['        """"""Dump info in `file` file-like object open in bytes mode (to support remote files)""""""\n']",['file.write'],1
repos/datasets/src/datasets/info.py:DatasetInfo:_dump_license,DatasetInfo:_dump_license,method,1,1,1,40,40.0,0,0,"['self', 'file']","[None, None]","[None, None]",266,"['        """"""Dump license in `file` file-like object open in bytes mode (to support remote files)""""""\n']",['file.write'],1
repos/datasets/src/datasets/info.py:DatasetInfo:_from_yaml_dict,DatasetInfo:_from_yaml_dict,method,12,34,25,355,10.44,1,2,"['cls', 'yaml_data']","[None, ' dict']","[None, None]",392,[],"['copy.deepcopy', 'yaml_data.get', 'Features._from_yaml_list', 'SplitDict._from_yaml_list', 'dataclasses.fields', 'cls', 'yaml_data.items']",7
repos/datasets/src/datasets/info.py:DatasetInfo:_to_yaml_dict,DatasetInfo:_to_yaml_dict,method,12,35,27,348,9.94,1,2,['self'],[None],[None],377,[],"['asdict', 'getattr', 'hasattr', 'value._to_yaml_list', 'value._to_yaml_string']",5
repos/datasets/src/datasets/info.py:DatasetInfo:copy,DatasetInfo:copy,method,2,8,8,73,9.12,0,0,['self'],[None],[None],374,[],"['self.__class__', 'copy.deepcopy']",2
repos/datasets/src/datasets/info.py:DatasetInfo:from_dict,DatasetInfo:from_dict,method,5,18,14,117,6.5,1,0,"['cls', 'dataset_info_dict']","[None, ' dict']","[None, None]",360,[],"['dataclasses.fields', 'cls', 'dataset_info_dict.items']",3
repos/datasets/src/datasets/info.py:DatasetInfo:from_directory,DatasetInfo:from_directory,method,16,66,62,674,10.21,0,2,"['cls', 'dataset_info_dir', 'fs', 'storage_options']","[None, ' str', None, ' Optional[dict] ']","[None, None, '""deprecated""', ' None']",306,"['        """"""Create [`DatasetInfo`] from the JSON file in `dataset_info_dir`.\n', '\n', '        This function updates all the dynamically generated fields (num_examples,\n', '        hash, time of creation,...) of the [`DatasetInfo`].\n', '\n', '        This will overwrite all previous metadata.\n', '\n', '        Args:\n', '            dataset_info_dir (`str`):\n', '                The directory containing the metadata file. This\n', '                should be the root directory of a specific dataset version.\n', '            fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n', '                Instance of the remote filesystem used to download the files from.\n', '\n', '                <Deprecated version=""2.9.0"">\n', '\n', '                `fs` was deprecated in version 2.9.0 and will be removed in 3.0.0.\n', '                Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`.\n', '\n', '                </Deprecated>\n', '\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.9.0""/>\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import DatasetInfo\n', '        >>> ds_info = DatasetInfo.from_directory(""/path/to/directory/"")\n', '        ```\n', '        """"""\n']","['warnings.warn', 'url_to_fs', 'logger.info', 'ValueError', 'DatasetInfo.from_directory', 'fs.open', 'json.load', 'cls.from_dict']",8
repos/datasets/src/datasets/info.py:DatasetInfo:from_merge,DatasetInfo:from_merge,method,15,91,50,1086,11.93,1,4,"['cls', 'dataset_infos']","[None, ' List[""DatasetInfo""]']","[None, None]",271,[],"['len', 'all', 'list', 'cls']",4
repos/datasets/src/datasets/info.py:DatasetInfo:update,DatasetInfo:update,method,6,21,20,143,6.81,1,1,"['self', 'other_dataset_info', 'ignore_none']","[None, ' ""DatasetInfo""', None]","[None, None, 'True']",364,[],"['self_dict.update', 'copy.deepcopy']",2
repos/datasets/src/datasets/info.py:DatasetInfo:write_to_directory,DatasetInfo:write_to_directory,method,12,58,50,585,10.09,0,2,"['self', 'dataset_info_dir', 'pretty_print', 'fs', 'storage_options']","[None, None, None, None, ' Optional[dict] ']","[None, None, 'False', '""deprecated""', ' None']",213,"['        """"""Write `DatasetInfo` and license (if present) as JSON files to `dataset_info_dir`.\n', '\n', '        Args:\n', '            dataset_info_dir (`str`):\n', '                Destination directory.\n', '            pretty_print (`bool`, defaults to `False`):\n', '                If `True`, the JSON will be pretty-printed with the indent level of 4.\n', '            fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n', '                Instance of the remote filesystem used to download the files from.\n', '\n', '                <Deprecated version=""2.9.0"">\n', '\n', '                `fs` was deprecated in version 2.9.0 and will be removed in 3.0.0.\n', '                Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`.\n', '\n', '                </Deprecated>\n', '\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.9.0""/>\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"")\n', '        >>> ds.info.write_to_directory(""/path/to/directory/"")\n', '        ```\n', '        """"""\n']","['warnings.warn', 'url_to_fs', 'fs.open', 'self._dump_info', 'self._dump_license']",5
repos/datasets/src/datasets/info.py:DatasetInfosDict:from_dataset_card_data,DatasetInfosDict:from_dataset_card_data,method,9,33,28,541,16.39,1,2,"['cls', 'dataset_card_data']","[None, ' DatasetCardData']","[None, None]",452,[],"['isinstance', 'cls', 'dataset_info_yaml_dict.get', 'DatasetInfo._from_yaml_dict']",4
repos/datasets/src/datasets/info.py:DatasetInfosDict:from_directory,DatasetInfosDict:from_directory,method,12,41,35,638,15.56,1,3,"['cls', 'dataset_infos_dir']","[None, None]","[None, None]",432,[],"['logger.info', 'DatasetCard.load', 'cls.from_dataset_card_data', 'open', 'cls', 'DatasetInfo.from_dict', 'json.load']",7
repos/datasets/src/datasets/info.py:DatasetInfosDict:to_dataset_card_data,DatasetInfosDict:to_dataset_card_data,method,22,84,51,1327,15.8,3,4,"['self', 'dataset_card_data']","[None, ' DatasetCardData']","[None, None]",470,[],"['isinstance', 'dset_info._to_yaml_dict', 'self.items', 'total_dataset_infos.items', 'len', 'next', 'sorted', 'dataset_info_yaml_dict.pop']",8
repos/datasets/src/datasets/info.py:DatasetInfosDict:write_to_directory,DatasetInfosDict:write_to_directory,method,21,66,54,974,14.76,0,4,"['self', 'dataset_infos_dir', 'overwrite', 'pretty_print']","[None, None, None, None]","[None, None, 'False', 'False']",403,[],"['self.from_directory', 'total_dataset_infos.update', 'open', 'asdict', 'total_dataset_infos.items', 'json.dump', 'DatasetCard.load', 'DatasetCardData', 'total_dataset_infos.to_dataset_card_data', 'DatasetCard', 'str', 'dataset_card.save']",12
repos/datasets/src/datasets/info.py:MetricInfo:__post_init__,MetricInfo:__post_init__,method,6,36,33,233,6.47,1,2,['self'],[None],[None],538,[],"['isinstance', 'ValueError']",2
repos/datasets/src/datasets/info.py:MetricInfo:from_dict,MetricInfo:from_dict,method,5,18,14,116,6.44,1,0,"['cls', 'metric_info_dict']","[None, ' dict']","[None, None]",591,[],"['dataclasses.fields', 'cls', 'metric_info_dict.items']",3
repos/datasets/src/datasets/info.py:MetricInfo:from_directory,MetricInfo:from_directory,method,9,24,23,316,13.17,0,1,"['cls', 'metric_info_dir']","[None, None]","[None, None]",568,"['        """"""Create MetricInfo from the JSON file in `metric_info_dir`.\n', '\n', '        Args:\n', '            metric_info_dir: `str` The directory containing the metadata file. This\n', '                should be the root directory of a specific dataset version.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import MetricInfo\n', '        >>> metric_info = MetricInfo.from_directory(""/path/to/directory/"")\n', '        ```\n', '        """"""\n']","['logger.info', 'ValueError', 'MetricInfo.from_directory', 'open', 'json.load', 'cls.from_dict']",6
repos/datasets/src/datasets/info.py:MetricInfo:write_to_directory,MetricInfo:write_to_directory,method,5,24,17,276,11.5,0,1,"['self', 'metric_info_dir', 'pretty_print']","[None, None, None]","[None, None, 'False']",547,"['        """"""Write `MetricInfo` as JSON to `metric_info_dir`.\n', '        Also save the license separately in LICENCE.\n', '        If `pretty_print` is True, the JSON will be pretty-printed with the indent level of 4.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_metric\n', '        >>> metric = load_metric(""accuracy"")\n', '        >>> metric.info.write_to_directory(""/path/to/directory/"")\n', '        ```\n', '        """"""\n']","['open', 'json.dump', 'f.write']",3
repos/datasets/src/datasets/info.py:PostProcessedInfo:__post_init__,PostProcessedInfo:__post_init__,method,4,11,9,113,10.27,0,1,['self'],[None],[None],82,[],"['isinstance', 'Features.from_dict']",2
repos/datasets/src/datasets/info.py:PostProcessedInfo:from_dict,PostProcessedInfo:from_dict,method,5,18,14,124,6.89,1,0,"['cls', 'post_processed_info_dict']","[None, ' dict']","[None, None]",88,[],"['dataclasses.fields', 'cls', 'post_processed_info_dict.items']",3
repos/datasets/src/datasets/inspect.py:get_dataset_config_info,get_dataset_config_info,function,10,40,32,290,7.25,0,0,"['path', 'config_name', 'data_files', 'Sequence[str]', 'Mapping[str', 'Union[str', 'Sequence[str]]]]] ', 'download_config', 'download_mode', 'str]] ', 'revision', 'Version]] ', 'token', 'str]] ', 'use_auth_token', '**config_kwargs', '']","[' str', ' Optional[str] ', ' Optional[Union[str', None, None, None, None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[Union[str', None, ' Optional[Union[bool', None, None, None, None]","[None, ' None', None, None, None, None, ' None', ' None', None, ' None', None, ' None', None, ' None', '""deprecated""', None, None]",429,"['    """"""Get the meta information (DatasetInfo) about a dataset for a particular config\n', '\n', '    Args:\n', '        path (``str``): path to the dataset processing script with the dataset builder. Can be either:\n', '\n', '            - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n', ""                e.g. ``'./dataset/squad'`` or ``'./dataset/squad/squad.py'``\n"", '            - a dataset identifier on the Hugging Face Hub (list all available datasets and ids with ``datasets.list_datasets()``)\n', ""                e.g. ``'squad'``, ``'glue'`` or ``'openai/webtext'``\n"", '        config_name (:obj:`str`, optional): Defining the name of the dataset configuration.\n', '        data_files (:obj:`str` or :obj:`Sequence` or :obj:`Mapping`, optional): Path(s) to source data file(s).\n', '        download_config (:class:`~download.DownloadConfig`, optional): Specific download configuration parameters.\n', '        download_mode (:class:`DownloadMode` or :obj:`str`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\n', '        revision (:class:`~utils.Version` or :obj:`str`, optional): Version of the dataset script to load.\n', '            As datasets have their own git repository on the Datasets Hub, the default version ""main"" corresponds to their ""main"" branch.\n', '            You can specify a different version than the default ""main"" by using a commit SHA or a git tag of the dataset repository.\n', '        token (``str`` or :obj:`bool`, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '            If True, or not specified, will get token from `""~/.huggingface""`.\n', '        use_auth_token (``str`` or :obj:`bool`, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '            If True, or not specified, will get token from `""~/.huggingface""`.\n', '\n', '            <Deprecated version=""2.14.0"">\n', '\n', '            `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n', '\n', '            </Deprecated>\n', '\n', '        **config_kwargs (additional keyword arguments): optional attributes for builder class which will override the attributes if supplied.\n', '\n', '    """"""\n']",[],0
repos/datasets/src/datasets/inspect.py:get_dataset_config_names,get_dataset_config_names,function,5,22,21,446,20.27,0,0,"['path', 'revision', 'Version]] ', 'download_config', 'download_mode', 'str]] ', 'dynamic_modules_path', 'data_files', 'List', 'str]] ', '**download_kwargs', '']","[' str', ' Optional[Union[str', None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[str] ', ' Optional[Union[Dict', None, None, None, None]","[None, None, ' None', ' None', None, ' None', ' None', None, None, ' None', None, None]",291,"['    """"""Get the list of available config names for a particular dataset.\n', '\n', '    Args:\n', '        path (`str`): path to the dataset processing script with the dataset builder. Can be either:\n', '\n', '            - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n', ""                e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`\n"", '            - a dataset identifier on the Hugging Face Hub (list all available datasets and ids with [`datasets.list_datasets`])\n', ""                e.g. `'squad'`, `'glue'` or `'openai/webtext'`\n"", '        revision (`Union[str, datasets.Version]`, *optional*):\n', '            If specified, the dataset module will be loaded from the datasets repository at this version.\n', '            By default:\n', '            - it is set to the local version of the lib.\n', ""            - it will also try to load it from the main branch if it's not available at the local version of the lib.\n"", '            Specifying a version that is different from your local version of the lib might cause compatibility issues.\n', '        download_config ([`DownloadConfig`], *optional*):\n', '            Specific download configuration parameters.\n', '        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n', '            Download/generate mode.\n', '        dynamic_modules_path (`str`, defaults to `~/.cache/huggingface/modules/datasets_modules`):\n', '            Optional path to the directory in which the dynamic modules are saved. It must have been initialized with `init_dynamic_modules`.\n', '            By default the datasets and metrics are stored inside the `datasets_modules` module.\n', '        data_files (`Union[Dict, List, str]`, *optional*):\n', '            Defining the data_files of the dataset configuration.\n', '        **download_kwargs (additional keyword arguments):\n', '            Optional attributes for [`DownloadConfig`] which will override the attributes in `download_config` if supplied,\n', '            for example `token`.\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> from datasets import get_dataset_config_names\n', '    >>> get_dataset_config_names(""glue"")\n', ""    ['cola',\n"", ""     'sst2',\n"", ""     'mrpc',\n"", ""     'qqp',\n"", ""     'stsb',\n"", ""     'mnli',\n"", ""     'mnli_mismatched',\n"", ""     'mnli_matched',\n"", ""     'qnli',\n"", ""     'rte',\n"", ""     'wnli',\n"", ""     'ax']\n"", '    ```\n', '    """"""\n']","['dataset_module_factory', 'get_dataset_builder_class', 'list']",3
repos/datasets/src/datasets/inspect.py:get_dataset_default_config_name,get_dataset_default_config_name,function,10,31,28,545,17.58,0,2,"['path', 'revision', 'Version]] ', 'download_config', 'download_mode', 'str]] ', 'dynamic_modules_path', 'data_files', 'List', 'str]] ', '**download_kwargs', '']","[' str', ' Optional[Union[str', None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[str] ', ' Optional[Union[Dict', None, None, None, None]","[None, None, ' None', ' None', None, ' None', ' None', None, None, ' None', None, None]",362,"['    """"""Get the default config name for a particular dataset.\n', '    Can return None only if the dataset has multiple configurations and no default configuration.\n', '\n', '    Args:\n', '        path (`str`): path to the dataset processing script with the dataset builder. Can be either:\n', '\n', '            - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n', ""                e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`\n"", '            - a dataset identifier on the Hugging Face Hub (list all available datasets and ids with [`datasets.list_datasets`])\n', ""                e.g. `'squad'`, `'glue'` or `'openai/webtext'`\n"", '        revision (`Union[str, datasets.Version]`, *optional*):\n', '            If specified, the dataset module will be loaded from the datasets repository at this version.\n', '            By default:\n', '            - it is set to the local version of the lib.\n', ""            - it will also try to load it from the main branch if it's not available at the local version of the lib.\n"", '            Specifying a version that is different from your local version of the lib might cause compatibility issues.\n', '        download_config ([`DownloadConfig`], *optional*):\n', '            Specific download configuration parameters.\n', '        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n', '            Download/generate mode.\n', '        dynamic_modules_path (`str`, defaults to `~/.cache/huggingface/modules/datasets_modules`):\n', '            Optional path to the directory in which the dynamic modules are saved. It must have been initialized with `init_dynamic_modules`.\n', '            By default the datasets and metrics are stored inside the `datasets_modules` module.\n', '        data_files (`Union[Dict, List, str]`, *optional*):\n', '            Defining the data_files of the dataset configuration.\n', '        **download_kwargs (additional keyword arguments):\n', '            Optional attributes for [`DownloadConfig`] which will override the attributes in `download_config` if supplied,\n', '            for example `token`.\n', '\n', '    Returns:\n', '        Optional[str]: the default config name if there is one\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> from datasets import get_dataset_default_config_name\n', '    >>> get_dataset_default_config_name(""openbookqa"")\n', ""    'main'\n"", '    ```\n', '    """"""\n']","['dataset_module_factory', 'get_dataset_builder_class', 'list', 'len']",4
repos/datasets/src/datasets/inspect.py:get_dataset_split_names,get_dataset_split_names,function,17,69,60,580,8.41,0,1,"['path', 'config_name', 'data_files', 'Sequence[str]', 'Mapping[str', 'Union[str', 'Sequence[str]]]]] ', 'download_config', 'download_mode', 'str]] ', 'revision', 'Version]] ', 'token', 'str]] ', 'use_auth_token', '**config_kwargs', '']","[' str', ' Optional[str] ', ' Optional[Union[str', None, None, None, None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[Union[str', None, ' Optional[Union[bool', None, None, None, None]","[None, ' None', None, None, None, None, ' None', ' None', None, ' None', None, ' None', None, ' None', '""deprecated""', None, None]",508,"['    """"""Get the list of available splits for a particular config and dataset.\n', '\n', '    Args:\n', '        path (`str`): path to the dataset processing script with the dataset builder. Can be either:\n', '\n', '            - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n', ""                e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`\n"", '            - a dataset identifier on the Hugging Face Hub (list all available datasets and ids with [`datasets.list_datasets`])\n', ""                e.g. `'squad'`, `'glue'` or `'openai/webtext'`\n"", '        config_name (`str`, *optional*):\n', '            Defining the name of the dataset configuration.\n', '        data_files (`str` or `Sequence` or `Mapping`, *optional*):\n', '            Path(s) to source data file(s).\n', '        download_config ([`DownloadConfig`], *optional*):\n', '            Specific download configuration parameters.\n', '        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n', '            Download/generate mode.\n', '        revision ([`Version`] or `str`, *optional*):\n', '            Version of the dataset script to load.\n', '            As datasets have their own git repository on the Datasets Hub, the default version ""main"" corresponds to their ""main"" branch.\n', '            You can specify a different version than the default ""main"" by using a commit SHA or a git tag of the dataset repository.\n', '        token (`str` or `bool`, *optional*):\n', '            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '            If `True`, or not specified, will get token from `""~/.huggingface""`.\n', '        use_auth_token (`str` or `bool`, *optional*):\n', '            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '            If `True`, or not specified, will get token from `""~/.huggingface""`.\n', '\n', '            <Deprecated version=""2.14.0"">\n', '\n', '            `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n', '\n', '            </Deprecated>\n', '\n', '        **config_kwargs (additional keyword arguments):\n', '            Optional attributes for builder class which will override the attributes if supplied.\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> from datasets import get_dataset_split_names\n', ""    >>> get_dataset_split_names('rotten_tomatoes')\n"", ""    ['train', 'validation', 'test']\n"", '    ```\n', '    """"""\n']","['warnings.warn', 'get_dataset_config_info', 'list']",3
repos/datasets/src/datasets/inspect.py:inspect_dataset,inspect_dataset,function,22,47,41,572,12.17,0,4,"['path', 'local_path', 'download_config', '**download_kwargs']","[' str', ' str', ' Optional[DownloadConfig] ', None]","[None, None, ' None', None]",125,"['    """"""\n', '    Allow inspection/modification of a dataset script by copying on local drive at local_path.\n', '\n', '    Args:\n', '        path (`str`): Path to the dataset processing script with the dataset builder. Can be either:\n', '\n', '            - a local path to processing script or the directory containing the script (if the script has the same name\n', '                as the directory),\n', ""                e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\n"", '            - a dataset identifier on the Hugging Face Hub (list all available datasets and ids with [`list_datasets`])\n', ""                e.g. `'squad'`, `'glue'` or `'openai/webtext'`.\n"", '        local_path (`str`):\n', '            Path to the local folder to copy the dataset script to.\n', '        download_config ([`DownloadConfig`], *optional*):\n', '            Specific download configuration parameters.\n', '        **download_kwargs (additional keyword arguments):\n', '            Optional arguments for [`DownloadConfig`] which will override\n', '            the attributes of `download_config` if supplied.\n', '    """"""\n']","['DownloadConfig', 'str', 'shutil.copytree', 'huggingface_hub.HfApi']",4
repos/datasets/src/datasets/inspect.py:inspect_metric,inspect_metric,function,35,128,102,1213,9.48,1,1,"['path', 'local_path', 'download_config', '**download_kwargs']","[' str', ' str', ' Optional[DownloadConfig] ', None]","[None, None, ' None', None]",164,[],"['DownloadConfig', 'print', 'warnings.warn', 'get_dataset_config_names', 'get_dataset_config_info']",5
repos/datasets/src/datasets/inspect.py:list_datasets,list_datasets,function,7,27,16,228,8.44,2,3,"['with_community_datasets', 'with_details']","[None, None]","['True', 'False']",53,"['    """"""List all the datasets scripts available on the Hugging Face Hub.\n', '\n', '    Args:\n', '        with_community_datasets (`bool`, *optional*, defaults to `True`):\n', '            Include the community provided datasets.\n', '        with_details (`bool`, *optional*, defaults to `False`):\n', '            Return the full details on the datasets instead of only the short name.\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> from datasets import list_datasets\n', '    >>> list_datasets()\n', ""    ['acronym_identification',\n"", ""     'ade_corpus_v2',\n"", ""     'adversarial_qa',\n"", ""     'aeslc',\n"", ""     'afrikaans_ner_corpus',\n"", ""     'ag_news',\n"", '     ...\n', '    ]\n', '    ```\n', '    """"""\n']","['huggingface_hub.list_datasets', 'list']",2
repos/datasets/src/datasets/inspect.py:list_metrics,list_metrics,function,7,27,15,192,7.11,2,3,"['with_community_metrics', 'with_details']","[None, None]","['True', 'False']",88,"['    """"""List all the metrics script available on the Hugging Face Hub.\n', '\n', '    <Deprecated version=""2.5.0"">\n', '\n', '    Use `evaluate.list_evaluation_modules` instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n', '\n', '    </Deprecated>\n', '\n', '    Args:\n', '        with_community_metrics (:obj:`bool`, optional, default ``True``): Include the community provided metrics.\n', '        with_details (:obj:`bool`, optional, default ``False``): Return the full details on the metrics instead of only the short name.\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> from datasets import list_metrics\n', '    >>> list_metrics()\n', ""    ['accuracy',\n"", ""     'bertscore',\n"", ""     'bleu',\n"", ""     'bleurt',\n"", ""     'cer',\n"", ""     'chrf',\n"", '     ...\n', '    ]\n', '    ```\n', '    """"""\n']",['huggingface_hub.list_metrics'],1
repos/datasets/src/datasets/inspect.py:SplitsNotFoundError,SplitsNotFoundError,class,0,1,1,4,4.0,0,0,[],[],[],48,[],[],0
repos/datasets/src/datasets/io/abc.py:AbstractDatasetInputStream,AbstractDatasetInputStream,class,14,39,34,381,9.77,0,0,[],[],[],34,[],[],0
repos/datasets/src/datasets/io/abc.py:AbstractDatasetReader,AbstractDatasetReader,class,20,58,50,610,10.52,0,1,[],[],[],8,[],[],0
repos/datasets/src/datasets/io/abc.py:AbstractDatasetInputStream:__init__,AbstractDatasetInputStream:__init__,method,12,12,12,149,12.42,0,0,"['self', 'features', 'cache_dir', 'keep_in_memory', 'streaming', 'num_proc', '**kwargs', '']","[None, ' Optional[Features] ', ' str ', ' bool ', ' bool ', ' Optional[int] ', None, None]","[None, ' None', ' None', ' False', ' False', ' None', None, None]",9,[],[],0
repos/datasets/src/datasets/io/abc.py:AbstractDatasetInputStream:read,AbstractDatasetInputStream:read,method,0,1,1,4,4.0,0,0,['self'],[None],[None],52,[],[],0
repos/datasets/src/datasets/io/abc.py:AbstractDatasetReader:__init__,AbstractDatasetReader:__init__,method,18,23,22,249,10.83,0,1,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', 'streaming', 'num_proc', '**kwargs', '']","[None, ' Optional[NestedDataStructureLike[PathLike]] ', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' bool ', ' Optional[int] ', None, None]","[None, ' None', ' None', ' None', ' None', ' False', ' False', ' None', None, None]",9,[],['isinstance'],1
repos/datasets/src/datasets/io/abc.py:AbstractDatasetReader:read,AbstractDatasetReader:read,method,0,1,1,4,4.0,0,0,['self'],[None],[None],30,[],[],0
repos/datasets/src/datasets/io/csv.py:CsvDatasetReader,CsvDatasetReader,class,24,80,60,1090,13.62,0,2,[],[],[],15,[],[],0
repos/datasets/src/datasets/io/csv.py:CsvDatasetWriter,CsvDatasetWriter,class,52,215,144,2083,9.69,2,5,[],[],[],69,[],[],0
repos/datasets/src/datasets/io/csv.py:CsvDatasetReader:__init__,CsvDatasetReader:__init__,method,12,25,21,354,14.16,0,1,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', 'streaming', 'num_proc', '**kwargs', '']","[None, ' NestedDataStructureLike[PathLike]', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' bool ', ' Optional[int] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' False', ' None', None, None]",16,[],"['super', 'isinstance', 'Csv']",3
repos/datasets/src/datasets/io/csv.py:CsvDatasetReader:read,CsvDatasetReader:read,method,10,28,22,474,16.93,0,1,['self'],[None],[None],45,[],[],0
repos/datasets/src/datasets/io/csv.py:CsvDatasetWriter:__init__,CsvDatasetWriter:__init__,method,17,37,34,332,8.97,0,2,"['self', 'dataset', 'path_or_buf', 'BinaryIO]', 'batch_size', 'num_proc', 'storage_options', '**to_csv_kwargs', '']","[None, ' Dataset', ' Union[PathLike', None, ' Optional[int] ', ' Optional[int] ', ' Optional[dict] ', None, None]","[None, None, None, None, ' None', ' None', ' None', None, None]",16,[],['ValueError'],1
repos/datasets/src/datasets/io/csv.py:CsvDatasetWriter:_batch_csv,CsvDatasetWriter:_batch_csv,method,14,27,26,310,11.48,0,1,"['self', 'args']","[None, None]","[None, None]",102,[],"['query_table', 'key=slice', 'batch.to_pandas', 'csv_str.encode']",4
repos/datasets/src/datasets/io/csv.py:CsvDatasetWriter:_write,CsvDatasetWriter:_write,method,16,81,52,667,8.23,2,1,"['self', 'file_obj', 'header', 'index', '**to_csv_kwargs']","[None, ' BinaryIO', None, None, None]","[None, None, None, None, None]",115,"['        """"""Writes the pyarrow table as CSV to a binary file handle.\n', '\n', '        Caller is responsible for opening and closing the handle.\n', '        """"""\n']","['hf_tqdm', 'range', 'len', 'self._batch_csv', 'file_obj.write', 'multiprocessing.Pool', 'pool.imap']",7
repos/datasets/src/datasets/io/csv.py:CsvDatasetWriter:write,CsvDatasetWriter:write,method,10,35,30,464,13.26,0,1,['self'],[None],[None],90,[],"['isinstance', 'fsspec.open', 'self._write']",3
repos/datasets/src/datasets/io/generator.py:GeneratorDatasetInputStream,GeneratorDatasetInputStream,class,20,71,53,960,13.52,0,1,[],[],[],8,[],[],0
repos/datasets/src/datasets/io/generator.py:GeneratorDatasetInputStream:__init__,GeneratorDatasetInputStream:__init__,method,8,16,13,260,16.25,0,0,"['self', 'generator', 'features', 'cache_dir', 'keep_in_memory', 'streaming', 'gen_kwargs', 'num_proc', '**kwargs', '']","[None, ' Callable', ' Optional[Features] ', ' str ', ' bool ', ' bool ', ' Optional[dict] ', ' Optional[int] ', None, None]","[None, None, ' None', ' None', ' False', ' False', ' None', ' None', None, None]",9,[],"['super', 'Generator']",2
repos/datasets/src/datasets/io/generator.py:GeneratorDatasetInputStream:read,GeneratorDatasetInputStream:read,method,10,28,22,468,16.71,0,1,['self'],[None],[None],36,[],[],0
repos/datasets/src/datasets/io/json.py:JsonDatasetReader,JsonDatasetReader,class,26,86,65,1147,13.34,0,2,[],[],[],15,[],[],0
repos/datasets/src/datasets/io/json.py:JsonDatasetWriter,JsonDatasetWriter,class,56,292,190,2769,9.48,2,9,[],[],[],73,[],[],0
repos/datasets/src/datasets/io/json.py:JsonDatasetReader:__init__,JsonDatasetReader:__init__,method,14,28,24,385,13.75,0,1,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', 'streaming', 'field', 'num_proc', '**kwargs', '']","[None, ' NestedDataStructureLike[PathLike]', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' bool ', ' Optional[str] ', ' Optional[int] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' False', ' None', ' None', None, None]",16,[],"['super', 'isinstance', 'Json']",3
repos/datasets/src/datasets/io/json.py:JsonDatasetReader:read,JsonDatasetReader:read,method,10,28,22,474,16.93,0,1,['self'],[None],[None],48,[],[],0
repos/datasets/src/datasets/io/json.py:JsonDatasetWriter:__init__,JsonDatasetWriter:__init__,method,17,37,34,334,9.03,0,2,"['self', 'dataset', 'path_or_buf', 'BinaryIO]', 'batch_size', 'num_proc', 'storage_options', '**to_json_kwargs', '']","[None, ' Dataset', ' Union[PathLike', None, ' Optional[int] ', ' Optional[int] ', ' Optional[dict] ', None, None]","[None, None, None, None, ' None', ' None', ' None', None, None]",16,[],['ValueError'],1
repos/datasets/src/datasets/io/json.py:JsonDatasetWriter:_batch_json,JsonDatasetWriter:_batch_json,method,13,26,25,336,12.92,0,1,"['self', 'args']","[None, None]","[None, None]",122,[],"['query_table', 'key=slice', 'batch.to_pandas', 'json_str.endswith', 'json_str.encode']",5
repos/datasets/src/datasets/io/json.py:JsonDatasetWriter:_write,JsonDatasetWriter:_write,method,16,81,52,677,8.36,2,1,"['self', 'file_obj', 'orient', 'lines', '**to_json_kwargs', '']","[None, ' BinaryIO', None, None, None, None]","[None, None, None, None, None, None]",135,"['        """"""Writes the pyarrow table as JSON lines to a binary file handle.\n', '\n', '        Caller is responsible for opening and closing the handle.\n', '        """"""\n']","['hf_tqdm', 'range', 'len', 'self._batch_json', 'file_obj.write', 'multiprocessing.Pool', 'pool.imap']",7
repos/datasets/src/datasets/io/json.py:JsonDatasetWriter:write,JsonDatasetWriter:write,method,15,111,86,1102,9.93,0,5,['self'],[None],[None],94,[],"['isinstance', 'NotImplementedError', 'fsspec.open', 'self._write']",4
repos/datasets/src/datasets/io/parquet.py:get_writer_batch_size,get_writer_batch_size,function,8,40,29,505,12.62,0,2,['features'],[' Features'],[None],18,"['    """"""\n', '    Get the writer_batch_size that defines the maximum row group size in the parquet files.\n', '    The default in `datasets` is 1,000 but we lower it to 100 for image datasets.\n', '    This allows to optimize random access to parquet file, since accessing 1 row requires\n', '    to read its entire row group.\n', '\n', '    This can be improved to get optimized size for querying/iterating\n', '    but at least it matches the dataset viewer expectations on HF.\n', '\n', '    Args:\n', '        ds_config_info (`datasets.info.DatasetInfo`):\n', '            Dataset info from `datasets`.\n', '    Returns:\n', '        writer_batch_size (`Optional[int]`):\n', '            Writer batch size to pass to a dataset builder.\n', '            If `None`, then it will use the `datasets` default.\n', '    """"""\n']","['set_batch_size', 'isinstance', 'min', '_visit']",4
repos/datasets/src/datasets/io/parquet.py:ParquetDatasetReader,ParquetDatasetReader,class,26,83,63,1151,13.87,0,2,[],[],[],53,[],[],0
repos/datasets/src/datasets/io/parquet.py:ParquetDatasetWriter,ParquetDatasetWriter,class,38,113,91,1369,12.12,1,2,[],[],[],109,[],[],0
repos/datasets/src/datasets/io/parquet.py:ParquetDatasetReader:__init__,ParquetDatasetReader:__init__,method,14,28,24,415,14.82,0,1,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', 'streaming', 'num_proc', '**kwargs', '']","[None, ' NestedDataStructureLike[PathLike]', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' bool ', ' Optional[int] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' False', ' None', None, None]",54,[],"['super', 'isinstance', 'Parquet']",3
repos/datasets/src/datasets/io/parquet.py:ParquetDatasetReader:read,ParquetDatasetReader:read,method,10,28,22,474,16.93,0,1,['self'],[None],[None],85,[],[],0
repos/datasets/src/datasets/io/parquet.py:ParquetDatasetWriter:__init__,ParquetDatasetWriter:__init__,method,12,14,13,207,14.79,0,0,"['self', 'dataset', 'path_or_buf', 'BinaryIO]', 'batch_size', 'storage_options', '**parquet_writer_kwargs', '']","[None, ' Dataset', ' Union[PathLike', None, ' Optional[int] ', ' Optional[dict] ', None, None]","[None, None, None, None, ' None', ' None', None, None]",54,[],['get_writer_batch_size'],1
repos/datasets/src/datasets/io/parquet.py:ParquetDatasetWriter:_write,ParquetDatasetWriter:_write,method,18,40,36,475,11.88,1,0,"['self', 'file_obj', 'batch_size', '**parquet_writer_kwargs']","[None, ' BinaryIO', ' int', None]","[None, None, None, None]",134,"['        """"""Writes the pyarrow table as Parquet to a binary file handle.\n', '\n', '        Caller is responsible for opening and closing the handle.\n', '        """"""\n']","['parquet_writer_kwargs.pop', 'pq.ParquetWriter', 'hf_tqdm', 'range', 'len', 'query_table', 'key=slice', 'writer.write_table', 'writer.close']",9
repos/datasets/src/datasets/io/parquet.py:ParquetDatasetWriter:write,ParquetDatasetWriter:write,method,9,30,24,415,13.83,0,2,['self'],[None],[None],124,[],"['isinstance', 'fsspec.open', 'self._write']",3
repos/datasets/src/datasets/io/spark.py:SparkDatasetReader,SparkDatasetReader,class,21,69,51,904,13.1,0,2,[],[],[],11,[],[],0
repos/datasets/src/datasets/io/spark.py:SparkDatasetReader:__init__,SparkDatasetReader:__init__,method,12,20,17,316,15.8,0,0,"['self', 'df', 'split', 'features', 'streaming', 'cache_dir', 'keep_in_memory', 'working_dir', 'load_from_cache_file', 'file_format', '**kwargs', '']","[None, ' pyspark.sql.DataFrame', ' Optional[NamedSplit] ', ' Optional[Features] ', ' bool ', ' str ', ' bool ', ' str ', ' bool ', ' str ', None, None]","[None, None, ' None', ' None', ' True', ' None', ' False', ' None', ' True', ' ""arrow""', None, None]",18,[],"['super', 'Spark']",2
repos/datasets/src/datasets/io/spark.py:SparkDatasetReader:read,SparkDatasetReader:read,method,8,16,14,300,18.75,0,2,['self'],[None],[None],49,[],[],0
repos/datasets/src/datasets/io/sql.py:SqlDatasetReader,SqlDatasetReader,class,13,58,48,813,14.02,0,0,[],[],[],17,[],[],0
repos/datasets/src/datasets/io/sql.py:SqlDatasetWriter,SqlDatasetWriter,class,45,190,128,1717,9.04,2,3,[],[],[],56,[],[],0
repos/datasets/src/datasets/io/sql.py:SqlDatasetReader:__init__,SqlDatasetReader:__init__,method,3,12,12,182,15.17,0,0,"['self', 'sql', '""sqlalchemy.sql.Selectable""]', 'con', '""sqlalchemy.engine.Connection""', '""sqlalchemy.engine.Engine""', '""sqlite3.Connection""]', 'features', 'cache_dir', 'keep_in_memory', '**kwargs', '']","[None, ' Union[str', None, ' Union[str', None, None, None, ' Optional[Features] ', ' str ', ' bool ', None, None]","[None, None, None, None, None, None, None, ' None', ' None', ' False', None, None]",18,[],"['super', 'Sql']",2
repos/datasets/src/datasets/io/sql.py:SqlDatasetReader:read,SqlDatasetReader:read,method,8,22,17,363,16.5,0,0,['self'],[None],[None],36,[],[],0
repos/datasets/src/datasets/io/sql.py:SqlDatasetWriter:__init__,SqlDatasetWriter:__init__,method,15,33,30,268,8.12,0,2,"['self', 'dataset', 'name', 'con', '""sqlalchemy.engine.Connection""', '""sqlalchemy.engine.Engine""', '""sqlite3.Connection""]', 'batch_size', 'num_proc', '**to_sql_kwargs', '']","[None, ' Dataset', ' str', ' Union[str', None, None, None, ' Optional[int] ', ' Optional[int] ', None, None]","[None, None, None, None, None, None, None, ' None', ' None', None, None]",18,[],['ValueError'],1
repos/datasets/src/datasets/io/sql.py:SqlDatasetWriter:_batch_sql,SqlDatasetWriter:_batch_sql,method,13,33,30,342,10.36,0,0,"['self', 'args']","[None, None]","[None, None]",84,[],"['query_table', 'key=slice', 'batch.to_pandas', 'df.to_sql', 'len']",5
repos/datasets/src/datasets/io/sql.py:SqlDatasetWriter:_write,SqlDatasetWriter:_write,method,15,77,50,607,7.88,2,1,"['self', 'index', '**to_sql_kwargs']","[None, None, None]","[None, None, None]",96,"['        """"""Writes the pyarrow table as SQL to a database.\n', '\n', '        Caller is responsible for opening and closing the SQL connection.\n', '        """"""\n']","['hf_tqdm', 'range', 'len', 'self._batch_sql', 'multiprocessing.Pool', 'pool.imap']",6
repos/datasets/src/datasets/io/sql.py:SqlDatasetWriter:write,SqlDatasetWriter:write,method,6,14,11,185,13.21,0,0,['self'],[None],[None],76,[],['self._write'],1
repos/datasets/src/datasets/io/text.py:TextDatasetReader,TextDatasetReader,class,24,80,60,1091,13.64,0,2,[],[],[],9,[],[],0
repos/datasets/src/datasets/io/text.py:TextDatasetReader:__init__,TextDatasetReader:__init__,method,12,25,21,355,14.2,0,1,"['self', 'path_or_paths', 'split', 'features', 'cache_dir', 'keep_in_memory', 'streaming', 'num_proc', '**kwargs', '']","[None, ' NestedDataStructureLike[PathLike]', ' Optional[NamedSplit] ', ' Optional[Features] ', ' str ', ' bool ', ' bool ', ' Optional[int] ', None, None]","[None, None, ' None', ' None', ' None', ' False', ' False', ' None', None, None]",10,[],"['super', 'isinstance', 'Text']",3
repos/datasets/src/datasets/io/text.py:TextDatasetReader:read,TextDatasetReader:read,method,10,28,22,474,16.93,0,1,['self'],[None],[None],39,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:_apply_feature_types_on_batch,_apply_feature_types_on_batch,function,10,23,20,296,12.87,1,1,"['batch', 'features', 'token_per_repo_id', 'Union[str', 'bool', 'None]]']","[' dict', ' Features', ' Dict[str', None, None, None]","[None, None, None, None, None, None]",1086,[],"['dict', 'len', 'features.encode_batch', 'features.decode_batch']",4
repos/datasets/src/datasets/iterable_dataset.py:_apply_feature_types_on_example,_apply_feature_types_on_example,function,9,20,17,264,13.2,1,1,"['example', 'features', 'token_per_repo_id', 'Union[str', 'bool', 'None]]']","[' dict', ' Features', ' Dict[str', None, None, None]","[None, None, None, None, None, None]",1071,[],"['dict', 'features.encode_example', 'features.decode_example']",3
repos/datasets/src/datasets/iterable_dataset.py:_batch_arrow_tables,_batch_arrow_tables,function,21,106,54,1238,11.68,2,3,"['iterable', 'pa.Table]]', 'batch_size', 'drop_last_batch', '']","[' Iterable[Tuple[Key', None, ' Optional[int]', ' bool ', None]","[None, None, None, ' False', None]",146,"['    """"""Iterate over sub-tables of size `batch_size`.\n', '\n', '    Args:\n', '        iterable (`Iterable[Tuple[Key, pa.Table]]`):\n', '            A tables iterable containing tuples (table_key, table) of type (int/str, pa.Table)\n', '        batch_size (`Optional[int]`):\n', '            Size of each sub-table to yield. If None or <= 0, yields the full table.\n', '        drop_last_batch (`bool`, defaults to `False`):\n', '            Drop the last batch if it is smaller than `batch_size`.\n', '    """"""\n']","['pa.concat_tables', 'pa_table.to_reader', 'len', 'keys_buffer.append', 'chunks_buffer.append']",5
repos/datasets/src/datasets/iterable_dataset.py:_batch_to_examples,_batch_to_examples,function,7,14,12,112,8.0,2,0,"['batch', 'list]']","[' Dict[str', None]","[None, None]",78,"['    """"""Convert a batch (dict of examples) to examples list""""""\n']","['len', 'range', 'batch.items']",3
repos/datasets/src/datasets/iterable_dataset.py:_check_column_names,_check_column_names,function,7,35,30,244,6.97,1,2,['column_names'],[' List[str]'],[None],515,"['    """"""Check the column names to make sure they don\'t contain duplicates.""""""\n']","['Counter', 'all', 'counter.values', 'ValueError']",4
repos/datasets/src/datasets/iterable_dataset.py:_concatenate_iterable_datasets,_concatenate_iterable_datasets,function,23,88,48,835,9.49,3,3,"['dsets', 'info', 'split', 'axis', '']","[' List[IterableDataset]', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' int ', None]","[None, ' None', ' None', ' 0', None]",2230,"['    """"""\n', '    Converts a list of `IterableDataset` with the same schema into a single `IterableDataset`.\n', '    Missing data are filled with None values.\n', '\n', '    <Added version=""2.4.0""/>\n', '\n', '    Args:\n', '        dsets (`List[datasets.IterableDataset]`): List of Datasets to concatenate.\n', '        info (`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n', '        split (`NamedSplit`, optional): Name of the dataset split.\n', '        axis (``{0, 1}``, default ``0``, meaning over rows):\n', '            Axis to concatenate over, where ``0`` means over rows (vertically) and ``1`` means over columns\n', '            (horizontally).\n', '\n', '            *New in version 1.6.0*\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> ds3 = _concatenate_iterable_datasets([ds1, ds2])\n', '    ```\n', '    """"""\n']","['_check_if_features_can_be_aligned', '_check_column_names', 'Features', '_align_features', 'features.items', 'VerticallyConcatenatedMultiSourcesExamplesIterable', 'HorizontallyConcatenatedMultiSourcesExamplesIterable', 'DatasetInfo.from_merge', 'info.copy', 'IterableDataset']",10
repos/datasets/src/datasets/iterable_dataset.py:_convert_to_arrow,_convert_to_arrow,function,16,54,43,539,9.98,1,2,"['iterable', 'dict]]', 'batch_size', 'drop_last_batch', '']","[' Iterable[Tuple[Key', None, ' int', ' bool ', None]","[None, None, None, ' False', None]",114,"['    """"""Convert and group examples in Arrow tables of size `batch_size`.\n', '\n', '    Args:\n', '        iterable (`Iterable[Tuple[Key, dict]]`):\n', '            An examples iterable containing tuples (example_key, example) of type (int/str, dict)\n', '        batch_size (`Optional[int]`):\n', '            Size of each sub-table to yield. If None or <= 0, yields the full table.\n', '        drop_last_batch (`bool`, defaults to `False`):\n', '            Drop the last batch if it is smaller than `batch_size`.\n', '    """"""\n']","['iter', 'islice', 'list', 'len', 'zip']",5
repos/datasets/src/datasets/iterable_dataset.py:_examples_to_batch,_examples_to_batch,function,7,24,16,139,5.79,2,0,"['examples', 'Any]]']","[' List[Dict[str', None]","[None, None]",69,[],['dict'],1
repos/datasets/src/datasets/iterable_dataset.py:_infer_features_from_batch,_infer_features_from_batch,function,6,18,17,243,13.5,0,1,"['batch', 'list]', 'try_features']","[' Dict[str', None, ' Optional[Features] ']","[None, None, ' None']",59,[],"['table_cast', 'pa.schema', 'Features.from_arrow_schema']",3
repos/datasets/src/datasets/iterable_dataset.py:_interleave_iterable_datasets,_interleave_iterable_datasets,function,25,86,53,917,10.66,3,2,"['datasets', 'probabilities', 'seed', 'info', 'split', 'stopping_strategy', '""all_exhausted""] ', '']","[' List[IterableDataset]', ' Optional[List[float]] ', ' Optional[int] ', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Literal[""first_exhausted""', None, None]","[None, ' None', ' None', ' None', ' None', None, ' ""first_exhausted""', None]",2290,"['    """"""\n', '    Interleave several iterable datasets (sources) into a single iterable dataset.\n', '    The new iterable dataset alternates between the sources to yield examples.\n', '    If `probabilities = None` (default) the iterable dataset will cycles through the sources in order for each next example in the iteration.\n', '    If `probabilities` is not `None, the iterable dataset will sample a random source according to the provided probabilities for each next examples in the iteration.\n', '\n', '    <Added version=""2.4.0""/>\n', '\n', '    Args:\n', '        datasets (`List[IterableDataset]`): list of datasets to interleave\n', '        probabilities (`List[float]`, optional, default None): If specified, the new iterable dataset samples\n', '            examples from one source at a time according to these probabilities.\n', '        seed (`int`, optional, default None): The random seed used to choose a source for each example.\n', '        stopping_strategy (`str`, defaults to `first_exhausted`):\n', '            Two strategies are proposed right now.\n', '            By default, `first_exhausted` is an undersampling strategy, i.e the dataset construction is stopped as soon as one dataset has ran out of samples.\n', '            If the strategy is `all_exhausted`,  we use an oversampling strategy, i.e the dataset construction is stopped as soon as every samples of every dataset has been added at least once.\n', '            Note that if the strategy is `all_exhausted`, the interleaved dataset size can get enormous:\n', '            - with no probabilities, the resulting dataset will have max_length_datasets*nb_dataset samples.\n', '            - with given probabilities, the resulting dataset will have more samples if some datasets have really low probability of visiting.\n', '\n', '    Output:\n', '        `datasets.IterableDataset`\n', '    """"""\n']","['_check_if_features_can_be_aligned', 'Features', '_align_features', 'features.items', 'CyclingMultiSourcesExamplesIterable', 'RandomlyCyclingMultiSourcesExamplesIterable', 'DatasetInfo.from_merge', 'info.copy', 'IterableDataset']",9
repos/datasets/src/datasets/iterable_dataset.py:_maybe_add_torch_iterable_dataset_parent_class,_maybe_add_torch_iterable_dataset_parent_class,function,5,12,11,153,12.75,0,2,['cls'],[None],[None],1182,"['    """"""Add torch.utils.data.IterableDataset as a parent class if \'torch\' is available""""""\n']",[],0
repos/datasets/src/datasets/iterable_dataset.py:_rename_columns_fn,_rename_columns_fn,function,7,61,35,542,8.89,1,2,"['example', 'column_mapping', 'str]']","[' Dict', ' Dict[str', None]","[None, None, None]",38,[],"['any', 'ValueError', 'set', 'column_mapping.values', 'column_mapping.items']",5
repos/datasets/src/datasets/iterable_dataset.py:_split_by_node_iterable_dataset,_split_by_node_iterable_dataset,function,14,22,20,443,20.14,0,1,"['dataset', 'rank', 'world_size']","[' IterableDataset', ' int', ' int']","[None, None, None]",2358,"['    """"""\n', '    Split an iterable dataset for the node at rank `rank` in a pool of nodes of size `world_size`.\n', '\n', '    If the dataset has a number of shards that is a factor of `world_size` (i.e. if `dataset.n_shards % world_size == 0`),\n', '    then the shards are evenly assigned across the nodes, which is the most optimized.\n', '    Otherwise, each node keeps 1 example out of `world_size`, skipping the other examples.\n', '\n', '    Args:\n', '        dataset ([`IterableDataset`]):\n', '            The iterable dataset to split by node.\n', '        rank (`int`):\n', '            Rank of the current node.\n', '        world_size (`int`):\n', '            Total number of nodes.\n', '\n', '    Returns:\n', '        [`IterableDataset`]: The iterable dataset to be used on the node at rank `rank`.\n', '    """"""\n']","['DistributedConfig', 'IterableDataset']",2
repos/datasets/src/datasets/iterable_dataset.py:add_column_fn,add_column_fn,function,5,19,18,118,6.21,0,1,"['example', 'idx', 'name', 'column']","[' Dict', ' int', ' str', ' List[Dict]']","[None, None, None, None]",53,[],['ValueError'],1
repos/datasets/src/datasets/iterable_dataset.py:identity_func,identity_func,function,2,2,2,7,3.5,0,0,['x'],[None],[None],34,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:ArrowExamplesIterable,ArrowExamplesIterable,class,34,82,64,1236,15.07,3,0,[],[],[],273,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:BufferShuffledExamplesIterable,BufferShuffledExamplesIterable,class,42,125,93,1392,11.14,2,1,[],[],[],968,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:CyclingMultiSourcesExamplesIterable,CyclingMultiSourcesExamplesIterable,class,32,118,89,1552,13.15,1,4,[],[],[],400,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:DistributedConfig,DistributedConfig,class,2,4,4,23,5.75,0,0,[],[],[],1177,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:ExamplesIterable,ExamplesIterable,class,21,58,49,868,14.97,0,0,[],[],[],227,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:FilteredExamplesIterable,FilteredExamplesIterable,class,76,348,188,3833,11.01,7,16,[],[],[],833,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:FormattingConfig,FormattingConfig,class,5,26,26,200,7.69,0,1,[],[],[],1160,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:HorizontallyConcatenatedMultiSourcesExamplesIterable,HorizontallyConcatenatedMultiSourcesExamplesIterable,class,29,104,71,1083,10.41,3,2,[],[],[],525,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset,IterableDataset,class,190,1402,597,17062,12.17,9,46,[],[],[],1191,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:MappedExamplesIterable,MappedExamplesIterable,class,102,505,275,5520,10.93,10,22,[],[],[],641,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable,RandomlyCyclingMultiSourcesExamplesIterable,class,20,109,82,1482,13.6,2,1,[],[],[],588,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:SelectColumnsIterable,SelectColumnsIterable,class,24,71,51,893,12.58,3,1,[],[],[],341,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:ShuffledDataSourcesArrowExamplesIterable,ShuffledDataSourcesArrowExamplesIterable,class,26,72,53,1111,15.43,3,0,[],[],[],306,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:ShuffledDataSourcesExamplesIterable,ShuffledDataSourcesExamplesIterable,class,15,45,38,668,14.84,0,0,[],[],[],251,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:ShufflingConfig,ShufflingConfig,class,4,5,5,63,12.6,0,0,[],[],[],1171,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:SkipExamplesIterable,SkipExamplesIterable,class,13,33,28,342,10.36,0,0,[],[],[],1017,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:StepExamplesIterable,StepExamplesIterable,class,20,66,52,793,12.02,1,1,[],[],[],368,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:TakeExamplesIterable,TakeExamplesIterable,class,24,71,56,702,9.89,1,0,[],[],[],1036,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:TypedExamplesIterable,TypedExamplesIterable,class,33,114,84,1454,12.75,3,3,[],[],[],1102,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:VerticallyConcatenatedMultiSourcesExamplesIterable,VerticallyConcatenatedMultiSourcesExamplesIterable,class,21,87,56,1095,12.59,2,1,[],[],[],465,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:_BaseExamplesIterable,_BaseExamplesIterable,class,12,69,39,782,11.33,0,0,[],[],[],198,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:_HasNextIterator,_HasNextIterator,class,12,42,27,363,8.64,0,2,[],[],[],85,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:ArrowExamplesIterable:__init__,ArrowExamplesIterable:__init__,method,7,7,7,113,16.14,0,0,"['self', 'generate_tables_fn', 'Tuple[Key', 'pa.Table]]', 'kwargs']","[None, ' Callable[...', None, None, ' dict']","[None, None, None, None, None]",274,[],['super'],1
repos/datasets/src/datasets/iterable_dataset.py:ArrowExamplesIterable:__iter__,ArrowExamplesIterable:__iter__,method,11,20,14,300,15.0,3,0,['self'],[None],[None],280,[],"['PythonFormatter', 'self.generate_tables_fn', 'pa_table.to_reader', 'formatter.format_batch', '_batch_to_examples']",5
repos/datasets/src/datasets/iterable_dataset.py:ArrowExamplesIterable:_iter_arrow,ArrowExamplesIterable:_iter_arrow,method,2,3,3,47,15.67,0,0,['self'],[None],[None],288,[],['self.generate_tables_fn'],1
repos/datasets/src/datasets/iterable_dataset.py:ArrowExamplesIterable:n_shards,ArrowExamplesIterable:n_shards,method,2,2,2,50,25.0,0,0,['self'],[None],[None],302,[],['_number_of_shards_in_gen_kwargs'],1
repos/datasets/src/datasets/iterable_dataset.py:ArrowExamplesIterable:shard_data_sources,ArrowExamplesIterable:shard_data_sources,method,8,15,15,299,19.93,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",294,"['        """"""Keep only the requested shard.""""""\n']","['_split_gen_kwargs', 'self.split_shard_indices_by_worker', '_merge_gen_kwargs', 'ArrowExamplesIterable']",4
repos/datasets/src/datasets/iterable_dataset.py:ArrowExamplesIterable:shuffle_data_sources,ArrowExamplesIterable:shuffle_data_sources,method,2,4,4,93,23.25,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",291,[],['ShuffledDataSourcesArrowExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:BufferShuffledExamplesIterable:__init__,BufferShuffledExamplesIterable:__init__,method,7,7,7,101,14.43,0,0,"['self', 'ex_iterable', 'buffer_size', 'generator']","[None, ' _BaseExamplesIterable', ' int', ' np.random.Generator']","[None, None, None, None]",969,[],['super'],1
repos/datasets/src/datasets/iterable_dataset.py:BufferShuffledExamplesIterable:__iter__,BufferShuffledExamplesIterable:__iter__,method,27,54,44,421,7.8,1,1,['self'],[None],[None],981,[],"['deepcopy', 'self._iter_random_indices', 'len', 'next', 'mem_buffer.append', 'rng.shuffle']",6
repos/datasets/src/datasets/iterable_dataset.py:BufferShuffledExamplesIterable:_iter_random_indices,BufferShuffledExamplesIterable:_iter_random_indices,method,1,11,11,84,7.64,1,0,"['rng', 'buffer_size', 'random_batch_size']","[' np.random.Generator', ' int', None]","[None, None, '1000']",977,[],['rng.integers'],1
repos/datasets/src/datasets/iterable_dataset.py:BufferShuffledExamplesIterable:n_shards,BufferShuffledExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],1013,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:BufferShuffledExamplesIterable:shard_data_sources,BufferShuffledExamplesIterable:shard_data_sources,method,4,7,7,155,22.14,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",1004,"['        """"""Keep only the requested shard.""""""\n']",['BufferShuffledExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:BufferShuffledExamplesIterable:shuffle_data_sources,BufferShuffledExamplesIterable:shuffle_data_sources,method,2,6,6,137,22.83,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",998,"['        """"""Shuffle the wrapped examples iterable as well as the shuffling buffer.""""""\n']",['BufferShuffledExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:__init__,CyclingMultiSourcesExamplesIterable:__init__,method,7,12,12,169,14.08,0,1,"['self', 'ex_iterables', 'stopping_strategy', '""all_exhausted""] ', '']","[None, ' List[_BaseExamplesIterable]', ' Literal[""first_exhausted""', None, None]","[None, None, None, ' ""first_exhausted""', None]",252,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']",['super'],1
repos/datasets/src/datasets/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:__iter__,CyclingMultiSourcesExamplesIterable:__iter__,method,15,46,39,505,10.98,1,3,['self'],[None],[None],258,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']","['self._get_indices_iterator', 'np.full', 'next', 'self.bool_strategy_func', '_HasNextIterator']",5
repos/datasets/src/datasets/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:_get_indices_iterator,CyclingMultiSourcesExamplesIterable:_get_indices_iterator,method,2,2,2,42,21.0,0,0,['self'],[None],[None],415,[],['cycle'],1
repos/datasets/src/datasets/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:n_shards,CyclingMultiSourcesExamplesIterable:n_shards,method,2,6,6,64,10.67,0,0,['self'],[None],[None],302,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']",['min'],1
repos/datasets/src/datasets/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:shard_data_sources,CyclingMultiSourcesExamplesIterable:shard_data_sources,method,3,10,10,170,17.0,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",457,"['        """"""Either keep only the requested shard, or propagate the request to the underlying iterable.""""""\n']",['CyclingMultiSourcesExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:CyclingMultiSourcesExamplesIterable:shuffle_data_sources,CyclingMultiSourcesExamplesIterable:shuffle_data_sources,method,3,9,9,170,18.89,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",448,"['        """"""Shuffle each underlying examples iterable.""""""\n']",['CyclingMultiSourcesExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:ExamplesIterable:__init__,ExamplesIterable:__init__,method,5,5,5,84,16.8,0,0,"['self', 'generate_examples_fn', 'Tuple[Key', 'dict]]', 'kwargs']","[None, ' Callable[...', None, None, ' dict']","[None, None, None, None, None]",228,[],['super'],1
repos/datasets/src/datasets/iterable_dataset.py:ExamplesIterable:__iter__,ExamplesIterable:__iter__,method,2,3,3,49,16.33,0,0,['self'],[None],[None],233,[],['self.generate_examples_fn'],1
repos/datasets/src/datasets/iterable_dataset.py:ExamplesIterable:n_shards,ExamplesIterable:n_shards,method,2,2,2,50,25.0,0,0,['self'],[None],[None],247,[],['_number_of_shards_in_gen_kwargs'],1
repos/datasets/src/datasets/iterable_dataset.py:ExamplesIterable:shard_data_sources,ExamplesIterable:shard_data_sources,method,8,15,15,296,19.73,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",239,"['        """"""Keep only the requested shard.""""""\n']","['_split_gen_kwargs', 'self.split_shard_indices_by_worker', '_merge_gen_kwargs', 'ExamplesIterable']",4
repos/datasets/src/datasets/iterable_dataset.py:ExamplesIterable:shuffle_data_sources,ExamplesIterable:shuffle_data_sources,method,2,4,4,90,22.5,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",236,[],['ShuffledDataSourcesExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:FilteredExamplesIterable:__init__,FilteredExamplesIterable:__init__,method,28,56,53,662,11.82,0,2,"['self', 'ex_iterable', 'function', 'with_indices', 'input_columns', 'batched', 'batch_size', 'fn_kwargs', 'formatting', 'format_type', '']","[None, ' _BaseExamplesIterable', ' Callable', ' bool ', ' Optional[List[str]] ', ' bool ', ' Optional[int] ', ' Optional[dict] ', ' Optional[""FormattingConfig""] ', None, None]","[None, None, None, ' False', ' None', ' False', ' 1000', ' None', ' None', '""deprecated""', None]",252,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']","['warnings.warn', 'FormattingConfig', 'super']",3
repos/datasets/src/datasets/iterable_dataset.py:FilteredExamplesIterable:__iter__,FilteredExamplesIterable:__iter__,method,6,13,11,137,10.54,0,1,['self'],[None],[None],258,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']","['ArrowExamplesIterable', 'self._iter']",2
repos/datasets/src/datasets/iterable_dataset.py:FilteredExamplesIterable:_iter,FilteredExamplesIterable:_iter,method,29,135,71,1277,9.46,5,8,['self'],[None],[None],681,[],"['get_formatter', 'isinstance', 'iter', 'islice', 'list', 'zip', '_examples_to_batch', 'format_dict', 'function_args.append', 'range', 'self.function', 'dict']",12
repos/datasets/src/datasets/iterable_dataset.py:FilteredExamplesIterable:_iter_arrow,FilteredExamplesIterable:_iter_arrow,method,18,71,47,718,10.11,2,5,['self'],[None],[None],288,"['        """"""Keep only the requested shard.""""""\n']","['_batch_arrow_tables', '_convert_to_arrow', 'function_args.append', 'range', 'self.function', 'pa_table.filter', 'mask.as_py', 'isinstance', 'len']",9
repos/datasets/src/datasets/iterable_dataset.py:FilteredExamplesIterable:n_shards,FilteredExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],302,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']",[],0
repos/datasets/src/datasets/iterable_dataset.py:FilteredExamplesIterable:shard_data_sources,FilteredExamplesIterable:shard_data_sources,method,7,10,10,233,23.3,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",952,"['        """"""Keep only the requested shard.""""""\n']",['FilteredExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:FilteredExamplesIterable:shuffle_data_sources,FilteredExamplesIterable:shuffle_data_sources,method,7,9,9,218,24.22,0,0,"['self', 'seed']","[None, ' Optional[int]']","[None, None]",941,"['        """"""Shuffle the wrapped examples iterable.""""""\n']",['FilteredExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:FormattingConfig:__post_init__,FormattingConfig:__post_init__,method,2,22,22,150,6.82,0,1,['self'],[None],[None],1163,[],['NotImplementedError'],1
repos/datasets/src/datasets/iterable_dataset.py:HorizontallyConcatenatedMultiSourcesExamplesIterable:__init__,HorizontallyConcatenatedMultiSourcesExamplesIterable:__init__,method,3,3,3,49,16.33,0,0,"['self', 'ex_iterables']","[None, ' List[_BaseExamplesIterable]']","[None, None]",478,[],['super'],1
repos/datasets/src/datasets/iterable_dataset.py:HorizontallyConcatenatedMultiSourcesExamplesIterable:__iter__,HorizontallyConcatenatedMultiSourcesExamplesIterable:__iter__,method,19,59,41,520,8.81,3,2,['self'],[None],[None],484,"['        """"""Shuffle the list of examples iterable, as well as each underlying examples iterable.""""""\n']","['itertools.count', 'list', 'next', 'keys.append', 'examples.append', 'ex_iterators.remove', '_check_column_names', 'new_example.update']",8
repos/datasets/src/datasets/iterable_dataset.py:HorizontallyConcatenatedMultiSourcesExamplesIterable:n_shards,HorizontallyConcatenatedMultiSourcesExamplesIterable:n_shards,method,1,2,2,7,3.5,0,0,['self'],[None],[None],503,"['        """"""Either keep only the requested shard, or propagate the request to the underlying iterable.""""""\n']",[],0
repos/datasets/src/datasets/iterable_dataset.py:HorizontallyConcatenatedMultiSourcesExamplesIterable:shard_data_sources,HorizontallyConcatenatedMultiSourcesExamplesIterable:shard_data_sources,method,2,9,9,144,16.0,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",506,"['        """"""Either keep only the requested shard, or propagate the request to the underlying iterable.""""""\n']",['HorizontallyConcatenatedMultiSourcesExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:HorizontallyConcatenatedMultiSourcesExamplesIterable:shuffle_data_sources,HorizontallyConcatenatedMultiSourcesExamplesIterable:shuffle_data_sources,method,1,2,2,10,5.0,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",492,"['        """"""Shuffle the list of examples iterable, as well as each underlying examples iterable.""""""\n']",[],0
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:__getstate__,IterableDataset:__getstate__,method,2,2,2,19,9.5,0,0,['self'],[None],[None],1230,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:__init__,IterableDataset:__init__,method,30,114,93,980,8.6,0,2,"['self', 'ex_iterable', 'info', 'split', 'formatting', 'shuffling', 'distributed', 'token_per_repo_id', 'Union[str', 'bool', 'None]]] ', 'format_type', '']","[None, ' _BaseExamplesIterable', ' Optional[DatasetInfo] ', ' Optional[NamedSplit] ', ' Optional[FormattingConfig] ', ' Optional[ShufflingConfig] ', ' Optional[DistributedConfig] ', ' Optional[Dict[str', None, None, None, None, None]","[None, None, ' None', ' None', ' None', ' None', ' None', None, None, None, ' None', '""deprecated""', None]",252,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']","['RuntimeError', 'warnings.warn', 'FormattingConfig', 'info.copy', 'DatasetInfo', 'DatasetInfoMixin.__init__', '_maybe_add_torch_iterable_dataset_parent_class']",7
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:__iter__,IterableDataset:__iter__,method,25,82,57,1003,12.23,2,6,['self'],[None],[None],258,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']","['isinstance', 'self._iter_pytorch', 'self._prepare_ex_iterable_for_iteration', 'get_formatter', '_batch_arrow_tables', '_convert_to_arrow', 'formatter.format_row', '_apply_feature_types_on_example', 'format_dict']",9
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:__repr__,IterableDataset:__repr__,method,2,13,13,147,11.31,0,0,['self'],[None],[None],1227,[],"['f""IterableDataset']",1
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:__setstate__,IterableDataset:__setstate__,method,3,3,3,78,26.0,0,0,"['self', 'd']","[None, None]","[None, None]",1233,[],['_maybe_add_torch_iterable_dataset_parent_class'],1
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:_effective_generator,IterableDataset:_effective_generator,method,9,35,28,337,9.63,0,1,['self'],[None],[None],1241,[],"['deepcopy', 'ValueError']",2
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:_head,IterableDataset:_head,method,2,2,2,44,22.0,0,0,"['self', 'n']","[None, None]","[None, '5']",1238,[],['_examples_to_batch'],1
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:_is_main_process,IterableDataset:_is_main_process,method,9,29,20,219,7.55,0,3,['self'],[None],[None],1319,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:_iter_pytorch,IterableDataset:_iter_pytorch,method,31,184,129,2180,11.85,2,6,['self'],[None],[None],1258,[],"['self._prepare_ex_iterable_for_iteration', 'self._is_main_process', 'logger.warning', 'logger.info', 'ex_iterable.split_shard_indices_by_worker', 'logger.debug', 'ex_iterable.shard_data_sources', 'get_formatter', 'isinstance', '_batch_arrow_tables', '_convert_to_arrow', 'formatter.format_row', '_apply_feature_types_on_example', 'format_dict']",14
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:_prepare_ex_iterable_for_iteration,IterableDataset:_prepare_ex_iterable_for_iteration,method,17,127,82,1038,8.17,0,5,['self'],[None],[None],1330,[],"['self._is_main_process', 'logger.info', 'ex_iterable.shard_data_sources', 'StepExamplesIterable']",4
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:_resolve_features,IterableDataset:_resolve_features,method,12,29,27,494,17.03,0,1,['self'],[None],[None],2210,[],"['isinstance', '_infer_features_from_batch', 'IterableDataset']",3
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:_step,IterableDataset:_step,method,9,14,14,328,23.43,0,0,"['self', 'step', 'offset']","[None, ' int', ' int']","[None, None, None]",2198,[],"['StepExamplesIterable', 'IterableDataset']",2
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:add_column,IterableDataset:add_column,method,2,5,5,80,16.0,0,0,"['self', 'name', 'column', 'np.array]']","[None, ' str', ' Union[list', None]","[None, None, None, None]",1928,"['        """"""Add column to Dataset.\n', '\n', '        Args:\n', '            name (str): Column name.\n', '            column (list or np.array): Column data to be added.\n', '\n', '        Returns:\n', '            `IterableDataset`\n', '        """"""\n']",['self.map'],1
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:cast,IterableDataset:cast,method,11,20,20,351,17.55,0,0,"['self', 'features', '']","[None, ' Features', None]","[None, None, None]",2147,"['        """"""\n', '        Cast the dataset to a new set of features.\n', '\n', '        Args:\n', '            features ([`Features`]):\n', '                New features to cast the dataset to.\n', '                The name of the fields in the features must match the current column names.\n', '                The type of the data must also be convertible from one type to the other.\n', '                For non-trivial conversion, e.g. `string` <-> `ClassLabel` you should use [`~Dataset.map`] to update the Dataset.\n', '\n', '        Returns:\n', '            `IterableDataset`: A copy of the dataset with casted features.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"", streaming=True)\n', '        >>> ds.features\n', ""        {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n"", ""         'text': Value(dtype='string', id=None)}\n"", '        >>> new_features = ds.features.copy()\n', '        >>> new_features[""label""] = ClassLabel(names=[""bad"", ""good""])\n', '        >>> new_features[""text""] = Value(""large_string"")\n', '        >>> ds = ds.cast(new_features)\n', '        >>> ds.features\n', ""        {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n"", ""         'text': Value(dtype='large_string', id=None)}\n"", '        ```\n', '        """"""\n']","['info.copy', 'IterableDataset']",2
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:cast_column,IterableDataset:cast_column,method,11,20,20,358,17.9,0,0,"['self', 'column', 'feature']","[None, ' str', ' FeatureType']","[None, None, None]",2096,"['        """"""Cast column to feature for decoding.\n', '\n', '        Args:\n', '            column (`str`):\n', '                Column name.\n', '            feature (`Feature`):\n', '                Target feature.\n', '\n', '        Returns:\n', '            `IterableDataset`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset, Audio\n', '        >>> ds = load_dataset(""PolyAI/minds14"", name=""en-US"", split=""train"", streaming=True)\n', '        >>> ds.features\n', ""        {'audio': Audio(sampling_rate=8000, mono=True, decode=True, id=None),\n"", ""         'english_transcription': Value(dtype='string', id=None),\n"", ""         'intent_class': ClassLabel(num_classes=14, names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan',  'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill'], id=None),\n"", ""         'lang_id': ClassLabel(num_classes=14, names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR',  'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'], id=None),\n"", ""         'path': Value(dtype='string', id=None),\n"", ""         'transcription': Value(dtype='string', id=None)}\n"", '        >>> ds = ds.cast_column(""audio"", Audio(sampling_rate=16000))\n', '        >>> ds.features\n', ""        {'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n"", ""         'english_transcription': Value(dtype='string', id=None),\n"", ""         'intent_class': ClassLabel(num_classes=14, names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan',  'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill'], id=None),\n"", ""         'lang_id': ClassLabel(num_classes=14, names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR',  'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'], id=None),\n"", ""         'path': Value(dtype='string', id=None),\n"", ""         'transcription': Value(dtype='string', id=None)}\n"", '        ```\n', '        """"""\n']","['info.copy', 'IterableDataset']",2
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:column_names,IterableDataset:column_names,method,1,9,8,76,8.44,0,0,['self'],[None],[None],1914,"['        """"""Names of the columns in the dataset.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""validation"", streaming=True)\n', '        >>> ds.column_names\n', ""        ['text', 'label']\n"", '        ```\n', '        """"""\n']",['list'],1
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:filter,IterableDataset:filter,method,20,39,36,714,18.31,0,2,"['self', 'function', 'with_indices', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'fn_kwargs', '']","[None, ' Optional[Callable] ', None, ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' Optional[dict] ', None]","[None, ' None', 'False', None, ' None', ' False', ' 1000', ' None', None]",1694,"['        """"""Apply a filter function to all the elements so that the dataset only includes examples according to the filter function.\n', '        The filtering is done on-the-fly when iterating over the dataset.\n', '\n', '        Args:\n', '            function (`Callable`):\n', '                Callable with one of the following signatures:\n', '\n', '                - `function(example: Dict[str, Any]) -> bool` if `with_indices=False, batched=False`\n', '                - `function(example: Dict[str, Any], indices: int) -> bool` if `with_indices=True, batched=False`\n', '                - `function(example: Dict[str, List]) -> List[bool]` if `with_indices=False, batched=True`\n', '                - `function(example: Dict[str, List], indices: List[int]) -> List[bool]` if `with_indices=True, batched=True`\n', '\n', '                If no function is provided, defaults to an always True function: `lambda x: True`.\n', '            with_indices (`bool`, defaults to `False`):\n', '                Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n', '            input_columns (`str` or `List[str]`, *optional*):\n', '                The columns to be passed into `function` as\n', '                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`):\n', '                Provide batch of examples to `function`.\n', '            batch_size (`int`, *optional*, default `1000`):\n', '                Number of examples per batch provided to `function` if `batched=True`.\n', '            fn_kwargs (`Dict`, *optional*, default `None`):\n', '                Keyword arguments to be passed to `function`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"", streaming=True)\n', '        >>> ds = ds.filter(lambda x: x[""label""] == 0)\n', '        >>> list(ds.take(3))\n', ""        [{'label': 0, 'movie_review': 'simplistic , silly and tedious .'},\n"", ""         {'label': 0,\n"", '         \'movie_review\': ""it\'s so laddish and juvenile , only teenage boys could possibly find it funny .""},\n', ""         {'label': 0,\n"", ""         'movie_review': 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .'}]\n"", '        ```\n', '        """"""\n']","['isinstance', 'copy.deepcopy', 'FilteredExamplesIterable', 'TypedExamplesIterable', 'IterableDataset']",5
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:from_file,IterableDataset:from_file,method,8,11,11,306,27.82,0,0,['filename'],[' str'],[None],1538,"['        """"""Instantiate a IterableDataset from Arrow table at filename.\n', '\n', '        Args:\n', '            filename (`str`):\n', '                File name of the dataset.\n', '\n', '        Returns:\n', '            [`IterableDataset`]\n', '        """"""\n']","['read_schema_from_file', 'Features.from_arrow_schema', 'ArrowExamplesIterable', 'IterableDataset', 'info=DatasetInfo']",5
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:from_generator,IterableDataset:from_generator,method,4,11,11,173,15.73,0,0,"['generator', 'features', 'gen_kwargs', '']","[' Callable', ' Optional[Features] ', ' Optional[dict] ', None]","[None, ' None', ' None', None]",1441,"['        """"""Create an Iterable Dataset from a generator.\n', '\n', '        Args:\n', '            generator (`Callable`):\n', '                A generator function that `yields` examples.\n', '            features (`Features`, *optional*):\n', '                Dataset features.\n', '            gen_kwargs(`dict`, *optional*):\n', '                Keyword arguments to be passed to the `generator` callable.\n', '                You can define a sharded iterable dataset by passing the list of shards in `gen_kwargs`.\n', '                This can be used to improve shuffling and when iterating over the dataset with multiple workers.\n', '\n', '        Returns:\n', '            `IterableDataset`\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> def gen():\n', '        ...     yield {""text"": ""Good"", ""label"": 0}\n', '        ...     yield {""text"": ""Bad"", ""label"": 1}\n', '        ...\n', '        >>> ds = IterableDataset.from_generator(gen)\n', '        ```\n', '\n', '        ```py\n', '        >>> def gen(shards):\n', '        ...     for shard in shards:\n', '        ...         with open(shard) as f:\n', '        ...             for line in f:\n', '        ...                 yield {""line"": line}\n', '        ...\n', '        >>> shards = [f""data{i}.txt"" for i in range(32)]\n', '        >>> ds = IterableDataset.from_generator(gen, gen_kwargs={""shards"": shards})\n', '        >>> ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer\n', '        >>> from torch.utils.data import DataLoader\n', '        >>> dataloader = DataLoader(ds.with_format(""torch""), num_workers=4)  # give each worker a subset of 32/4=8 shards\n', '        ```\n', '        """"""\n']",['GeneratorDatasetInputStream'],1
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:from_spark,IterableDataset:from_spark,method,6,23,23,243,10.57,0,1,"['df', 'split', 'features', '**kwargs', '']","[' ""pyspark.sql.DataFrame""', ' Optional[NamedSplit] ', ' Optional[Features] ', None, None]","[None, ' None', ' None', None, None]",1495,"['        """"""Create an IterableDataset from Spark DataFrame. The dataset is streamed to the driver in batches.\n', '\n', '        Args:\n', '            df (`pyspark.sql.DataFrame`):\n', '                The DataFrame containing the desired data.\n', '            split (`NamedSplit`, *optional*):\n', '                Split name to be assigned to the dataset.\n', '            features (`Features`, *optional*):\n', '                Dataset features.\n', '\n', '        Returns:\n', '            [`IterableDataset`]\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> df = spark.createDataFrame(\n', '        >>>     data=[[1, ""Elia""], [2, ""Teo""], [3, ""Fang""]],\n', '        >>>     columns=[""id"", ""name""],\n', '        >>> )\n', '        >>> ds = IterableDataset.from_spark(df)\n', '        ```\n', '        """"""\n']","['EnvironmentError', 'SparkDatasetReader']",2
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:iter,IterableDataset:iter,method,24,89,61,1060,11.91,3,5,"['self', 'batch_size', 'drop_last_batch']","[None, ' int', ' bool ']","[None, None, ' False']",1398,"['        """"""Iterate through the batches of size `batch_size`.\n', '\n', '        Args:\n', '            batch_size (:obj:`int`): size of each batch to yield.\n', '            drop_last_batch (:obj:`bool`, default `False`): Whether a last batch smaller than the batch_size should be\n', '                dropped\n', '        """"""\n']","['get_formatter', 'isinstance', 'self._prepare_ex_iterable_for_iteration', '_batch_arrow_tables', 'ex_iterable.iter_arrow', '_convert_to_arrow', 'formatter.format_batch', 'iter', 'islice', 'len', '_examples_to_batch', '_apply_feature_types_on_batch', 'format_dict']",13
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:map,IterableDataset:map,method,27,58,47,910,15.69,0,5,"['self', 'function', 'with_indices', 'input_columns', 'List[str]]] ', 'batched', 'batch_size', 'drop_last_batch', 'remove_columns', 'List[str]]] ', 'features', 'fn_kwargs', '']","[None, ' Optional[Callable] ', ' bool ', ' Optional[Union[str', None, ' bool ', ' Optional[int] ', ' bool ', ' Optional[Union[str', None, ' Optional[Features] ', ' Optional[dict] ', None]","[None, ' None', ' False', None, ' None', ' False', ' 1000', ' False', None, ' None', ' None', ' None', None]",1581,"['        """"""\n', '        Apply a function to all the examples in the iterable dataset (individually or in batches) and update them.\n', '        If your function returns a column that already exists, then it overwrites it.\n', '        The function is applied on-the-fly on the examples when iterating over the dataset.\n', '\n', '        You can specify whether the function should be batched or not with the `batched` parameter:\n', '\n', '        - If batched is `False`, then the function takes 1 example in and should return 1 example.\n', '          An example is a dictionary, e.g. `{""text"": ""Hello there !""}`.\n', '        - If batched is `True` and `batch_size` is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.\n', '          A batch is a dictionary, e.g. a batch of 1 example is {""text"": [""Hello there !""]}.\n', '        - If batched is `True` and `batch_size` is `n` > 1, then the function takes a batch of `n` examples as input and can return a batch with `n` examples, or with an arbitrary number of examples.\n', '          Note that the last batch may have less than `n` examples.\n', '          A batch is a dictionary, e.g. a batch of `n` examples is `{""text"": [""Hello there !""] * n}`.\n', '\n', '        Args:\n', '            function (`Callable`, *optional*, defaults to `None`):\n', '                Function applied on-the-fly on the examples when you iterate on the dataset.\n', '                It must have one of the following signatures:\n', '\n', '                - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False`\n', '                - `function(example: Dict[str, Any], idx: int) -> Dict[str, Any]` if `batched=False` and `with_indices=True`\n', '                - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False`\n', '                - `function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if `batched=True` and `with_indices=True`\n', '\n', '                For advanced usage, the function can also return a `pyarrow.Table`.\n', '                Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n', '                If no function is provided, default to identity function: `lambda x: x`.\n', '            with_indices (`bool`, defaults to `False`):\n', '                Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx[, rank]): ...`.\n', '            input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\n', '                The columns to be passed into `function`\n', '                as positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n', '            batched (`bool`, defaults to `False`):\n', '                Provide batch of examples to `function`.\n', '            batch_size (`int`, *optional*, defaults to `1000`):\n', '                Number of examples per batch provided to `function` if `batched=True`.\n', '                `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to `function`.\n', '            drop_last_batch (`bool`, defaults to `False`):\n', '                Whether a last batch smaller than the batch_size should be\n', '                dropped instead of being processed by the function.\n', '            remove_columns (`[List[str]]`, *optional*, defaults to `None`):\n', '                Remove a selection of columns while doing the mapping.\n', '                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n', '                columns with names in `remove_columns`, these columns will be kept.\n', '            features (`[Features]`, *optional*, defaults to `None`):\n', '                Feature types of the resulting dataset.\n', '            fn_kwargs (`Dict`, *optional*, default `None`):\n', '                Keyword arguments to be passed to `function`.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"", streaming=True)\n', '        >>> def add_prefix(example):\n', '        ...     example[""text""] = ""Review: "" + example[""text""]\n', '        ...     return example\n', '        >>> ds = ds.map(add_prefix)\n', '        >>> list(ds.take(3))\n', ""        [{'label': 1,\n"", '         \'text\': \'Review: the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'},\n', ""         {'label': 1,\n"", '         \'text\': \'Review: the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\'},\n', ""         {'label': 1, 'text': 'Review: effective but too-tepid biopic'}]\n"", '        ```\n', '        """"""\n']","['isinstance', 'MappedExamplesIterable', 'TypedExamplesIterable', 'IterableDataset']",4
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:n_shards,IterableDataset:n_shards,method,5,13,10,177,13.62,0,1,['self'],[None],[None],302,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']",[],0
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:remove_columns,IterableDataset:remove_columns,method,17,34,28,402,11.82,1,2,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",2000,"['        """"""\n', '        Remove one or several column(s) in the dataset and the features associated to them.\n', '        The removal is done on-the-fly on the examples when iterating over the dataset.\n', '\n', '\n', '        Args:\n', '            column_names (`Union[str, List[str]]`):\n', '                Name of the column(s) to remove.\n', '\n', '        Returns:\n', '            `IterableDataset`: A copy of the dataset object without the columns to remove.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"", streaming=True)\n', '        >>> next(iter(ds))\n', '        {\'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\', \'label\': 1}\n', '        >>> ds = ds.remove_columns(""label"")\n', '        >>> next(iter(ds))\n', '        {\'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'}\n', '        ```\n', '        """"""\n']","['self.map', 'original_features.copy', 'original_features.items']",3
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:rename_column,IterableDataset:rename_column,method,2,3,3,65,21.67,0,0,"['self', 'original_column_name', 'new_column_name']","[None, ' str', ' str']","[None, None, None]",1940,"['        """"""\n', '        Rename a column in the dataset, and move the features associated to the original column under the new column\n', '        name.\n', '\n', '        Args:\n', '            original_column_name (`str`):\n', '                Name of the column to rename.\n', '            new_column_name (`str`):\n', '                New name for the column.\n', '\n', '        Returns:\n', '            `IterableDataset`: A copy of the dataset with a renamed column.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"", streaming=True)\n', '        >>> next(iter(ds))\n', ""        {'label': 1,\n"", '         \'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'}\n', '        >>> ds = ds.rename_column(""text"", ""movie_review"")\n', '        >>> next(iter(ds))\n', ""        {'label': 1,\n"", '         \'movie_review\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'}\n', '        ```\n', '        """"""\n']",['self.rename_columns'],1
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:rename_columns,IterableDataset:rename_columns,method,13,43,35,473,11.0,1,1,"['self', 'column_mapping', 'str]']","[None, ' Dict[str', None]","[None, None, None]",1970,"['        """"""\n', '        Rename several columns in the dataset, and move the features associated to the original columns under\n', '        the new column names.\n', '\n', '        Args:\n', '            column_mapping (`Dict[str, str]`): A mapping of columns to rename to their new names\n', '\n', '        Returns:\n', '            `IterableDataset`: A copy of the dataset with renamed columns\n', '        """"""\n']","['self.map', 'partial', 'remove_columns=list', 'Features', 'column_mapping.keys', 'original_features.items']",6
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:select_columns,IterableDataset:select_columns,method,17,62,54,754,12.16,0,4,"['self', 'column_names', 'List[str]]']","[None, ' Union[str', None]","[None, None, None]",2040,"['        """"""Select one or several column(s) in the dataset and the features\n', '        associated to them. The selection is done on-the-fly on the examples\n', '        when iterating over the dataset.\n', '\n', '\n', '        Args:\n', '            column_names (`Union[str, List[str]]`):\n', '                Name of the column(s) to select.\n', '\n', '        Returns:\n', '            `IterableDataset`: A copy of the dataset object with selected columns.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"", streaming=True)\n', '        >>> next(iter(ds))\n', '        {\'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\', \'label\': 1}\n', '        >>> ds = ds.select_columns(""text"")\n', '        >>> next(iter(ds))\n', '        {\'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'}\n', '        ```\n', '        """"""\n']","['isinstance', 'copy.deepcopy', 'set', 'ValueError', 'Features', 'info.copy', 'SelectColumnsIterable', 'IterableDataset']",8
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:set_epoch,IterableDataset:set_epoch,method,2,2,2,17,8.5,0,0,"['self', 'epoch']","[None, ' int']","[None, None]",1839,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:shuffle,IterableDataset:shuffle,method,13,26,24,507,19.5,0,1,"['self', 'seed', 'generator', 'buffer_size']","[None, None, ' Optional[np.random.Generator] ', ' int ']","[None, 'None', ' None', ' 1000']",1772,"['        """"""\n', '        Randomly shuffles the elements of this dataset.\n', '\n', '        This dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer,\n', '        replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or\n', '        equal to the full size of the dataset is required.\n', '\n', '        For instance, if your dataset contains 10,000 elements but `buffer_size` is set to 1000, then `shuffle` will\n', '        initially select a random element from only the first 1000 elements in the buffer. Once an element is\n', '        selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n', '        maintaining the 1000 element buffer.\n', '\n', '        If the dataset is made of several shards, it also does shuffle the order of the shards.\n', '        However if the order has been fixed by using [`~datasets.IterableDataset.skip`] or [`~datasets.IterableDataset.take`]\n', '        then the order of the shards is kept unchanged.\n', '\n', '        Args:\n', '            seed (`int`, *optional*, defaults to `None`):\n', '                Random seed that will be used to shuffle the dataset.\n', '                It is used to sample from the shuffle buffer and also to shuffle the data shards.\n', '            generator (`numpy.random.Generator`, *optional*):\n', '                Numpy random Generator to use to compute the permutation of the dataset rows.\n', '                If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n', '            buffer_size (`int`, defaults to `1000`):\n', '                Size of the buffer.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"", streaming=True)\n', '        >>> list(ds.take(3))\n', ""        [{'label': 1,\n"", '         \'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'},\n', ""         {'label': 1,\n"", '         \'text\': \'the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\'},\n', ""         {'label': 1, 'text': 'effective but too-tepid biopic'}]\n"", '        >>> shuffled_ds = ds.shuffle(seed=42)\n', '        >>> list(shuffled_ds.take(3))\n', ""        [{'label': 1,\n"", '         \'text\': ""a sports movie with action that\'s exciting on the field and a story you care about off it .""},\n', ""         {'label': 1,\n"", ""         'text': 'at its best , the good girl is a refreshingly adult take on adultery . . .'},\n"", ""         {'label': 1,\n"", '         \'text\': ""sam jones became a very lucky filmmaker the day wilco got dropped from their record label , proving that one man\'s ruin may be another\'s fortune .""}]\n', '        ```\n', '        """"""\n']","['deepcopy', 'ShufflingConfig', 'IterableDataset', 'ex_iterable=BufferShuffledExamplesIterable']",4
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:skip,IterableDataset:skip,method,9,13,13,306,23.54,0,0,"['self', 'n']","[None, ' int']","[None, None]",1842,"['        """"""\n', '        Create a new [`IterableDataset`] that skips the first `n` elements.\n', '\n', '        Args:\n', '            n (`int`):\n', '                Number of elements to skip.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"", streaming=True)\n', '        >>> list(ds.take(3))\n', ""        [{'label': 1,\n"", '         \'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'},\n', ""         {'label': 1,\n"", '         \'text\': \'the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\'},\n', ""         {'label': 1, 'text': 'effective but too-tepid biopic'}]\n"", '        >>> ds = ds.skip(1)\n', '        >>> list(ds.take(3))\n', ""        [{'label': 1,\n"", '         \'text\': \'the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\'},\n', ""         {'label': 1, 'text': 'effective but too-tepid biopic'},\n"", ""         {'label': 1,\n"", ""         'text': 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .'}]\n"", '        ```\n', '        """"""\n']","['SkipExamplesIterable', 'IterableDataset']",2
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:take,IterableDataset:take,method,9,13,13,306,23.54,0,0,"['self', 'n']","[None, ' int']","[None, None]",1881,"['        """"""\n', '        Create a new [`IterableDataset`] with only the first `n` elements.\n', '\n', '        Args:\n', '            n (`int`):\n', '                Number of elements to take.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_dataset\n', '        >>> ds = load_dataset(""rotten_tomatoes"", split=""train"", streaming=True)\n', '        >>> small_ds = ds.take(2)\n', '        >>> list(small_ds)\n', ""        [{'label': 1,\n"", '         \'text\': \'the rock is destined to be the 21st century\\\'s new "" conan "" and that he\\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\'},\n', ""         {'label': 1,\n"", '         \'text\': \'the gorgeously elaborate continuation of "" the lord of the rings "" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\\'s expanded vision of j . r . r . tolkien\\\'s middle-earth .\'}]\n', '        ```\n', '        """"""\n']","['TakeExamplesIterable', 'IterableDataset']",2
repos/datasets/src/datasets/iterable_dataset.py:IterableDataset:with_format,IterableDataset:with_format,method,9,12,12,314,26.17,0,0,"['self', 'type', '']","[None, ' Optional[str] ', None]","[None, ' None', None]",1553,"['        """"""\n', '        Return a dataset with the specified format.\n', '        Supported formats: ""arrow"", or None for regular python objects.\n', '        The other formats are currently not implemented.\n', '\n', '        Args:\n', '\n', '            type (`str`, optional, default None): if set to ""torch"", the returned dataset\n', '                will be a subclass of torch.utils.data.IterableDataset to be used in a DataLoader\n', '        """"""\n']","['get_format_type_from_alias', 'IterableDataset', 'formatting=FormattingConfig']",3
repos/datasets/src/datasets/iterable_dataset.py:MappedExamplesIterable:__init__,MappedExamplesIterable:__init__,method,32,60,57,734,12.23,0,2,"['self', 'ex_iterable', 'function', 'with_indices', 'input_columns', 'batched', 'batch_size', 'drop_last_batch', 'remove_columns', 'fn_kwargs', 'formatting', 'format_type', '']","[None, ' _BaseExamplesIterable', ' Callable', ' bool ', ' Optional[List[str]] ', ' bool ', ' Optional[int] ', ' bool ', ' Optional[List[str]] ', ' Optional[dict] ', ' Optional[""FormattingConfig""] ', None, None]","[None, None, None, ' False', ' None', ' False', ' 1000', ' False', ' None', ' None', ' None', '""deprecated""', None]",252,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']","['warnings.warn', 'FormattingConfig', 'super']",3
repos/datasets/src/datasets/iterable_dataset.py:MappedExamplesIterable:__iter__,MappedExamplesIterable:__iter__,method,6,13,11,137,10.54,0,1,['self'],[None],[None],258,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']","['ArrowExamplesIterable', 'self._iter']",2
repos/datasets/src/datasets/iterable_dataset.py:MappedExamplesIterable:_iter,MappedExamplesIterable:_iter,method,43,233,127,2151,9.23,7,11,['self'],[None],[None],681,[],"['iter', 'get_formatter', 'isinstance', 'islice', 'list', 'zip', 'len', '_examples_to_batch', 'format_dict', 'function_args.append', 'range', 'dict', 'transformed_batch.update', 'next', 'ValueError', '_batch_to_examples', 'transformed_example.update']",17
repos/datasets/src/datasets/iterable_dataset.py:MappedExamplesIterable:_iter_arrow,MappedExamplesIterable:_iter_arrow,method,24,109,71,1095,10.05,3,8,['self'],[None],[None],353,"['        """"""Shuffle each underlying examples iterable.""""""\n']","['_batch_arrow_tables', '_convert_to_arrow', 'function_args.append', 'range', 'self.function', 'isinstance', 'TypeError', 'output_table.remove_column', 'len']",9
repos/datasets/src/datasets/iterable_dataset.py:MappedExamplesIterable:n_shards,MappedExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],302,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']",[],0
repos/datasets/src/datasets/iterable_dataset.py:MappedExamplesIterable:shard_data_sources,MappedExamplesIterable:shard_data_sources,method,11,14,14,359,25.64,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",813,"['        """"""Keep only the requested shard.""""""\n']",['MappedExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:MappedExamplesIterable:shuffle_data_sources,MappedExamplesIterable:shuffle_data_sources,method,11,13,13,349,26.85,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",798,"['        """"""Shuffle the wrapped examples iterable.""""""\n']",['MappedExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable:__init__,RandomlyCyclingMultiSourcesExamplesIterable:__init__,method,5,6,6,116,19.33,0,0,"['self', 'ex_iterables', 'generator', 'probabilities', 'stopping_strategy', '""all_exhausted""] ', '']","[None, ' List[_BaseExamplesIterable]', ' np.random.Generator', ' Optional[List[float]] ', ' Literal[""first_exhausted""', None, None]","[None, None, None, ' None', None, ' ""first_exhausted""', None]",252,[],"['super', 'deepcopy']",2
repos/datasets/src/datasets/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable:_get_indices_iterator,RandomlyCyclingMultiSourcesExamplesIterable:_get_indices_iterator,method,4,6,6,109,18.17,0,0,['self'],[None],[None],415,"['        """"""Shuffle each underlying examples iterable.""""""\n']","['deepcopy', 'self._iter_random_indices', 'len']",3
repos/datasets/src/datasets/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable:_iter_random_indices,RandomlyCyclingMultiSourcesExamplesIterable:_iter_random_indices,method,2,27,19,186,6.89,2,1,"['rng', 'num_sources', 'random_batch_size', 'p', '']","[' np.random.Generator', ' int', None, ' Optional[List[float]] ', None]","[None, None, '1000', ' None', None]",602,"['        """"""Get an infinite iterator that randomly samples the index of the source to pick examples from.""""""\n']","['rng.integers', 'rng.choice']",2
repos/datasets/src/datasets/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable:shard_data_sources,RandomlyCyclingMultiSourcesExamplesIterable:shard_data_sources,method,5,12,12,196,16.33,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",631,"['        """"""Either keep only the requested shard, or propagate the request to the underlying iterable.""""""\n']",['RandomlyCyclingMultiSourcesExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:RandomlyCyclingMultiSourcesExamplesIterable:shuffle_data_sources,RandomlyCyclingMultiSourcesExamplesIterable:shuffle_data_sources,method,3,13,13,255,19.62,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",621,"['        """"""Shuffle the data sources of each wrapped examples iterable.""""""\n']",['RandomlyCyclingMultiSourcesExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:SelectColumnsIterable:__init__,SelectColumnsIterable:__init__,method,8,9,9,142,15.78,0,1,"['self', 'ex_iterable', 'column_names']","[None, ' _BaseExamplesIterable', ' List[str]']","[None, None, None]",342,[],['super'],1
repos/datasets/src/datasets/iterable_dataset.py:SelectColumnsIterable:__iter__,SelectColumnsIterable:__iter__,method,6,13,10,72,5.54,2,0,['self'],[None],[None],349,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:SelectColumnsIterable:_iter_arrow,SelectColumnsIterable:_iter_arrow,method,4,8,7,91,11.38,1,0,['self'],[None],[None],353,[],['pa_table.select'],1
repos/datasets/src/datasets/iterable_dataset.py:SelectColumnsIterable:n_shards,SelectColumnsIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],364,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:SelectColumnsIterable:shard_data_sources,SelectColumnsIterable:shard_data_sources,method,2,4,4,105,26.25,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",360,[],['SelectColumnsIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:SelectColumnsIterable:shuffle_data_sources,SelectColumnsIterable:shuffle_data_sources,method,2,3,3,95,31.67,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",357,[],['SelectColumnsIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:ShuffledDataSourcesArrowExamplesIterable:__init__,ShuffledDataSourcesArrowExamplesIterable:__init__,method,3,4,4,78,19.5,0,0,"['self', 'generate_tables_fn', 'Tuple[Key', 'pa.Table]]', 'kwargs', 'generator', '']","[None, ' Callable[...', None, None, ' dict', ' np.random.Generator', None]","[None, None, None, None, None, None, None]",252,[],"['super', 'deepcopy']",2
repos/datasets/src/datasets/iterable_dataset.py:ShuffledDataSourcesArrowExamplesIterable:__iter__,ShuffledDataSourcesArrowExamplesIterable:__iter__,method,15,25,19,410,16.4,3,0,['self'],[None],[None],258,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']","['deepcopy', '_shuffle_gen_kwargs', 'PythonFormatter', 'self.generate_tables_fn', 'pa_table.to_reader', 'formatter.format_batch', '_batch_to_examples']",7
repos/datasets/src/datasets/iterable_dataset.py:ShuffledDataSourcesArrowExamplesIterable:_iter_arrow,ShuffledDataSourcesArrowExamplesIterable:_iter_arrow,method,6,8,8,157,19.62,0,0,['self'],[None],[None],288,"['        """"""Keep only the requested shard.""""""\n']","['deepcopy', '_shuffle_gen_kwargs', 'self.generate_tables_fn']",3
repos/datasets/src/datasets/iterable_dataset.py:ShuffledDataSourcesArrowExamplesIterable:shard_data_sources,ShuffledDataSourcesArrowExamplesIterable:shard_data_sources,method,8,11,11,218,19.82,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",294,"['        """"""Keep only the requested shard.""""""\n']","['deepcopy', '_shuffle_gen_kwargs', 'ArrowExamplesIterable']",3
repos/datasets/src/datasets/iterable_dataset.py:ShuffledDataSourcesExamplesIterable:__init__,ShuffledDataSourcesExamplesIterable:__init__,method,3,4,4,80,20.0,0,0,"['self', 'generate_examples_fn', 'Tuple[Key', 'dict]]', 'kwargs', 'generator']","[None, ' Callable[...', None, None, ' dict', ' np.random.Generator']","[None, None, None, None, None, None]",252,[],"['super', 'deepcopy']",2
repos/datasets/src/datasets/iterable_dataset.py:ShuffledDataSourcesExamplesIterable:__iter__,ShuffledDataSourcesExamplesIterable:__iter__,method,6,8,8,159,19.88,0,0,['self'],[None],[None],258,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']","['deepcopy', '_shuffle_gen_kwargs', 'self.generate_examples_fn']",3
repos/datasets/src/datasets/iterable_dataset.py:ShuffledDataSourcesExamplesIterable:shard_data_sources,ShuffledDataSourcesExamplesIterable:shard_data_sources,method,8,11,11,215,19.55,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",264,"['        """"""Keep only the requested shard.""""""\n']","['deepcopy', '_shuffle_gen_kwargs', 'ExamplesIterable']",3
repos/datasets/src/datasets/iterable_dataset.py:SkipExamplesIterable:__init__,SkipExamplesIterable:__init__,method,5,5,5,56,11.2,0,0,"['self', 'ex_iterable', 'n']","[None, ' _BaseExamplesIterable', ' int']","[None, None, None]",1018,[],['super'],1
repos/datasets/src/datasets/iterable_dataset.py:SkipExamplesIterable:__iter__,SkipExamplesIterable:__iter__,method,2,5,5,45,9.0,0,0,['self'],[None],[None],1024,[],['islice'],1
repos/datasets/src/datasets/iterable_dataset.py:SkipExamplesIterable:n_shards,SkipExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],1032,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:SkipExamplesIterable:shuffle_data_sources,SkipExamplesIterable:shuffle_data_sources,method,1,2,2,10,5.0,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",1027,"['        """"""Doesn\'t shuffle the wrapped examples iterable since it would skip examples from other shards instead.""""""\n']",[],0
repos/datasets/src/datasets/iterable_dataset.py:StepExamplesIterable:__init__,StepExamplesIterable:__init__,method,7,7,7,81,11.57,0,0,"['self', 'ex_iterable', 'step', 'offset']","[None, ' _BaseExamplesIterable', ' int', ' int']","[None, None, None, None]",369,[],['super'],1
repos/datasets/src/datasets/iterable_dataset.py:StepExamplesIterable:__iter__,StepExamplesIterable:__iter__,method,5,15,15,149,9.93,1,1,['self'],[None],[None],376,[],"['iter', 'list', 'len']",3
repos/datasets/src/datasets/iterable_dataset.py:StepExamplesIterable:n_shards,StepExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],396,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:StepExamplesIterable:shard_data_sources,StepExamplesIterable:shard_data_sources,method,2,7,7,122,17.43,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",390,[],['StepExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:StepExamplesIterable:shuffle_data_sources,StepExamplesIterable:shuffle_data_sources,method,2,6,6,112,18.67,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",385,[],['StepExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:TakeExamplesIterable:__init__,TakeExamplesIterable:__init__,method,5,5,5,56,11.2,0,0,"['self', 'ex_iterable', 'n']","[None, ' _BaseExamplesIterable', ' int']","[None, None, None]",1018,[],['super'],1
repos/datasets/src/datasets/iterable_dataset.py:TakeExamplesIterable:__iter__,TakeExamplesIterable:__iter__,method,2,4,4,40,10.0,0,0,['self'],[None],[None],1024,"['        """"""Doesn\'t shuffle the wrapped examples iterable since it would skip examples from other shards instead.""""""\n']",['islice'],1
repos/datasets/src/datasets/iterable_dataset.py:TakeExamplesIterable:n_shards,TakeExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],1032,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:TakeExamplesIterable:shard_data_sources,TakeExamplesIterable:shard_data_sources,method,3,7,7,141,20.14,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",1059,"['        """"""Keep only the requested shard.""""""\n']",['TakeExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:TakeExamplesIterable:shuffle_data_sources,TakeExamplesIterable:shuffle_data_sources,method,1,2,2,10,5.0,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",1046,"['        """"""Doesn\'t shuffle the wrapped examples iterable since it would take examples from other shards instead.""""""\n']",[],0
repos/datasets/src/datasets/iterable_dataset.py:TakeExamplesIterable:split_number,TakeExamplesIterable:split_number,method,9,20,16,101,5.05,1,0,"['num', 'n']","[None, None]","[None, None]",1051,[],['range'],1
repos/datasets/src/datasets/iterable_dataset.py:TypedExamplesIterable:__init__,TypedExamplesIterable:__init__,method,10,14,14,184,13.14,0,1,"['self', 'ex_iterable', 'features', 'token_per_repo_id', 'Union[str', 'bool', 'None]]', '']","[None, ' _BaseExamplesIterable', ' Features', ' Dict[str', None, None, None, None]","[None, None, None, None, None, None, None, None]",252,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']",['super'],1
repos/datasets/src/datasets/iterable_dataset.py:TypedExamplesIterable:__iter__,TypedExamplesIterable:__iter__,method,3,12,12,144,12.0,1,0,['self'],[None],[None],258,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']",['_apply_feature_types_on_example'],1
repos/datasets/src/datasets/iterable_dataset.py:TypedExamplesIterable:_iter_arrow,TypedExamplesIterable:_iter_arrow,method,13,34,25,386,11.35,2,2,['self'],[None],[None],353,"['        """"""Shuffle each underlying examples iterable.""""""\n']","['set', 'len', 'pa_table.append_column', 'cast_table_to_features']",4
repos/datasets/src/datasets/iterable_dataset.py:TypedExamplesIterable:n_shards,TypedExamplesIterable:n_shards,method,2,2,2,31,15.5,0,0,['self'],[None],[None],302,"['        """"""Shuffle the kwargs order to shuffle shards""""""\n']",[],0
repos/datasets/src/datasets/iterable_dataset.py:TypedExamplesIterable:shard_data_sources,TypedExamplesIterable:shard_data_sources,method,4,7,7,156,22.29,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",1146,"['        """"""Keep only the requested shard.""""""\n']",['TypedExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:TypedExamplesIterable:shuffle_data_sources,TypedExamplesIterable:shuffle_data_sources,method,4,6,6,146,24.33,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",1138,"['        """"""Shuffle the wrapped examples iterable.""""""\n']",['TypedExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:VerticallyConcatenatedMultiSourcesExamplesIterable:__init__,VerticallyConcatenatedMultiSourcesExamplesIterable:__init__,method,6,14,14,150,10.71,0,1,"['self', 'ex_iterables']","[None, ' List[_BaseExamplesIterable]']","[None, None]",478,[],"['super', 'all']",2
repos/datasets/src/datasets/iterable_dataset.py:VerticallyConcatenatedMultiSourcesExamplesIterable:__iter__,VerticallyConcatenatedMultiSourcesExamplesIterable:__iter__,method,3,7,6,55,7.86,1,0,['self'],[None],[None],484,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:VerticallyConcatenatedMultiSourcesExamplesIterable:_iter_arrow,VerticallyConcatenatedMultiSourcesExamplesIterable:_iter_arrow,method,4,7,7,68,9.71,1,0,['self'],[None],[None],488,[],['ex_iterable.iter_arrow'],1
repos/datasets/src/datasets/iterable_dataset.py:VerticallyConcatenatedMultiSourcesExamplesIterable:n_shards,VerticallyConcatenatedMultiSourcesExamplesIterable:n_shards,method,2,6,6,64,10.67,0,0,['self'],[None],[None],503,[],['min'],1
repos/datasets/src/datasets/iterable_dataset.py:VerticallyConcatenatedMultiSourcesExamplesIterable:shard_data_sources,VerticallyConcatenatedMultiSourcesExamplesIterable:shard_data_sources,method,2,9,9,142,15.78,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",506,"['        """"""Either keep only the requested shard, or propagate the request to the underlying iterable.""""""\n']",['VerticallyConcatenatedMultiSourcesExamplesIterable'],1
repos/datasets/src/datasets/iterable_dataset.py:VerticallyConcatenatedMultiSourcesExamplesIterable:shuffle_data_sources,VerticallyConcatenatedMultiSourcesExamplesIterable:shuffle_data_sources,method,6,13,12,244,18.77,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",492,"['        """"""Shuffle the list of examples iterable, as well as each underlying examples iterable.""""""\n']","['deepcopy', 'list', 'rng.shuffle', 'VerticallyConcatenatedMultiSourcesExamplesIterable']",4
repos/datasets/src/datasets/iterable_dataset.py:_BaseExamplesIterable:__init__,_BaseExamplesIterable:__init__,method,4,5,5,73,14.6,0,0,['self'],[None],[None],201,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:_BaseExamplesIterable:__iter__,_BaseExamplesIterable:__iter__,method,1,6,6,68,11.33,0,0,['self'],[None],[None],204,"['        """"""An examples iterable should yield tuples (example_key, example) of type (int/str, dict)""""""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/iterable_dataset.py:_BaseExamplesIterable:n_shards,_BaseExamplesIterable:n_shards,method,1,6,6,68,11.33,0,0,['self'],[None],[None],223,[],['NotImplementedError'],1
repos/datasets/src/datasets/iterable_dataset.py:_BaseExamplesIterable:shard_data_sources,_BaseExamplesIterable:shard_data_sources,method,1,6,6,78,13.0,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",215,"['        """"""Either keep only the requested shard, or propagate the request to the underlying iterable.""""""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/iterable_dataset.py:_BaseExamplesIterable:shuffle_data_sources,_BaseExamplesIterable:shuffle_data_sources,method,1,6,6,80,13.33,0,0,"['self', 'generator']","[None, ' np.random.Generator']","[None, None]",208,"['        """"""\n', '        Either shuffle the shards/sources of the dataset, or propagate the shuffling to the underlying iterable.\n', '        If the order of the shards must stay fixed (when using .skip or .take for example), then this method returns self.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/iterable_dataset.py:_BaseExamplesIterable:split_shard_indices_by_worker,_BaseExamplesIterable:split_shard_indices_by_worker,method,1,4,4,54,13.5,0,0,"['self', 'worker_id', 'num_workers']","[None, ' int', ' int']","[None, None, None]",219,[],['list'],1
repos/datasets/src/datasets/iterable_dataset.py:_HasNextIterator:__init__,_HasNextIterator:__init__,method,3,4,4,35,8.75,0,0,"['self', 'it']","[None, None]","[None, None]",88,[],['iter'],1
repos/datasets/src/datasets/iterable_dataset.py:_HasNextIterator:__iter__,_HasNextIterator:__iter__,method,1,2,2,10,5.0,0,0,['self'],[None],[None],92,[],[],0
repos/datasets/src/datasets/iterable_dataset.py:_HasNextIterator:__next__,_HasNextIterator:__next__,method,5,11,9,96,8.73,0,1,['self'],[None],[None],95,[],['next'],1
repos/datasets/src/datasets/iterable_dataset.py:_HasNextIterator:hasnext,_HasNextIterator:hasnext,method,5,16,13,141,8.81,0,1,['self'],[None],[None],103,[],['next'],1
repos/datasets/src/datasets/keyhash.py:_as_bytes,_as_bytes,function,7,21,16,241,11.48,0,1,"['hash_data', 'int', 'bytes]']","[' Union[str', None, None]","[None, None, None]",38,"['    """"""\n', '    Returns the input hash_data in its bytes form\n', '\n', '    Args:\n', '    hash_data: the hash salt/key to be converted to bytes\n', '    """"""\n']","['isinstance', 'hash_data.replace', 'str', 'InvalidKeyError', 'hash_data.encode']",5
repos/datasets/src/datasets/keyhash.py:DuplicatedKeysError,DuplicatedKeysError,class,15,57,43,554,9.72,0,1,[],[],[],71,[],[],0
repos/datasets/src/datasets/keyhash.py:InvalidKeyError,InvalidKeyError,class,9,30,29,268,8.93,0,0,[],[],[],61,[],[],0
repos/datasets/src/datasets/keyhash.py:KeyHasher,KeyHasher,class,10,22,21,238,10.82,0,0,[],[],[],87,[],[],0
repos/datasets/src/datasets/keyhash.py:DuplicatedKeysError:__init__,DuplicatedKeysError:__init__,method,14,52,38,498,9.58,0,1,"['self', 'key', 'duplicate_key_indices', 'fix_msg']","[None, None, None, None]","[None, None, None, '""""']",74,[],"['len', 'super']",2
repos/datasets/src/datasets/keyhash.py:InvalidKeyError:__init__,InvalidKeyError:__init__,method,8,27,26,239,8.85,0,0,"['self', 'hash_data']","[None, None]","[None, None]",64,[],['super'],1
repos/datasets/src/datasets/keyhash.py:KeyHasher:__init__,KeyHasher:__init__,method,2,2,2,58,29.0,0,0,"['self', 'hash_salt']","[None, ' str']","[None, None]",90,[],['insecure_hashlib.md5'],1
repos/datasets/src/datasets/keyhash.py:KeyHasher:hash,KeyHasher:hash,method,6,8,8,101,12.62,0,0,"['self', 'key', 'int', 'bytes]']","[None, ' Union[str', None, None]","[None, None, None, None]",93,"['        """"""Returns 128-bits unique hash of input key\n', '\n', '        Args:\n', '        key: the input key to be hashed (should be str, int or bytes)\n', '\n', '        Returns: 128-bit int hash key""""""\n']","['_as_bytes', 'md5.update', 'int']",3
repos/datasets/src/datasets/load.py:_copy_script_and_other_resources_in_importable_dir,_copy_script_and_other_resources_in_importable_dir,function,32,122,82,1870,15.33,2,9,"['name', 'importable_directory_path', 'subdirectory_name', 'original_local_path', 'local_imports', 'str]]', 'additional_files', 'str]]', 'download_mode', 'str]]', '']","[' str', ' str', ' str', ' str', ' List[Tuple[str', None, ' List[Tuple[str', None, ' Optional[Union[DownloadMode', None, None]","[None, None, None, None, None, None, None, None, None, None, None]",361,"['    """"""Copy a script and its required imports to an importable directory\n', '\n', '    Args:\n', '        name (str): name of the resource to load\n', '        importable_directory_path (str): path to the loadable folder in the dynamic modules directory\n', '        subdirectory_name (str): name of the subdirectory in importable_directory_path in which to place the script\n', '        original_local_path (str): local path to the resource script\n', '        local_imports (List[Tuple[str, str]]): list of (destination_filename, import_file_to_copy)\n', '        additional_files (List[Tuple[str, str]]): list of (destination_filename, additional_file_to_copy)\n', '        download_mode (Optional[Union[DownloadMode, str]]): download mode\n', '\n', '    Return:\n', '        importable_file: path to an importable module with importlib.import_module\n', '    """"""\n']","['lock_importable_file', 'shutil.rmtree', 'os.makedirs', 'open', 'shutil.copyfile', 'json.dump', 'shutil.copytree', 'ImportError', 'filecmp.cmp']",9
repos/datasets/src/datasets/load.py:_create_importable_file,_create_importable_file,function,12,25,25,630,25.2,0,0,"['local_path', 'local_imports', 'str]]', 'additional_files', 'str]]', 'dynamic_modules_path', 'module_namespace', 'subdirectory_name', 'name', 'download_mode', '']","[' str', ' List[Tuple[str', None, ' List[Tuple[str', None, ' str', ' str', ' str', ' str', ' DownloadMode', None]","[None, None, None, None, None, None, None, None, None, None, None]",455,[],"['name.replace', 'Path', '_copy_script_and_other_resources_in_importable_dir', 'logger.debug']",4
repos/datasets/src/datasets/load.py:_download_additional_modules,_download_additional_modules,function,38,198,137,1934,9.77,2,11,"['name', 'base_path', 'imports', 'str', 'str', 'str]', 'download_config']","[' str', ' str', ' Tuple[str', None, None, None, ' Optional[DownloadConfig]']","[None, None, None, None, None, None, None]",294,"['    """"""\n', '    Download additional module for a module <name>.py at URL (or local path) <base_path>/<name>.py\n', '    The imports must have been parsed first using ``get_imports``.\n', '\n', '    If some modules need to be installed with pip, an error is raised showing how to install them.\n', '    This function return the list of downloaded modules as tuples (import_name, module_file_path).\n', '\n', '    The downloaded modules can then be moved into an importable directory with ``_copy_script_and_other_resources_in_importable_dir``.\n', '    """"""\n']","['download_config.copy', 'library_imports.append', 'ValueError', 'url_or_path_join', 'cached_path', 'local_imports.append', 'importlib.import_module', 'len', 'needs_to_be_installed.keys', 'ImportError']",10
repos/datasets/src/datasets/load.py:_get_importable_file_path,_get_importable_file_path,function,3,10,10,190,19.0,0,0,"['dynamic_modules_path', 'module_namespace', 'subdirectory_name', 'name', '']","[' str', ' str', ' str', ' str', None]","[None, None, None, None, None]",445,[],"['name.replace', 'name.split']",2
repos/datasets/src/datasets/load.py:_load_importable_file,_load_importable_file,function,8,14,14,185,13.21,0,0,"['dynamic_modules_path', 'module_namespace', 'subdirectory_name', 'name', '']","[' str', ' str', ' str', ' str', None]","[None, None, None, None, None]",480,[],"['name.replace', 'name.split']",2
repos/datasets/src/datasets/load.py:_raise_timeout_error,_raise_timeout_error,function,1,33,29,193,5.85,0,0,"['signum', 'frame']","[None, None]","[None, None]",100,[],['ValueError'],1
repos/datasets/src/datasets/load.py:configure_builder_class,configure_builder_class,function,23,52,43,783,15.06,0,0,"['builder_cls', 'builder_configs', 'default_config_name', 'dataset_name', '']","[' Type[DatasetBuilder]', ' List[BuilderConfig]', ' Optional[str]', ' str', None]","[None, None, None, None, None]",205,"['    """"""\n', '    Dynamically create a builder class with custom builder configs parsed from README.md file,\n', '    i.e. set BUILDER_CONFIGS class variable of a builder class to custom configs list.\n', '    """"""\n']","['ConfiguredDatasetBuilder', '__reduce__', '_InitializeConfiguredDatasetBuilder']",3
repos/datasets/src/datasets/load.py:create_builder_configs_from_metadata_configs,create_builder_configs_from_metadata_configs,function,37,210,139,2218,10.56,1,7,"['module_path', 'metadata_configs', 'supports_metadata', 'base_path', 'default_builder_kwargs', 'Any] ', 'download_config', '']","[' str', ' MetadataConfigs', ' bool', ' Optional[str] ', ' Dict[str', None, ' Optional[DownloadConfig] ', None]","[None, None, None, ' None', None, ' None', ' None', None]",601,[],"['import_main_class', 'metadata_configs.get_default_config_name', 'metadata_configs.items', 'config_params.get', 'xjoin', 'sanitize_patterns', 'get_data_patterns', 'DataFilesPatternsDict.from_patterns', 'EmptyDatasetError', 'get_metadata_patterns', 'DataFilesPatternsList.from_patterns', 'DataFilesPatternsDict', 'config_data_files_dict.items', 'hasattr', 'logger.warning', 'builder_configs.append', 'builder_config_cls']",17
repos/datasets/src/datasets/load.py:dataset_module_factory,dataset_module_factory,function,74,390,200,4311,11.05,1,11,"['path', 'revision', 'Version]] ', 'download_config', 'download_mode', 'str]] ', 'dynamic_modules_path', 'data_dir', 'data_files', 'List', 'str', 'DataFilesDict]] ', 'cache_dir', 'trust_remote_code', '_require_default_config_name', '_require_custom_configs', '**download_kwargs', '']","[' str', ' Optional[Union[str', None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[str] ', ' Optional[str] ', ' Optional[Union[Dict', None, None, None, ' Optional[str] ', ' Optional[bool] ', None, None, None, None]","[None, None, ' None', ' None', None, ' None', ' None', ' None', None, None, None, ' None', ' None', ' None', 'True', 'False', None, None]",1689,"['    """"""\n', '    Download/extract/cache a dataset module.\n', '\n', '    Dataset codes are cached inside the dynamic modules cache to allow easy import (avoid ugly sys.path tweaks).\n', '\n', '    Args:\n', '\n', '        path (str): Path or name of the dataset.\n', '            Depending on ``path``, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\n', '\n', '            For local datasets:\n', '\n', '            - if ``path`` is a local directory (containing data files only)\n', '              -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n', ""              e.g. ``'./path/to/directory/with/my/csv/data'``.\n"", '            - if ``path`` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory):\n', '              -> load the dataset builder from the dataset script\n', ""              e.g. ``'./dataset/squad'`` or ``'./dataset/squad/squad.py'``.\n"", '\n', '            For datasets on the Hugging Face Hub (list all available datasets with ``huggingface_hub.list_datasets()``)\n', '\n', '            - if ``path`` is a dataset repository on the HF hub (containing data files only)\n', '              -> load a generic dataset builder (csv, text etc.) based on the content of the repository\n', ""              e.g. ``'username/dataset_name'``, a dataset repository on the HF hub containing your data files.\n"", '            - if ``path`` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n', '              -> load the dataset builder from the dataset script in the dataset repository\n', ""              e.g. ``glue``, ``squad``, ``'username/dataset_name'``, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n"", '\n', '        revision (:class:`~utils.Version` or :obj:`str`, optional): Version of the dataset script to load.\n', '            As datasets have their own git repository on the Datasets Hub, the default version ""main"" corresponds to their ""main"" branch.\n', '            You can specify a different version than the default ""main"" by using a commit SHA or a git tag of the dataset repository.\n', '        download_config (:class:`DownloadConfig`, optional): Specific download configuration parameters.\n', '        download_mode (:class:`DownloadMode` or :obj:`str`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\n', '        dynamic_modules_path (Optional str, defaults to HF_MODULES_CACHE / ""datasets_modules"", i.e. ~/.cache/huggingface/modules/datasets_modules):\n', '            Optional path to the directory in which the dynamic modules are saved. It must have been initialized with :obj:`init_dynamic_modules`.\n', '            By default, the datasets and metrics are stored inside the `datasets_modules` module.\n', '        data_dir (:obj:`str`, optional): Directory with the data files. Used only if `data_files` is not specified,\n', '            in which case it\'s equal to pass `os.path.join(data_dir, ""**"")` as `data_files`.\n', '        data_files (:obj:`Union[Dict, List, str]`, optional): Defining the data_files of the dataset configuration.\n', '        cache_dir (`str`, *optional*):\n', '            Directory to read/write data. Defaults to `""~/.cache/huggingface/datasets""`.\n', '\n', '            <Added version=""2.16.0""/>\n', '        trust_remote_code (`bool`, defaults to `True`):\n', '            Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n', '            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n', '            execute code present on the Hub on your local machine.\n', '\n', '            <Tip warning={true}>\n', '\n', '            `trust_remote_code` will default to False in the next major release.\n', '\n', '            </Tip>\n', '\n', '            <Added version=""2.16.0""/>\n', '        **download_kwargs (additional keyword arguments): optional attributes for DownloadConfig() which will override\n', '            the attributes in download_config if supplied.\n', '\n', '    Returns:\n', '        DatasetModule\n', '    """"""\n']","['DownloadConfig', 'DownloadMode', 'list', 'path.replace', 'filename.endswith', 'PackagedDatasetModuleFactory', 'path.endswith', 'LocalDatasetModuleFactoryWithScript', 'FileNotFoundError', 'LocalDatasetModuleFactoryWithoutScript', 'is_relative_path', 'path.count', '_raise_if_offline_mode_is_enabled', 'HfApi', 'hf_api.dataset_info', 'isinstance', 'ConnectionError', 'str', 'DatasetNotFoundError', 'HfFileSystem', 'fs.open', 'f.read', 'HubDatasetModuleFactoryWithParquetExport', 'HubDatasetModuleFactoryWithScript', 'HubDatasetModuleFactoryWithoutScript', 'CachedDatasetModuleFactory']",26
repos/datasets/src/datasets/load.py:files_to_hash,files_to_hash,function,14,27,23,329,12.19,2,1,['file_paths'],[' List[str]'],[None],265,"['    """"""\n', '    Convert a list of scripts or text files provided in file_paths into a hashed filename in a repeatable way.\n', '    """"""\n']","['to_use_files.extend', 'to_use_files.append', 'open', 'lines.extend', '_hash_python_lines']",5
repos/datasets/src/datasets/load.py:get_dataset_builder_class,get_dataset_builder_class,function,11,37,31,650,17.57,0,2,"['dataset_module', 'dataset_name']","[' ""DatasetModule""', ' Optional[str] ']","[None, ' None']",245,[],"['lock_importable_file', 'nullcontext', 'import_main_class', 'ValueError', 'configure_builder_class']",5
repos/datasets/src/datasets/load.py:import_main_class,import_main_class,function,18,39,30,387,9.92,1,4,"['module_path', 'dataset']","[None, None]","[None, 'True']",161,"['    """"""Import a module at module_path and return its main class:\n', '    - a DatasetBuilder if dataset is True\n', '    - a Metric if dataset is False\n', '    """"""\n']","['importlib.import_module', 'inspect.isclass', 'issubclass', 'inspect.isabstract', 'inspect.getmodule']",5
repos/datasets/src/datasets/load.py:increase_load_count,increase_load_count,function,4,14,14,166,11.86,0,1,"['name', 'resource_type']","[' str', ' str']","[None, None]",285,"['    """"""Update the download count of a dataset or metric.""""""\n']",['head_hf_s3'],1
repos/datasets/src/datasets/load.py:infer_module_for_data_files,infer_module_for_data_files,function,14,54,43,548,10.15,1,2,"['data_files', 'path', 'download_config']","[' DataFilesDict', ' Optional[str] ', ' Optional[DownloadConfig] ']","[None, ' None', ' None']",573,"['    """"""Infer module (and builder kwargs) from data files. Raise if module names for different splits don\'t match.\n', '\n', '    Args:\n', '        data_files ([`DataFilesDict`]): Dict of list of data files.\n', '        path (str, *optional*): Dataset name or path.\n', '        download_config ([`DownloadConfig`], *optional*):\n', '            Specific download configuration parameters to authenticate on the Hugging Face Hub for private remote files.\n', '\n', '    Returns:\n', '        tuple[str, dict[str, Any]]: Tuple with\n', '            - inferred module name\n', '            - builder kwargs\n', '    """"""\n']","['infer_module_for_data_files_list', 'data_files.items', 'next', 'any', 'split_modules.values', 'ValueError', 'DataFilesNotFoundError']",7
repos/datasets/src/datasets/load.py:infer_module_for_data_files_list,infer_module_for_data_files_list,function,14,62,49,673,10.85,3,2,"['data_files_list', 'download_config']","[' DataFilesList', ' Optional[DownloadConfig] ']","[None, ' None']",498,"['    """"""Infer module (and builder kwargs) from list of data files.\n', '\n', '    It picks the module based on the most common file extension.\n', '    In case of a draw "".parquet"" is the favorite, and then alphabetical order.\n', '\n', '    Args:\n', '        data_files_list (DataFilesList): List of data files.\n', '        download_config (bool or str, optional): mainly use use_auth_token or storage_options to support different platforms and auth types.\n', '\n', '    Returns:\n', '        tuple[str, dict[str, Any]]: Tuple with\n', '            - inferred module name\n', '            - dict of builder kwargs\n', '    """"""\n']","['Counter', 'suffix.lower', 'xbasename', 'sort_key', 'sorted', 'infer_module_for_data_files_list_in_archives']",6
repos/datasets/src/datasets/load.py:infer_module_for_data_files_list_in_archives,infer_module_for_data_files_list_in_archives,function,19,61,42,742,12.16,2,4,"['data_files_list', 'download_config']","[' DataFilesList', ' Optional[DownloadConfig] ']","[None, ' None']",535,"['    """"""Infer module (and builder kwargs) from list of archive data files.\n', '\n', '    Args:\n', '        data_files_list (DataFilesList): List of data files.\n', '        download_config (bool or str, optional): mainly use use_auth_token or storage_options to support different platforms and auth types.\n', '\n', '    Returns:\n', '        tuple[str, dict[str, Any]]: Tuple with\n', '            - inferred module name\n', '            - dict of builder kwargs\n', '    """"""\n']","['str', 'xjoin', 'f.split', 'xglob', 'Counter', 'suffix.lower', 'xbasename', 'extensions_counter.most_common']",8
repos/datasets/src/datasets/load.py:init_dynamic_modules,init_dynamic_modules,function,9,18,17,322,17.89,0,1,"['name', 'hf_modules_cache', 'str]] ']","[' str ', ' Optional[Union[Path', None]","[' config.MODULE_NAME_FOR_DYNAMIC_MODULES', None, ' None']",143,"['    """"""\n', '    Create a module with name `name` in which you can add dynamic modules\n', '    such as metrics or datasets. The module can be imported using its name.\n', '    The module is created in the HF_MODULE_CACHE directory by default (~/.cache/huggingface/modules) but it can\n', '    be overridden by specifying a path to another directory in `hf_modules_cache`.\n', '    """"""\n']","['init_hf_modules', 'os.makedirs', 'open']",3
repos/datasets/src/datasets/load.py:load_dataset,load_dataset,function,32,249,143,2513,10.09,0,10,"['path', 'name', 'data_dir', 'data_files', 'Sequence[str]', 'Mapping[str', 'Union[str', 'Sequence[str]]]]] ', 'split', 'Split]] ', 'cache_dir', 'features', 'download_config', 'download_mode', 'str]] ', 'verification_mode', 'str]] ', 'ignore_verifications', 'keep_in_memory', 'save_infos', 'revision', 'Version]] ', 'token', 'str]] ', 'use_auth_token', 'task', 'streaming', 'num_proc', 'storage_options', 'trust_remote_code', '**config_kwargs', '']","[' str', ' Optional[str] ', ' Optional[str] ', ' Optional[Union[str', None, None, None, None, ' Optional[Union[str', None, ' Optional[str] ', ' Optional[Features] ', ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[Union[VerificationMode', None, None, ' Optional[bool] ', ' bool ', ' Optional[Union[str', None, ' Optional[Union[bool', None, None, None, ' bool ', ' Optional[int] ', ' Optional[Dict] ', ' bool ', None, None]","[None, ' None', ' None', None, None, None, None, ' None', None, ' None', ' None', ' None', ' None', None, ' None', None, ' None', '""deprecated""', ' None', ' False', None, ' None', None, ' None', '""deprecated""', '""deprecated""', ' False', ' None', ' None', ' None', None, None]",2315,"['    """"""Load a dataset from the Hugging Face Hub, or a local dataset.\n', '\n', '    You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n', '\n', '    A dataset is a directory that contains:\n', '\n', '    - some data files in generic formats (JSON, CSV, Parquet, text, etc.).\n', '    - and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n', '\n', '    Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n', '\n', '    This function does the following under the hood:\n', '\n', ""        1. Download and import in the library the dataset script from `path` if it's not already cached inside the library.\n"", '\n', '            If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\n', '\n', '            Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\n', '            contain the path or URL to the original data files and the code to load examples from the original data files.\n', '\n', '            You can find the complete list of datasets in the Datasets [Hub](https://huggingface.co/datasets).\n', '\n', '        2. Run the dataset script which will:\n', '\n', ""            * Download the dataset file from the original URL (see the script) if it's not already available locally or cached.\n"", '            * Process and cache the dataset in typed Arrow tables for caching.\n', '\n', '                Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n', '                They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n', '\n', '        3. Return a dataset built from the requested splits in `split` (default: all).\n', '\n', '    It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\n', '    In this case, it automatically loads all the data files from the directory or the dataset repository.\n', '\n', '    Args:\n', '\n', '        path (`str`):\n', '            Path or name of the dataset.\n', '            Depending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\n', '\n', '            For local datasets:\n', '\n', '            - if `path` is a local directory (containing data files only)\n', '              -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n', ""              e.g. `'./path/to/directory/with/my/csv/data'`.\n"", '            - if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)\n', '              -> load the dataset builder from the dataset script\n', ""              e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\n"", '\n', '            For datasets on the Hugging Face Hub (list all available datasets with [`huggingface_hub.list_datasets`])\n', '\n', '            - if `path` is a dataset repository on the HF hub (containing data files only)\n', '              -> load a generic dataset builder (csv, text etc.) based on the content of the repository\n', ""              e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.\n"", '            - if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n', '              -> load the dataset builder from the dataset script in the dataset repository\n', ""              e.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n"", '\n', '        name (`str`, *optional*):\n', '            Defining the name of the dataset configuration.\n', '        data_dir (`str`, *optional*):\n', '            Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n', '            the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n', '        data_files (`str` or `Sequence` or `Mapping`, *optional*):\n', '            Path(s) to source data file(s).\n', '        split (`Split` or `str`):\n', '            Which split of the data to load.\n', '            If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n', '            If given, will return a single Dataset.\n', '            Splits can be combined and specified like in tensorflow-datasets.\n', '        cache_dir (`str`, *optional*):\n', '            Directory to read/write data. Defaults to `""~/.cache/huggingface/datasets""`.\n', '        features (`Features`, *optional*):\n', '            Set the features type to use for this dataset.\n', '        download_config ([`DownloadConfig`], *optional*):\n', '            Specific download configuration parameters.\n', '        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n', '            Download/generate mode.\n', '        verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n', '            Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n', '\n', '            <Added version=""2.9.1""/>\n', '        ignore_verifications (`bool`, defaults to `False`):\n', '            Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).\n', '\n', '            <Deprecated version=""2.9.1"">\n', '\n', '            `ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\n', '            Please use `verification_mode` instead.\n', '\n', '            </Deprecated>\n', '        keep_in_memory (`bool`, defaults to `None`):\n', '            Whether to copy the dataset in-memory. If `None`, the dataset\n', '            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n', '            nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n', '        save_infos (`bool`, defaults to `False`):\n', '            Save the dataset information (checksums/size/splits/...).\n', '        revision ([`Version`] or `str`, *optional*):\n', '            Version of the dataset script to load.\n', '            As datasets have their own git repository on the Datasets Hub, the default version ""main"" corresponds to their ""main"" branch.\n', '            You can specify a different version than the default ""main"" by using a commit SHA or a git tag of the dataset repository.\n', '        token (`str` or `bool`, *optional*):\n', '            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '            If `True`, or not specified, will get token from `""~/.huggingface""`.\n', '        use_auth_token (`str` or `bool`, *optional*):\n', '            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '            If `True`, or not specified, will get token from `""~/.huggingface""`.\n', '\n', '            <Deprecated version=""2.14.0"">\n', '\n', '            `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n', '\n', '            </Deprecated>\n', '        task (`str`):\n', ""            The task to prepare the dataset for during training and evaluation. Casts the dataset's [`Features`] to standardized column names and types as detailed in `datasets.tasks`.\n"", '\n', '            <Deprecated version=""2.13.0"">\n', '\n', '            `task` was deprecated in version 2.13.0 and will be removed in 3.0.0.\n', '\n', '            </Deprecated>\n', '        streaming (`bool`, defaults to `False`):\n', ""            If set to `True`, don't download the data files. Instead, it streams the data progressively while\n"", '            iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.\n', '\n', '            Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n', '            Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n', ""            like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n"", '        num_proc (`int`, *optional*, defaults to `None`):\n', '            Number of processes when downloading and generating the dataset locally.\n', '            Multiprocessing is disabled by default.\n', '\n', '            <Added version=""2.7.0""/>\n', '        storage_options (`dict`, *optional*, defaults to `None`):\n', '            **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n', '\n', '            <Added version=""2.11.0""/>\n', '        trust_remote_code (`bool`, defaults to `True`):\n', '            Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n', '            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n', '            execute code present on the Hub on your local machine.\n', '\n', '            <Tip warning={true}>\n', '\n', '            `trust_remote_code` will default to False in the next major release.\n', '\n', '            </Tip>\n', '\n', '            <Added version=""2.16.0""/>\n', '        **config_kwargs (additional keyword arguments):\n', '            Keyword arguments to be passed to the `BuilderConfig`\n', '            and used in the [`DatasetBuilder`].\n', '\n', '    Returns:\n', '        [`Dataset`] or [`DatasetDict`]:\n', '        - if `split` is not `None`: the dataset requested,\n', '        - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.\n', '\n', '        or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`\n', '\n', '        - if `split` is not `None`, the dataset is requested\n', '        - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.\n', '\n', '    Example:\n', '\n', '    Load a dataset from the Hugging Face Hub:\n', '\n', '    ```py\n', '    >>> from datasets import load_dataset\n', ""    >>> ds = load_dataset('rotten_tomatoes', split='train')\n"", '\n', '    # Map data files to splits\n', ""    >>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n"", ""    >>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n"", '    ```\n', '\n', '    Load a local dataset:\n', '\n', '    ```py\n', '    # Load a CSV file\n', '    >>> from datasets import load_dataset\n', ""    >>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n"", '\n', '    # Load a JSON file\n', '    >>> from datasets import load_dataset\n', ""    >>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n"", '\n', '    # Load from a local loading script\n', '    >>> from datasets import load_dataset\n', ""    >>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n"", '    ```\n', '\n', '    Load an [`~datasets.IterableDataset`]:\n', '\n', '    ```py\n', '    >>> from datasets import load_dataset\n', ""    >>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True)\n"", '    ```\n', '\n', '    Load an image dataset with the `ImageFolder` dataset builder:\n', '\n', '    ```py\n', '    >>> from datasets import load_dataset\n', ""    >>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n"", '    ```\n', '    """"""\n']","['warnings.warn', 'ValueError', 'Path', 'NotImplementedError', 'DownloadMode', 'VerificationMode', 'load_dataset_builder', 'builder_instance.as_streaming_dataset', 'builder_instance.download_and_prepare', 'is_small_dataset', 'builder_instance.as_dataset', 'warnings.catch_warnings', 'warnings.simplefilter', 'ds.prepare_for_task', 'builder_instance._save_infos']",15
repos/datasets/src/datasets/load.py:load_dataset_builder,load_dataset_builder,function,32,172,129,2292,13.33,0,5,"['path', 'name', 'data_dir', 'data_files', 'Sequence[str]', 'Mapping[str', 'Union[str', 'Sequence[str]]]]] ', 'cache_dir', 'features', 'download_config', 'download_mode', 'str]] ', 'revision', 'Version]] ', 'token', 'str]] ', 'use_auth_token', 'storage_options', 'trust_remote_code', '_require_default_config_name', '**config_kwargs', '']","[' str', ' Optional[str] ', ' Optional[str] ', ' Optional[Union[str', None, None, None, None, ' Optional[str] ', ' Optional[Features] ', ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[Union[str', None, ' Optional[Union[bool', None, None, ' Optional[Dict] ', ' Optional[bool] ', None, None, None]","[None, ' None', ' None', None, None, None, None, ' None', ' None', ' None', ' None', None, ' None', None, ' None', None, ' None', '""deprecated""', ' None', ' None', 'True', None, None]",2128,"['    """"""Load a dataset builder from the Hugging Face Hub, or a local dataset. A dataset builder can be used to inspect general information that is required to build a dataset (cache directory, config, dataset info, etc.)\n', '    without downloading the dataset itself.\n', '\n', '    You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n', '\n', '    A dataset is a directory that contains:\n', '\n', '    - some data files in generic formats (JSON, CSV, Parquet, text, etc.)\n', '    - and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n', '\n', '    Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n', '\n', '    Args:\n', '\n', '        path (`str`):\n', '            Path or name of the dataset.\n', '            Depending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\n', '\n', '            For local datasets:\n', '\n', '            - if `path` is a local directory (containing data files only)\n', '              -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n', ""              e.g. `'./path/to/directory/with/my/csv/data'`.\n"", '            - if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)\n', '              -> load the dataset builder from the dataset script\n', ""              e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\n"", '\n', '            For datasets on the Hugging Face Hub (list all available datasets with [`huggingface_hub.list_datasets`])\n', '\n', '            - if `path` is a dataset repository on the HF hub (containing data files only)\n', '              -> load a generic dataset builder (csv, text etc.) based on the content of the repository\n', ""              e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.\n"", '            - if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n', '              -> load the dataset builder from the dataset script in the dataset repository\n', ""              e.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n"", '\n', '        name (`str`, *optional*):\n', '            Defining the name of the dataset configuration.\n', '        data_dir (`str`, *optional*):\n', '            Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n', '            the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n', '        data_files (`str` or `Sequence` or `Mapping`, *optional*):\n', '            Path(s) to source data file(s).\n', '        cache_dir (`str`, *optional*):\n', '            Directory to read/write data. Defaults to `""~/.cache/huggingface/datasets""`.\n', '        features ([`Features`], *optional*):\n', '            Set the features type to use for this dataset.\n', '        download_config ([`DownloadConfig`], *optional*):\n', '            Specific download configuration parameters.\n', '        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n', '            Download/generate mode.\n', '        revision ([`Version`] or `str`, *optional*):\n', '            Version of the dataset script to load.\n', '            As datasets have their own git repository on the Datasets Hub, the default version ""main"" corresponds to their ""main"" branch.\n', '            You can specify a different version than the default ""main"" by using a commit SHA or a git tag of the dataset repository.\n', '        token (`str` or `bool`, *optional*):\n', '            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '            If `True`, or not specified, will get token from `""~/.huggingface""`.\n', '        use_auth_token (`str` or `bool`, *optional*):\n', '            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n', '            If `True`, or not specified, will get token from `""~/.huggingface""`.\n', '\n', '            <Deprecated version=""2.14.0"">\n', '\n', '            `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n', '\n', '            </Deprecated>\n', '        storage_options (`dict`, *optional*, defaults to `None`):\n', '            **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n', '\n', '            <Added version=""2.11.0""/>\n', '        trust_remote_code (`bool`, defaults to `True`):\n', '            Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n', '            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n', '            execute code present on the Hub on your local machine.\n', '\n', '            <Tip warning={true}>\n', '\n', '            `trust_remote_code` will default to False in the next major release.\n', '\n', '            </Tip>\n', '\n', '            <Added version=""2.16.0""/>\n', '        **config_kwargs (additional keyword arguments):\n', '            Keyword arguments to be passed to the [`BuilderConfig`]\n', '            and used in the [`DatasetBuilder`].\n', '\n', '    Returns:\n', '        [`DatasetBuilder`]\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> from datasets import load_dataset_builder\n', ""    >>> ds_builder = load_dataset_builder('rotten_tomatoes')\n"", '    >>> ds_builder.info.features\n', ""    {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n"", ""     'text': Value(dtype='string', id=None)}\n"", '    ```\n', '    """"""\n']","['warnings.warn', 'DownloadMode', 'download_config.copy', 'DownloadConfig', 'dataset_module_factory', '_require_custom_configs=bool', 'builder_kwargs.pop', 'ValueError', 'get_dataset_builder_class', 'builder_cls', 'builder_instance._use_legacy_cache_dir_if_possible']",11
repos/datasets/src/datasets/load.py:load_from_disk,load_from_disk,function,14,83,70,981,11.82,0,3,"['dataset_path', 'fs', 'keep_in_memory', 'storage_options']","[' str', None, ' Optional[bool] ', ' Optional[dict] ']","[None, '""deprecated""', ' None', ' None']",2634,"['    """"""\n', '    Loads a dataset that was previously saved using [`~Dataset.save_to_disk`] from a dataset directory, or\n', '    from a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n', '\n', '    Args:\n', '        dataset_path (`str`):\n', '            Path (e.g. `""dataset/train""`) or remote URI (e.g.\n', '            `""s3://my-bucket/dataset/train""`) of the [`Dataset`] or [`DatasetDict`] directory where the dataset will be\n', '            loaded from.\n', '        fs (`~filesystems.S3FileSystem` or `fsspec.spec.AbstractFileSystem`, *optional*):\n', '            Instance of the remote filesystem used to download the files from.\n', '\n', '            <Deprecated version=""2.9.0"">\n', '\n', '            `fs` was deprecated in version 2.9.0 and will be removed in 3.0.0.\n', '            Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`.\n', '\n', '            </Deprecated>\n', '\n', '        keep_in_memory (`bool`, defaults to `None`):\n', '            Whether to copy the dataset in-memory. If `None`, the dataset\n', '            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n', '            nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n', '\n', '        storage_options (`dict`, *optional*):\n', '            Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '            <Added version=""2.9.0""/>\n', '\n', '    Returns:\n', '        [`Dataset`] or [`DatasetDict`]:\n', '        - If `dataset_path` is a path of a dataset directory: the dataset requested.\n', '        - If `dataset_path` is a path of a dataset dict directory, a [`DatasetDict`] with each split.\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> from datasets import load_from_disk\n', ""    >>> ds = load_from_disk('path/to/dataset/directory')\n"", '    ```\n', '    """"""\n']","['warnings.warn', 'url_to_fs', 'fs.exists', 'FileNotFoundError', 'fs.isfile', 'posixpath.join', 'Dataset.load_from_disk', 'DatasetDict.load_from_disk']",8
repos/datasets/src/datasets/load.py:load_metric,load_metric,function,12,33,32,721,21.85,0,0,"['path', 'config_name', 'process_id', 'num_process', 'cache_dir', 'experiment_id', 'keep_in_memory', 'download_config', 'download_mode', 'str]] ', 'revision', 'Version]] ', 'trust_remote_code', '**metric_init_kwargs', '']","[' str', ' Optional[str] ', ' int ', ' int ', ' Optional[str] ', ' Optional[str] ', ' bool ', ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[Union[str', None, ' Optional[bool] ', None, None]","[None, ' None', ' 0', ' 1', ' None', ' None', ' False', ' None', None, ' None', None, ' None', ' None', None, None]",2032,"['    """"""Load a `datasets.Metric`.\n', '\n', '    <Deprecated version=""2.5.0"">\n', '\n', '    Use `evaluate.load` instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n', '\n', '    </Deprecated>\n', '\n', '    Args:\n', '\n', '        path (``str``):\n', '            path to the metric processing script with the metric builder. Can be either:\n', '                - a local path to processing script or the directory containing the script (if the script has the same name as the directory),\n', ""                    e.g. ``'./metrics/rouge'`` or ``'./metrics/rogue/rouge.py'``\n"", '                - a metric identifier on the HuggingFace datasets repo (list all available metrics with ``datasets.list_metrics()``)\n', ""                    e.g. ``'rouge'`` or ``'bleu'``\n"", '        config_name (:obj:`str`, optional): selecting a configuration for the metric (e.g. the GLUE metric has a configuration for each subset)\n', '        process_id (:obj:`int`, optional): for distributed evaluation: id of the process\n', '        num_process (:obj:`int`, optional): for distributed evaluation: total number of processes\n', '        cache_dir (Optional str): path to store the temporary predictions and references (default to `~/.cache/huggingface/metrics/`)\n', '        experiment_id (``str``): A specific experiment id. This is used if several distributed evaluations share the same file system.\n', '            This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n', '        keep_in_memory (bool): Whether to store the temporary results in memory (defaults to False)\n', '        download_config (Optional ``datasets.DownloadConfig``: specific download configuration parameters.\n', '        download_mode (:class:`DownloadMode` or :obj:`str`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\n', '        revision (Optional ``Union[str, datasets.Version]``): if specified, the module will be loaded from the datasets repository\n', '            at this version. By default, it is set to the local version of the lib. Specifying a version that is different from\n', '            your local version of the lib might cause compatibility issues.\n', '        trust_remote_code (`bool`, defaults to `True`):\n', '            Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n', '            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n', '            execute code present on the Hub on your local machine.\n', '\n', '            <Tip warning={true}>\n', '\n', '            `trust_remote_code` will default to False in the next major release.\n', '\n', '            </Tip>\n', '\n', '            <Added version=""2.16.0""/>\n', '\n', '    Returns:\n', '        `datasets.Metric`\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> from datasets import load_metric\n', ""    >>> accuracy = load_metric('accuracy')\n"", '    >>> accuracy.compute(references=[1, 0], predictions=[1, 1])\n', ""    {'accuracy': 0.5}\n"", '    ```\n', '    """"""\n']","['warnings.catch_warnings', 'warnings.filterwarnings', 'DownloadMode', 'metric_module_factory', 'import_main_class', 'metric_cls', 'metric.download_and_prepare']",7
repos/datasets/src/datasets/load.py:metric_module_factory,metric_module_factory,function,39,158,108,1874,11.86,0,7,"['path', 'revision', 'Version]] ', 'download_config', 'download_mode', 'str]] ', 'dynamic_modules_path', 'trust_remote_code', '**download_kwargs', '']","[' str', ' Optional[Union[str', None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[str] ', ' Optional[bool] ', None, None]","[None, None, ' None', ' None', None, ' None', ' None', ' None', None, None]",1918,"['    """"""\n', '    Download/extract/cache a metric module.\n', '\n', '    <Deprecated version=""2.5.0"">\n', '\n', '    Use the new library 🤗 Evaluate instead: https://huggingface.co/docs/evaluate\n', '\n', '    </Deprecated>\n', '\n', '    Metrics codes are cached inside the dynamic modules cache to allow easy import (avoid ugly sys.path tweaks).\n', '\n', '    Args:\n', '\n', '        path (str): Path or name of the metric script.\n', '\n', '            - if ``path`` is a local metric script or a directory containing a local metric script (if the script has the same name as the directory):\n', '              -> load the module from the metric script\n', ""              e.g. ``'./metrics/accuracy'`` or ``'./metrics/accuracy/accuracy.py'``.\n"", '            - if ``path`` is a metric on the Hugging Face Hub (ex: `glue`, `squad`)\n', '              -> load the module from the metric script in the GitHub repository at huggingface/datasets\n', ""              e.g. ``'accuracy'`` or ``'rouge'``.\n"", '\n', '        revision (Optional ``Union[str, datasets.Version]``):\n', '            If specified, the module will be loaded from the datasets repository at this version.\n', '            By default:\n', '            - it is set to the local version of the lib.\n', ""            - it will also try to load it from the main branch if it's not available at the local version of the lib.\n"", '            Specifying a version that is different from your local version of the lib might cause compatibility issues.\n', '        download_config (:class:`DownloadConfig`, optional): Specific download configuration parameters.\n', '        download_mode (:class:`DownloadMode` or :obj:`str`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\n', '        dynamic_modules_path (Optional str, defaults to HF_MODULES_CACHE / ""datasets_modules"", i.e. ~/.cache/huggingface/modules/datasets_modules):\n', '            Optional path to the directory in which the dynamic modules are saved. It must have been initialized with :obj:`init_dynamic_modules`.\n', '            By default, the datasets and metrics are stored inside the `datasets_modules` module.\n', '        trust_remote_code (`bool`, defaults to `True`):\n', '            Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n', '            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n', '            execute code present on the Hub on your local machine.\n', '\n', '            <Tip warning={true}>\n', '\n', '            `trust_remote_code` will default to False in the next major release.\n', '\n', '            </Tip>\n', '\n', '            <Added version=""2.16.0""/>\n', '        **download_kwargs (additional keyword arguments): optional attributes for DownloadConfig() which will override\n', '            the attributes in download_config if supplied.\n', '\n', '    Returns:\n', '        MetricModule\n', '    """"""\n']","['warnings.catch_warnings', 'warnings.filterwarnings', 'DownloadConfig', 'DownloadMode', 'list', 'path.replace', 'filename.endswith', 'path.endswith', 'LocalMetricModuleFactory', 'FileNotFoundError', 'is_relative_path', 'path.count', 'GithubMetricModuleFactory', 'CachedMetricModuleFactory', 'isinstance']",15
repos/datasets/src/datasets/load.py:resolve_trust_remote_code,resolve_trust_remote_code,function,11,140,83,1067,7.62,1,4,"['trust_remote_code', 'repo_id']","[' Optional[bool]', ' str']","[None, None]",107,"['    """"""\n', '    Copied and adapted from Transformers\n', '    https://github.com/huggingface/transformers/blob/2098d343cc4b4b9d2aea84b3cf1eb5a1e610deff/src/transformers/dynamic_module_utils.py#L589\n', '    """"""\n']","['signal.signal', 'signal.alarm', 'input', 'answer.lower', 'ValueError', '_raise_timeout_error']",6
repos/datasets/src/datasets/load.py:BuilderConfigsParameters,BuilderConfigsParameters,class,6,9,8,137,15.22,0,0,[],[],[],673,[],[],0
repos/datasets/src/datasets/load.py:CachedDatasetModuleFactory,CachedDatasetModuleFactory,class,53,215,149,2621,12.19,0,6,[],[],[],1552,[],[],0
repos/datasets/src/datasets/load.py:CachedMetricModuleFactory,CachedMetricModuleFactory,class,22,118,99,1271,10.77,0,3,[],[],[],1640,[],[],0
repos/datasets/src/datasets/load.py:DatasetModule,DatasetModule,class,11,15,14,230,15.33,0,0,[],[],[],691,[],[],0
repos/datasets/src/datasets/load.py:GithubMetricModuleFactory,GithubMetricModuleFactory,class,52,307,218,3421,11.14,0,7,[],[],[],716,[],[],0
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithParquetExport,HubDatasetModuleFactoryWithParquetExport,class,46,109,91,1731,15.88,1,0,[],[],[],1361,[],[],0
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithScript,HubDatasetModuleFactoryWithScript,class,59,314,212,4064,12.94,0,9,[],[],[],1426,[],[],0
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithoutScript,HubDatasetModuleFactoryWithoutScript,class,108,369,226,5686,15.41,2,14,[],[],[],1174,[],[],0
repos/datasets/src/datasets/load.py:LocalDatasetModuleFactoryWithScript,LocalDatasetModuleFactoryWithScript,class,47,222,169,2760,12.43,0,6,[],[],[],909,[],[],0
repos/datasets/src/datasets/load.py:LocalDatasetModuleFactoryWithoutScript,LocalDatasetModuleFactoryWithoutScript,class,78,253,185,3936,15.56,1,12,[],[],[],992,[],[],0
repos/datasets/src/datasets/load.py:LocalMetricModuleFactory,LocalMetricModuleFactory,class,39,209,158,2323,11.11,0,4,[],[],[],827,[],[],0
repos/datasets/src/datasets/load.py:MetricModule,MetricModule,class,3,4,4,24,6.0,0,0,[],[],[],701,[],[],0
repos/datasets/src/datasets/load.py:PackagedDatasetModuleFactory,PackagedDatasetModuleFactory,class,34,120,92,1519,12.66,0,4,[],[],[],1116,[],[],0
repos/datasets/src/datasets/load.py:_DatasetModuleFactory,_DatasetModuleFactory,class,2,6,6,60,10.0,0,0,[],[],[],706,[],[],0
repos/datasets/src/datasets/load.py:_InitializeConfiguredDatasetBuilder,_InitializeConfiguredDatasetBuilder,class,6,17,14,252,14.82,0,0,[],[],[],187,[],[],0
repos/datasets/src/datasets/load.py:_MetricModuleFactory,_MetricModuleFactory,class,2,6,6,59,9.83,0,0,[],[],[],711,[],[],0
repos/datasets/src/datasets/load.py:CachedDatasetModuleFactory:__init__,CachedDatasetModuleFactory:__init__,method,7,9,9,116,12.89,0,0,"['self', 'name', 'cache_dir', 'dynamic_modules_path', '']","[None, ' str', ' Optional[str] ', ' Optional[str] ', None]","[None, None, ' None', ' None', None]",727,[],[],0
repos/datasets/src/datasets/load.py:CachedDatasetModuleFactory:get_module,CachedDatasetModuleFactory:get_module,method,47,190,129,2365,12.45,0,6,['self'],[None],[None],927,[],"['init_dynamic_modules', 'os.listdir', 'len', '_get_modification_time', 'sorted', 'logger.warning', '_get_importable_file_path', '_load_importable_file', 'importlib.invalidate_caches', 'DatasetModule', 'camelcase_to_snakecase', 'glob.glob', 'FileNotFoundError']",13
repos/datasets/src/datasets/load.py:CachedMetricModuleFactory:__init__,CachedMetricModuleFactory:__init__,method,5,7,7,91,13.0,0,0,"['self', 'name', 'dynamic_modules_path', '']","[None, ' str', ' Optional[str] ', None]","[None, None, ' None', None]",727,[],[],0
repos/datasets/src/datasets/load.py:CachedMetricModuleFactory:get_module,CachedMetricModuleFactory:get_module,method,17,90,75,986,10.96,0,3,['self'],[None],[None],754,[],"['init_dynamic_modules', 'os.listdir', 'len', 'FileNotFoundError', '_get_modification_time', 'sorted', 'logger.warning', 'importlib.invalidate_caches', 'MetricModule']",9
repos/datasets/src/datasets/load.py:GithubMetricModuleFactory:__init__,GithubMetricModuleFactory:__init__,method,15,27,25,391,14.48,0,1,"['self', 'name', 'revision', 'Version]] ', 'download_config', 'download_mode', 'str]] ', 'dynamic_modules_path', 'trust_remote_code', '']","[None, ' str', ' Optional[Union[str', None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[str] ', ' Optional[str] ', None]","[None, None, None, ' None', ' None', None, ' None', ' None', ' None', None]",727,[],"['download_config.copy', 'DownloadConfig', 'increase_load_count']",3
repos/datasets/src/datasets/load.py:GithubMetricModuleFactory:download_loading_script,GithubMetricModuleFactory:download_loading_script,method,7,19,18,293,15.42,0,1,"['self', 'revision']","[None, ' Optional[str]']","[None, None]",747,[],"['hf_github_url', 'cached_path']",2
repos/datasets/src/datasets/load.py:GithubMetricModuleFactory:get_module,GithubMetricModuleFactory:get_module,method,35,220,156,2297,10.44,0,5,['self'],[None],[None],754,[],"['hf_github_url', 'warnings.warn', 'self.download_loading_script', 'logger.warning', 'get_imports', '_download_additional_modules', 'base_path=hf_github_url', 'init_dynamic_modules', 'files_to_hash', '_get_importable_file_path', 'resolve_trust_remote_code', '_create_importable_file', 'ValueError', '_load_importable_file', 'importlib.invalidate_caches', 'MetricModule']",16
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithParquetExport:__init__,HubDatasetModuleFactoryWithParquetExport:__init__,method,9,10,10,142,14.2,0,0,"['self', 'name', 'revision', 'download_config', '']","[None, ' str', ' Optional[str] ', ' Optional[DownloadConfig] ', None]","[None, None, ' None', ' None', None]",727,[],"['DownloadConfig', 'increase_load_count']",2
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithParquetExport:get_module,HubDatasetModuleFactoryWithParquetExport:get_module,method,37,83,67,1444,17.4,1,0,['self'],[None],[None],927,[],"['_dataset_viewer.get_exported_parquet_files', '_dataset_viewer.get_exported_dataset_infos', 'DatasetInfosDict', 'DatasetInfo.from_dict', 'HfApi', 'MetadataConfigs._from_exported_parquet_files_and_dataset_infos', 'create_builder_configs_from_metadata_configs', 'camelcase_to_snakecase', 'DatasetModule', 'builder_configs_parameters=BuilderConfigsParameters']",10
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithScript:__init__,HubDatasetModuleFactoryWithScript:__init__,method,15,16,16,263,16.44,0,0,"['self', 'name', 'revision', 'Version]] ', 'download_config', 'download_mode', 'str]] ', 'dynamic_modules_path', 'trust_remote_code', '']","[None, ' str', ' Optional[Union[str', None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[str] ', ' Optional[bool] ', None]","[None, None, None, ' None', ' None', None, ' None', ' None', ' None', None]",727,[],"['DownloadConfig', 'increase_load_count']",2
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithScript:download_dataset_infos_file,HubDatasetModuleFactoryWithScript:download_dataset_infos_file,method,7,24,22,359,14.96,0,1,['self'],[None],[None],1456,[],"['hf_dataset_url', 'cached_path']",2
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithScript:download_dataset_readme_file,HubDatasetModuleFactoryWithScript:download_dataset_readme_file,method,7,24,22,342,14.25,0,1,['self'],[None],[None],1470,[],"['hf_dataset_url', 'cached_path']",2
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithScript:download_loading_script,HubDatasetModuleFactoryWithScript:download_loading_script,method,7,18,17,290,16.11,0,1,['self'],[None],[None],1449,[],"['hf_dataset_url', 'cached_path']",2
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithScript:get_module,HubDatasetModuleFactoryWithScript:get_module,method,37,193,146,2387,12.37,0,6,['self'],[None],[None],927,[],"['warnings.warn', 'self.download_loading_script', 'self.download_dataset_infos_file', 'self.download_dataset_readme_file', 'get_imports', '_download_additional_modules', 'base_path=hf_dataset_url', 'additional_files.append', 'init_dynamic_modules', 'files_to_hash', '_get_importable_file_path', 'resolve_trust_remote_code', '_create_importable_file', 'ValueError', '_load_importable_file', 'importlib.invalidate_caches', 'hf_dataset_url', 'DatasetModule']",18
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithoutScript:__init__,HubDatasetModuleFactoryWithoutScript:__init__,method,15,16,16,225,14.06,0,0,"['self', 'name', 'revision', 'Version]] ', 'data_dir', 'data_files', 'List', 'Dict]] ', 'download_config', 'download_mode', 'str]] ', '']","[None, ' str', ' Optional[Union[str', None, ' Optional[str] ', ' Optional[Union[str', None, None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, None]","[None, None, None, ' None', ' None', None, None, ' None', ' None', None, ' None', None]",727,[],"['DownloadConfig', 'increase_load_count']",2
repos/datasets/src/datasets/load.py:HubDatasetModuleFactoryWithoutScript:get_module,HubDatasetModuleFactoryWithoutScript:get_module,method,97,324,193,5170,15.96,2,14,['self'],[None],[None],927,[],"['HfApi', 'cached_path', 'hf_dataset_url', 'DatasetCard.load', 'DatasetCardData', 'open', 'yaml.safe_load', 'dataset_card_data.to_dict', '_dataset_card_data_dict.update', 'MetadataConfigs.from_dataset_card_data', 'DatasetInfosDict.from_dataset_card_data', '_dataset_viewer.get_exported_dataset_infos', 'DatasetInfosDict', 'DatasetInfo.from_dict', 'exported_dataset_infos.update', 'sanitize_patterns', 'next', 'get_data_patterns', 'DataFilesDict.from_patterns', 'infer_module_for_data_files', 'data_files.filter_extensions', 'get_metadata_patterns', 'DataFilesList.from_patterns', 'DataFilesDict', 'data_files.items', 'create_builder_configs_from_metadata_configs', 'import_main_class', 'camelcase_to_snakecase', 'json.load', 'len', 'legacy_dataset_infos.pop', 'legacy_dataset_infos.update', 'DatasetModule', 'builder_configs_parameters=BuilderConfigsParameters']",34
repos/datasets/src/datasets/load.py:LocalDatasetModuleFactoryWithScript:__init__,LocalDatasetModuleFactoryWithScript:__init__,method,14,14,14,216,15.43,0,0,"['self', 'path', 'download_config', 'download_mode', 'str]] ', 'dynamic_modules_path', 'trust_remote_code', '']","[None, ' str', ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[str] ', ' Optional[bool] ', None]","[None, None, ' None', None, ' None', ' None', ' None', None]",727,[],"['Path', 'DownloadConfig']",2
repos/datasets/src/datasets/load.py:LocalDatasetModuleFactoryWithScript:get_module,LocalDatasetModuleFactoryWithScript:get_module,method,36,185,138,2294,12.4,0,6,['self'],[None],[None],927,[],"['warnings.warn', 'Path', 'get_imports', '_download_additional_modules', 'base_path=str', 'dataset_infos_path.is_file', 'additional_files.append', 'str', 'dataset_readme_path.is_file', 'init_dynamic_modules', 'files_to_hash', '_get_importable_file_path', 'resolve_trust_remote_code', '_create_importable_file', 'ValueError', '_load_importable_file', 'importlib.invalidate_caches', 'DatasetModule']",18
repos/datasets/src/datasets/load.py:LocalDatasetModuleFactoryWithoutScript:__init__,LocalDatasetModuleFactoryWithoutScript:__init__,method,12,25,25,255,10.2,0,1,"['self', 'path', 'data_dir', 'data_files', 'List', 'Dict]] ', 'download_mode', 'str]] ', '']","[None, ' str', ' Optional[str] ', ' Optional[Union[str', None, None, ' Optional[Union[DownloadMode', None, None]","[None, None, ' None', None, None, ' None', None, ' None', None]",727,[],"['ValueError', 'Path']",2
repos/datasets/src/datasets/load.py:LocalDatasetModuleFactoryWithoutScript:get_module,LocalDatasetModuleFactoryWithoutScript:get_module,method,69,206,146,3481,16.9,1,11,['self'],[None],[None],927,[],"['DatasetCard.load', 'DatasetCardData', 'open', 'yaml.safe_load', 'dataset_card_data.to_dict', '_dataset_card_data_dict.update', 'MetadataConfigs.from_dataset_card_data', 'DatasetInfosDict.from_dataset_card_data', 'Path', 'sanitize_patterns', 'next', 'get_data_patterns', 'DataFilesDict.from_patterns', 'infer_module_for_data_files', 'data_files.filter_extensions', 'get_metadata_patterns', 'DataFilesList.from_patterns', 'DataFilesDict', 'data_files.items', 'create_builder_configs_from_metadata_configs', 'import_main_class', 'camelcase_to_snakecase', 'DatasetInfosDict', 'DatasetInfo.from_dict', 'json.load', 'len', 'legacy_dataset_infos.pop', 'legacy_dataset_infos.update', 'Hasher.hash', 'DatasetModule', 'builder_configs_parameters=BuilderConfigsParameters']",31
repos/datasets/src/datasets/load.py:LocalMetricModuleFactory:__init__,LocalMetricModuleFactory:__init__,method,14,14,14,216,15.43,0,0,"['self', 'path', 'download_config', 'download_mode', 'str]] ', 'dynamic_modules_path', 'trust_remote_code', '']","[None, ' str', ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, ' Optional[str] ', ' Optional[str] ', None]","[None, None, ' None', None, ' None', ' None', ' None', None]",727,[],"['Path', 'DownloadConfig']",2
repos/datasets/src/datasets/load.py:LocalMetricModuleFactory:get_module,LocalMetricModuleFactory:get_module,method,27,164,121,1774,10.82,0,4,['self'],[None],[None],754,[],"['warnings.warn', 'get_imports', '_download_additional_modules', 'base_path=str', 'init_dynamic_modules', 'files_to_hash', '_get_importable_file_path', 'resolve_trust_remote_code', '_create_importable_file', 'ValueError', '_load_importable_file', 'importlib.invalidate_caches', 'MetricModule']",13
repos/datasets/src/datasets/load.py:PackagedDatasetModuleFactory:__init__,PackagedDatasetModuleFactory:__init__,method,11,12,12,184,15.33,0,0,"['self', 'name', 'data_dir', 'data_files', 'List', 'Dict]] ', 'download_config', 'download_mode', 'str]] ', '']","[None, ' str', ' Optional[str] ', ' Optional[Union[str', None, None, ' Optional[DownloadConfig] ', ' Optional[Union[DownloadMode', None, None]","[None, None, ' None', None, None, ' None', ' None', None, ' None', None]",727,[],['increase_load_count'],1
repos/datasets/src/datasets/load.py:PackagedDatasetModuleFactory:get_module,PackagedDatasetModuleFactory:get_module,method,24,83,61,1088,13.11,0,4,['self'],[None],[None],927,[],"['Path', 'sanitize_patterns', 'get_data_patterns', 'DataFilesDict.from_patterns', 'get_metadata_patterns', 'DataFilesList.from_patterns', 'DataFilesDict', 'data_files.items', 'DatasetModule']",9
repos/datasets/src/datasets/load.py:_DatasetModuleFactory:get_module,_DatasetModuleFactory:get_module,method,1,2,2,24,12.0,0,0,['self'],[None],[None],707,[],[],0
repos/datasets/src/datasets/load.py:_InitializeConfiguredDatasetBuilder:__call__,_InitializeConfiguredDatasetBuilder:__call__,method,5,11,10,179,16.27,0,0,"['self', 'builder_cls', 'metadata_configs', 'default_config_name', 'name']","[None, None, None, None, None]","[None, None, None, None, None]",196,[],"['_InitializeConfiguredDatasetBuilder', 'configure_builder_class']",2
repos/datasets/src/datasets/load.py:_MetricModuleFactory:get_module,_MetricModuleFactory:get_module,method,1,2,2,24,12.0,0,0,['self'],[None],[None],712,[],[],0
repos/datasets/src/datasets/metric.py:summarize_if_long_list,summarize_if_long_list,function,4,25,23,182,7.28,0,1,['obj'],[None],[None],69,[],"['type', 'len', 'format_chunk']",3
repos/datasets/src/datasets/metric.py:FileFreeLock,FileFreeLock,class,11,32,27,407,12.72,0,0,[],[],[],45,[],[],0
repos/datasets/src/datasets/metric.py:Metric,Metric,class,191,1199,525,11949,9.97,14,42,[],[],[],148,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin,MetricInfoMixin,class,28,95,40,1033,10.87,0,0,[],[],[],79,[],[],0
repos/datasets/src/datasets/metric.py:FileFreeLock:__init__,FileFreeLock:__init__,method,3,5,5,90,18.0,0,0,"['self', 'lock_file', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",48,[],"['FileLock', 'super']",2
repos/datasets/src/datasets/metric.py:FileFreeLock:_acquire,FileFreeLock:_acquire,method,5,16,15,202,12.62,0,0,['self'],[None],[None],52,[],[],0
repos/datasets/src/datasets/metric.py:FileFreeLock:_release,FileFreeLock:_release,method,1,2,2,31,15.5,0,0,['self'],[None],[None],63,[],[],0
repos/datasets/src/datasets/metric.py:Metric:__del__,Metric:__del__,method,6,42,24,307,7.31,0,4,['self'],[None],[None],644,[],['hasattr'],1
repos/datasets/src/datasets/metric.py:Metric:__init__,Metric:__init__,method,56,149,112,1594,10.7,0,5,"['self', 'config_name', 'keep_in_memory', 'cache_dir', 'num_process', 'process_id', 'seed', 'experiment_id', 'max_concurrent_cache_files', 'timeout', 'float] ', '**kwargs', '']","[None, ' Optional[str] ', ' bool ', ' Optional[str] ', ' int ', ' int ', ' Optional[int] ', ' Optional[str] ', ' int ', ' Union[int', None, None, None]","[None, ' None', ' False', ' None', ' 1', ' 0', ' None', ' None', ' 10000', None, ' 100', None, None]",175,[],"['self._info', 'camelcase_to_snakecase', 'MetricInfoMixin.__init__', 'isinstance', 'ValueError', 'self._build_data_dir', 'types.MethodType']",7
repos/datasets/src/datasets/metric.py:Metric:__len__,Metric:__len__,method,2,8,8,46,5.75,0,1,['self'],[None],[None],243,"['        """"""Return the number of examples (predictions or predictions/references pair)\n', ""        currently stored in the metric's cache.\n"", '        """"""\n']",['len'],1
repos/datasets/src/datasets/metric.py:Metric:__repr__,Metric:__repr__,method,1,14,13,137,9.79,0,0,['self'],[None],[None],249,[],"[""f'Metric""]",1
repos/datasets/src/datasets/metric.py:Metric:_build_data_dir,Metric:_build_data_dir,method,5,10,8,178,17.8,0,0,['self'],[None],[None],256,"['        """"""Path of this metric in cache_dir:\n', '        Will be:\n', '            self._data_dir_root/self.name/self.config_name/self.hash (if not none)/\n', '        If any of these element is missing or if ``with_version=False`` the corresponding subfolders are dropped.\n', '        """"""\n']",['os.makedirs'],1
repos/datasets/src/datasets/metric.py:Metric:_check_all_processes_locks,Metric:_check_all_processes_locks,method,8,39,36,479,12.28,1,0,['self'],[None],[None],335,[],"['range', 'FileFreeLock', 'nofilelock.acquire', 'ValueError', 'nofilelock.release']",5
repos/datasets/src/datasets/metric.py:Metric:_check_rendez_vous,Metric:_check_rendez_vous,method,13,51,40,702,13.76,0,0,['self'],[None],[None],351,[],"['FileFreeLock', 'nofilelock.acquire', 'ValueError', 'nofilelock.release', 'FileLock', 'rendez_vous_lock.acquire', 'rendez_vous_lock.release']",7
repos/datasets/src/datasets/metric.py:Metric:_compute,Metric:_compute,method,1,2,2,24,12.0,0,0,"['self', '*', 'predictions', 'references', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",640,"['        """"""This method defines the common API for all the metrics in the library""""""\n']",[],0
repos/datasets/src/datasets/metric.py:Metric:_create_cache_file,Metric:_create_cache_file,method,17,106,83,985,9.29,1,2,"['self', 'timeout']","[None, None]","[None, '1']",267,"['        """"""Create a new cache file. If the default cache file is used, we generated a new hash.""""""\n']","['range', 'FileLock', 'filelock.acquire', 'ValueError', 'str']",5
repos/datasets/src/datasets/metric.py:Metric:_download_and_prepare,Metric:_download_and_prepare,method,1,2,2,10,5.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",629,"['        """"""Downloads and prepares resources for the metric.\n', '\n', '        This is the internal implementation to overwrite called when user calls\n', '        `download_and_prepare`. It should download all required resources for the metric.\n', '\n', '        Args:\n', '            dl_manager (:class:`DownloadManager`): `DownloadManager` used to download and cache data.\n', '        """"""\n']",[],0
repos/datasets/src/datasets/metric.py:Metric:_finalize,Metric:_finalize,method,20,78,63,771,9.88,0,3,['self'],[None],[None],371,"['        """"""Close all the writing process and load/gather the data\n', '        from all the nodes if main node or all_process is True.\n', '        """"""\n']","['ArrowReader', 'info=DatasetInfo', 'Dataset.from_buffer', 'self._get_all_cache_files', 'Dataset', 'ValueError']",6
repos/datasets/src/datasets/metric.py:Metric:_get_all_cache_files,Metric:_get_all_cache_files,method,22,90,75,762,8.47,1,3,['self'],[None],[None],300,"['        """"""Get a lock on all the cache files in a distributed setup.\n', '        We wait for timeout second to let all the distributed node finish their tasks (default is 100 seconds).\n', '        """"""\n']","['ValueError', 'range', 'enumerate', 'filelocks.append', 'FileLock', 'filelock.acquire']",6
repos/datasets/src/datasets/metric.py:Metric:_info,Metric:_info,method,1,2,2,24,12.0,0,0,['self'],[None],[None],595,"['        """"""Construct the MetricInfo object. See `MetricInfo` for details.\n', '\n', '        Warning: This function is only called once and the result is cached for all\n', '        following .info() calls.\n', '\n', '        Returns:\n', '            info: (MetricInfo) The metrics information\n', '        """"""\n']",[],0
repos/datasets/src/datasets/metric.py:Metric:_init_writer,Metric:_init_writer,method,25,122,85,1206,9.89,0,6,"['self', 'timeout']","[None, None]","[None, '1']",556,[],"['FileLock', 'ValueError', 'pa.BufferOutputStream', 'ArrowWriter', 'self._create_cache_file', 'self._check_all_processes_locks', 'self._check_rendez_vous']",7
repos/datasets/src/datasets/metric.py:Metric:add,Metric:add,method,16,76,60,749,9.86,2,3,"['self', '*', 'prediction', 'reference', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",523,"['        """"""Add one prediction and reference for the metric\'s stack.\n', '\n', '        Args:\n', '            prediction (list/array/tensor, optional): Predictions.\n', '            reference (list/array/tensor, optional): References.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_metric\n', '        >>> metric = load_metric(""accuracy"")\n', '        >>> metric.add(predictions=model_predictions, references=labels)\n', '        ```\n', '        """"""\n']","['ValueError', 'self._init_writer']",2
repos/datasets/src/datasets/metric.py:Metric:add_batch,Metric:add_batch,method,24,132,92,1281,9.7,3,5,"['self', '*', 'predictions', 'references', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",476,"['        """"""Add a batch of predictions and references for the metric\'s stack.\n', '\n', '        Args:\n', '            predictions (list/array/tensor, optional): Predictions.\n', '            references (list/array/tensor, optional): References.\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_metric\n', '        >>> metric = load_metric(""accuracy"")\n', '        >>> metric.add_batch(predictions=model_prediction, references=labels)\n', '        ```\n', '        """"""\n']","['ValueError', 'self._init_writer', 'any', 'len', 'next', 'sorted']",6
repos/datasets/src/datasets/metric.py:Metric:compute,Metric:compute,method,42,139,78,1185,8.53,6,8,"['self', '*', 'predictions', 'references', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",405,"['        """"""Compute the metrics.\n', '\n', '        Usage of positional arguments is not allowed to prevent mistakes.\n', '\n', '        Args:\n', '            predictions (list/array/tensor, optional): Predictions.\n', '            references (list/array/tensor, optional): References.\n', '            **kwargs (optional): Keyword arguments that will be forwarded to the metrics :meth:`_compute`\n', '                method (see details in the docstring).\n', '\n', '        Return:\n', '            dict or None\n', '\n', '            - Dictionary with the metrics if this metric is run on the main process (``process_id == 0``).\n', '            - None if the metric is not run on the main process (``process_id != 0``).\n', '\n', '        Example:\n', '\n', '        ```py\n', '        >>> from datasets import load_metric\n', '        >>> metric = load_metric(""accuracy"")\n', '        >>> accuracy = metric.compute(predictions=model_prediction, references=labels)\n', '        ```\n', '        """"""\n']","['all_kwargs.update', 'ValueError', 'any', 'inputs.values', 'self.add_batch', 'self._finalize', 'temp_seed', 'self._compute', 'reversed', 'logger.info', 'os.remove', 'filelock.release']",12
repos/datasets/src/datasets/metric.py:Metric:download_and_prepare,Metric:download_and_prepare,method,8,22,17,327,14.86,0,2,"['self', 'download_config', 'dl_manager', '']","[None, ' Optional[DownloadConfig] ', ' Optional[DownloadManager] ', None]","[None, ' None', ' None', None]",606,"['        """"""Downloads and prepares dataset for reading.\n', '\n', '        Args:\n', '            download_config (:class:`DownloadConfig`, optional): Specific download configuration parameters.\n', '            dl_manager (:class:`DownloadManager`, optional): Specific download manager to use.\n', '        """"""\n']","['DownloadConfig', 'DownloadManager', 'self._download_and_prepare']",3
repos/datasets/src/datasets/metric.py:MetricInfoMixin:__init__,MetricInfoMixin:__init__,method,2,2,2,22,11.0,0,0,"['self', 'info']","[None, ' MetricInfo']","[None, None]",91,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:citation,MetricInfoMixin:citation,method,2,2,2,32,16.0,0,0,['self'],[None],[None],112,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:codebase_urls,MetricInfoMixin:codebase_urls,method,2,2,2,37,18.5,0,0,['self'],[None],[None],132,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:description,MetricInfoMixin:description,method,2,2,2,35,17.5,0,0,['self'],[None],[None],108,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:experiment_id,MetricInfoMixin:experiment_id,method,2,2,2,37,18.5,0,0,['self'],[None],[None],104,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:features,MetricInfoMixin:features,method,2,2,2,32,16.0,0,0,['self'],[None],[None],116,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:format,MetricInfoMixin:format,method,2,2,2,30,15.0,0,0,['self'],[None],[None],144,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:homepage,MetricInfoMixin:homepage,method,2,2,2,32,16.0,0,0,['self'],[None],[None],124,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:info,MetricInfoMixin:info,method,2,2,2,23,11.5,0,0,['self'],[None],[None],95,"['        """""":class:`datasets.MetricInfo` object containing all the metadata in the metric.""""""\n']",[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:inputs_description,MetricInfoMixin:inputs_description,method,2,2,2,42,21.0,0,0,['self'],[None],[None],120,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:license,MetricInfoMixin:license,method,2,2,2,31,15.5,0,0,['self'],[None],[None],128,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:name,MetricInfoMixin:name,method,2,2,2,35,17.5,0,0,['self'],[None],[None],100,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:reference_urls,MetricInfoMixin:reference_urls,method,2,2,2,38,19.0,0,0,['self'],[None],[None],136,[],[],0
repos/datasets/src/datasets/metric.py:MetricInfoMixin:streamable,MetricInfoMixin:streamable,method,2,2,2,34,17.0,0,0,['self'],[None],[None],140,[],[],0
repos/datasets/src/datasets/naming.py:camelcase_to_snakecase,camelcase_to_snakecase,function,5,8,6,114,14.25,0,0,['name'],[None],[None],34,"['    """"""Convert camel-case string to snake-case.""""""\n']","['_uppercase_uppercase_re.sub', '_lowercase_uppercase_re.sub', 'name.lower']",3
repos/datasets/src/datasets/naming.py:filename_prefix_for_name,filename_prefix_for_name,function,4,15,14,123,8.2,0,1,['name'],[None],[None],48,[],"['ValueError', 'camelcase_to_snakecase']",2
repos/datasets/src/datasets/naming.py:filename_prefix_for_split,filename_prefix_for_split,function,5,28,24,241,8.61,0,2,"['name', 'split']","[None, None]","[None, None]",54,[],"['ValueError', 're.match']",2
repos/datasets/src/datasets/naming.py:filenames_for_dataset_split,filenames_for_dataset_split,function,11,37,27,411,11.11,1,3,"['path', 'dataset_name', 'split', 'filetype_suffix', 'shard_lengths']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",70,[],"['filename_prefix_for_split', 'len', 'range']",3
repos/datasets/src/datasets/naming.py:filepattern_for_dataset_split,filepattern_for_dataset_split,function,6,13,12,161,12.38,0,1,"['dataset_name', 'split', 'data_dir', 'filetype_suffix']","[None, None, None, None]","[None, None, None, 'None']",62,[],['filename_prefix_for_split'],1
repos/datasets/src/datasets/naming.py:snakecase_to_camelcase,snakecase_to_camelcase,function,3,17,12,167,9.82,0,0,['name'],[None],[None],41,"['    """"""Convert snake-case string to camel-case string.""""""\n']",['_single_underscore_re.split'],1
repos/datasets/src/datasets/packaged_modules/__init__.py:_hash_python_lines,_hash_python_lines,function,10,22,21,237,10.77,1,1,['lines'],[' List[str]'],[None],20,[],"['re.sub', 'filtered_lines.append', 'full_str.encode', 'insecure_hashlib.sha256']",4
repos/datasets/src/datasets/parallel/parallel.py:_map_with_joblib,_map_with_joblib,function,9,20,20,232,11.6,1,0,"['function', 'iterable', 'num_proc', 'batched', 'batch_size', 'types', 'disable_tqdm', 'desc', 'single_map_nested_func']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None]",77,[],"['joblib.parallel_backend', 'joblib.Parallel', 'joblib.delayed']",3
repos/datasets/src/datasets/parallel/parallel.py:_map_with_multiprocessing_pool,_map_with_multiprocessing_pool,function,30,135,95,1037,7.68,3,3,"['function', 'iterable', 'num_proc', 'batched', 'batch_size', 'types', 'disable_tqdm', 'desc', 'single_map_nested_func']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None]",43,[],"['len', 'range', 'min', 'split_kwds.append', 'sum', 'ValueError', 'logger.info', 'Pool', 'pool.map']",9
repos/datasets/src/datasets/parallel/parallel.py:parallel_backend,parallel_backend,function,7,15,14,185,12.33,0,1,['backend_name'],[' str'],[None],93,"['    """"""\n', '    **Experimental.**  Configures the parallel backend for parallelized dataset loading, which uses the parallelization\n', '    implemented by joblib.\n', '\n', '    Args:\n', '        backend_name (str): Name of backend for parallelization implementation, has to be supported by joblib.\n', '\n', '     Example usage:\n', '     ```py\n', ""     with parallel_backend('spark'):\n"", '       dataset = load_dataset(..., num_proc=2)\n', '     ```\n', '    """"""\n']",['register_spark'],1
repos/datasets/src/datasets/parallel/parallel.py:parallel_map,parallel_map,function,4,28,18,295,10.54,0,1,"['function', 'iterable', 'num_proc', 'batched', 'batch_size', 'types', 'disable_tqdm', 'desc', 'single_map_nested_func']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None]",17,"['    """"""\n', '    **Experimental.** Apply a function to iterable elements in parallel, where the implementation uses either\n', '    multiprocessing.Pool or joblib for parallelization.\n', '\n', '    Args:\n', '        function (`Callable[[Any], Any]`): Function to be applied to `iterable`.\n', '        iterable (`list`, `tuple` or `np.ndarray`): Iterable elements to apply function to.\n', '        num_proc (`int`): Number of processes (if no backend specified) or jobs (using joblib).\n', '        types (`tuple`): Additional types (besides `dict` values) to apply `function` recursively to their elements.\n', '        disable_tqdm (`bool`): Whether to disable the tqdm progressbar.\n', '        desc (`str`): Prefix for the tqdm progressbar.\n', '        single_map_nested_func (`Callable`): Map function that applies `function` to an element from `iterable`.\n', '            Takes a tuple of function, data_struct, types, rank, disable_tqdm, desc as input, where data_struct is an\n', '            element of `iterable`, and `rank` is used for progress bar.\n', '    """"""\n']","['_map_with_multiprocessing_pool', '_map_with_joblib']",2
repos/datasets/src/datasets/parallel/parallel.py:ParallelBackendConfig,ParallelBackendConfig,class,1,2,2,17,8.5,0,0,[],[],[],12,[],[],0
repos/datasets/src/datasets/search.py:BaseIndex,BaseIndex,class,16,54,40,507,9.39,1,0,[],[],[],60,[],[],0
repos/datasets/src/datasets/search.py:BatchedNearestExamplesResults,BatchedNearestExamplesResults,class,4,4,4,56,14.0,0,0,[],[],[],55,[],[],0
repos/datasets/src/datasets/search.py:BatchedSearchResults,BatchedSearchResults,class,4,4,4,60,15.0,0,0,[],[],[],45,[],[],0
repos/datasets/src/datasets/search.py:ElasticSearchIndex,ElasticSearchIndex,class,74,311,219,2872,9.23,4,5,[],[],[],98,[],[],0
repos/datasets/src/datasets/search.py:FaissIndex,FaissIndex,class,74,576,300,4960,8.61,1,23,[],[],[],215,[],[],0
repos/datasets/src/datasets/search.py:IndexableMixin,IndexableMixin,class,62,477,229,5061,10.61,4,7,[],[],[],417,[],[],0
repos/datasets/src/datasets/search.py:MissingIndex,MissingIndex,class,0,1,1,4,4.0,0,0,[],[],[],36,[],[],0
repos/datasets/src/datasets/search.py:NearestExamplesResults,NearestExamplesResults,class,3,4,4,32,8.0,0,0,[],[],[],50,[],[],0
repos/datasets/src/datasets/search.py:SearchResults,SearchResults,class,4,4,4,36,9.0,0,0,[],[],[],40,[],[],0
repos/datasets/src/datasets/search.py:BaseIndex:load,BaseIndex:load,method,1,2,2,24,12.0,0,0,"['cls', 'file', 'PurePath]']","[None, ' Union[str', None]","[None, None, None]",93,"['        """"""Deserialize the index from disk""""""\n']",[],0
repos/datasets/src/datasets/search.py:BaseIndex:save,BaseIndex:save,method,1,2,2,24,12.0,0,0,"['self', 'file', 'PurePath]']","[None, ' Union[str', None]","[None, None, None]",88,"['        """"""Serialize the index on disk""""""\n']",[],0
repos/datasets/src/datasets/search.py:BaseIndex:search,BaseIndex:search,method,1,2,2,24,12.0,0,0,"['self', 'query', 'k', '**kwargs']","[None, None, ' int ', None]","[None, None, ' 10', None]",63,"['        """"""\n', '        To implement.\n', '        This method has to return the scores and the indices of the retrieved examples given a certain query.\n', '        """"""\n']",[],0
repos/datasets/src/datasets/search.py:BaseIndex:search_batch,BaseIndex:search_batch,method,11,17,17,200,11.76,1,0,"['self', 'queries', 'k', '**kwargs']","[None, None, ' int ', None]","[None, None, ' 10', None]",70,"['        """"""Find the nearest examples indices to the query.\n', '\n', '        Args:\n', '            queries (`Union[List[str], np.ndarray]`): The queries as a list of strings if `column` is a text index or as a numpy array if `column` is a vector index.\n', '            k (`int`): The number of examples to retrieve per query.\n', '\n', '        Ouput:\n', '            total_scores (`List[List[float]`): The retrieval scores of the retrieved examples per query.\n', '            total_indices (`List[List[int]]`): The indices of the retrieved examples per query.\n', '        """"""\n']","['self.search', 'total_scores.append', 'total_indices.append', 'BatchedSearchResults']",4
repos/datasets/src/datasets/search.py:ElasticSearchIndex:__init__,ElasticSearchIndex:__init__,method,28,123,86,857,6.97,0,3,"['self', 'host', 'port', 'es_client', 'es_index_name', 'es_index_config', '']","[None, ' Optional[str] ', ' Optional[int] ', ' Optional[""Elasticsearch""] ', ' Optional[str] ', ' Optional[dict] ', None]","[None, ' None', ' None', ' None', ' None', ' None', None]",108,[],"['ImportError', 'ValueError', 'Elasticsearch', 'str']",4
repos/datasets/src/datasets/search.py:ElasticSearchIndex:add_documents,ElasticSearchIndex:add_documents,method,24,76,61,775,10.2,3,2,"['self', 'documents', '""Dataset""]', 'column']","[None, ' Union[List[str]', None, ' Optional[str] ']","[None, None, None, ' None']",146,"['        """"""\n', '        Add documents to the index.\n', '        If the documents are inside a certain column, you can specify it using the `column` argument.\n', '        """"""\n']","['len', 'hf_tqdm', 'passage_generator', 'enumerate', 'actions=passage_generator', 'progress.update', 'logger.warning', 'logger.info']",8
repos/datasets/src/datasets/search.py:ElasticSearchIndex:search,ElasticSearchIndex:search,method,6,28,25,275,9.82,0,0,"['self', 'query', 'k', '**kwargs']","[None, ' str', None, None]","[None, None, '10', None]",182,"['        """"""Find the nearest examples indices to the query.\n', '\n', '        Args:\n', '            query (`str`): The query as a string.\n', '            k (`int`): The number of examples to retrieve.\n', '\n', '        Ouput:\n', '            scores (`List[List[float]`): The retrieval scores of the retrieved examples.\n', '            indices (`List[List[int]]`): The indices of the retrieved examples.\n', '        """"""\n']",['SearchResults'],1
repos/datasets/src/datasets/search.py:ElasticSearchIndex:search_batch,ElasticSearchIndex:search_batch,method,20,39,36,548,14.05,1,0,"['self', 'queries', 'k', 'max_workers', '**kwargs']","[None, None, ' int ', None, None]","[None, None, ' 10', '10', None]",201,[],"['len', 'enumerate', 'future.result', 'BatchedSearchResults']",4
repos/datasets/src/datasets/search.py:FaissIndex:__init__,FaissIndex:__init__,method,12,130,86,775,5.96,0,3,"['self', 'device', 'List[int]]] ', 'string_factory', 'metric_type', 'custom_index', '']","[None, ' Optional[Union[int', None, ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', None]","[None, None, ' None', ' None', ' None', ' None', None]",108,[],"['ValueError', 'ImportError']",2
repos/datasets/src/datasets/search.py:FaissIndex:_faiss_index_to_device,FaissIndex:_faiss_index_to_device,method,14,65,51,486,7.48,0,3,"['index', 'device', 'List[int]]] ']","[' ""faiss.Index""', ' Optional[Union[int', None]","[None, None, ' None']",316,"['        """"""\n', '        Sends a faiss index to a device.\n', '        A device can either be a positive integer (GPU id), a negative integer (all GPUs),\n', '            or a list of positive integers (select GPUs to use), or `None` for CPU.\n', '        """"""\n']","['isinstance', 'faiss.StandardGpuResources', 'faiss.index_cpu_to_gpu', 'faiss.index_cpu_to_all_gpus', 'faiss.index_cpu_to_gpus_list', 'gpus=list', 'TypeError']",7
repos/datasets/src/datasets/search.py:FaissIndex:add_vectors,FaissIndex:add_vectors,method,35,174,93,1654,9.51,1,12,"['self', 'vectors', '""Dataset""]', 'column', 'batch_size', 'train_size', 'faiss_verbose', '']","[None, ' Union[np.array', None, ' Optional[str] ', ' int ', ' Optional[int] ', ' Optional[bool] ', None]","[None, None, None, ' None', ' 1000', ' None', ' None', None]",255,"['        """"""\n', '        Add vectors to the index.\n', '        If the arrays are inside a certain column, you can specify it using the `column` argument.\n', '        """"""\n']","['isinstance', 'ValueError', 'len', 'faiss.index_factory', 'faiss.IndexFlat', 'self._faiss_index_to_device', 'logger.info', 'hasattr', 'hf_tqdm']",9
repos/datasets/src/datasets/search.py:FaissIndex:load,FaissIndex:load,method,13,22,21,296,13.45,0,0,"['cls', 'file', 'PurePath]', 'device', 'List[int]]] ', 'storage_options', '']","[None, ' Union[str', None, ' Optional[Union[int', None, ' Optional[Dict] ', None]","[None, None, None, None, ' None', ' None', None]",400,"['        """"""Deserialize the FaissIndex from disk""""""\n']","['cls', 'fsspec.open', 'faiss.read_index', 'faiss_index._faiss_index_to_device']",4
repos/datasets/src/datasets/search.py:FaissIndex:save,FaissIndex:save,method,13,30,29,307,10.23,0,1,"['self', 'file', 'PurePath]', 'storage_options']","[None, ' Union[str', None, ' Optional[Dict] ']","[None, None, None, ' None']",387,"['        """"""Serialize the FaissIndex on disk""""""\n']","['isinstance', 'faiss.index_gpu_to_cpu', 'fsspec.open', 'faiss.write_index', 'faiss.BufferedIOWriter']",5
repos/datasets/src/datasets/search.py:FaissIndex:search,FaissIndex:search,method,10,44,41,353,8.02,0,2,"['self', 'query', 'k', '**kwargs']","[None, ' np.array', None, None]","[None, None, '10', None]",349,"['        """"""Find the nearest examples indices to the query.\n', '\n', '        Args:\n', '            query (`np.array`): The query as a numpy array.\n', '            k (`int`): The number of examples to retrieve.\n', '\n', '        Ouput:\n', '            scores (`List[List[float]`): The retrieval scores of the retrieved examples.\n', '            indices (`List[List[int]]`): The indices of the retrieved examples.\n', '        """"""\n']","['len', 'ValueError', 'query.reshape', 'np.asarray', 'SearchResults']",5
repos/datasets/src/datasets/search.py:FaissIndex:search_batch,FaissIndex:search_batch,method,9,24,23,249,10.38,0,2,"['self', 'queries', 'k', '**kwargs']","[None, ' np.array', None, None]","[None, None, '10', None]",369,"['        """"""Find the nearest examples indices to the queries.\n', '\n', '        Args:\n', '            queries (`np.array`): The queries as a numpy array.\n', '            k (`int`): The number of examples to retrieve.\n', '\n', '        Ouput:\n', '            total_scores (`List[List[float]`): The retrieval scores of the retrieved examples per query.\n', '            total_indices (`List[List[int]]`): The indices of the retrieved examples per query.\n', '        """"""\n']","['len', 'ValueError', 'np.asarray', 'BatchedSearchResults', 'indices.astype']",5
repos/datasets/src/datasets/search.py:IndexableMixin:__getitem__,IndexableMixin:__getitem__,method,1,2,2,24,12.0,0,0,"['self', 'key']","[None, None]","[None, None]",426,[],[],0
repos/datasets/src/datasets/search.py:IndexableMixin:__init__,IndexableMixin:__init__,method,3,4,4,36,9.0,0,0,['self'],[None],[None],420,[],[],0
repos/datasets/src/datasets/search.py:IndexableMixin:__len__,IndexableMixin:__len__,method,1,2,2,24,12.0,0,0,['self'],[None],[None],423,[],[],0
repos/datasets/src/datasets/search.py:IndexableMixin:_check_index_is_initialized,IndexableMixin:_check_index_is_initialized,method,2,23,22,194,8.43,0,1,"['self', 'index_name']","[None, ' str']","[None, None]",432,[],"['self.is_index_initialized', 'MissingIndex']",2
repos/datasets/src/datasets/search.py:IndexableMixin:add_elasticsearch_index,IndexableMixin:add_elasticsearch_index,method,0,13,13,124,9.54,0,0,"['self', 'column', 'index_name', 'host', 'port', 'es_client', 'es_index_name', 'es_index_config', '']","[None, ' str', ' Optional[str] ', ' Optional[str] ', ' Optional[int] ', ' Optional[""Elasticsearch""] ', ' Optional[str] ', ' Optional[dict] ', None]","[None, None, ' None', ' None', ' None', ' None', ' None', ' None', None]",585,"['        """"""Add a text index using ElasticSearch for fast retrieval.\n', '\n', '        Args:\n', '            column (`str`): The column of the documents to add to the index.\n', '            index_name (Optional `str`): The index_name/identifier of the index. This is the index name that is used to call `.get_nearest` or `.search`.\n', '                By default it corresponds to `column`.\n', '            host (Optional `str`, defaults to localhost):\n', '                host of where ElasticSearch is running\n', '            port (Optional `str`, defaults to 9200):\n', '                port of where ElasticSearch is running\n', '            es_client (Optional `elasticsearch.Elasticsearch`):\n', '                The elasticsearch client used to create the index if host and port are None.\n', '            es_index_name (Optional `str`): The elasticsearch index name used to create the index.\n', '            es_index_config (Optional `dict`):\n', '                The configuration of the elasticsearch index.\n', '                Default config is:\n', '\n', '        Config::\n', '\n', '            {\n', '                ""settings"": {\n', '                    ""number_of_shards"": 1,\n', '                    ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},\n', '                },\n', '                ""mappings"": {\n', '                    ""properties"": {\n', '                        ""text"": {\n', '                            ""type"": ""text"",\n', '                            ""analyzer"": ""standard"",\n', '                            ""similarity"": ""BM25""\n', '                        },\n', '                    }\n', '                },\n', '            }\n', '        """"""\n']",[],0
repos/datasets/src/datasets/search.py:IndexableMixin:add_faiss_index,IndexableMixin:add_faiss_index,method,6,25,21,328,13.12,0,1,"['self', 'column', 'index_name', 'device', 'List[int]]] ', 'string_factory', 'metric_type', 'custom_index', 'batch_size', 'train_size', 'faiss_verbose', '']","[None, ' str', ' Optional[str] ', ' Optional[Union[int', None, ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', ' int ', ' Optional[int] ', ' bool ', None]","[None, None, ' None', None, ' None', ' None', ' None', ' None', ' 1000', ' None', ' False', None]",454,"['        """"""Add a dense index using Faiss for fast retrieval.\n', '        The index is created using the vectors of the specified column.\n', '        You can specify `device` if you want to run it on GPU (`device` must be the GPU index, see more below).\n', '        You can find more information about Faiss here:\n', '        - For `string factory`: https://github.com/facebookresearch/faiss/wiki/The-index-factory\n', '\n', '        Args:\n', '            column (`str`): The column of the vectors to add to the index.\n', '            index_name (Optional `str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n', '                By default it corresponds to `column`.\n', '            device (Optional `Union[int, List[int]]`): If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n', '                If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n', '            string_factory (Optional `str`): This is passed to the index factory of Faiss to create the index. Default index class is IndexFlatIP.\n', '            metric_type (Optional `int`): Type of metric. Ex: `faiss.METRIC_INNER_PRODUCT` or `faiss.METRIC_L2`.\n', '            custom_index (Optional `faiss.Index`): Custom Faiss index that you already have instantiated and configured for your needs.\n', '            batch_size (Optional `int`): Size of the batch to use while adding vectors to the FaissIndex. Default value is 1000.\n', '                <Added version=""2.4.0""/>\n', '            train_size (Optional `int`): If the index needs a training step, specifies how many vectors will be used to train the index.\n', '            faiss_verbose (`bool`, defaults to False): Enable the verbosity of the Faiss index.\n', '        """"""\n']","['FaissIndex', 'faiss_index.add_vectors']",2
repos/datasets/src/datasets/search.py:IndexableMixin:add_faiss_index_from_external_arrays,IndexableMixin:add_faiss_index_from_external_arrays,method,4,16,14,284,17.75,0,0,"['self', 'external_arrays', 'index_name', 'device', 'List[int]]] ', 'string_factory', 'metric_type', 'custom_index', 'batch_size', 'train_size', 'faiss_verbose', '']","[None, ' np.array', ' str', ' Optional[Union[int', None, ' Optional[str] ', ' Optional[int] ', ' Optional[""faiss.Index""] ', ' int ', ' Optional[int] ', ' bool ', None]","[None, None, None, None, ' None', ' None', ' None', ' None', ' 1000', ' None', ' False', None]",495,"['        """"""Add a dense index using Faiss for fast retrieval.\n', '        The index is created using the vectors of `external_arrays`.\n', '        You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n', '        You can find more information about Faiss here:\n', '        - For `string factory`: https://github.com/facebookresearch/faiss/wiki/The-index-factory\n', '\n', '        Args:\n', '            external_arrays (`np.array`): If you want to use arrays from outside the lib for the index, you can set `external_arrays`.\n', '                It will use `external_arrays` to create the Faiss index instead of the arrays in the given `column`.\n', '            index_name (`str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n', '            device (Optional `Union[int, List[int]]`): If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n', '                If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n', '            string_factory (Optional `str`): This is passed to the index factory of Faiss to create the index. Default index class is IndexFlatIP.\n', '            metric_type (Optional `int`): Type of metric. Ex: `faiss.METRIC_INNER_PRODUCT` or `faiss.METRIC_L2`.\n', '            custom_index (Optional `faiss.Index`): Custom Faiss index that you already have instantiated and configured for your needs.\n', '            batch_size (Optional `int`): Size of the batch to use while adding vectors to the FaissIndex. Default value is 1000.\n', '                <Added version=""2.4.0""/>\n', '            train_size (Optional `int`): If the index needs a training step, specifies how many vectors will be used to train the index.\n', '            faiss_verbose (`bool`, defaults to False): Enable the verbosity of the Faiss index.\n', '        """"""\n']","['FaissIndex', 'faiss_index.add_vectors']",2
repos/datasets/src/datasets/search.py:IndexableMixin:drop_index,IndexableMixin:drop_index,method,2,2,2,28,14.0,0,0,"['self', 'index_name']","[None, ' str']","[None, None]",684,"['        """"""Drop the index with the specified column.\n', '\n', '        Args:\n', '            index_name (`str`):\n', '                The `index_name`/identifier of the index.\n', '        """"""\n']",[],0
repos/datasets/src/datasets/search.py:IndexableMixin:get_index,IndexableMixin:get_index,method,3,3,3,76,25.33,0,0,"['self', 'index_name']","[None, ' str']","[None, None]",442,"['        """"""List the `index_name`/identifiers of all the attached indexes.\n', '\n', '        Args:\n', '            index_name (`str`): Index name.\n', '\n', '        Returns:\n', '            [`BaseIndex`]\n', '        """"""\n']",['self._check_index_is_initialized'],1
repos/datasets/src/datasets/search.py:IndexableMixin:get_nearest_examples,IndexableMixin:get_nearest_examples,method,8,20,18,209,10.45,1,1,"['self', 'index_name', 'query', 'np.array]', 'k', '**kwargs']","[None, ' str', ' Union[str', None, ' int ', None]","[None, None, None, None, ' 10', None]",735,"['        """"""Find the nearest examples in the dataset to the query.\n', '\n', '        Args:\n', '            index_name (`str`):\n', '                The index_name/identifier of the index.\n', '            query (`Union[str, np.ndarray]`):\n', '                The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n', '            k (`int`):\n', '                The number of examples to retrieve.\n', '\n', '        Returns:\n', '            `(scores, examples)`:\n', '                A tuple of `(scores, examples)` where:\n', '                - **scores** (`List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples\n', '                - **examples** (`dict`): the retrieved examples\n', '        """"""\n']","['self._check_index_is_initialized', 'self.search', 'NearestExamplesResults', 'len']",4
repos/datasets/src/datasets/search.py:IndexableMixin:get_nearest_examples_batch,IndexableMixin:get_nearest_examples_batch,method,14,41,29,367,8.95,3,1,"['self', 'index_name', 'queries', 'np.array]', 'k', '**kwargs']","[None, ' str', ' Union[List[str]', None, ' int ', None]","[None, None, None, None, ' 10', None]",759,"['        """"""Find the nearest examples in the dataset to the query.\n', '\n', '        Args:\n', '            index_name (`str`):\n', '                The `index_name`/identifier of the index.\n', '            queries (`Union[List[str], np.ndarray]`):\n', '                The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n', '            k (`int`):\n', '                The number of examples to retrieve per query.\n', '\n', '        Returns:\n', '            `(total_scores, total_examples)`:\n', '                A tuple of `(total_scores, total_examples)` where:\n', '                - **total_scores** (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query\n', '                - **total_examples** (`List[dict]`): the retrieved examples per query\n', '        """"""\n']","['self._check_index_is_initialized', 'self.search_batch', 'len', 'zip', 'BatchedNearestExamplesResults']",5
repos/datasets/src/datasets/search.py:IndexableMixin:is_index_initialized,IndexableMixin:is_index_initialized,method,3,4,4,31,7.75,0,0,"['self', 'index_name']","[None, ' str']","[None, None]",429,[],[],0
repos/datasets/src/datasets/search.py:IndexableMixin:list_indexes,IndexableMixin:list_indexes,method,1,2,2,25,12.5,0,0,['self'],[None],[None],438,"['        """"""List the `colindex_nameumns`/identifiers of all the attached indexes.""""""\n']",['list'],1
repos/datasets/src/datasets/search.py:IndexableMixin:load_elasticsearch_index,IndexableMixin:load_elasticsearch_index,method,2,8,8,147,18.38,0,0,"['self', 'index_name', 'es_index_name', 'host', 'port', 'es_client', 'es_index_config', '']","[None, ' str', ' str', ' Optional[str] ', ' Optional[int] ', ' Optional[""Elasticsearch""] ', ' Optional[dict] ', None]","[None, None, None, ' None', ' None', ' None', ' None', None]",637,"['        """"""Load an existing text index using ElasticSearch for fast retrieval.\n', '\n', '        Args:\n', '            index_name (`str`):\n', '                The `index_name`/identifier of the index. This is the index name that is used to call `get_nearest` or `search`.\n', '            es_index_name (`str`):\n', '                The name of elasticsearch index to load.\n', '            host (`str`, *optional*, defaults to `localhost`):\n', '                Host of where ElasticSearch is running.\n', '            port (`str`, *optional*, defaults to `9200`):\n', '                Port of where ElasticSearch is running.\n', '            es_client (`elasticsearch.Elasticsearch`, *optional*):\n', '                The elasticsearch client used to create the index if host and port are `None`.\n', '            es_index_config (`dict`, *optional*):\n', '                The configuration of the elasticsearch index.\n', '                Default config is:\n', '                    ```\n', '                    {\n', '                        ""settings"": {\n', '                            ""number_of_shards"": 1,\n', '                            ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},\n', '                        },\n', '                        ""mappings"": {\n', '                            ""properties"": {\n', '                                ""text"": {\n', '                                    ""type"": ""text"",\n', '                                    ""analyzer"": ""standard"",\n', '                                    ""similarity"": ""BM25""\n', '                                },\n', '                            }\n', '                        },\n', '                    }\n', '                    ```\n', '        """"""\n']",['ElasticSearchIndex'],1
repos/datasets/src/datasets/search.py:IndexableMixin:load_faiss_index,IndexableMixin:load_faiss_index,method,6,37,36,359,9.7,0,1,"['self', 'index_name', 'file', 'PurePath]', 'device', 'List[int]]] ', 'storage_options', '']","[None, ' str', ' Union[str', None, ' Optional[Union[int', None, ' Optional[Dict] ', None]","[None, None, None, None, None, ' None', ' None', None]",553,"['        """"""Load a FaissIndex from disk.\n', '\n', '        If you want to do additional configurations, you can have access to the faiss index object by doing\n', '        `.get_index(index_name).faiss_index` to make it fit your needs.\n', '\n', '        Args:\n', '            index_name (`str`): The index_name/identifier of the index. This is the index_name that is used to\n', '                call `.get_nearest` or `.search`.\n', '            file (`str`): The path to the serialized faiss index on disk or remote URI (e.g. `""s3://my-bucket/index.faiss""`).\n', '            device (Optional `Union[int, List[int]]`): If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n', '                If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.11.0""/>\n', '\n', '        """"""\n']","['FaissIndex.load', 'len', 'ValueError', 'logger.info']",4
repos/datasets/src/datasets/search.py:IndexableMixin:save_faiss_index,IndexableMixin:save_faiss_index,method,6,23,20,243,10.57,0,1,"['self', 'index_name', 'file', 'PurePath]', 'storage_options']","[None, ' str', ' Union[str', None, ' Optional[Dict] ']","[None, None, None, None, ' None']",535,"['        """"""Save a FaissIndex on disk.\n', '\n', '        Args:\n', '            index_name (`str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n', '            file (`str`): The path to the serialized faiss index on disk or remote URI (e.g. `""s3://my-bucket/index.faiss""`).\n', '            storage_options (`dict`, *optional*):\n', '                Key/value pairs to be passed on to the file-system backend, if any.\n', '\n', '                <Added version=""2.11.0""/>\n', '\n', '        """"""\n']","['self.get_index', 'isinstance', 'ValueError', 'index.save', 'logger.info']",5
repos/datasets/src/datasets/search.py:IndexableMixin:search,IndexableMixin:search,method,3,5,5,101,20.2,0,0,"['self', 'index_name', 'query', 'np.array]', 'k', '**kwargs']","[None, ' str', ' Union[str', None, ' int ', None]","[None, None, None, None, ' 10', None]",693,"['        """"""Find the nearest examples indices in the dataset to the query.\n', '\n', '        Args:\n', '            index_name (`str`):\n', '                The name/identifier of the index.\n', '            query (`Union[str, np.ndarray]`):\n', '                The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n', '            k (`int`):\n', '                The number of examples to retrieve.\n', '\n', '        Returns:\n', '            `(scores, indices)`:\n', '                A tuple of `(scores, indices)` where:\n', '                - **scores** (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples\n', '                - **indices** (`List[List[int]]`): the indices of the retrieved examples\n', '        """"""\n']",['self._check_index_is_initialized'],1
repos/datasets/src/datasets/search.py:IndexableMixin:search_batch,IndexableMixin:search_batch,method,3,5,5,109,21.8,0,0,"['self', 'index_name', 'queries', 'np.array]', 'k', '**kwargs']","[None, ' str', ' Union[List[str]', None, ' int ', None]","[None, None, None, None, ' 10', None]",713,"['        """"""Find the nearest examples indices in the dataset to the query.\n', '\n', '        Args:\n', '            index_name (`str`):\n', '                The `index_name`/identifier of the index.\n', '            queries (`Union[List[str], np.ndarray]`):\n', '                The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n', '            k (`int`):\n', '                The number of examples to retrieve per query.\n', '\n', '        Returns:\n', '            `(total_scores, total_indices)`:\n', '                A tuple of `(total_scores, total_indices)` where:\n', '                - **total_scores** (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query\n', '                - **total_indices** (`List[List[int]]`): the indices of the retrieved examples per query\n', '        """"""\n']",['self._check_index_is_initialized'],1
repos/datasets/src/datasets/splits.py:NamedSplit,NamedSplit,class,0,1,1,1,1.0,0,0,[],[],[],314,[],[],0
repos/datasets/src/datasets/splits.py:NamedSplitAll,NamedSplitAll,class,7,19,16,248,13.05,0,0,[],[],[],391,[],[],0
repos/datasets/src/datasets/splits.py:PercentSlice,PercentSlice,class,0,1,1,4,4.0,0,0,[],[],[],260,[],[],0
repos/datasets/src/datasets/splits.py:PercentSliceMeta,PercentSliceMeta,class,5,19,18,167,8.79,0,1,[],[],[],253,[],[],0
repos/datasets/src/datasets/splits.py:Split,Split,class,8,18,18,182,10.11,0,0,[],[],[],406,[],[],0
repos/datasets/src/datasets/splits.py:SplitBase,SplitBase,class,30,220,139,1569,7.13,2,6,[],[],[],81,[],[],0
repos/datasets/src/datasets/splits.py:SplitDict,SplitDict,class,43,167,113,1852,11.09,4,6,[],[],[],519,[],[],0
repos/datasets/src/datasets/splits.py:SplitGenerator,SplitGenerator,class,11,31,30,295,9.52,0,0,[],[],[],602,[],[],0
repos/datasets/src/datasets/splits.py:SplitInfo,SplitInfo,class,14,36,28,633,17.58,0,0,[],[],[],32,[],[],0
repos/datasets/src/datasets/splits.py:SplitReadInstruction,SplitReadInstruction,class,22,75,58,938,12.51,1,2,[],[],[],464,[],[],0
repos/datasets/src/datasets/splits.py:SubSplitInfo,SubSplitInfo,class,7,12,9,179,14.92,0,0,[],[],[],59,[],[],0
repos/datasets/src/datasets/splits.py:_SplitMerged,_SplitMerged,class,13,23,19,356,15.48,0,0,[],[],[],276,[],[],0
repos/datasets/src/datasets/splits.py:_SubSplit,_SubSplit,class,13,45,34,550,12.22,0,1,[],[],[],292,[],[],0
repos/datasets/src/datasets/splits.py:NamedSplitAll:__init__,NamedSplitAll:__init__,method,1,1,1,23,23.0,0,0,['self'],[None],[None],394,[],['super'],1
repos/datasets/src/datasets/splits.py:NamedSplitAll:__repr__,NamedSplitAll:__repr__,method,1,2,2,23,11.5,0,0,['self'],[None],[None],397,[],[],0
repos/datasets/src/datasets/splits.py:NamedSplitAll:get_read_instruction,NamedSplitAll:get_read_instruction,method,3,9,9,120,13.33,0,0,"['self', 'split_dict']","[None, None]","[None, None]",400,[],"['split_dict.values', 'sum', 'SplitReadInstruction']",3
repos/datasets/src/datasets/splits.py:PercentSliceMeta:__getitem__,PercentSliceMeta:__getitem__,method,4,16,15,134,8.38,0,1,"['cls', 'slice_value']","[None, None]","[None, None]",254,[],"['isinstance', 'ValueError']",2
repos/datasets/src/datasets/splits.py:Split:__new__,Split:__new__,method,2,7,7,54,7.71,0,0,"['cls', 'name']","[None, None]","[None, None]",449,"['        """"""Create a custom split with datasets.Split(\'custom_name\').""""""\n']","['NamedSplitAll', 'NamedSplit']",2
repos/datasets/src/datasets/splits.py:SplitBase:__add__,SplitBase:__add__,method,2,3,3,30,10.0,0,0,"['self', 'other']","[None, None]","[None, None]",138,"['        """"""Merging: datasets.Split.TRAIN + datasets.Split.TEST.""""""\n']",['_SplitMerged'],1
repos/datasets/src/datasets/splits.py:SplitBase:__eq__,SplitBase:__eq__,method,3,14,14,126,9.0,0,1,"['self', 'other']","[None, None]","[None, None]",128,"['        """"""Equality: datasets.Split.TRAIN == \'train\'.""""""\n']","['isinstance', 'NotImplementedError']",2
repos/datasets/src/datasets/splits.py:SplitBase:__ne__,SplitBase:__ne__,method,2,3,3,27,9.0,0,0,"['self', 'other']","[None, None]","[None, None]",134,"['        """"""InEquality: datasets.Split.TRAIN != \'test\'.""""""\n']",['self.__eq__'],1
repos/datasets/src/datasets/splits.py:SplitBase:get_read_instruction,SplitBase:get_read_instruction,method,1,3,3,42,14.0,0,0,"['self', 'split_dict']","[None, None]","[None, None]",117,"['        """"""Parse the descriptor tree and compile all read instructions together.\n', '\n', '        Args:\n', '            split_dict: `dict`, The `dict[split_name, SplitInfo]` of the dataset\n', '\n', '        Returns:\n', '            split_read_instruction: `SplitReadInstruction`\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/splits.py:SplitBase:subsplit,SplitBase:subsplit,method,1,9,9,56,6.22,0,0,"['self', 'arg', 'k', 'percent', 'weighted']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",142,"['        """"""Divides this split into subsplits.\n', '\n', '        There are 3 ways to define subsplits, which correspond to the 3\n', '        arguments `k` (get `k` even subsplits), `percent` (get a slice of the\n', '        dataset with `datasets.percent`), and `weighted` (get subsplits with proportions\n', '        specified by `weighted`).\n', '\n', '        Example::\n', '\n', '        ```\n', '        # 50% train, 50% test\n', '        train, test = split.subsplit(k=2)\n', '        # 50% train, 25% test, 25% validation\n', '        train, test, validation = split.subsplit(weighted=[2, 1, 1])\n', '        # Extract last 20%\n', '        subsplit = split.subsplit(datasets.percent[-20:])\n', '        ```\n', '\n', '        Warning: k and weighted will be converted into percent which mean that\n', '        values below the percent will be rounded up or down. The final split may be\n', '        bigger to deal with remainders. For instance:\n', '\n', '        ```\n', '        train, test, valid = split.subsplit(k=3)  # 33%, 33%, 34%\n', '        s1, s2, s3, s4 = split.subsplit(weighted=[2, 2, 1, 1])  # 33%, 33%, 16%, 18%\n', '        ```\n', '\n', '        Args:\n', '            arg: If no kwargs are given, `arg` will be interpreted as one of\n', '                `k`, `percent`, or `weighted` depending on the type.\n', '                For example:\n', '                ```\n', '                split.subsplit(10)  # Equivalent to split.subsplit(k=10)\n', '                split.subsplit(datasets.percent[:-20])  # percent=datasets.percent[:-20]\n', '                split.subsplit([1, 1, 2])  # weighted=[1, 1, 2]\n', '                ```\n', '            k: `int` If set, subdivide the split into `k` equal parts.\n', '            percent: `datasets.percent slice`, return a single subsplit corresponding to\n', '                a slice of the original split. For example:\n', '                `split.subsplit(datasets.percent[-20:])  # Last 20% of the dataset`.\n', '            weighted: `list[int]`, return a list of subsplits whose proportions match\n', '                the normalized sum of the list. For example:\n', '                `split.subsplit(weighted=[1, 1, 2])  # 25%, 25%, 50%`.\n', '\n', '        Returns:\n', '            A subsplit or list of subsplits extracted from this split object.\n', '        """"""\n']",['ValueError'],1
repos/datasets/src/datasets/splits.py:SplitDict:__getitem__,SplitDict:__getitem__,method,7,15,14,199,13.27,0,1,"['self', 'key', 'str]']","[None, ' Union[SplitBase', None]","[None, None, None]",526,[],"['str', 'super', 'make_file_instructions', 'SubSplitInfo']",4
repos/datasets/src/datasets/splits.py:SplitDict:__init__,SplitDict:__init__,method,3,4,4,63,15.75,0,0,"['self', '*args', 'dataset_name', '**kwargs']","[None, None, None, None]","[None, None, 'None', None]",522,[],['super'],1
repos/datasets/src/datasets/splits.py:SplitDict:__setitem__,SplitDict:__setitem__,method,4,13,13,121,9.31,0,1,"['self', 'key', 'str]', 'value']","[None, ' Union[SplitBase', None, ' SplitInfo']","[None, None, None, None]",539,[],"['ValueError', 'super']",2
repos/datasets/src/datasets/splits.py:SplitDict:_from_yaml_list,SplitDict:_from_yaml_list,method,2,2,2,36,18.0,0,0,"['cls', 'yaml_data']","[None, ' list']","[None, None]",597,[],['cls.from_split_dict'],1
repos/datasets/src/datasets/splits.py:SplitDict:_to_yaml_list,SplitDict:_to_yaml_list,method,4,20,12,184,9.2,2,0,['self'],[None],[None],586,[],"['self.to_split_dict', 'split_info_dict.pop']",2
repos/datasets/src/datasets/splits.py:SplitDict:add,SplitDict:add,method,5,13,13,171,13.15,0,1,"['self', 'split_info']","[None, ' SplitInfo']","[None, None]",544,"['        """"""Add the split info.""""""\n']","['ValueError', 'super']",2
repos/datasets/src/datasets/splits.py:SplitDict:copy,SplitDict:copy,method,2,3,3,71,23.67,0,0,['self'],[None],[None],583,[],['SplitDict.from_split_dict'],1
repos/datasets/src/datasets/splits.py:SplitDict:from_split_dict,SplitDict:from_split_dict,method,10,29,21,341,11.76,1,3,"['cls', 'split_infos', 'Dict]', 'dataset_name']","[None, ' Union[List', None, ' Optional[str] ']","[None, None, None, ' None']",557,"['        """"""Returns a new SplitDict initialized from a Dict or List of `split_infos`.""""""\n']","['isinstance', 'list', 'cls', 'SplitInfo', 'split_dict.add']",5
repos/datasets/src/datasets/splits.py:SplitDict:to_split_dict,SplitDict:to_split_dict,method,9,14,12,143,10.21,1,0,['self'],[None],[None],574,"['        """"""Returns a list of SplitInfo protos that we have.""""""\n']","['self.items', 'copy.deepcopy', 'out.append']",3
repos/datasets/src/datasets/splits.py:SplitDict:total_num_examples,SplitDict:total_num_examples,method,2,6,6,44,7.33,0,0,['self'],[None],[None],552,"['        """"""Return the total number of examples.""""""\n']","['sum', 'self.values']",2
repos/datasets/src/datasets/splits.py:SplitGenerator:__post_init__,SplitGenerator:__post_init__,method,5,21,20,155,7.38,0,0,['self'],[None],[None],632,[],"['str', 'NamedSplit', 'SplitInfo']",3
repos/datasets/src/datasets/splits.py:SplitInfo:file_instructions,SplitInfo:file_instructions,method,4,8,8,147,18.38,0,0,['self'],[None],[None],47,"['        """"""Returns the list of dict(filename, take, skip).""""""\n']","['make_file_instructions', 'instruction=str']",2
repos/datasets/src/datasets/splits.py:SplitReadInstruction:__add__,SplitReadInstruction:__add__,method,4,12,8,223,18.58,0,0,"['self', 'other']","[None, None]","[None, None]",493,"['        """"""Merging split together.""""""\n']",['SplitReadInstruction'],1
repos/datasets/src/datasets/splits.py:SplitReadInstruction:__getitem__,SplitReadInstruction:__getitem__,method,10,29,27,289,9.97,1,1,"['self', 'slice_value']","[None, None]","[None, None]",503,"['        """"""Sub-splits.""""""\n']","['SplitReadInstruction', 'ValueError', 'v._asdict', 'split_instruction.add']",4
repos/datasets/src/datasets/splits.py:SplitReadInstruction:__init__,SplitReadInstruction:__init__,method,4,16,16,177,11.06,0,1,"['self', 'split_info']","[None, None]","[None, 'None']",480,[],"['NonMutableDict', 'self.add']",2
repos/datasets/src/datasets/splits.py:SplitReadInstruction:add,SplitReadInstruction:add,method,2,2,2,55,27.5,0,0,"['self', 'sliced_split']","[None, None]","[None, None]",486,"['        """"""Add a SlicedSplitInfo the read instructions.""""""\n']",[],0
repos/datasets/src/datasets/splits.py:SplitReadInstruction:get_list_sliced_split_info,SplitReadInstruction:get_list_sliced_split_info,method,1,2,2,33,16.5,0,0,['self'],[None],[None],515,[],['list'],1
repos/datasets/src/datasets/splits.py:SubSplitInfo:file_instructions,SubSplitInfo:file_instructions,method,2,2,2,41,20.5,0,0,['self'],[None],[None],76,"['        """"""Returns the list of dict(filename, take, skip).""""""\n']",[],0
repos/datasets/src/datasets/splits.py:SubSplitInfo:num_examples,SubSplitInfo:num_examples,method,2,2,2,36,18.0,0,0,['self'],[None],[None],71,"['        """"""Returns the number of example in the subsplit.""""""\n']",[],0
repos/datasets/src/datasets/splits.py:_SplitMerged:__init__,_SplitMerged:__init__,method,4,4,4,39,9.75,0,0,"['self', 'split1', 'split2']","[None, None, None]","[None, None, None]",279,[],[],0
repos/datasets/src/datasets/splits.py:_SplitMerged:__repr__,_SplitMerged:__repr__,method,2,3,3,52,17.33,0,0,['self'],[None],[None],288,[],[],0
repos/datasets/src/datasets/splits.py:_SplitMerged:get_read_instruction,_SplitMerged:get_read_instruction,method,5,7,5,169,24.14,0,0,"['self', 'split_dict']","[None, None]","[None, None]",283,[],[],0
repos/datasets/src/datasets/splits.py:_SubSplit:__init__,_SubSplit:__init__,method,4,4,4,47,11.75,0,0,"['self', 'split', 'slice_value']","[None, None, None]","[None, None, None]",295,[],[],0
repos/datasets/src/datasets/splits.py:_SubSplit:__repr__,_SubSplit:__repr__,method,5,30,22,334,11.13,0,1,['self'],[None],[None],302,[],['slice_str.format'],1
repos/datasets/src/datasets/splits.py:_SubSplit:get_read_instruction,_SubSplit:get_read_instruction,method,2,2,2,69,34.5,0,0,"['self', 'split_dict']","[None, None]","[None, None]",299,[],[],0
repos/datasets/src/datasets/streaming.py:extend_dataset_builder_for_streaming,extend_dataset_builder_for_streaming,function,22,80,61,1011,12.64,2,3,['builder'],"[' ""DatasetBuilder""']",[None],112,"['    """"""Extend the dataset builder module and the modules imported by it to support streaming.\n', '\n', '    Args:\n', '        builder (:class:`DatasetBuilder`): Dataset builder instance.\n', '    """"""\n']","['DownloadConfig', 'extend_module_for_streaming', 'inspect.getfile', 'lock_importable_file', 'get_imports', 'type', 'issubclass']",7
repos/datasets/src/datasets/streaming.py:extend_module_for_streaming,extend_module_for_streaming,function,17,106,78,2236,21.09,0,3,"['module_path', 'download_config']","[None, ' Optional[DownloadConfig] ']","[None, ' None']",44,"['    """"""Extend the module to support streaming.\n', '\n', '    We patch some functions in the module to use `fsspec` to support data streaming:\n', '    - We use `fsspec.open` to open and read remote files. We patch the module function:\n', '      - `open`\n', '    - We use the ""::"" hop separator to join paths and navigate remote compressed/archive files. We patch the module\n', '      functions:\n', '      - `os.path.join`\n', '      - `pathlib.Path.joinpath` and `pathlib.Path.__truediv__` (called when using the ""/"" operator)\n', '\n', '    The patched functions are replaced with custom functions defined to work with the\n', '    :class:`~download.streaming_download_manager.StreamingDownloadManager`.\n', '\n', '    Args:\n', '        module_path: Path to the module to be extended.\n', '        download_config : mainly use use_auth_token or storage_options to support different platforms and auth types.\n', '    """"""\n']","['importlib.import_module', 'hasattr', 'isinstance', 'wrap_auth', 'wrapper', 'function', 'patch_submodule']",7
repos/datasets/src/datasets/table.py:_are_list_values_of_length,_are_list_values_of_length,function,2,6,6,90,15.0,0,0,"['array', 'length']","[' pa.ListArray', ' int']","[None, None]",1809,"['    """"""Check if all the sub-lists of a `pa.ListArray` have the specified length.""""""\n']","['pc.all', 'len']",2
repos/datasets/src/datasets/table.py:_combine_list_array_offsets_with_mask,_combine_list_array_offsets_with_mask,function,6,18,16,187,10.39,0,1,['array'],[' pa.ListArray'],[None],1814,"['    """"""Add the null bitmap to the offsets of a `pa.ListArray`.""""""\n']","['pa.concat_arrays', 'pc.replace_with_mask', 'array.is_null', 'pa.nulls', 'pa.int32']",5
repos/datasets/src/datasets/table.py:_deepcopy,_deepcopy,function,10,17,15,139,8.18,1,0,"['x', 'memo']","[None, ' dict']","[None, None]",69,"['    """"""deepcopy a regular class instance""""""\n']","['cls.__new__', 'memo[id', 'setattr', 'copy.deepcopy']",4
repos/datasets/src/datasets/table.py:_in_memory_arrow_table_from_buffer,_in_memory_arrow_table_from_buffer,function,7,8,7,114,14.25,0,0,['buffer'],[' pa.Buffer'],[None],41,[],"['pa.BufferReader', 'opened_stream.read_all']",2
repos/datasets/src/datasets/table.py:_in_memory_arrow_table_from_file,_in_memory_arrow_table_from_file,function,7,8,7,142,17.75,0,0,['filename'],[' str'],[None],34,[],"['pa.input_stream', 'opened_stream.read_all']",2
repos/datasets/src/datasets/table.py:_interpolation_search,_interpolation_search,function,12,56,40,228,4.07,1,1,"['arr', 'x']","[' List[int]', ' int']","[None, None]",79,"['    """"""\n', '    Return the position i of a sorted array so that arr[i] <= x < arr[i+1]\n', '\n', '    Args:\n', '        arr (`List[int]`): non-empty sorted list of integers\n', '        x (`int`): query\n', '\n', '    Returns:\n', '        `int`: the position i so that arr[i] <= x < arr[i+1]\n', '\n', '    Raises:\n', '        `IndexError`: if the array is empty or if the query is outside the array values\n', '    """"""\n']","['len', 'IndexError']",2
repos/datasets/src/datasets/table.py:_memory_mapped_arrow_table_from_file,_memory_mapped_arrow_table_from_file,function,5,6,5,117,19.5,0,0,['filename'],[' str'],[None],63,[],"['_memory_mapped_record_batch_reader_from_file', 'opened_stream.read_all']",2
repos/datasets/src/datasets/table.py:_memory_mapped_record_batch_reader_from_file,_memory_mapped_record_batch_reader_from_file,function,4,4,4,91,22.75,0,0,['filename'],[' str'],[None],48,[],['pa.memory_map'],1
repos/datasets/src/datasets/table.py:_short_str,_short_str,function,4,12,10,78,6.5,0,1,['value'],[' Any'],[None],1840,[],"['str', 'len']",2
repos/datasets/src/datasets/table.py:_storage_type,_storage_type,function,6,28,19,388,13.86,0,1,['type'],[' pa.DataType'],[None],1827,"['    """"""Convert a (possibly nested) `pa.ExtensionType` to its storage type.""""""\n']","['isinstance', '_storage_type', 'pa.struct', 'pa.list_']",4
repos/datasets/src/datasets/table.py:_wrap_for_chunked_arrays,_wrap_for_chunked_arrays,function,5,22,18,198,9.0,0,1,['func'],[None],[None],1797,"['    """"""Apply the function on each chunk of a `pyarrow.ChunkedArray`, or on the array directly""""""\n']","['wrapper', 'isinstance', 'pa.chunked_array', 'func']",4
repos/datasets/src/datasets/table.py:array_cast,array_cast,function,42,253,124,3239,12.8,0,16,"['array', 'pa_type', 'allow_primitive_to_str', 'allow_decimal_to_str']","[' pa.Array', ' pa.DataType', ' bool ', ' bool ']","[None, None, ' True', ' True']",1848,"['    """"""Improved version of `pa.Array.cast`\n', '\n', '    It supports casting `pa.StructArray` objects to re-order the fields.\n', '    It also let you control certain aspects of the casting, e.g. whether\n', '    to disable casting primitives (`booleans`, `floats` or `ints`) or\n', '    disable casting decimals to strings.\n', '\n', '    Args:\n', '        array (`pa.Array`):\n', '            PyArrow array to cast\n', '        pa_type (`pa.DataType`):\n', '            Target PyArrow type\n', '        allow_primitive_to_str (`bool`, defaults to `True`):\n', '            Whether to allow casting primitives to strings.\n', '            Defaults to `True`.\n', '        allow_decimal_to_str (`bool`, defaults to `True`):\n', '            Whether to allow casting decimals to strings.\n', '            Defaults to `True`.\n', '\n', '    Raises:\n', '        `pa.ArrowInvalidError`: if the arrow data casting fails\n', '        `TypeError`: if the target type is not supported according, e.g.\n', '\n', '            - if a field is missing\n', '            - if casting from primitives to strings and `allow_primitive_to_str` is `False`\n', '            - if casting from decimals to strings and `allow_decimal_to_str` is `False`\n', '\n', '    Returns:\n', '        `List[pyarrow.Array]`: the casted array\n', '    """"""\n']","['partial', 'isinstance', 'pa_type.wrap_array', 'fields=list', '_are_list_values_of_length', '_storage_type', '_c', 'pc.list_slice', 'len', '_combine_list_array_offsets_with_mask', 'TypeError', 'array.cast']",12
repos/datasets/src/datasets/table.py:cast_array_to_feature,cast_array_to_feature,function,48,281,132,3970,14.13,0,18,"['array', 'feature', 'allow_primitive_to_str', 'allow_decimal_to_str']","[' pa.Array', ' ""FeatureType""', ' bool ', ' bool ']","[None, None, ' True', ' True']",1968,"['    """"""Cast an array to the arrow type that corresponds to the requested feature type.\n', '    For custom features like [`Audio`] or [`Image`], it takes into account the ""cast_storage"" methods\n', '    they defined to enable casting from other arrow types.\n', '\n', '    Args:\n', '        array (`pa.Array`):\n', '            The PyArrow array to cast.\n', '        feature (`datasets.features.FeatureType`):\n', '            The target feature type.\n', '        allow_primitive_to_str (`bool`, defaults to `True`):\n', '            Whether to allow casting primitives to strings.\n', '            Defaults to `True`.\n', '        allow_decimal_to_str (`bool`, defaults to `True`):\n', '            Whether to allow casting decimals to strings.\n', '            Defaults to `True`.\n', '\n', '    Raises:\n', '        `pa.ArrowInvalidError`: if the arrow data casting fails\n', '        `TypeError`: if the target type is not supported according, e.g.\n', '\n', '            - if a field is missing\n', '            - if casting from primitives and `allow_primitive_to_str` is `False`\n', '            - if casting from decimals and `allow_decimal_to_str` is `False`\n', '\n', '    Returns:\n', '        array (`pyarrow.Array`): the casted array\n', '    """"""\n']","['partial', 'isinstance', 'hasattr', 'feature.cast_storage', 'Sequence', 'set', 'feature.items', 'names=list', '_c', '_combine_list_array_offsets_with_mask', '_are_list_values_of_length', '_storage_type', 'array_cast', 'pc.list_slice', 'pa.list_', 'len', 'get_nested_type', 'feature', 'TypeError']",19
repos/datasets/src/datasets/table.py:cast_table_to_features,cast_table_to_features,function,7,25,25,395,15.8,0,1,"['table', 'features']","[' pa.Table', ' ""Features""']","[None, None]",2218,"['    """"""Cast a table to the arrow schema that corresponds to the requested features.\n', '\n', '    Args:\n', '        table (`pyarrow.Table`):\n', '            PyArrow table to cast.\n', '        features ([`Features`]):\n', '            Target features.\n', '\n', '    Returns:\n', '        table (`pyarrow.Table`): the casted table\n', '    """"""\n']","['sorted', 'CastError', 'requested_column_names=list', 'features.items']",4
repos/datasets/src/datasets/table.py:cast_table_to_schema,cast_table_to_schema,function,12,31,31,452,14.58,0,1,"['table', 'schema']","[' pa.Table', ' pa.Schema']","[None, None]",2240,"['    """"""Cast a table to the arrow schema. Different from `cast_table_to_features`, this method can preserve nullability.\n', '\n', '    Args:\n', '        table (`pa.Table`):\n', '            PyArrow table to cast.\n', '        features ([`Features`]):\n', '            Target features.\n', '\n', '    Returns:\n', '        `pa.Table`: the casted table\n', '    """"""\n']","['Features.from_arrow_schema', 'sorted', 'CastError', 'requested_column_names=list', 'features.items']",5
repos/datasets/src/datasets/table.py:concat_tables,concat_tables,function,4,10,9,108,10.8,0,1,"['tables', 'axis']","[' List[Table]', ' int ']","[None, ' 0']",1753,"['    """"""\n', '    Concatenate tables.\n', '\n', '    Args:\n', '        tables (list of `Table`):\n', '            List of tables to be concatenated.\n', '        axis (`{0, 1}`, defaults to `0`, meaning over rows):\n', '            Axis to concatenate over, where `0` means over rows (vertically) and `1` means over columns\n', '            (horizontally).\n', '\n', '            <Added version=""1.6.0""/>\n', '    Returns:\n', '        `datasets.table.Table`:\n', '            If the number of input tables is > 1, then the returned table is a `datasets.table.ConcatenationTable`.\n', ""            Otherwise if there's only one table, it is returned as is.\n"", '    """"""\n']","['list', 'len', 'ConcatenationTable.from_tables']",3
repos/datasets/src/datasets/table.py:embed_array_storage,embed_array_storage,function,29,124,88,1620,13.06,0,9,"['array', 'feature']","[' pa.Array', ' ""FeatureType""']","[None, None]",2126,"['    """"""Embed data into an arrays\'s storage.\n', '    For custom features like Audio or Image, it takes into account the ""embed_storage"" methods\n', '    they define to embed external data (e.g. an image file) into an array.\n', '\n', '    <Added version=""2.4.0""/>\n', '\n', '    Args:\n', '        array (`pa.Array`):\n', '            The PyArrow array in which to embed data.\n', '        feature (`datasets.features.FeatureType`):\n', '            Array features.\n', '\n', '    Raises:\n', '        `TypeError`: if the target type is not supported according, e.g.\n', '\n', '            - if a field is missing\n', '\n', '    Returns:\n', '         array (`pyarrow.Array`): the casted array\n', '    """"""\n']","['isinstance', 'hasattr', 'feature.embed_storage', 'Sequence', 'feature.items', 'names=list', '_combine_list_array_offsets_with_mask', '_e', 'len', 'pa.list_', 'TypeError']",11
repos/datasets/src/datasets/table.py:embed_table_storage,embed_table_storage,function,12,24,24,305,12.71,1,0,['table'],[' pa.Table'],[None],2265,"['    """"""Embed external data into a table\'s storage.\n', '\n', '    <Added version=""2.4.0""/>\n', '\n', '    Args:\n', '        table (`pyarrow.Table`):\n', '            PyArrow table in which to embed data.\n', '\n', '    Returns:\n', '        table (`pyarrow.Table`): the table with embedded data\n', '    """"""\n']","['Features.from_arrow_schema', 'embed_array_storage', 'require_storage_embed', 'features.items']",4
repos/datasets/src/datasets/table.py:inject_arrow_table_documentation,inject_arrow_table_documentation,function,9,24,20,274,11.42,0,1,['arrow_table_method'],[None],[None],23,[],"['wrapper', 'hasattr']",2
repos/datasets/src/datasets/table.py:list_table_cache_files,list_table_cache_files,function,7,26,20,245,9.42,2,1,['table'],[' Table'],[None],1776,"['    """"""\n', '    Get the cache files that are loaded by the table.\n', '    Cache file are used when parts of the table come from the disk via memory mapping.\n', '\n', '    Returns:\n', '        `List[str]`:\n', '            A list of paths to the cache files loaded by the table.\n', '    """"""\n']","['isinstance', 'list_table_cache_files']",2
repos/datasets/src/datasets/table.py:read_schema_from_file,read_schema_from_file,function,5,8,7,118,14.75,0,0,['filename'],[' str'],[None],53,"['    """"""\n', '    Infer arrow table schema from file without loading whole file into memory.\n', '    Usefull especially while having very big files.\n', '    """"""\n']",['pa.memory_map'],1
repos/datasets/src/datasets/table.py:table_cast,table_cast,function,8,14,12,179,12.79,0,1,"['table', 'schema']","[' pa.Table', ' pa.Schema']","[None, None]",2287,"['    """"""Improved version of `pa.Table.cast`.\n', '\n', '    It supports casting to feature types stored in the schema metadata.\n', '\n', '    Args:\n', '        table (`pyarrow.Table`):\n', '            PyArrow table to cast.\n', '        schema (`pyarrow.Schema`):\n', '            Target PyArrow schema.\n', '\n', '    Returns:\n', '        table (`pyarrow.Table`): the casted table\n', '    """"""\n']","['cast_table_to_schema', 'table.replace_schema_metadata']",2
repos/datasets/src/datasets/table.py:table_flatten,table_flatten,function,27,67,51,932,13.91,1,2,['table'],[' pa.Table'],[None],2309,"['    """"""Improved version of `pa.Table.flatten`.\n', '\n', '    It behaves as `pa.Table.flatten` in a sense it does 1-step flatten of the columns with a struct type into one column per struct field,\n', '    but updates the metadata and skips decodable features unless the `decode` attribute of these features is set to False.\n', '\n', '    Args:\n', '        table (`pa.Table`):\n', '            PyArrow table to flatten.\n', '\n', '    Returns:\n', '        `Table`: the flattened table\n', '    """"""\n']","['Features.from_arrow_schema', 'any', 'subfeature.flatten', 'features.values', 'table.column', 'hasattr', 'flat_arrays.extend', 'flat_column_names.extend', 'flat_arrays.append', 'flat_column_names.append', 'table.flatten', 'features.flatten', 'Features', 'flat_table.replace_schema_metadata']",14
repos/datasets/src/datasets/table.py:table_iter,table_iter,function,11,55,34,755,13.73,1,2,"['table', 'batch_size', 'drop_last_batch']","[' Table', ' int', None]","[None, None, 'False']",2390,"['    """"""Iterate over sub-tables of size `batch_size`.\n', '\n', '    Args:\n', '        table (`pyarrow.Table`):\n', '            PyArrow table to iterate over.\n', '        batch_size (`int`):\n', '            Size of each sub-table to yield.\n', '        drop_last_batch (`bool`, defaults to `False`):\n', '            Drop the last batch if it is smaller than `batch_size`.\n', '    """"""\n']","['table.to_reader', 'len', 'chunks_buffer.append']",3
repos/datasets/src/datasets/table.py:table_visitor,table_visitor,function,22,76,55,828,10.89,4,5,"['table', 'function', 'None]']","[' pa.Table', ' Callable[[pa.Array]', None]","[None, None, None]",2351,"['    """"""Visit all arrays in a table and apply a function to them.\n', '\n', '    Args:\n', '        table (`pyarrow.Table`):\n', '            PyArrow table to visit.\n', '        function (`Callable[[pa.Array], None]`):\n', '            Function to apply to each array.\n', '    """"""\n']","['Features.from_arrow_schema', '_visit', 'isinstance', 'function', 'hasattr', 'Sequence', 'feature.items', 'features.items']",8
repos/datasets/src/datasets/table.py:CastError,CastError,class,15,64,46,883,13.8,0,1,[],[],[],2193,[],[],0
repos/datasets/src/datasets/table.py:ConcatenationTable,ConcatenationTable,class,135,864,372,8434,9.76,24,19,[],[],[],1274,[],[],0
repos/datasets/src/datasets/table.py:InMemoryTable,InMemoryTable,class,26,153,57,1993,13.03,0,0,[],[],[],639,[],[],0
repos/datasets/src/datasets/table.py:IndexedTableMixin,IndexedTableMixin,class,36,131,93,1276,9.74,2,3,[],[],[],105,[],[],0
repos/datasets/src/datasets/table.py:MemoryMappedTable,MemoryMappedTable,class,46,310,131,3950,12.74,1,3,[],[],[],990,[],[],0
repos/datasets/src/datasets/table.py:Table,Table,class,72,256,106,2729,10.66,0,1,[],[],[],154,[],[],0
repos/datasets/src/datasets/table.py:TableBlock,TableBlock,class,0,1,1,4,4.0,0,0,[],[],[],629,[],[],0
repos/datasets/src/datasets/table.py:CastError:__init__,CastError:__init__,method,5,5,5,117,23.4,0,0,"['self', '*args', 'table_column_names', 'requested_column_names']","[None, None, ' List[str]', ' List[str]']","[None, None, None, None]",2196,[],['super'],1
repos/datasets/src/datasets/table.py:CastError:__reduce__,CastError:__reduce__,method,2,7,7,123,17.57,0,0,['self'],[None],[None],2201,[],['partial'],1
repos/datasets/src/datasets/table.py:CastError:details,CastError:details,method,6,39,24,509,13.05,0,1,['self'],[None],[None],2207,[],['set'],1
repos/datasets/src/datasets/table.py:ConcatenationTable:__getstate__,ConcatenationTable:__getstate__,method,1,5,5,55,11.0,0,0,['self'],[None],[None],1313,[],[],0
repos/datasets/src/datasets/table.py:ConcatenationTable:__init__,ConcatenationTable:__init__,method,7,33,31,256,7.76,2,1,"['self', 'table', 'blocks']","[None, ' pa.Table', ' List[List[TableBlock]]']","[None, None, None]",1300,[],"['super', 'isinstance', 'TypeError']",3
repos/datasets/src/datasets/table.py:ConcatenationTable:__setstate__,ConcatenationTable:__setstate__,method,12,33,27,423,12.82,0,2,"['self', 'state']","[None, None]","[None, None]",1316,[],"['self._concat_blocks_horizontally_and_vertically', 'pa.concat_tables', 'ConcatenationTable.__init__']",3
repos/datasets/src/datasets/table.py:ConcatenationTable:_concat_blocks,ConcatenationTable:_concat_blocks,method,16,59,42,464,7.86,2,4,"['blocks', 'pa.Table]]', 'axis']","[' List[Union[TableBlock', None, ' int ']","[None, None, ' 0']",1331,[],"['hasattr', 'pa.concat_tables', 'enumerate', 'zip', 'pa_table.append_column', 'ValueError']",6
repos/datasets/src/datasets/table.py:ConcatenationTable:_concat_blocks_horizontally_and_vertically,ConcatenationTable:_concat_blocks_horizontally_and_vertically,method,9,18,18,294,16.33,1,1,"['cls', 'blocks']","[None, ' List[List[TableBlock]]']","[None, None]",1351,[],"['enumerate', 'cls._concat_blocks', 'pa_tables_to_concat_vertically.append']",3
repos/datasets/src/datasets/table.py:ConcatenationTable:_consolidate_blocks,ConcatenationTable:_consolidate_blocks,method,4,14,11,160,11.43,0,1,"['cls', 'blocks']","[None, ' TableBlockContainer']","[None, None]",1377,[],"['isinstance', 'cls._merge_blocks']",2
repos/datasets/src/datasets/table.py:ConcatenationTable:_merge_blocks,ConcatenationTable:_merge_blocks,method,10,56,38,497,8.88,1,3,"['cls', 'blocks', 'axis']","[None, ' TableBlockContainer', ' Optional[int] ']","[None, None, ' None']",1361,[],"['groupby', 'isinstance', 'list', 'all', 'cls._merge_blocks']",5
repos/datasets/src/datasets/table.py:ConcatenationTable:_slices,ConcatenationTable:_slices,method,4,14,12,90,6.43,1,0,['self'],[None],[None],1482,[],['len'],1
repos/datasets/src/datasets/table.py:ConcatenationTable:add_column,ConcatenationTable:add_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1618,"['        """"""\n', '        Add column to Table at position.\n', '\n', '        A new table is returned with the column added, the original table\n', '        object is left unchanged.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index to place the column at.\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`: New table with the passed column added.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:ConcatenationTable:append_column,ConcatenationTable:append_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1639,"['        """"""\n', '        Append column at end of columns.\n', '\n', '        Args:\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table with the passed column added.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:ConcatenationTable:cast,ConcatenationTable:cast,method,24,56,45,625,11.16,3,0,"['self', 'target_schema', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",1569,"['        """"""\n', '        Cast table values to another schema.\n', '\n', '        Args:\n', '            target_schema (`Schema`):\n', '                Schema to cast to, the names and order of fields must match.\n', '            safe (`bool`, defaults to `True`):\n', '                Check for overflows or other unsafe conversions.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']","['table_cast', 'Features.from_arrow_schema', 'list', 'subfields.append', 'enumerate', 'Features', 'new_tables.append', 'blocks.append', 'ConcatenationTable']",9
repos/datasets/src/datasets/table.py:ConcatenationTable:combine_chunks,ConcatenationTable:combine_chunks,method,8,18,16,182,10.11,1,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1549,"['        """"""\n', '        Make a new table by combining the chunks this table has.\n', '\n', '        All the underlying chunks in the `ChunkedArray` of each column are\n', '        concatenated into zero or one chunk.\n', '\n', '        Args:\n', '            memory_pool (`MemoryPool`, defaults to `None`):\n', '                For memory allocations, if required, otherwise use default pool.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']","['blocks.append', 'ConcatenationTable']",2
repos/datasets/src/datasets/table.py:ConcatenationTable:drop,ConcatenationTable:drop,method,8,28,21,206,7.36,1,0,"['self', 'columns', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",1712,"['        """"""\n', '        Drop one or more columns and return a new table.\n', '\n', '        Args:\n', '            columns (`List[str]`):\n', '                List of field names referencing existing columns.\n', '\n', '        Raises:\n', '            `KeyError` : if any of the passed columns name are not existing.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table without the columns.\n', '        """"""\n']","['blocks.append', 'ConcatenationTable']",2
repos/datasets/src/datasets/table.py:ConcatenationTable:filter,ConcatenationTable:filter,method,8,26,23,247,9.5,1,0,"['self', 'mask', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",1520,"['        """"""\n', '        Select records from a Table. See `pyarrow.compute.filter` for full usage.\n', '        """"""\n']","['zip', 'mask.slice', 'blocks.append', 'ConcatenationTable']",4
repos/datasets/src/datasets/table.py:ConcatenationTable:flatten,ConcatenationTable:flatten,method,8,19,17,174,9.16,1,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1531,"['        """"""\n', '        Flatten this Table.  Each column with a struct type is flattened\n', '        into one column per struct field.  Other columns are left unchanged.\n', '\n', '        Args:\n', '            memory_pool (`MemoryPool`, defaults to `None`):\n', '                For memory allocations, if required, otherwise use default pool.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']","['table_flatten', 'blocks.append', 'ConcatenationTable']",3
repos/datasets/src/datasets/table.py:ConcatenationTable:from_blocks,ConcatenationTable:from_blocks,method,10,31,24,335,10.81,1,1,"['cls', 'blocks']","[None, ' TableBlockContainer']","[None, None]",1386,[],"['cls._consolidate_blocks', 'isinstance', 'cls', 'cls._concat_blocks', 'cls._concat_blocks_horizontally_and_vertically']",5
repos/datasets/src/datasets/table.py:ConcatenationTable:from_tables,ConcatenationTable:from_tables,method,32,157,110,1682,10.71,3,4,"['cls', 'tables', 'Table]]', 'axis']","[None, ' List[Union[pa.Table', None, ' int ']","[None, None, None, ' 0']",1400,"['        """"""Create `ConcatenationTable` from list of tables.\n', '\n', '        Args:\n', '            tables (list of `Table` or list of `pyarrow.Table`):\n', '                List of tables.\n', '            axis (`{0, 1}`, defaults to `0`, meaning over rows):\n', '                Axis to concatenate over, where `0` means over rows (vertically) and `1` means over columns\n', '                (horizontally).\n', '\n', '                <Added version=""1.6.0""/>\n', '        """"""\n']","['to_blocks', 'isinstance', 'copy.deepcopy', '_slice_row_block', 'len', '_split_both_like', 'list', 'new_blocks.append', 'new_result.append', 'ValueError', '_extend_blocks', 'result.extend', 'enumerate', 'cls.from_blocks']",14
repos/datasets/src/datasets/table.py:ConcatenationTable:remove_column,ConcatenationTable:remove_column,method,11,32,27,273,8.53,2,0,"['self', 'i', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",1656,"['        """"""\n', '        Create new Table with the indicated column removed.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index of column to remove.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table without the column.\n', '        """"""\n']","['blocks.append', 't.remove_column', 'ConcatenationTable']",3
repos/datasets/src/datasets/table.py:ConcatenationTable:rename_columns,ConcatenationTable:rename_columns,method,9,29,24,274,9.45,1,0,"['self', 'names', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",1699,"['        """"""\n', '        Create new table with columns renamed to provided names.\n', '        """"""\n']","['dict', 'blocks.append', 'ConcatenationTable']",3
repos/datasets/src/datasets/table.py:ConcatenationTable:replace_schema_metadata,ConcatenationTable:replace_schema_metadata,method,8,18,16,205,11.39,1,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1600,"['        """"""\n', '        EXPERIMENTAL: Create shallow copy of table by replacing schema\n', '        key-value metadata with the indicated new metadata (which may be `None`,\n', '        which deletes any existing metadata).\n', '\n', '        Args:\n', '            metadata (`dict`, defaults to `None`):\n', '\n', '        Returns:\n', '            `datasets.table.Table`: shallow_copy\n', '        """"""\n']","['blocks.append', 'ConcatenationTable']",2
repos/datasets/src/datasets/table.py:ConcatenationTable:select,ConcatenationTable:select,method,8,28,21,210,7.5,1,0,"['self', 'columns', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",1733,"['        """"""\n', '        Select columns of the table.\n', '\n', '        Returns a new table with the specified columns, and metadata preserved.\n', '\n', '        Args:\n', '            columns (:obj:`Union[List[str], List[int]]`):\n', '                The column names or integer indices to select.\n', '\n', '        Returns:\n', '            :class:`datasets.table.Table`: New table with the specified columns, and metadata preserved.\n', '        """"""\n']","['blocks.append', 'ConcatenationTable']",2
repos/datasets/src/datasets/table.py:ConcatenationTable:set_column,ConcatenationTable:set_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1680,"['        """"""\n', '        Replace column in Table at position.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index to place the column at.\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table with the passed column set.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:ConcatenationTable:slice,ConcatenationTable:slice,method,15,60,39,439,7.32,1,2,"['self', 'offset', 'length']","[None, None, None]","[None, '0', 'None']",1489,"['        """"""\n', '        Compute zero-copy slice of this Table.\n', '\n', '        Args:\n', '            offset (`int`, defaults to `0`):\n', '                Offset from start of table to slice.\n', '            length (`int`, defaults to `None`):\n', '                Length of slice (default is until end of table starting from\n', '                offset).\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']","['len', 'blocks.append', 'ConcatenationTable']",3
repos/datasets/src/datasets/table.py:InMemoryTable:add_column,InMemoryTable:add_column,method,2,3,3,58,19.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",876,"['        """"""\n', '        Add column to Table at position.\n', '\n', '        A new table is returned with the column added, the original table\n', '        object is left unchanged.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index to place the column at.\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`: New table with the passed column added.\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:append_column,InMemoryTable:append_column,method,2,3,3,61,20.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",897,"['        """"""\n', '        Append column at end of columns.\n', '\n', '        Args:\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table with the passed column added.\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:cast,InMemoryTable:cast,method,2,4,4,58,14.5,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",847,"['        """"""\n', '        Cast table values to another schema.\n', '\n', '        Args:\n', '            target_schema (`Schema`):\n', '                Schema to cast to, the names and order of fields must match.\n', '            safe (`bool`, defaults to `True`):\n', '                Check for overflows or other unsafe conversions.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:combine_chunks,InMemoryTable:combine_chunks,method,2,3,3,62,20.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",831,"['        """"""\n', '        Make a new table by combining the chunks this table has.\n', '\n', '        All the underlying chunks in the `ChunkedArray` of each column are\n', '        concatenated into zero or one chunk.\n', '\n', '        Args:\n', '            memory_pool (`MemoryPool`, defaults to `None`):\n', '                For memory allocations, if required, otherwise use default pool.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:drop,InMemoryTable:drop,method,2,3,3,52,17.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",953,"['        """"""\n', '        Drop one or more columns and return a new table.\n', '\n', '        Args:\n', '            columns (`List[str]`):\n', '                List of field names referencing existing columns.\n', '\n', '        Raises:\n', '            `KeyError` : if any of the passed columns name are not existing.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table without the columns.\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:filter,InMemoryTable:filter,method,2,3,3,54,18.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",811,"['        """"""\n', '        Select records from a Table. See `pyarrow.compute.filter` for full usage.\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:flatten,InMemoryTable:flatten,method,2,4,4,61,15.25,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",817,"['        """"""\n', '        Flatten this Table.  Each column with a struct type is flattened\n', '        into one column per struct field.  Other columns are left unchanged.\n', '\n', '        Args:\n', '            memory_pool (`MemoryPool`, defaults to `None`):\n', '                For memory allocations, if required, otherwise use default pool.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:from_arrays,InMemoryTable:from_arrays,method,2,3,3,47,15.67,0,0,"['cls', '*args', '**kwargs']","[None, None, None]","[None, None, None]",723,"['        """"""\n', '        Construct a Table from Arrow arrays.\n', '\n', '        Args:\n', '            arrays (`List[Union[pyarrow.Array, pyarrow.ChunkedArray]]`):\n', '                Equal-length arrays that should form the table.\n', '            names (`List[str]`, *optional*):\n', '                Names for the table columns. If not passed, schema must be passed.\n', '            schema (`Schema`, defaults to `None`):\n', '                Schema for the created table. If not passed, names must be passed.\n', '            metadata (`Union[dict, Mapping]`, defaults to `None`):\n', '                Optional metadata for the schema (if inferred).\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['cls'],1
repos/datasets/src/datasets/table.py:InMemoryTable:from_batches,InMemoryTable:from_batches,method,2,3,3,48,16.0,0,0,"['cls', '*args', '**kwargs']","[None, None, None]","[None, None, None]",779,"['        """"""\n', '        Construct a Table from a sequence or iterator of Arrow `RecordBatches`.\n', '\n', '        Args:\n', '            batches (`Union[Sequence[pyarrow.RecordBatch], Iterator[pyarrow.RecordBatch]]`):\n', '                Sequence of `RecordBatch` to be converted, all schemas must be equal.\n', '            schema (`Schema`, defaults to `None`):\n', '                If not passed, will be inferred from the first `RecordBatch`.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '        """"""\n']",['cls'],1
repos/datasets/src/datasets/table.py:InMemoryTable:from_buffer,InMemoryTable:from_buffer,method,4,4,4,65,16.25,0,0,"['cls', 'buffer']","[None, ' pa.Buffer']","[None, None]",660,[],"['_in_memory_arrow_table_from_buffer', 'cls']",2
repos/datasets/src/datasets/table.py:InMemoryTable:from_file,InMemoryTable:from_file,method,4,4,4,65,16.25,0,0,"['cls', 'filename']","[None, ' str']","[None, None]",655,[],"['_in_memory_arrow_table_from_file', 'cls']",2
repos/datasets/src/datasets/table.py:InMemoryTable:from_pandas,InMemoryTable:from_pandas,method,2,3,3,47,15.67,0,0,"['cls', '*args', '**kwargs']","[None, None, None]","[None, None, None]",665,"['        """"""\n', '        Convert pandas.DataFrame to an Arrow Table.\n', '\n', '        The column types in the resulting Arrow Table are inferred from the\n', '        dtypes of the pandas.Series in the DataFrame. In the case of non-object\n', '        Series, the NumPy dtype is translated to its Arrow equivalent. In the\n', '        case of `object`, we need to guess the datatype by looking at the\n', '        Python objects in this Series.\n', '\n', ""        Be aware that Series of the `object` dtype don't carry enough\n"", '        information to always lead to a meaningful Arrow type. In the case that\n', '        we cannot infer a type, e.g. because the DataFrame is of length 0 or\n', '        the Series only contains `None/nan` objects, the type is set to\n', '        null. This behavior can be avoided by constructing an explicit schema\n', '        and passing it to this function.\n', '\n', '        Args:\n', '            df (`pandas.DataFrame`):\n', '            schema (`pyarrow.Schema`, *optional*):\n', '                The expected schema of the Arrow Table. This can be used to\n', '                indicate the type of columns if we cannot infer it automatically.\n', '                If passed, the output will have exactly this schema. Columns\n', '                specified in the schema that are not found in the DataFrame columns\n', '                or its index will raise an error. Additional columns or index\n', '                levels in the DataFrame which are not specified in the schema will\n', '                be ignored.\n', '            preserve_index (`bool`, *optional*):\n', '                Whether to store the index as an additional column in the resulting\n', '                `Table`. The default of None will store the index as a column,\n', '                except for RangeIndex which is stored as metadata only. Use\n', '                `preserve_index=True` to force it to be stored as a column.\n', '            nthreads (`int`, defaults to `None` (may use up to system CPU count threads))\n', '                If greater than 1, convert columns to Arrow in parallel using\n', '                indicated number of threads.\n', '            columns (`List[str]`, *optional*):\n', '               List of column to be converted. If `None`, use all columns.\n', '            safe (`bool`, defaults to `True`):\n', '               Check for overflows or other unsafe conversions,\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '\n', '        Examples:\n', '        ```python\n', '        >>> import pandas as pd\n', '        >>> import pyarrow as pa\n', '        >>> df = pd.DataFrame({\n', ""            ...     'int': [1, 2],\n"", ""            ...     'str': ['a', 'b']\n"", '            ... })\n', '        >>> pa.Table.from_pandas(df)\n', '        <pyarrow.lib.Table object at 0x7f05d1fb1b40>\n', '        ```\n', '        """"""\n']",['cls'],1
repos/datasets/src/datasets/table.py:InMemoryTable:from_pydict,InMemoryTable:from_pydict,method,2,3,3,47,15.67,0,0,"['cls', '*args', '**kwargs']","[None, None, None]","[None, None, None]",743,"['        """"""\n', '        Construct a Table from Arrow arrays or columns.\n', '\n', '        Args:\n', '            mapping (`Union[dict, Mapping]`):\n', '                A mapping of strings to Arrays or Python lists.\n', '            schema (`Schema`, defaults to `None`):\n', '                If not passed, will be inferred from the Mapping values\n', '            metadata (`Union[dict, Mapping]`, defaults to `None`):\n', '                Optional metadata for the schema (if inferred).\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['cls'],1
repos/datasets/src/datasets/table.py:InMemoryTable:from_pylist,InMemoryTable:from_pylist,method,2,4,4,55,13.75,0,0,"['cls', 'mapping', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",761,"['        """"""\n', '        Construct a Table from list of rows / dictionaries.\n', '\n', '        Args:\n', '            mapping (`List[dict]`):\n', '                A mapping of strings to row values.\n', '            schema (`Schema`, defaults to `None`):\n', '                If not passed, will be inferred from the Mapping values\n', '            metadata (`Union[dict, Mapping]`, defaults to `None`):\n', '                Optional metadata for the schema (if inferred).\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['cls'],1
repos/datasets/src/datasets/table.py:InMemoryTable:remove_column,InMemoryTable:remove_column,method,2,3,3,61,20.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",914,"['        """"""\n', '        Create new Table with the indicated column removed.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index of column to remove.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table without the column.\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:rename_columns,InMemoryTable:rename_columns,method,2,3,3,62,20.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",947,"['        """"""\n', '        Create new table with columns renamed to provided names.\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:replace_schema_metadata,InMemoryTable:replace_schema_metadata,method,2,3,3,71,23.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",862,"['        """"""\n', '        EXPERIMENTAL: Create shallow copy of table by replacing schema\n', '        key-value metadata with the indicated new metadata (which may be `None`,\n', '        which deletes any existing metadata).\n', '\n', '        Args:\n', '            metadata (`dict`, defaults to `None`):\n', '\n', '        Returns:\n', '            `datasets.table.Table`: shallow_copy\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:select,InMemoryTable:select,method,2,3,3,54,18.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",970,"['        """"""\n', '        Select columns of the table.\n', '\n', '        Returns a new table with the specified columns, and metadata preserved.\n', '\n', '        Args:\n', '            columns (:obj:`Union[List[str], List[int]]`):\n', '                The column names or integer indices to select.\n', '\n', '        Returns:\n', '            :class:`datasets.table.Table`: New table with the specified columns, and metadata preserved.\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:set_column,InMemoryTable:set_column,method,2,3,3,58,19.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",928,"['        """"""\n', '        Replace column in Table at position.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index to place the column at.\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table with the passed column set.\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:InMemoryTable:slice,InMemoryTable:slice,method,2,3,3,65,21.67,0,0,"['self', 'offset', 'length']","[None, None, None]","[None, '0', 'None']",794,"['        """"""\n', '        Compute zero-copy slice of this Table.\n', '\n', '        Args:\n', '            offset (`int`, defaults to `0`):\n', '                Offset from start of table to slice.\n', '            length (`int`, defaults to `None`):\n', '                Length of slice (default is until end of table starting from\n', '                offset).\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['InMemoryTable'],1
repos/datasets/src/datasets/table.py:IndexedTableMixin:__init__,IndexedTableMixin:__init__,method,10,25,22,223,8.92,1,0,"['self', 'table']","[None, ' pa.Table']","[None, None]",106,[],"['table.to_batches', 'len', 'np.cumsum']",3
repos/datasets/src/datasets/table.py:IndexedTableMixin:fast_gather,IndexedTableMixin:fast_gather,method,9,28,28,289,10.32,1,1,"['self', 'indices', 'np.ndarray]']","[None, ' Union[List[int]', None]","[None, None, None]",113,"['        """"""\n', '        Create a pa.Table by gathering the records at the records at the specified indices. Should be faster\n', '        than pa.concat_tables(table.fast_slice(int(i) % table.num_rows, 1) for i in indices) since NumPy can compute\n', '        the binary searches in parallel, highly optimized C\n', '        """"""\n']","['len', 'ValueError', 'np.searchsorted', 'zip']",4
repos/datasets/src/datasets/table.py:IndexedTableMixin:fast_slice,IndexedTableMixin:fast_slice,method,17,61,42,608,9.97,0,2,"['self', 'offset', 'length']","[None, None, None]","[None, '0', 'None']",130,"['        """"""\n', '        Slice the Table using interpolation search.\n', ""        The behavior is the same as `pyarrow.Table.slice` but it's significantly faster.\n"", '\n', '        Interpolation search is used to find the start and end indexes of the batches we want to keep.\n', '        The batches to keep are then concatenated to form the sliced Table.\n', '        """"""\n']","['IndexError', '_interpolation_search']",2
repos/datasets/src/datasets/table.py:MemoryMappedTable:__getstate__,MemoryMappedTable:__getstate__,method,1,5,5,47,9.4,0,0,['self'],[None],[None],1022,[],[],0
repos/datasets/src/datasets/table.py:MemoryMappedTable:__init__,MemoryMappedTable:__init__,method,6,13,12,113,8.69,0,1,"['self', 'table', 'path', 'replays']","[None, ' pa.Table', ' str', ' Optional[List[Replay]] ']","[None, None, None, ' None']",1011,[],['super'],1
repos/datasets/src/datasets/table.py:MemoryMappedTable:__setstate__,MemoryMappedTable:__setstate__,method,7,13,12,198,15.23,0,0,"['self', 'state']","[None, None]","[None, None]",1025,[],"['_memory_mapped_arrow_table_from_file', 'self._apply_replays', 'MemoryMappedTable.__init__']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:_append_replay,MemoryMappedTable:_append_replay,method,4,5,4,72,14.4,0,0,"['self', 'replay']","[None, ' Replay']","[None, None]",1044,[],"['copy.deepcopy', 'replays.append']",2
repos/datasets/src/datasets/table.py:MemoryMappedTable:_apply_replays,MemoryMappedTable:_apply_replays,method,10,32,24,227,7.09,1,2,"['table', 'replays']","[' pa.Table', ' Optional[List[Replay]] ']","[None, ' None']",1033,[],"['table_cast', 'table_flatten', 'getattr']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:add_column,MemoryMappedTable:add_column,method,5,11,11,180,16.36,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1143,"['        """"""\n', '        Add column to Table at position.\n', '\n', '        A new table is returned with the column added, the original table\n', '        object is left unchanged.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index to place the column at.\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`: New table with the passed column added.\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:append_column,MemoryMappedTable:append_column,method,5,11,11,186,16.91,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1166,"['        """"""\n', '        Append column at end of columns.\n', '\n', '        Args:\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table with the passed column added.\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:cast,MemoryMappedTable:cast,method,5,12,12,174,14.5,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1110,"['        """"""\n', '        Cast table values to another schema\n', '\n', '        Args:\n', '            target_schema (`Schema`):\n', '                Schema to cast to, the names and order of fields must match.\n', '            safe (`bool`, defaults to `True`):\n', '                Check for overflows or other unsafe conversions.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:combine_chunks,MemoryMappedTable:combine_chunks,method,5,11,11,188,17.09,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1092,"['        """"""\n', '        Make a new table by combining the chunks this table has.\n', '\n', '        All the underlying chunks in the ChunkedArray of each column are\n', '        concatenated into zero or one chunk.\n', '\n', '        Args:\n', '            memory_pool (`MemoryPool`, defaults to `None`):\n', '                For memory allocations, if required, otherwise use default pool.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:drop,MemoryMappedTable:drop,method,5,11,11,168,15.27,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1230,"['        """"""\n', '        Drop one or more columns and return a new table.\n', '\n', '        Args:\n', '            columns (`List[str]`):\n', '                List of field names referencing existing columns.\n', '\n', '        Raises:\n', '            `KeyError` : if any of the passed columns name are not existing.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table without the columns.\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:filter,MemoryMappedTable:filter,method,5,11,11,172,15.64,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1068,"['        """"""\n', '        Select records from a Table. See `pyarrow.compute.filter` for full usage.\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:flatten,MemoryMappedTable:flatten,method,5,12,12,180,15.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1076,"['        """"""\n', '        Flatten this Table.  Each column with a struct type is flattened\n', '        into one column per struct field.  Other columns are left unchanged.\n', '\n', '        Args:\n', '            memory_pool (`MemoryPool`, defaults to `None`):\n', '                For memory allocations, if required, otherwise use default pool.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:from_file,MemoryMappedTable:from_file,method,5,9,8,126,14.0,0,0,"['cls', 'filename', 'replays']","[None, ' str', None]","[None, None, 'None']",1017,[],"['_memory_mapped_arrow_table_from_file', 'cls._apply_replays', 'cls']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:remove_column,MemoryMappedTable:remove_column,method,5,11,11,186,16.91,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1185,"['        """"""\n', '        Create new Table with the indicated column removed.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index of column to remove.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table without the column.\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:rename_columns,MemoryMappedTable:rename_columns,method,5,11,11,188,17.09,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1222,"['        """"""\n', '        Create new table with columns renamed to provided names.\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:replace_schema_metadata,MemoryMappedTable:replace_schema_metadata,method,5,11,11,206,18.73,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1127,"['        """"""\n', '        EXPERIMENTAL: Create shallow copy of table by replacing schema\n', '        key-value metadata with the indicated new metadata (which may be None,\n', '        which deletes any existing metadata.\n', '\n', '        Args:\n', '            metadata (`dict`, defaults to `None`):\n', '\n', '        Returns:\n', '            `datasets.table.Table`: shallow_copy\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:select,MemoryMappedTable:select,method,5,11,11,172,15.64,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1249,"['        """"""\n', '        Select columns of the table.\n', '\n', '        Returns a new table with the specified columns, and metadata preserved.\n', '\n', '        Args:\n', '            columns (:obj:`Union[List[str], List[int]]`):\n', '                The column names or integer indices to select.\n', '\n', '        Returns:\n', '            :class:`datasets.table.Table`: New table with the specified columns, and metadata preserved.\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:set_column,MemoryMappedTable:set_column,method,5,11,11,180,16.36,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1201,"['        """"""\n', '        Replace column in Table at position.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index to place the column at.\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:\n', '                New table with the passed column set.\n', '        """"""\n']","['copy.deepcopy', 'self._append_replay', 'MemoryMappedTable']",3
repos/datasets/src/datasets/table.py:MemoryMappedTable:slice,MemoryMappedTable:slice,method,5,12,12,159,13.25,0,0,"['self', 'offset', 'length']","[None, None, None]","[None, '0', 'None']",1049,"['        """"""\n', '        Compute zero-copy slice of this Table.\n', '\n', '        Args:\n', '            offset (`int`, defaults to `0`):\n', '                Offset from start of table to slice.\n', '            length (`int`, defaults to `None`):\n', '                Length of slice (default is until end of table starting from\n', '                offset).\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']","['self._append_replay', 'MemoryMappedTable']",2
repos/datasets/src/datasets/table.py:Table:__deepcopy__,Table:__deepcopy__,method,3,7,7,102,14.57,0,0,"['self', 'memo']","[None, ' dict']","[None, None]",170,[],"['memo[id', 'list', '_deepcopy']",3
repos/datasets/src/datasets/table.py:Table:__eq__,Table:__eq__,method,2,2,2,24,12.0,0,0,"['self', 'other']","[None, None]","[None, None]",427,[],['self.equals'],1
repos/datasets/src/datasets/table.py:Table:__getitem__,Table:__getitem__,method,2,2,2,19,9.5,0,0,"['self', 'i']","[None, None]","[None, None]",430,[],[],0
repos/datasets/src/datasets/table.py:Table:__init__,Table:__init__,method,3,3,3,40,13.33,0,0,"['self', 'table']","[None, ' pa.Table']","[None, None]",106,"['        """"""\n', '        Create a pa.Table by gathering the records at the records at the specified indices. Should be faster\n', '        than pa.concat_tables(table.fast_slice(int(i) % table.num_rows, 1) for i in indices) since NumPy can compute\n', '        the binary searches in parallel, highly optimized C\n', '        """"""\n']",['super'],1
repos/datasets/src/datasets/table.py:Table:__len__,Table:__len__,method,1,2,2,21,10.5,0,0,['self'],[None],[None],433,[],['len'],1
repos/datasets/src/datasets/table.py:Table:__repr__,Table:__repr__,method,2,3,3,76,25.33,0,0,['self'],[None],[None],436,[],[],0
repos/datasets/src/datasets/table.py:Table:__str__,Table:__str__,method,2,3,3,75,25.0,0,0,['self'],[None],[None],439,[],[],0
repos/datasets/src/datasets/table.py:Table:add_column,Table:add_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",523,"['        """"""\n', '        Add column to Table at position.\n', '\n', '        A new table is returned with the column added, the original table\n', '        object is left unchanged.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index to place the column at.\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`: New table with the passed column added.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:append_column,Table:append_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",544,"['        """"""\n', '        Append column at end of columns.\n', '\n', '        Args:\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`:  New table with the passed column added.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:cast,Table:cast,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",494,"['        """"""\n', '        Cast table values to another schema.\n', '\n', '        Args:\n', '            target_schema (`Schema`):\n', '                Schema to cast to, the names and order of fields must match.\n', '            safe (`bool`, defaults to `True`):\n', '                Check for overflows or other unsafe conversions.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:column,Table:column,method,2,3,3,39,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",338,"['        """"""\n', '        Select a column by its column name, or numeric index.\n', '\n', '        Args:\n', '            i (`Union[int, str]`):\n', '                The index or name of the column to retrieve.\n', '\n', '        Returns:\n', '            `pyarrow.ChunkedArray`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:column_names,Table:column_names,method,2,2,2,29,14.5,0,0,['self'],[None],[None],421,"['        """"""\n', ""        Names of the table's columns.\n"", '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:columns,Table:columns,method,2,2,2,24,12.0,0,0,['self'],[None],[None],371,"['        """"""\n', '        List of all columns in numerical order.\n', '\n', '        Returns:\n', '            `List[pa.ChunkedArray]`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:combine_chunks,Table:combine_chunks,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",478,"['        """"""\n', '        Make a new table by combining the chunks this table has.\n', '\n', '        All the underlying chunks in the `ChunkedArray` of each column are\n', '        concatenated into zero or one chunk.\n', '\n', '        Args:\n', '            memory_pool (`MemoryPool`, defaults to `None`):\n', '                For memory allocations, if required, otherwise use default pool.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:drop,Table:drop,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",597,"['        """"""\n', '        Drop one or more columns and return a new table.\n', '\n', '        Args:\n', '            columns (`List[str]`):\n', '                List of field names referencing existing columns.\n', '\n', '        Raises:\n', '            `KeyError` : if any of the passed columns name are not existing.\n', '\n', '        Returns:\n', '            `datasets.table.Table`: New table without the columns.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:equals,Table:equals,method,7,27,20,162,6.0,0,1,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",195,"['        """"""\n', '        Check if contents of two tables are equal.\n', '\n', '        Args:\n', '            other ([`~datasets.table.Table`]):\n', '                Table to compare against.\n', '            check_metadata `bool`, defaults to `False`):\n', '                Whether schema metadata equality should be checked as well.\n', '\n', '        Returns:\n', '            `bool`\n', '        """"""\n']","['tuple', 'isinstance']",2
repos/datasets/src/datasets/table.py:Table:field,Table:field,method,2,3,3,38,12.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",325,"['        """"""\n', '        Select a schema field by its column name or numeric index.\n', '\n', '        Args:\n', '            i (`Union[int, str]`):\n', '                The index or name of the field to retrieve.\n', '\n', '        Returns:\n', '            `pyarrow.Field`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:filter,Table:filter,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",458,"['        """"""\n', '        Select records from a Table. See `pyarrow.compute.filter` for full usage.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:flatten,Table:flatten,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",464,"['        """"""\n', '        Flatten this Table.  Each column with a struct type is flattened\n', '        into one column per struct field.  Other columns are left unchanged.\n', '\n', '        Args:\n', '            memory_pool (`MemoryPool`, defaults to `None`):\n', '                For memory allocations, if required, otherwise use default pool.\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:itercolumns,Table:itercolumns,method,2,3,3,44,14.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",351,"['        """"""\n', '        Iterator over all columns in their numerical order.\n', '\n', '        Yields:\n', '            `pyarrow.ChunkedArray`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:nbytes,Table:nbytes,method,2,2,2,23,11.5,0,0,['self'],[None],[None],414,"['        """"""\n', '        Total number of bytes consumed by the elements of the table.\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:num_columns,Table:num_columns,method,2,2,2,28,14.0,0,0,['self'],[None],[None],381,"['        """"""\n', '        Number of columns in this table.\n', '\n', '        Returns:\n', '            int\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:num_rows,Table:num_rows,method,2,2,2,25,12.5,0,0,['self'],[None],[None],391,"['        """"""\n', '        Number of rows in this table.\n', '\n', '        Due to the definition of a table, all columns have the same number of\n', '        rows.\n', '\n', '        Returns:\n', '            int\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:remove_column,Table:remove_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",560,"['        """"""\n', '        Create new Table with the indicated column removed.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index of column to remove.\n', '\n', '        Returns:\n', '            `datasets.table.Table`: New table without the column.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:rename_columns,Table:rename_columns,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",591,"['        """"""\n', '        Create new table with columns renamed to provided names.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:replace_schema_metadata,Table:replace_schema_metadata,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",509,"['        """"""\n', '        EXPERIMENTAL: Create shallow copy of table by replacing schema\n', '        key-value metadata with the indicated new metadata (which may be None,\n', '        which deletes any existing metadata\n', '\n', '        Args:\n', '            metadata (`dict`, defaults to `None`):\n', '\n', '        Returns:\n', '            `datasets.table.Table`: shallow_copy\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:schema,Table:schema,method,2,2,2,23,11.5,0,0,['self'],[None],[None],361,"['        """"""\n', '        Schema of the table and its columns.\n', '\n', '        Returns:\n', '            `pyarrow.Schema`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:select,Table:select,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",613,"['        """"""\n', '        Select columns of the table.\n', '\n', '        Returns a new table with the specified columns, and metadata preserved.\n', '\n', '        Args:\n', '            columns (:obj:`Union[List[str], List[int]]`):\n', '                The column names or integer indices to select.\n', '\n', '        Returns:\n', '            `datasets.table.Table`: table with only a subset of the columns\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:set_column,Table:set_column,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",573,"['        """"""\n', '        Replace column in Table at position.\n', '\n', '        Args:\n', '            i (`int`):\n', '                Index to place the column at.\n', '            field_ (`Union[str, pyarrow.Field]`):\n', '                If a string is passed then the type is deduced from the column\n', '                data.\n', '            column (`Union[pyarrow.Array, List[pyarrow.Array]]`):\n', '                Column data.\n', '\n', '        Returns:\n', '            `datasets.table.Table`: New table with the passed column set.\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:shape,Table:shape,method,2,2,2,22,11.0,0,0,['self'],[None],[None],404,"['        """"""\n', '        Dimensions of the table: (#rows, #columns).\n', '\n', '        Returns:\n', '            `(int, int)`: Number of rows and number of columns.\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:slice,Table:slice,method,1,2,2,26,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",442,"['        """"""\n', '        Compute zero-copy slice of this Table.\n', '\n', '        Args:\n', '            offset (`int`, defaults to `0`):\n', '                Offset from start of table to slice.\n', '            length (`int`, defaults to `None`):\n', '                Length of slice (default is until end of table starting from\n', '                offset).\n', '\n', '        Returns:\n', '            `datasets.table.Table`\n', '        """"""\n']",['NotImplementedError'],1
repos/datasets/src/datasets/table.py:Table:to_batches,Table:to_batches,method,2,3,3,43,14.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",212,"['        """"""\n', '        Convert Table to list of (contiguous) `RecordBatch` objects.\n', '\n', '        Args:\n', '            max_chunksize (`int`, defaults to `None`):\n', '                Maximum size for `RecordBatch` chunks. Individual chunks may be\n', '                smaller depending on the chunk layout of individual columns.\n', '\n', '        Returns:\n', '            `List[pyarrow.RecordBatch]`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:to_pandas,Table:to_pandas,method,2,3,3,42,14.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",244,"['        """"""\n', '        Convert to a pandas-compatible NumPy array or DataFrame, as appropriate.\n', '\n', '        Args:\n', '            memory_pool (`MemoryPool`, defaults to `None`):\n', '                Arrow MemoryPool to use for allocations. Uses the default memory\n', '                pool is not passed.\n', '            strings_to_categorical (`bool`, defaults to `False`):\n', '                Encode string (UTF8) and binary types to `pandas.Categorical`.\n', '            categories (`list`, defaults to `empty`):\n', '                List of fields that should be returned as `pandas.Categorical`. Only\n', '                applies to table-like data structures.\n', '            zero_copy_only (`bool`, defaults to `False`):\n', '                Raise an `ArrowException` if this function call would require copying\n', '                the underlying data.\n', '            integer_object_nulls (`bool`, defaults to `False`):\n', '                Cast integers with nulls to objects.\n', '            date_as_object (`bool`, defaults to `True`):\n', '                Cast dates to objects. If `False`, convert to `datetime64[ns]` dtype.\n', '            timestamp_as_object (`bool`, defaults to `False`):\n', '                Cast non-nanosecond timestamps (`np.datetime64`) to objects. This is\n', ""                useful if you have timestamps that don't fit in the normal date\n"", '                range of nanosecond timestamps (1678 CE-2262 CE).\n', '                If `False`, all timestamps are converted to `datetime64[ns]` dtype.\n', '            use_threads (`bool`, defaults to `True`):\n', '                Whether to parallelize the conversion using multiple threads.\n', '            deduplicate_objects (`bool`, defaults to `False`):\n', '                Do not create multiple copies Python objects when created, to save\n', '                on memory use. Conversion will be slower.\n', '            ignore_metadata (`bool`, defaults to `False`):\n', ""                If `True`, do not use the 'pandas' metadata to reconstruct the\n"", '                DataFrame index, if present.\n', '            safe (`bool`, defaults to `True`):\n', '                For certain data types, a cast is needed in order to store the\n', '                data in a pandas DataFrame or Series (e.g. timestamps are always\n', '                stored as nanoseconds in pandas). This option controls whether it\n', '                is a safe cast or not.\n', '            split_blocks (`bool`, defaults to `False`):\n', '                If `True`, generate one internal ""block"" for each column when\n', '                creating a pandas.DataFrame from a `RecordBatch` or `Table`. While this\n', '                can temporarily reduce memory note that various pandas operations\n', '                can trigger ""consolidation"" which may balloon memory use.\n', '            self_destruct (`bool`, defaults to `False`):\n', '                EXPERIMENTAL: If `True`, attempt to deallocate the originating Arrow\n', '                memory while converting the Arrow object to pandas. If you use the\n', '                object after calling `to_pandas` with this option it will crash your\n', '                program.\n', '            types_mapper (`function`, defaults to `None`):\n', '                A function mapping a pyarrow DataType to a pandas `ExtensionDtype`.\n', '                This can be used to override the default pandas type for conversion\n', '                of built-in pyarrow types or in absence of `pandas_metadata` in the\n', '                Table schema. The function receives a pyarrow DataType and is\n', '                expected to return a pandas `ExtensionDtype` or `None` if the\n', '                default conversion should be used for that type. If you have\n', '                a dictionary mapping, you can pass `dict.get` as function.\n', '\n', '        Returns:\n', '            `pandas.Series` or `pandas.DataFrame`: `pandas.Series` or `pandas.DataFrame` depending on type of object\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:to_pydict,Table:to_pydict,method,2,3,3,42,14.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",226,"['        """"""\n', '        Convert the Table to a `dict` or `OrderedDict`.\n', '\n', '        Returns:\n', '            `dict`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:to_pylist,Table:to_pylist,method,2,3,3,42,14.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",235,"['        """"""\n', '        Convert the Table to a list\n', '\n', '        Returns:\n', '            `list`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:to_reader,Table:to_reader,method,2,2,2,55,27.5,0,0,"['self', 'max_chunksize']","[None, ' Optional[int] ']","[None, ' None']",309,"['        """"""\n', '        Convert the Table to a RecordBatchReader.\n', '\n', '        Note that this method is zero-copy, it merely exposes the same data under a different API.\n', '\n', '        Args:\n', '            max_chunksize (`int`, defaults to `None`)\n', '                Maximum size for RecordBatch chunks. Individual chunks may be smaller depending\n', '                on the chunk layout of individual columns.\n', '\n', '        Returns:\n', '            `pyarrow.RecordBatchReader`\n', '        """"""\n']",[],0
repos/datasets/src/datasets/table.py:Table:to_string,Table:to_string,method,2,3,3,42,14.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",306,[],[],0
repos/datasets/src/datasets/table.py:Table:validate,Table:validate,method,2,3,3,41,13.67,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",179,"['        """"""\n', '        Perform validation checks.  An exception is raised if validation fails.\n', '\n', '        By default only cheap validation checks are run.  Pass `full=True`\n', '        for thorough validation checks (potentially `O(n)`).\n', '\n', '        Args:\n', '            full (`bool`, defaults to `False`):\n', '                If `True`, run expensive checks, otherwise cheap checks only.\n', '\n', '        Raises:\n', '            `pa.lib.ArrowInvalid`: if validation fails\n', '        """"""\n']",[],0
repos/datasets/src/datasets/tasks/__init__.py:task_template_from_dict,task_template_from_dict,function,7,21,18,254,12.1,0,1,['task_template_dict'],[' dict'],[None],39,"['    """"""Create one of the supported task templates in :py:mod:`datasets.tasks` from a dictionary.""""""\n']","['task_template_dict.get', 'logger.warning', 'NAME2TEMPLATE.get', 'template.from_dict']",4
repos/datasets/src/datasets/tasks/audio_classification.py:AudioClassification,AudioClassification,class,22,69,55,848,12.29,0,2,[],[],[],10,[],[],0
repos/datasets/src/datasets/tasks/audio_classification.py:AudioClassification:align_with_features,AudioClassification:align_with_features,method,11,34,24,417,12.26,0,2,"['self', 'features']","[None, None]","[None, None]",17,[],"['ValueError', 'isinstance', 'copy.deepcopy']",3
repos/datasets/src/datasets/tasks/audio_classification.py:AudioClassification:column_mapping,AudioClassification:column_mapping,method,3,7,7,64,9.14,0,0,['self'],[None],[None],29,[],[],0
repos/datasets/src/datasets/tasks/automatic_speech_recognition.py:AutomaticSpeechRecognition,AutomaticSpeechRecognition,class,21,68,54,888,13.06,0,2,[],[],[],10,[],[],0
repos/datasets/src/datasets/tasks/automatic_speech_recognition.py:AutomaticSpeechRecognition:align_with_features,AutomaticSpeechRecognition:align_with_features,method,11,35,25,411,11.74,0,2,"['self', 'features']","[None, None]","[None, None]",17,[],"['ValueError', 'isinstance', 'copy.deepcopy']",3
repos/datasets/src/datasets/tasks/automatic_speech_recognition.py:AutomaticSpeechRecognition:column_mapping,AutomaticSpeechRecognition:column_mapping,method,1,5,5,75,15.0,0,0,['self'],[None],[None],29,[],[],0
repos/datasets/src/datasets/tasks/base.py:TaskTemplate,TaskTemplate,class,17,58,43,517,8.91,1,0,[],[],[],14,[],[],0
repos/datasets/src/datasets/tasks/base.py:TaskTemplate:align_with_features,TaskTemplate:align_with_features,method,2,2,2,25,12.5,0,0,"['self', 'features']","[' T', ' Features']","[None, None]",20,"['        """"""\n', '        Align features with the task template.\n', '        """"""\n']",['copy.deepcopy'],1
repos/datasets/src/datasets/tasks/base.py:TaskTemplate:column_mapping,TaskTemplate:column_mapping,method,1,2,2,24,12.0,0,0,['self'],[None],[None],33,[],[],0
repos/datasets/src/datasets/tasks/base.py:TaskTemplate:features,TaskTemplate:features,method,2,3,3,55,18.33,0,0,['self'],[None],[None],28,[],['Features'],1
repos/datasets/src/datasets/tasks/base.py:TaskTemplate:from_dict,TaskTemplate:from_dict,method,5,18,14,113,6.28,1,0,"['cls', 'template_dict']","[' Type[T]', ' dict']","[None, None]",37,[],"['dataclasses.fields', 'cls', 'template_dict.items']",3
repos/datasets/src/datasets/tasks/image_classification.py:ImageClassification,ImageClassification,class,22,69,55,848,12.29,0,2,[],[],[],10,[],[],0
repos/datasets/src/datasets/tasks/image_classification.py:ImageClassification:align_with_features,ImageClassification:align_with_features,method,11,34,24,417,12.26,0,2,"['self', 'features']","[None, None]","[None, None]",17,[],"['ValueError', 'isinstance', 'copy.deepcopy']",3
repos/datasets/src/datasets/tasks/image_classification.py:ImageClassification:column_mapping,ImageClassification:column_mapping,method,3,7,7,64,9.14,0,0,['self'],[None],[None],29,[],[],0
repos/datasets/src/datasets/tasks/language_modeling.py:LanguageModeling,LanguageModeling,class,10,24,22,315,13.12,0,0,[],[],[],9,[],[],0
repos/datasets/src/datasets/tasks/language_modeling.py:LanguageModeling:column_mapping,LanguageModeling:column_mapping,method,1,3,3,31,10.33,0,0,['self'],[None],[None],17,[],[],0
repos/datasets/src/datasets/tasks/question_answering.py:QuestionAnsweringExtractive,QuestionAnsweringExtractive,class,13,48,41,575,11.98,0,0,[],[],[],9,[],[],0
repos/datasets/src/datasets/tasks/question_answering.py:QuestionAnsweringExtractive:column_mapping,QuestionAnsweringExtractive:column_mapping,method,1,7,7,99,14.14,0,0,['self'],[None],[None],28,[],[],0
repos/datasets/src/datasets/tasks/summarization.py:Summarization,Summarization,class,11,30,26,395,13.17,0,0,[],[],[],9,[],[],0
repos/datasets/src/datasets/tasks/summarization.py:Summarization:column_mapping,Summarization:column_mapping,method,1,5,5,61,12.2,0,0,['self'],[None],[None],18,[],[],0
repos/datasets/src/datasets/tasks/text_classification.py:TextClassification,TextClassification,class,22,69,55,850,12.32,0,2,[],[],[],10,[],[],0
repos/datasets/src/datasets/tasks/text_classification.py:TextClassification:align_with_features,TextClassification:align_with_features,method,11,34,24,417,12.26,0,2,"['self', 'features']","[None, None]","[None, None]",18,[],"['ValueError', 'isinstance', 'copy.deepcopy']",3
repos/datasets/src/datasets/tasks/text_classification.py:TextClassification:column_mapping,TextClassification:column_mapping,method,3,7,7,62,8.86,0,0,['self'],[None],[None],30,[],[],0
repos/datasets/src/datasets/utils/_dataset_viewer.py:get_exported_dataset_infos,get_exported_dataset_infos,function,27,107,79,1099,10.27,0,3,"['dataset', 'revision', 'token', 'bool]]']","[' str', ' str', ' Optional[Union[str', None]","[None, None, None, None]",61,"['    """"""\n', '    Get the dataset information, can be useful to get e.g. the dataset features.\n', '    Docs: https://huggingface.co/docs/datasets-server/info\n', '    """"""\n']","['http_get', 'headers=get_authentication_headers_for_url', 'info_response.raise_for_status', 'info_response.json', 'info_response.get', 'logger.debug', 'DatasetViewerError']",7
repos/datasets/src/datasets/utils/_dataset_viewer.py:get_exported_parquet_files,get_exported_parquet_files,function,29,107,82,1316,12.3,0,3,"['dataset', 'revision', 'token', 'bool]]']","[' str', ' str', ' Optional[Union[str', None]","[None, None, None, None]",25,"['    """"""\n', '    Get the dataset exported parquet files\n', '    Docs: https://huggingface.co/docs/datasets-server/parquet\n', '    """"""\n']","['http_get', 'headers=get_authentication_headers_for_url', 'parquet_data_files_response.raise_for_status', 'parquet_data_files_response.json', 'parquet_data_files_response_json.get', 'logger.debug', 'DatasetViewerError']",7
repos/datasets/src/datasets/utils/_dataset_viewer.py:DatasetViewerError,DatasetViewerError,class,0,0,0,0,0,0,0,[],[],[],15,[],[],0
repos/datasets/src/datasets/utils/_dill.py:_save_regexPattern,_save_regexPattern,function,7,17,16,144,8.47,0,0,"['pickler', 'obj']","[None, None]","[None, None]",143,[],"['log', 'pickler.save_reduce']",2
repos/datasets/src/datasets/utils/_dill.py:_save_set,_save_set,function,10,25,23,236,9.44,0,0,"['pickler', 'obj']","[None, None]","[None, None]",129,[],"['log', 'pickler.save_reduce']",2
repos/datasets/src/datasets/utils/_dill.py:_save_spacyLanguage,_save_spacyLanguage,function,14,26,25,325,12.5,0,0,"['pickler', 'obj']","[None, None]","[None, None]",188,[],"['create_spacyLanguage', 'lang_cls.from_config', 'lang_inst.from_bytes', 'log', 'obj.to_bytes', 'pickler.save_reduce']",6
repos/datasets/src/datasets/utils/_dill.py:_save_tiktokenEncoding,_save_tiktokenEncoding,function,7,19,18,194,10.21,0,0,"['pickler', 'obj']","[None, None]","[None, None]",152,[],"['log', 'pickler.save_reduce']",2
repos/datasets/src/datasets/utils/_dill.py:_save_torchGenerator,_save_torchGenerator,function,12,23,22,251,10.91,0,0,"['pickler', 'obj']","[None, None]","[None, None]",174,[],"['create_torchGenerator', 'torch.Generator', 'generator.set_state', 'log', 'pickler.save_reduce']",5
repos/datasets/src/datasets/utils/_dill.py:_save_torchTensor,_save_torchTensor,function,10,20,19,221,11.05,0,0,"['pickler', 'obj']","[None, None]","[None, None]",161,[],"['create_torchTensor', 'torch.from_numpy', 'log', 'pickler.save_reduce']",4
repos/datasets/src/datasets/utils/_dill.py:_save_transformersPreTrainedTokenizerBase,_save_transformersPreTrainedTokenizerBase,function,7,21,19,188,8.95,0,1,"['pickler', 'obj']","[None, None]","[None, None]",202,[],"['log', 'isinstance', 'pickler.save_reduce']",3
repos/datasets/src/datasets/utils/_dill.py:dump,dump,function,1,2,2,36,18.0,0,0,"['obj', 'file']","[None, None]","[None, None]",101,"['    """"""Pickle an object to a file.""""""\n']",['Pickler'],1
repos/datasets/src/datasets/utils/_dill.py:dumps,dumps,function,5,6,6,51,8.5,0,0,['obj'],[None],[None],106,"['    """"""Pickle an object to a string.""""""\n']","['BytesIO', 'dump', 'file.getvalue']",3
repos/datasets/src/datasets/utils/_dill.py:pklregister,pklregister,function,4,8,6,63,7.88,0,0,['t'],[None],[None],91,"['    """"""Register a custom reducer for the type.""""""\n']",['proxy'],1
repos/datasets/src/datasets/utils/_dill.py:Pickler,Pickler,class,38,145,82,1573,10.85,0,16,[],[],[],27,[],[],0
repos/datasets/src/datasets/utils/_dill.py:Pickler:_batch_setitems,Pickler:_batch_setitems,method,13,24,23,283,11.79,0,1,"['self', 'items']","[None, None]","[None, None]",72,[],"['super', 'sorted', 'Hasher.hash']",3
repos/datasets/src/datasets/utils/_dill.py:Pickler:memoize,Pickler:memoize,method,2,10,10,61,6.1,0,1,"['self', 'obj']","[None, None]","[None, None]",85,[],['type'],1
repos/datasets/src/datasets/utils/_dill.py:Pickler:save,Pickler:save,method,19,97,45,1028,10.6,0,14,"['self', 'obj', 'save_persistent_id']","[None, None, None]","[None, None, 'True']",31,[],"['type', 'pklregister', 'issubclass', 'getattr']",4
repos/datasets/src/datasets/utils/_filelock.py:FileLock,FileLock,class,27,70,61,877,12.53,0,3,[],[],[],26,[],[],0
repos/datasets/src/datasets/utils/_filelock.py:FileLock:__init__,FileLock:__init__,method,9,20,20,241,12.05,0,1,"['self', 'lock_file', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",34,[],"['version.parse', 'os.umask', 'self.hash_filename_if_too_long', 'super']",4
repos/datasets/src/datasets/utils/_filelock.py:FileLock:hash_filename_if_too_long,FileLock:hash_filename_if_too_long,method,15,36,30,505,14.03,0,2,"['cls', 'path']","[None, ' str']","[None, None]",45,[],"['issubclass', 'min', 'os.statvfs', 'len', 'str']",5
repos/datasets/src/datasets/utils/beam_utils.py:download_remote_to_local,download_remote_to_local,function,12,42,30,454,10.81,1,2,"['remote_file_path', 'local_file_path', 'force_download']","[None, None, None]","[None, None, 'False']",38,"['    """"""Use the Beam Filesystems to download from a remote directory on gcs/s3/hdfs...""""""\n']","['logger.info', 'fs.open', 'open', 'remote_file.read', 'local_file.write']",5
repos/datasets/src/datasets/utils/beam_utils.py:upload_local_to_remote,upload_local_to_remote,function,12,42,30,451,10.74,1,2,"['local_file_path', 'remote_file_path', 'force_upload']","[None, None, None]","[None, None, 'False']",21,"['    """"""Use the Beam Filesystems to upload to a remote directory on gcs/s3/hdfs...""""""\n']","['fs.exists', 'logger.info', 'fs.create', 'open', 'local_file.read', 'remote_file.write']",6
repos/datasets/src/datasets/utils/beam_utils.py:BeamPipeline,BeamPipeline,class,4,10,9,124,12.4,0,0,[],[],[],13,[],[],0
repos/datasets/src/datasets/utils/beam_utils.py:BeamPipeline:is_local,BeamPipeline:is_local,method,3,8,7,105,13.12,0,0,['self'],[None],[None],16,[],[],0
repos/datasets/src/datasets/utils/deprecation_utils.py:deprecated,deprecated,function,18,77,57,1025,13.31,0,4,['help_message'],[' Optional[str] '],[' None'],14,"['    """"""Decorator to mark a class or a function as deprecated.\n', '\n', '    Args:\n', '        help_message (:obj:`str`, optional): An optional message to guide the user on how to\n', '            switch to non-deprecated usage of the library.\n', '    """"""\n']","['decorator', 'inspect.isclass', 'wrapper', 'hash', 'warnings.warn', '_emitted_deprecation_warnings.add', 'deprecated_function']",7
repos/datasets/src/datasets/utils/deprecation_utils.py:DeprecatedEnum,DeprecatedEnum,class,11,44,40,396,9.0,0,0,[],[],[],83,[],[],0
repos/datasets/src/datasets/utils/deprecation_utils.py:OnAccess,OnAccess,class,10,47,33,493,10.49,0,3,[],[],[],59,[],[],0
repos/datasets/src/datasets/utils/deprecation_utils.py:DeprecatedEnum:__new__,DeprecatedEnum:__new__,method,7,8,7,95,11.88,0,0,"['cls', 'value']","[None, None]","[None, None]",88,[],['object.__new__'],1
repos/datasets/src/datasets/utils/deprecation_utils.py:DeprecatedEnum:deprecate,DeprecatedEnum:deprecate,method,2,26,26,215,8.27,0,0,['self'],[None],[None],98,[],['warnings.warn'],1
repos/datasets/src/datasets/utils/deprecation_utils.py:DeprecatedEnum:help_message,DeprecatedEnum:help_message,method,1,2,2,8,4.0,0,0,['self'],[None],[None],95,[],[],0
repos/datasets/src/datasets/utils/deprecation_utils.py:OnAccess:__call__,OnAccess:__call__,method,5,15,14,160,10.67,0,1,"['cls', 'value', 'names', '*', 'module', 'qualname', 'type', 'start']","[None, None, None, None, None, None, None, None]","[None, None, 'None', None, 'None', 'None', 'None', '1']",76,[],"['super', 'isinstance', 'obj._on_access']",3
repos/datasets/src/datasets/utils/deprecation_utils.py:OnAccess:__getattribute__,OnAccess:__getattribute__,method,5,10,9,107,10.7,0,1,"['cls', 'name']","[None, None]","[None, None]",64,[],"['super', 'isinstance', 'obj._on_access']",3
repos/datasets/src/datasets/utils/deprecation_utils.py:OnAccess:__getitem__,OnAccess:__getitem__,method,4,7,6,86,12.29,0,1,"['cls', 'name']","[None, None]","[None, None]",70,[],"['super', 'member._on_access']",2
repos/datasets/src/datasets/utils/doc_utils.py:is_documented_by,is_documented_by,function,5,8,7,120,15.0,0,0,['function_with_docstring'],[' Callable'],[None],4,"['    """"""Decorator to share docstrings across common functions.\n', '\n', '    Args:\n', '        function_with_docstring (`Callable`): Name of the function with the docstring.\n', '    """"""\n']",['wrapper'],1
repos/datasets/src/datasets/utils/experimental.py:experimental,experimental,function,5,25,24,193,7.72,0,0,['fn'],[' Callable'],[None],8,"['    """"""Decorator to flag a feature as experimental.\n', '\n', '    An experimental feature trigger a warning when used as it might be subject to breaking changes in the future.\n', '\n', '    Args:\n', '        fn (`Callable`):\n', '            The function to flag as experimental.\n', '\n', '    Returns:\n', '        `Callable`: The decorated function.\n', '\n', '    Example:\n', '\n', '    ```python\n', '    >>> from datasets.utils import experimental\n', '\n', '    >>> @experimental\n', '    ... def my_function():\n', '    ...     print(""Hello world!"")\n', '\n', '    >>> my_function()\n', ""    UserWarning: 'my_function' is experimental and might be subject to breaking changes in the future.\n"", '    Hello world!\n', '    ```\n', '    """"""\n']","['_inner_fn', 'warnings.warn', 'fn']",3
repos/datasets/src/datasets/utils/extract.py:BaseExtractor,BaseExtractor,class,2,23,17,198,8.61,0,0,[],[],[],52,[],[],0
repos/datasets/src/datasets/utils/extract.py:Bzip2Extractor,Bzip2Extractor,class,6,24,21,262,10.92,0,0,[],[],[],231,[],[],0
repos/datasets/src/datasets/utils/extract.py:ExtractManager,ExtractManager,class,23,77,58,898,11.66,0,2,[],[],[],22,[],[],0
repos/datasets/src/datasets/utils/extract.py:Extractor,Extractor,class,38,251,151,2635,10.5,4,9,[],[],[],269,[],[],0
repos/datasets/src/datasets/utils/extract.py:GzipExtractor,GzipExtractor,class,6,24,21,247,10.29,0,0,[],[],[],130,[],[],0
repos/datasets/src/datasets/utils/extract.py:Lz4Extractor,Lz4Extractor,class,10,34,31,355,10.44,0,1,[],[],[],255,[],[],0
repos/datasets/src/datasets/utils/extract.py:MagicNumberBaseExtractor,MagicNumberBaseExtractor,class,14,51,43,543,10.65,0,1,[],[],[],62,[],[],0
repos/datasets/src/datasets/utils/extract.py:RarExtractor,RarExtractor,class,11,33,31,359,10.88,0,1,[],[],[],202,[],[],0
repos/datasets/src/datasets/utils/extract.py:SevenZipExtractor,SevenZipExtractor,class,10,30,29,334,11.13,0,1,[],[],[],241,[],[],0
repos/datasets/src/datasets/utils/extract.py:TarExtractor,TarExtractor,class,21,109,71,1114,10.22,1,1,[],[],[],81,[],[],0
repos/datasets/src/datasets/utils/extract.py:XzExtractor,XzExtractor,class,6,23,20,270,11.74,0,0,[],[],[],192,[],[],0
repos/datasets/src/datasets/utils/extract.py:ZipExtractor,ZipExtractor,class,52,163,124,1375,8.44,0,6,[],[],[],140,[],[],0
repos/datasets/src/datasets/utils/extract.py:ZstdExtractor,ZstdExtractor,class,12,37,34,339,9.16,0,1,[],[],[],217,[],[],0
repos/datasets/src/datasets/utils/extract.py:BaseExtractor:extract,BaseExtractor:extract,method,0,0,0,0,0.0,0,0,[],[],[],59,"['        """"""\n', '        Fix for CVE-2007-4559\n', '        Desc:\n', '            Directory traversal vulnerability in the (1) extract and (2) extractall functions in the tarfile\n', '            module in Python allows user-assisted remote attackers to overwrite arbitrary files via a .. (dot dot)\n', '            sequence in filenames in a TAR archive, a related issue to CVE-2001-1267.\n', '        See: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2007-4559\n', '        From: https://stackoverflow.com/a/10077309\n', '        """"""\n']",[],0
repos/datasets/src/datasets/utils/extract.py:BaseExtractor:is_extractable,BaseExtractor:is_extractable,method,2,21,16,169,8.05,0,0,"['cls', 'path', 'str]', '**kwargs) -> bool', 'str]', 'output_path', 'str]) -> None']","[None, ' Union[Path', None, ' ...@staticmethod@abstractmethodinput_path: Union[Path', None, ' Union[Path', ' ...']","[None, None, None, None, None, None, None]",55,[],"['is_extractable', 'extract']",2
repos/datasets/src/datasets/utils/extract.py:Bzip2Extractor:extract,Bzip2Extractor:extract,method,4,12,10,142,11.83,0,0,"['input_path', 'str]', 'output_path', 'str]']","[' Union[Path', None, ' Union[Path', None]","[None, None, None, None]",235,[],"['bz2.open', 'open', 'shutil.copyfileobj']",3
repos/datasets/src/datasets/utils/extract.py:ExtractManager:__init__,ExtractManager:__init__,method,3,11,11,144,13.09,0,0,"['self', 'cache_dir']","[None, ' Optional[str] ']","[None, ' None']",23,[],[],0
repos/datasets/src/datasets/utils/extract.py:ExtractManager:_do_extract,ExtractManager:_do_extract,method,3,12,10,115,9.58,0,0,"['self', 'output_path', 'force_extract']","[None, ' str', ' bool']","[None, None, None]",37,[],['os.listdir'],1
repos/datasets/src/datasets/utils/extract.py:ExtractManager:_get_output_path,ExtractManager:_get_output_path,method,7,9,9,140,15.56,0,0,"['self', 'path']","[None, ' str']","[None, None]",29,[],['hash_url_to_filename'],1
repos/datasets/src/datasets/utils/extract.py:ExtractManager:extract,ExtractManager:extract,method,8,17,14,281,16.53,0,2,"['self', 'input_path', 'force_extract']","[None, ' str', ' bool ']","[None, None, ' False']",42,[],"['self._get_output_path', 'self._do_extract']",2
repos/datasets/src/datasets/utils/extract.py:Extractor:_get_magic_number_max_length,Extractor:_get_magic_number_max_length,method,7,15,13,178,11.87,2,1,['cls'],[None],[None],284,[],"['max', 'len', 'issubclass']",3
repos/datasets/src/datasets/utils/extract.py:Extractor:_read_magic_number,Extractor:_read_magic_number,method,3,8,7,124,15.5,0,0,"['path', 'str]', 'magic_number_length']","[' Union[Path', None, ' int']","[None, None, None]",293,[],['MagicNumberBaseExtractor.read_magic_number'],1
repos/datasets/src/datasets/utils/extract.py:Extractor:extract,Extractor:extract,method,16,90,60,901,10.01,1,4,"['cls', 'input_path', 'str]', 'output_path', 'str]', 'extractor_format', '# <Added version=""2.4.0""/>extractor', '']","[None, ' Union[Path', None, ' Union[Path', None, ' Optional[str] ', '', None]","[None, None, None, None, None, ' None', '""2.4.0""/>extractor: Optional[BaseExtractor] = ""deprecated""', None]",320,[],"['os.makedirs', 'str', 'FileLock', 'shutil.rmtree', 'isinstance', 'warnings.warn', 'extractor.extract', 'extractor.is_extractable']",8
repos/datasets/src/datasets/utils/extract.py:Extractor:infer_extractor_format,Extractor:infer_extractor_format,method,3,5,5,82,16.4,0,1,"['cls', 'path', 'str]) -> Optional[str]', 'magic_number_max_length))']","[None, ' Union[Path', '  # <Added version', '']","[None, None, '""2.4.0""/>)path', None]",312,[],['extractor.is_extractable'],1
repos/datasets/src/datasets/utils/extract.py:Extractor:is_extractable,Extractor:is_extractable,method,5,40,33,349,8.72,0,3,"['cls', 'path', 'str]', 'return_extractor']","[None, ' Union[Path', None, ' bool ']","[None, None, None, ' False']",300,[],"['warnings.warn', 'cls.infer_extractor_format']",2
repos/datasets/src/datasets/utils/extract.py:GzipExtractor:extract,GzipExtractor:extract,method,4,12,10,131,10.92,0,0,"['input_path', 'str]', 'output_path', 'str]']","[' Union[Path', None, ' Union[Path', None]","[None, None, None, None]",134,[],"['gzip.open', 'open', 'shutil.copyfileobj']",3
repos/datasets/src/datasets/utils/extract.py:Lz4Extractor:extract,Lz4Extractor:extract,method,8,22,20,231,10.5,0,1,"['input_path', 'str]', 'output_path', 'str]']","[' Union[Path', None, ' Union[Path', None]","[None, None, None, None]",259,[],"['ImportError', 'open', 'shutil.copyfileobj']",3
repos/datasets/src/datasets/utils/extract.py:MagicNumberBaseExtractor:is_extractable,MagicNumberBaseExtractor:is_extractable,method,7,23,19,287,12.48,0,1,"['cls', 'path', 'str]', 'magic_number']","[None, ' Union[Path', None, ' bytes ']","[None, None, None, ' b""""']",71,[],"['max', 'cls.read_magic_number', 'any']",3
repos/datasets/src/datasets/utils/extract.py:MagicNumberBaseExtractor:read_magic_number,MagicNumberBaseExtractor:read_magic_number,method,4,7,7,57,8.14,0,0,"['path', 'str]', 'magic_number_length']","[' Union[Path', None, ' int']","[None, None, None]",66,[],"['open', 'f.read']",2
repos/datasets/src/datasets/utils/extract.py:RarExtractor:extract,RarExtractor:extract,method,9,16,16,196,12.25,0,1,"['input_path', 'str]', 'output_path', 'str]']","[' Union[Path', None, ' Union[Path', None]","[None, None, None, None]",206,[],"['ImportError', 'os.makedirs', 'rarfile.RarFile', 'rf.extractall', 'rf.close']",5
repos/datasets/src/datasets/utils/extract.py:SevenZipExtractor:extract,SevenZipExtractor:extract,method,8,18,18,202,11.22,0,1,"['input_path', 'str]', 'output_path', 'str]']","[' Union[Path', None, ' Union[Path', None]","[None, None, None, None]",245,[],"['ImportError', 'os.makedirs', 'py7zr.SevenZipFile', 'archive.extractall']",4
repos/datasets/src/datasets/utils/extract.py:TarExtractor:extract,TarExtractor:extract,method,5,8,8,177,22.12,0,0,"['input_path', 'str]', 'output_path', 'str]']","[' Union[Path', None, ' Union[Path', None]","[None, None, None, None]",123,[],"['os.makedirs', 'tarfile.open', 'tar_file.extractall', 'tar_file.close']",4
repos/datasets/src/datasets/utils/extract.py:TarExtractor:is_extractable,TarExtractor:is_extractable,method,2,2,2,30,15.0,0,0,"['cls', 'path', 'str]', '**kwargs']","[None, ' Union[Path', None, None]","[None, None, None, None]",83,[],['tarfile.is_tarfile'],1
repos/datasets/src/datasets/utils/extract.py:TarExtractor:safemembers,TarExtractor:safemembers,method,12,76,48,693,9.12,1,1,"['members', 'output_path']","[None, None]","[None, None]",87,"['        """"""\n', '        Fix for CVE-2007-4559\n', '        Desc:\n', '            Directory traversal vulnerability in the (1) extract and (2) extractall functions in the tarfile\n', '            module in Python allows user-assisted remote attackers to overwrite arbitrary files via a .. (dot dot)\n', '            sequence in filenames in a TAR archive, a related issue to CVE-2001-1267.\n', '        See: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2007-4559\n', '        From: https://stackoverflow.com/a/10077309\n', '        """"""\n']","['resolved', 'badpath', 'badlink', 'logger.error', 'finfo.issym', 'finfo.islnk']",6
repos/datasets/src/datasets/utils/extract.py:XzExtractor:extract,XzExtractor:extract,method,4,11,9,138,12.55,0,0,"['input_path', 'str]', 'output_path', 'str]']","[' Union[Path', None, ' Union[Path', None]","[None, None, None, None]",196,[],"['lzma.open', 'open', 'shutil.copyfileobj']",3
repos/datasets/src/datasets/utils/extract.py:ZipExtractor:extract,ZipExtractor:extract,method,5,9,9,135,15.0,0,0,"['input_path', 'str]', 'output_path', 'str]']","[' Union[Path', None, ' Union[Path', None]","[None, None, None, None]",185,[],"['os.makedirs', 'zipfile.ZipFile', 'zip_file.extractall', 'zip_file.close']",4
repos/datasets/src/datasets/utils/extract.py:ZipExtractor:is_extractable,ZipExtractor:is_extractable,method,45,121,93,973,8.04,0,6,"['cls', 'path', 'str]', 'magic_number']","[None, ' Union[Path', None, ' bytes ']","[None, None, None, ' b""""']",148,[],"['super', 'open', '_EndRecData', 'fp.seek', 'fp.tell', 'fp.read', 'len', 'struct.unpack']",8
repos/datasets/src/datasets/utils/extract.py:ZstdExtractor:extract,ZstdExtractor:extract,method,10,25,23,215,8.6,0,1,"['input_path', 'str]', 'output_path', 'str]']","[' Union[Path', None, ' Union[Path', None]","[None, None, None, None]",221,[],"['ImportError', 'zstd.ZstdDecompressor', 'open', 'dctx.copy_stream']",4
repos/datasets/src/datasets/utils/file_utils.py:_add_retries_to_file_obj_read_method,_add_retries_to_file_obj_read_method,function,26,70,60,740,10.57,1,0,['file_obj'],[None],[None],1093,[],"['read_with_retries', 'range', 'read', 'logger.warning', 'time.sleep', 'ConnectionError', 'io.RawIOBase', 'getattr']",8
repos/datasets/src/datasets/utils/file_utils.py:_as_str,_as_str,function,2,7,7,64,9.14,0,0,"['path', 'Path', 'xPath]']","[' Union[str', None, None]","[None, None, None]",1455,[],"['str', 'isinstance']",2
repos/datasets/src/datasets/utils/file_utils.py:_get_extraction_protocol,_get_extraction_protocol,function,17,70,54,716,10.23,0,2,"['urlpath', 'download_config']","[' str', ' Optional[DownloadConfig] ']","[None, ' None']",820,[],"['str', 'urlpath.split', '_get_path_extension', 'path.endswith', '_prepare_path_and_storage_options', 'fsspec.open', '_get_extraction_protocol_with_magic_number', 'urlpath.startswith', 'FileNotFoundError']",9
repos/datasets/src/datasets/utils/file_utils.py:_get_extraction_protocol_with_magic_number,_get_extraction_protocol_with_magic_number,function,10,40,28,513,12.82,1,2,['f'],[None],[None],802,"['    """"""read the magic number from a file-like object and return the compression protocol""""""\n']","['f.seek', 'f.read', 'range', 'MAGIC_NUMBER_TO_COMPRESSION_PROTOCOL.get', 'MAGIC_NUMBER_TO_UNSUPPORTED_COMPRESSION_PROTOCOL.get', 'NotImplementedError']",6
repos/datasets/src/datasets/utils/file_utils.py:_get_path_extension,_get_path_extension,function,5,10,8,96,9.6,1,0,['path'],[' str'],[None],792,[],"['path.split', 'extension.split']",2
repos/datasets/src/datasets/utils/file_utils.py:_prepare_path_and_storage_options,_prepare_path_and_storage_options,function,10,17,16,306,18.0,1,0,"['urlpath', 'download_config']","[' str', ' Optional[DownloadConfig] ']","[None, ' None']",1123,[],"['urlpath.split', '_prepare_single_hop_path_and_storage_options', 'prepared_urlpath.append', 'prepared_storage_options.update']",4
repos/datasets/src/datasets/utils/file_utils.py:_prepare_single_hop_path_and_storage_options,_prepare_single_hop_path_and_storage_options,function,34,162,98,1733,10.7,2,11,"['urlpath', 'download_config']","[' str', ' Optional[DownloadConfig] ']","[None, ' None']",1135,"['    """"""\n', '    Prepare the URL and the kwargs that must be passed to the HttpFileSystem or to requests.get/head\n', '\n', '    In particular it resolves google drive URLs\n', '    It also adds the authentication headers for the Hugging Face Hub, for both https:// and hf:// paths.\n', '\n', '    Storage options are formatted in the form {protocol: storage_options_for_protocol}\n', '    """"""\n']","['urlpath.startswith', 'urlpath[len', 'urlpath.split', 'fsspec.available_protocols', 'get_datasets_user_agent', 'http_head', 'k.startswith', 'version.parse']",8
repos/datasets/src/datasets/utils/file_utils.py:_raise_if_offline_mode_is_enabled,_raise_if_offline_mode_is_enabled,function,2,20,15,129,6.45,0,1,['msg'],[' Optional[str] '],[' None'],301,"['    """"""Raise an OfflineModeIsEnabled error (subclass of ConnectionError) if HF_DATASETS_OFFLINE is True.""""""\n']","['OfflineModeIsEnabled', 'str']",2
repos/datasets/src/datasets/utils/file_utils.py:_request_with_retry,_request_with_retry,function,13,55,51,519,9.44,1,1,"['method', 'url', 'max_retries', 'base_wait_time', 'max_wait_time', 'timeout', '**params', '']","[' str', ' str', ' int ', ' float ', ' float ', ' float ', None, None]","[None, None, ' 0', ' 0.5', ' 2', ' 10.0', None, None]",309,"['    """"""Wrapper around requests to retry in case it fails with a ConnectTimeout, with exponential backoff.\n', '\n', '    Note that if the environment variable HF_DATASETS_OFFLINE is set to 1, then a OfflineModeIsEnabled error is raised.\n', '\n', '    Args:\n', ""        method (str): HTTP method, such as 'GET' or 'HEAD'.\n"", '        url (str): The URL of the resource to fetch.\n', '        max_retries (int): Maximum number of retries, defaults to 0 (no retries).\n', '        base_wait_time (float): Duration (in seconds) to wait before retrying the first time. Wait time between\n', '            retries then grows exponentially, capped by max_wait_time.\n', '        max_wait_time (float): Maximum amount of time between two retries, in seconds.\n', '        **params (additional keyword arguments): Params to pass to :obj:`requests.request`.\n', '    """"""\n']","['_raise_if_offline_mode_is_enabled', 'requests.request', 'logger.info', 'min', 'time.sleep']",5
repos/datasets/src/datasets/utils/file_utils.py:add_end_docstrings,add_end_docstrings,function,4,17,15,136,8.0,0,0,['*docstr'],[None],[None],715,[],['docstring_decorator'],1
repos/datasets/src/datasets/utils/file_utils.py:add_start_docstrings,add_start_docstrings,function,4,17,15,136,8.0,0,0,['*docstr'],[None],[None],707,[],['docstring_decorator'],1
repos/datasets/src/datasets/utils/file_utils.py:cached_path,cached_path,function,34,130,89,2117,16.28,0,10,"['url_or_filename', 'download_config', '**download_kwargs', '']","[None, None, None, None]","[None, 'None', None, None]",164,"['    """"""\n', '    Given something that might be a URL (or might be a local path),\n', ""    determine which. If it's a URL, download the file and cache it, and\n"", ""    return the path to the cached file. If it's already a local path,\n"", '    make sure the file exists and then return the path.\n', '\n', '    Return:\n', '        Local path (string)\n', '\n', '    Raises:\n', '        FileNotFoundError: in case of non-recoverable file\n', '            (non-existent or no cache on disk)\n', '        ConnectionError: in case of unreachable url\n', '            and no cache on disk\n', ""        ValueError: if it couldn't parse the url or filename correctly\n"", '        requests.exceptions.ConnectionError: in case of internet connection issue\n', '    """"""\n']","['DownloadConfig', 'isinstance', 'str', 'can_be_local', 'strip_protocol', 'is_remote_url', 'get_from_cache', 'is_local_path', 'FileNotFoundError', 'ValueError', '_get_extraction_protocol', '_get_path_extension', 'url_or_filename.split', 'relative_to_absolute_path', 'inner_file.rindex', 'ExtractManager']",16
repos/datasets/src/datasets/utils/file_utils.py:estimate_dataset_size,estimate_dataset_size,function,2,6,6,44,7.33,0,0,['paths'],[None],[None],723,[],['sum'],1
repos/datasets/src/datasets/utils/file_utils.py:fsspec_get,fsspec_get,function,11,43,42,506,11.77,0,1,"['url', 'temp_file', 'storage_options', 'desc', 'disable_tqdm']","[None, None, None, None, None]","[None, None, 'None', 'None', 'False']",370,[],"['_raise_if_offline_mode_is_enabled', 'url_to_fs', 'TqdmCallback', 'multiprocessing.current_process', 'fs.get_file']",5
repos/datasets/src/datasets/utils/file_utils.py:fsspec_head,fsspec_head,function,6,12,12,122,10.17,0,0,"['url', 'storage_options']","[None, None]","[None, 'None']",348,[],"['_raise_if_offline_mode_is_enabled', 'url_to_fs', 'fs.info']",3
repos/datasets/src/datasets/utils/file_utils.py:ftp_get,ftp_get,function,7,26,25,276,10.62,0,0,"['url', 'temp_file', 'timeout']","[None, None, None]","[None, None, '10.0']",398,[],"['_raise_if_offline_mode_is_enabled', 'logger.info', 'closing', 'shutil.copyfileobj', 'ConnectionError']",5
repos/datasets/src/datasets/utils/file_utils.py:ftp_head,ftp_head,function,5,17,16,171,10.06,0,0,"['url', 'timeout']","[None, None]","[None, '10.0']",388,[],"['_raise_if_offline_mode_is_enabled', 'closing', 'r.read']",3
repos/datasets/src/datasets/utils/file_utils.py:get_authentication_headers_for_url,get_authentication_headers_for_url,function,6,44,39,407,9.25,0,2,"['url', 'token', 'bool]] ', 'use_auth_token', 'bool]] ']","[' str', ' Optional[Union[str', None, ' Optional[Union[str', None]","[None, None, ' None', None, ' ""deprecated""']",278,"['    """"""Handle the HF authentication""""""\n']","['warnings.warn', 'url.startswith']",2
repos/datasets/src/datasets/utils/file_utils.py:get_datasets_user_agent,get_datasets_user_agent,function,8,61,31,555,9.1,0,5,"['user_agent', 'dict]] ']","[' Optional[Union[str', None]","[None, ' None']",258,[],"['isinstance', 'user_agent.items']",2
repos/datasets/src/datasets/utils/file_utils.py:get_from_cache,get_from_cache,function,79,487,267,4427,9.09,1,20,"['url', 'cache_dir', 'force_download', 'proxies', 'etag_timeout', 'resume_download', 'user_agent', 'local_files_only', 'use_etag', 'max_retries', 'token', 'use_auth_token', 'ignore_url_params', 'storage_options', 'download_desc', 'disable_tqdm', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, 'None', 'False', 'None', '100', 'False', 'None', 'False', 'True', '0', 'None', '""deprecated""', 'False', 'None', 'None', 'False', None]",494,"['    """"""\n', '    Given a URL, look for the corresponding file in the local cache.\n', ""    If it's not there, download it. Then return the path to the cached file.\n"", '\n', '    Return:\n', '        Local path (string)\n', '\n', '    Raises:\n', '        FileNotFoundError: in case of non-recoverable file\n', '            (non-existent or no cache on disk)\n', '        ConnectionError: in case of unreachable url\n', '            and no cache on disk\n', '    """"""\n']","['warnings.warn', 'isinstance', 'str', 'os.makedirs', 'urljoin', 'urlparse', 'hash_url_to_filename', 'get_authentication_headers_for_url', 'ftp_head', 'fsspec_head', 'response.get', 'http_head', 'k.startswith', 're.match', 'logger.info', 'ConnectionError', 'FileNotFoundError', '_raise_if_offline_mode_is_enabled', 'FileLock', 'temp_file_manager', 'open', 'partial', 'os.stat', 'ftp_get', 'fsspec_get', 'http_get', 'shutil.move', 'os.umask', 'os.chmod', 'json.dump']",30
repos/datasets/src/datasets/utils/file_utils.py:hash_url_to_filename,hash_url_to_filename,function,13,23,18,279,12.13,0,2,"['url', 'etag']","[None, None]","[None, 'None']",140,"['    """"""\n', '    Convert `url` into a hashed filename in a repeatable way.\n', ""    If `etag` is specified, append its hash to the url's, delimited\n"", '    by a period.\n', ""    If the url ends with .h5 (Keras HDF5 weights) adds '.h5' to the name\n"", '    so that TF 2.0 can identify it as a HDF5 file\n', '    (see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n', '    """"""\n']","['url.encode', 'insecure_hashlib.sha256', 'url_hash.hexdigest', 'etag.encode', 'etag_hash.hexdigest', 'url.endswith']",6
repos/datasets/src/datasets/utils/file_utils.py:head_hf_s3,head_hf_s3,function,3,8,8,131,16.38,0,0,"['identifier', 'filename', 'use_cdn', 'dataset', 'max_retries']","[' str', ' str', None, None, None]","[None, None, 'False', 'True', '0']",108,[],"['http_head', 'hf_bucket_url']",2
repos/datasets/src/datasets/utils/file_utils.py:hf_bucket_url,hf_bucket_url,function,8,19,14,253,13.32,0,3,"['identifier', 'filename', 'use_cdn', 'dataset']","[' str', ' str', None, None]","[None, None, 'False', 'True']",100,[],[],0
repos/datasets/src/datasets/utils/file_utils.py:hf_github_url,hf_github_url,function,8,21,17,286,13.62,0,2,"['path', 'name', 'dataset', 'revision']","[' str', ' str', None, ' Optional[str] ']","[None, None, 'True', ' None']",117,[],['version.parse'],1
repos/datasets/src/datasets/utils/file_utils.py:http_get,http_get,function,27,87,71,1028,11.82,1,4,"['url', 'temp_file', 'proxies', 'resume_size', 'headers', 'cookies', 'timeout', 'max_retries', 'desc', 'disable_tqdm', '']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'None', '0', 'None', 'None', '100.0', '0', 'None', 'False', None]",408,[],"['dict', 'get_datasets_user_agent', '_request_with_retry', 'int', 'hf_tqdm', 'multiprocessing.current_process', 'response.iter_content', 'progress.update', 'temp_file.write']",9
repos/datasets/src/datasets/utils/file_utils.py:http_head,http_head,function,7,19,18,315,16.58,0,0,"['url', 'proxies', 'headers', 'cookies', 'allow_redirects', 'timeout', 'max_retries']","[None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'True', '10.0', '0']",457,[],"['copy.deepcopy', 'get_datasets_user_agent', '_request_with_retry']",3
repos/datasets/src/datasets/utils/file_utils.py:init_hf_modules,init_hf_modules,function,10,30,21,392,13.07,0,3,"['hf_modules_cache', 'str]] ']","[' Optional[Union[Path', None]","[None, ' None']",60,"['    """"""\n', '    Add hf_modules_cache to the python path.\n', ""    By default hf_modules_cache='~/.cache/huggingface/modules'.\n"", '    It can also be set with the environment variable HF_MODULES_CACHE.\n', '    This is used to add modules such as `datasets_modules`\n', '    """"""\n']","['str', 'os.makedirs', 'open']",3
repos/datasets/src/datasets/utils/file_utils.py:is_local_path,is_local_path,function,2,6,6,98,16.33,0,0,['url_or_filename'],[' str'],[None],83,[],['urlparse'],1
repos/datasets/src/datasets/utils/file_utils.py:is_relative_path,is_relative_path,function,2,6,6,78,13.0,0,0,['url_or_filename'],[' str'],[None],90,[],['urlparse'],1
repos/datasets/src/datasets/utils/file_utils.py:is_remote_url,is_remote_url,function,2,7,7,102,14.57,0,0,['url_or_filename'],[' str'],[None],79,[],['urlparse'],1
repos/datasets/src/datasets/utils/file_utils.py:readline,readline,function,8,18,15,105,5.83,1,2,['f'],[' io.RawIOBase'],[None],727,[],"['bytearray', 'f.read', 'res.endswith', 'bytes']",4
repos/datasets/src/datasets/utils/file_utils.py:relative_to_absolute_path,relative_to_absolute_path,function,4,9,8,143,15.89,0,0,['path'],[' T'],[None],94,"['    """"""Convert relative path to absolute path.""""""\n']","['Path', 'isinstance']",2
repos/datasets/src/datasets/utils/file_utils.py:request_etag,request_etag,function,12,57,49,506,8.88,0,2,"['url', 'token', 'bool]] ', 'use_auth_token', 'bool]] ']","[' str', ' Optional[Union[str', None, ' Optional[Union[str', None]","[None, None, ' None', None, ' ""deprecated""']",475,[],"['warnings.warn', 'urlparse', 'get_authentication_headers_for_url', 'http_head', 'response.raise_for_status']",5
repos/datasets/src/datasets/utils/file_utils.py:stack_multiprocessing_download_progress_bars,stack_multiprocessing_download_progress_bars,function,2,4,4,93,23.25,0,0,[],[],[],354,[],['patch.dict'],1
repos/datasets/src/datasets/utils/file_utils.py:url_or_path_join,url_or_path_join,function,4,14,13,180,12.86,0,1,"['base_name', '*pathnames']","[' str', ' str']","[None, None]",126,[],"['is_remote_url', 'posixpath.join', 'Path']",3
repos/datasets/src/datasets/utils/file_utils.py:url_or_path_parent,url_or_path_parent,function,5,8,7,114,14.25,0,1,['url_or_path'],[' str'],[None],133,[],"['is_remote_url', 'url_or_path.rindex']",2
repos/datasets/src/datasets/utils/file_utils.py:xbasename,xbasename,function,6,10,9,120,12.0,0,1,['a'],[None],[None],925,"['    """"""\n', '    This function extends os.path.basename to support the ""::"" hop separator. It supports both paths and urls.\n', '\n', '    A shorthand, particularly useful where you have multiple hops, is to “chain” the URLs with the special separator ""::"".\n', '    This is used to access files inside a zip file over http for example.\n', '\n', ""    Let's say you have a zip file at https://host.com/archive.zip, and you want to access the file inside the zip file at /folder1/file.txt.\n"", '    Then you can just chain the url this way:\n', '\n', '        zip://folder1/file.txt::https://host.com/archive.zip\n', '\n', '    The xbasename function allows you to apply the basename on the first path of the chain.\n', '\n', '    Example::\n', '\n', '        >>> xbasename(""zip://folder1/file.txt::https://host.com/archive.zip"")\n', '        file.txt\n', '    """"""\n']","['str', 'is_local_path', 'posixpath.basename']",3
repos/datasets/src/datasets/utils/file_utils.py:xdirname,xdirname,function,8,18,15,160,8.89,0,2,['a'],[None],[None],873,"['    """"""\n', '    This function extends os.path.dirname to support the ""::"" hop separator. It supports both paths and urls.\n', '\n', '    A shorthand, particularly useful where you have multiple hops, is to “chain” the URLs with the special separator ""::"".\n', '    This is used to access files inside a zip file over http for example.\n', '\n', ""    Let's say you have a zip file at https://host.com/archive.zip, and you want to access the file inside the zip file at /folder1/file.txt.\n"", '    Then you can just chain the url this way:\n', '\n', '        zip://folder1/file.txt::https://host.com/archive.zip\n', '\n', '    The xdirname function allows you to apply the dirname on the first path of the chain.\n', '\n', '    Example::\n', '\n', '        >>> xdirname(""zip://folder1/file.txt::https://host.com/archive.zip"")\n', '        zip://folder1::https://host.com/archive.zip\n', '    """"""\n']","['str', 'is_local_path', 'posixpath.dirname', 'a.endswith']",4
repos/datasets/src/datasets/utils/file_utils.py:xet_parse,xet_parse,function,5,16,15,159,9.94,0,1,"['source', 'parser', 'download_config']","[None, None, ' Optional[DownloadConfig] ']","[None, 'None', ' None']",1528,"['    """"""Extend `xml.etree.ElementTree.parse` function to support remote files.\n', '\n', '    Args:\n', '        source: File path or file object.\n', '        parser (`XMLParser`, *optional*, default `XMLParser`): Parser instance.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '\n', '    Returns:\n', '        `xml.etree.ElementTree.Element`: Root element of the given source document.\n', '    """"""\n']","['hasattr', 'ET.parse', 'xopen']",3
repos/datasets/src/datasets/utils/file_utils.py:xexists,xexists,function,12,21,18,320,15.24,0,1,"['urlpath', 'download_config']","[' str', ' Optional[DownloadConfig] ']","[None, ' None']",904,"['    """"""Extend `os.path.exists` function to support both local and remote files.\n', '\n', '    Args:\n', '        urlpath (`str`): URL path.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '\n', '    Returns:\n', '        `bool`\n', '    """"""\n']","['_as_str', 'is_local_path', '_prepare_path_and_storage_options', 'urlpath.split', 'url_to_fs', 'fs.exists']",6
repos/datasets/src/datasets/utils/file_utils.py:xgetsize,xgetsize,function,17,44,35,473,10.75,0,2,"['path', 'download_config']","[None, ' Optional[DownloadConfig] ']","[None, ' None']",1025,"['    """"""Extend `os.path.getsize` function to support remote files.\n', '\n', '    Args:\n', '        path (`str`): URL path.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '\n', '    Returns:\n', '        `int`: optional\n', '    """"""\n']","['str', 'is_local_path', '_prepare_path_and_storage_options', 'path.split', 'url_to_fs', 'fs.size', 'FileNotFoundError', 'xopen', 'len']",9
repos/datasets/src/datasets/utils/file_utils.py:xglob,xglob,function,18,38,34,540,14.21,0,2,"['urlpath', '*', 'recursive', 'download_config']","[None, None, None, ' Optional[DownloadConfig] ']","[None, None, 'False', ' None']",1269,"['    """"""Extend `glob.glob` function to support remote files.\n', '\n', '    Args:\n', '        urlpath (`str`): URL path with shell-style wildcard patterns.\n', '        recursive (`bool`, default `False`): Whether to match the ""**"" pattern recursively to zero or more\n', '            directories or subdirectories.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '\n', '    Returns:\n', '        `list` of `str`\n', '    """"""\n']","['_as_str', 'is_local_path', 'glob.glob', '_prepare_path_and_storage_options', 'urlpath.split', 'url_to_fs', 'main_hop.split', 'fs.glob', 'isinstance']",9
repos/datasets/src/datasets/utils/file_utils.py:xgzip_open,xgzip_open,function,7,18,16,243,13.5,0,1,"['filepath_or_buffer', '*args', 'download_config', '**kwargs']","[None, None, ' Optional[DownloadConfig] ', None]","[None, None, ' None', None]",1459,[],"['hasattr', 'gzip.open', 'str']",3
repos/datasets/src/datasets/utils/file_utils.py:xisdir,xisdir,function,15,30,23,379,12.63,0,2,"['path', 'download_config']","[None, ' Optional[DownloadConfig] ']","[None, ' None']",1053,"['    """"""Extend `os.path.isdir` function to support remote files.\n', '\n', '    Args:\n', '        path (`str`): URL path.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '\n', '    Returns:\n', '        `bool`\n', '    """"""\n']","['str', 'is_local_path', '_prepare_path_and_storage_options', 'path.split', 'url_to_fs', 'main_hop.split', 'inner_path.strip', 'fs.isdir']",8
repos/datasets/src/datasets/utils/file_utils.py:xisfile,xisfile,function,12,21,18,297,14.14,0,1,"['path', 'download_config']","[None, ' Optional[DownloadConfig] ']","[None, ' None']",1005,"['    """"""Extend `os.path.isfile` function to support remote files.\n', '\n', '    Args:\n', '        path (`str`): URL path.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '\n', '    Returns:\n', '        `bool`\n', '    """"""\n']","['str', 'is_local_path', '_prepare_path_and_storage_options', 'path.split', 'url_to_fs', 'fs.isfile']",6
repos/datasets/src/datasets/utils/file_utils.py:xjoin,xjoin,function,7,15,13,120,8.0,0,1,"['a', '*p']","[None, None]","[None, None]",846,"['    """"""\n', '    This function extends os.path.join to support the ""::"" hop separator. It supports both paths and urls.\n', '\n', '    A shorthand, particularly useful where you have multiple hops, is to “chain” the URLs with the special separator ""::"".\n', '    This is used to access files inside a zip file over http for example.\n', '\n', ""    Let's say you have a zip file at https://host.com/archive.zip, and you want to access the file inside the zip file at /folder1/file.txt.\n"", '    Then you can just chain the url this way:\n', '\n', '        zip://folder1/file.txt::https://host.com/archive.zip\n', '\n', '    The xjoin function allows you to apply the join on the first path of the chain.\n', '\n', '    Example::\n', '\n', '        >>> xjoin(""zip://folder1::https://host.com/archive.zip"", ""file.txt"")\n', '        zip://folder1/file.txt::https://host.com/archive.zip\n', '    """"""\n']","['str', 'is_local_path', 'posixpath.join']",3
repos/datasets/src/datasets/utils/file_utils.py:xlistdir,xlistdir,function,17,40,36,514,12.85,0,2,"['path', 'download_config']","[' str', ' Optional[DownloadConfig] ']","[None, ' None']",1244,"['    """"""Extend `os.listdir` function to support remote files.\n', '\n', '    Args:\n', '        path (`str`): URL path.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '\n', '    Returns:\n', '        `list` of `str`\n', '    """"""\n']","['_as_str', 'is_local_path', 'os.listdir', '_prepare_path_and_storage_options', 'path.split', 'url_to_fs', 'main_hop.split', 'inner_path.strip', 'fs.isdir', 'FileNotFoundError', 'fs.listdir']",11
repos/datasets/src/datasets/utils/file_utils.py:xnumpy_load,xnumpy_load,function,8,20,18,244,12.2,0,1,"['filepath_or_buffer', '*args', 'download_config', '**kwargs']","[None, None, ' Optional[DownloadConfig] ', None]","[None, None, ' None', None]",1469,[],"['hasattr', 'np.load', 'str']",3
repos/datasets/src/datasets/utils/file_utils.py:xopen,xopen,function,21,108,90,911,8.44,0,3,"['file', 'mode', '*args', 'download_config', '**kwargs']","[' str', None, None, ' Optional[DownloadConfig] ', None]","[None, '""r""', None, ' None', None]",1197,"['    """"""Extend `open` function to support remote files using `fsspec`.\n', '\n', '    It also has a retry mechanism in case connection fails.\n', '    The `args` and `kwargs` are passed to `fsspec.open`, except `token` which is used for queries to private repos on huggingface.co\n', '\n', '    Args:\n', '        file (`str`): Path name of the file to be opened.\n', '        mode (`str`, *optional*, default ""r""): Mode in which the file is opened.\n', '        *args: Arguments to be passed to `fsspec.open`.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '        **kwargs: Keyword arguments to be passed to `fsspec.open`.\n', '\n', '    Returns:\n', '        file object\n', '    """"""\n']","['_as_str', 'file_str.split', 'is_local_path', 'kwargs.pop', 'open', '_prepare_path_and_storage_options', 'fsspec.open', 'str', 'NonStreamableDatasetError', 'file.startswith', 'FileNotFoundError', '_add_retries_to_file_obj_read_method']",12
repos/datasets/src/datasets/utils/file_utils.py:xpandas_read_csv,xpandas_read_csv,function,11,25,23,386,15.44,0,2,"['filepath_or_buffer', 'download_config', '**kwargs']","[None, ' Optional[DownloadConfig] ', None]","[None, ' None', None]",1479,[],"['hasattr', 'pd.read_csv', 'str', 'kwargs.get', '_get_extraction_protocol']",5
repos/datasets/src/datasets/utils/file_utils.py:xpandas_read_excel,xpandas_read_excel,function,14,46,31,523,11.37,0,1,"['filepath_or_buffer', 'download_config', '**kwargs']","[None, ' Optional[DownloadConfig] ', None]","[None, ' None', None]",1491,[],"['hasattr', 'pd.read_excel', 'str', 'BytesIO']",4
repos/datasets/src/datasets/utils/file_utils.py:xpyarrow_parquet_read_table,xpyarrow_parquet_read_table,function,8,18,17,259,14.39,0,1,"['filepath_or_buffer', 'download_config', '**kwargs']","[None, ' Optional[DownloadConfig] ', None]","[None, ' None', None]",1509,[],"['hasattr', 'pq.read_table', 'str']",3
repos/datasets/src/datasets/utils/file_utils.py:xrelpath,xrelpath,function,6,20,15,256,12.8,0,1,"['path', 'start']","[None, None]","[None, 'None']",1076,"['    """"""Extend `os.path.relpath` function to support remote files.\n', '\n', '    Args:\n', '        path (`str`): URL path.\n', '        start (`str`): Start URL directory path.\n', '\n', '    Returns:\n', '        `str`\n', '    """"""\n']","['str', 'is_local_path', 'posixpath.relpath', 'start=str']",4
repos/datasets/src/datasets/utils/file_utils.py:xsio_loadmat,xsio_loadmat,function,6,16,15,201,12.56,0,1,"['filepath_or_buffer', 'download_config', '**kwargs']","[None, ' Optional[DownloadConfig] ', None]","[None, ' None', None]",1519,[],"['hasattr', 'sio.loadmat']",2
repos/datasets/src/datasets/utils/file_utils.py:xsplit,xsplit,function,7,20,16,170,8.5,0,1,['a'],[None],[None],951,"['    """"""\n', '    This function extends os.path.split to support the ""::"" hop separator. It supports both paths and urls.\n', '\n', '    A shorthand, particularly useful where you have multiple hops, is to “chain” the URLs with the special separator ""::"".\n', '    This is used to access files inside a zip file over http for example.\n', '\n', ""    Let's say you have a zip file at https://host.com/archive.zip, and you want to access the file inside the zip file at /folder1/file.txt.\n"", '    Then you can just chain the url this way:\n', '\n', '        zip://folder1/file.txt::https://host.com/archive.zip\n', '\n', '    The xsplit function allows you to apply the xsplit on the first path of the chain.\n', '\n', '    Example::\n', '\n', '        >>> xsplit(""zip://folder1/file.txt::https://host.com/archive.zip"")\n', ""        ('zip://folder1::https://host.com/archive.zip', 'file.txt')\n"", '    """"""\n']","['str', 'is_local_path', 'posixpath.split', 'a.endswith']",4
repos/datasets/src/datasets/utils/file_utils.py:xsplitext,xsplitext,function,7,15,12,147,9.8,0,1,['a'],[None],[None],978,"['    """"""\n', '    This function extends os.path.splitext to support the ""::"" hop separator. It supports both paths and urls.\n', '\n', '    A shorthand, particularly useful where you have multiple hops, is to “chain” the URLs with the special separator ""::"".\n', '    This is used to access files inside a zip file over http for example.\n', '\n', ""    Let's say you have a zip file at https://host.com/archive.zip, and you want to access the file inside the zip file at /folder1/file.txt.\n"", '    Then you can just chain the url this way:\n', '\n', '        zip://folder1/file.txt::https://host.com/archive.zip\n', '\n', '    The xsplitext function allows you to apply the splitext on the first path of the chain.\n', '\n', '    Example::\n', '\n', '        >>> xsplitext(""zip://folder1/file.txt::https://host.com/archive.zip"")\n', ""        ('zip://folder1/file::https://host.com/archive.zip', '.txt')\n"", '    """"""\n']","['str', 'is_local_path', 'posixpath.splitext']",3
repos/datasets/src/datasets/utils/file_utils.py:xwalk,xwalk,function,22,49,42,599,12.22,1,3,"['urlpath', 'download_config', '**kwargs']","[None, ' Optional[DownloadConfig] ', None]","[None, ' None', None]",1295,"['    """"""Extend `os.walk` function to support remote files.\n', '\n', '    Args:\n', '        urlpath (`str`): URL root path.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '        **kwargs: Additional keyword arguments forwarded to the underlying filesystem.\n', '\n', '\n', '    Yields:\n', '        `tuple`: 3-tuple (dirpath, dirnames, filenames).\n', '    """"""\n']","['_as_str', 'is_local_path', 'os.walk', '_prepare_path_and_storage_options', 'urlpath.split', 'url_to_fs', 'main_hop.split', 'inner_path.strip', 'fs.isdir', 'isinstance', 'fs.walk']",11
repos/datasets/src/datasets/utils/file_utils.py:xxml_dom_minidom_parse,xxml_dom_minidom_parse,function,5,16,15,205,12.81,0,1,"['filename_or_file', 'download_config', '**kwargs']","[None, ' Optional[DownloadConfig] ', None]","[None, ' None', None]",1546,"['    """"""Extend `xml.dom.minidom.parse` function to support remote files.\n', '\n', '    Args:\n', '        filename_or_file (`str` or file): File path or file object.\n', '        download_config : mainly use token or storage_options to support different platforms and auth types.\n', '        **kwargs (optional): Additional keyword arguments passed to `xml.dom.minidom.parse`.\n', '\n', '    Returns:\n', '        :obj:`xml.dom.minidom.Document`: Parsed document.\n', '    """"""\n']","['hasattr', 'xopen']",2
repos/datasets/src/datasets/utils/file_utils.py:ArchiveIterable,ArchiveIterable,class,36,138,79,1454,10.54,2,8,[],[],[],1580,[],[],0
repos/datasets/src/datasets/utils/file_utils.py:FilesIterable,FilesIterable,class,21,81,59,866,10.69,3,4,[],[],[],1645,[],[],0
repos/datasets/src/datasets/utils/file_utils.py:NonStreamableDatasetError,NonStreamableDatasetError,class,0,1,1,4,4.0,0,0,[],[],[],788,[],[],0
repos/datasets/src/datasets/utils/file_utils.py:OfflineModeIsEnabled,OfflineModeIsEnabled,class,0,1,1,4,4.0,0,0,[],[],[],297,[],[],0
repos/datasets/src/datasets/utils/file_utils.py:TqdmCallback,TqdmCallback,class,12,27,24,289,10.7,0,1,[],[],[],360,[],[],0
repos/datasets/src/datasets/utils/file_utils.py:_IterableFromGenerator,_IterableFromGenerator,class,11,26,23,241,9.27,1,0,[],[],[],1564,[],[],0
repos/datasets/src/datasets/utils/file_utils.py:xPath,xPath,class,46,169,107,2065,12.22,1,5,[],[],[],1323,[],[],0
repos/datasets/src/datasets/utils/file_utils.py:ArchiveIterable:_iter_from_fileobj,ArchiveIterable:_iter_from_fileobj,method,5,12,9,137,11.42,0,1,"['cls', 'f']","[None, None]","[None, None]",1616,[],"['_get_extraction_protocol_with_magic_number', 'cls._iter_zip', 'cls._iter_tar']",3
repos/datasets/src/datasets/utils/file_utils.py:ArchiveIterable:_iter_from_urlpath,ArchiveIterable:_iter_from_urlpath,method,7,20,17,230,11.5,0,1,"['cls', 'urlpath', 'download_config']","[None, ' str', ' Optional[DownloadConfig] ']","[None, None, ' None']",1624,[],"['_get_extraction_protocol', 'xopen', 'cls._iter_zip', 'cls._iter_tar']",4
repos/datasets/src/datasets/utils/file_utils.py:ArchiveIterable:_iter_tar,ArchiveIterable:_iter_tar,method,13,31,25,295,9.52,1,3,['f'],[None],[None],1584,[],"['tarfile.open', 'tarinfo.isreg', 'stream.extractfile']",3
repos/datasets/src/datasets/utils/file_utils.py:ArchiveIterable:_iter_zip,ArchiveIterable:_iter_zip,method,12,25,19,247,9.88,1,3,['f'],[None],[None],1601,[],"['zipfile.ZipFile', 'zipf.infolist', 'member.is_dir', 'zipf.open']",4
repos/datasets/src/datasets/utils/file_utils.py:ArchiveIterable:from_buf,ArchiveIterable:from_buf,method,2,3,3,41,13.67,0,0,"['cls', 'fileobj']","[None, None]","[None, None]",1637,[],['cls'],1
repos/datasets/src/datasets/utils/file_utils.py:ArchiveIterable:from_urlpath,ArchiveIterable:from_urlpath,method,2,4,4,64,16.0,0,0,"['cls', 'urlpath_or_buf', 'download_config']","[None, None, ' Optional[DownloadConfig] ']","[None, None, ' None']",1641,[],['cls'],1
repos/datasets/src/datasets/utils/file_utils.py:FilesIterable:_iter_from_urlpaths,FilesIterable:_iter_from_urlpaths,method,17,53,37,552,10.42,3,4,"['cls', 'urlpaths', 'List[str]]', 'download_config']","[None, ' Union[str', None, ' Optional[DownloadConfig] ']","[None, None, None, ' None']",1649,[],"['isinstance', 'xisfile', 'xisdir', 'xwalk', 'sorted', 'dirname.startswith', 'xbasename', 'filename.startswith', 'xjoin', 'FileNotFoundError']",10
repos/datasets/src/datasets/utils/file_utils.py:FilesIterable:from_urlpaths,FilesIterable:from_urlpaths,method,2,4,4,59,14.75,0,0,"['cls', 'urlpaths', 'download_config']","[None, None, ' Optional[DownloadConfig] ']","[None, None, ' None']",1673,[],['cls'],1
repos/datasets/src/datasets/utils/file_utils.py:TqdmCallback:__init__,TqdmCallback:__init__,method,11,22,20,238,10.82,0,1,"['self', 'tqdm_kwargs', '*args', '**kwargs']","[None, None, None, None]","[None, 'None', None, None]",361,[],"['version.parse', 'super']",2
repos/datasets/src/datasets/utils/file_utils.py:_IterableFromGenerator:__init__,_IterableFromGenerator:__init__,method,7,7,7,77,11.0,0,0,"['self', 'generator', '*args', '**kwargs']","[None, ' Callable', None, None]","[None, None, None, None]",1567,[],['super'],1
repos/datasets/src/datasets/utils/file_utils.py:_IterableFromGenerator:__iter__,_IterableFromGenerator:__iter__,method,3,11,9,91,8.27,1,0,['self'],[None],[None],1573,[],['self.generator'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:__str__,xPath:__str__,method,11,32,27,330,10.31,0,2,['self'],[None],[None],1326,[],"['super', 'path_str.split', 'is_local_path', 'path_str.replace', 'SINGLE_SLASH_AFTER_PROTOCOL_PATTERN.sub', 'path_as_posix.endswith']",6
repos/datasets/src/datasets/utils/file_utils.py:xPath:__truediv__,xPath:__truediv__,method,2,2,2,22,11.0,0,0,"['self', 'p']","[None, ' str']","[None, None]",1445,[],['self.joinpath'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:exists,xPath:exists,method,2,3,3,56,18.67,0,0,"['self', 'download_config']","[None, ' Optional[DownloadConfig] ']","[None, ' None']",1336,"['        """"""Extend `pathlib.Path.exists` method to support both local and remote files.\n', '\n', '        Args:\n', '            download_config : mainly use token or storage_options to support different platforms and auth types.\n', '\n', '        Returns:\n', '            `bool`\n', '        """"""\n']",['xexists'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:glob,xPath:glob,method,19,46,39,634,13.78,1,2,"['self', 'pattern', 'download_config']","[None, None, ' Optional[DownloadConfig] ']","[None, None, ' None']",1347,"['        """"""Glob function for argument of type :obj:`~pathlib.Path` that supports both local paths end remote URLs.\n', '\n', '        Args:\n', '            pattern (`str`): Pattern that resulting paths must match.\n', '            download_config : mainly use token or storage_options to support different platforms and auth types.\n', '\n', '        Yields:\n', '            [`xPath`]\n', '        """"""\n']","['self.as_posix', 'posix_path.split', 'is_local_path', 'Path', '_prepare_path_and_storage_options', 'url_to_fs', 'fs.glob', 'type']",8
repos/datasets/src/datasets/utils/file_utils.py:xPath:joinpath,xPath:joinpath,method,2,3,3,43,14.33,0,0,"['self', '*p', '...]']","[None, ' Tuple[str', None]","[None, None, None]",1434,"['        """"""Extend :func:`xjoin` to support argument of type :obj:`~pathlib.Path`.\n', '\n', '        Args:\n', '            *p (`tuple` of `str`): Other path components.\n', '\n', '        Returns:\n', '            [`xPath`]\n', '        """"""\n']",['type'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:name,xPath:name,method,2,2,2,56,28.0,0,0,['self'],[None],[None],1396,"['        """"""Name function for argument of type :obj:`~pathlib.Path` that supports both local paths end remote URLs.\n', '\n', '        Returns:\n', '            `str`\n', '        """"""\n']",['PurePosixPath'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:open,xPath:open,method,2,4,4,37,9.25,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1422,"['        """"""Extend :func:`xopen` to support argument of type :obj:`~pathlib.Path`.\n', '\n', '        Args:\n', '            **args: Arguments passed to :func:`fsspec.open`.\n', '            **kwargs: Keyword arguments passed to :func:`fsspec.open`.\n', '\n', '        Returns:\n', '            `io.FileIO`: File-like object.\n', '        """"""\n']",['xopen'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:parent,xPath:parent,method,2,2,2,43,21.5,0,0,['self'],[None],[None],1387,"['        """"""Name function for argument of type :obj:`~pathlib.Path` that supports both local paths end remote URLs.\n', '\n', '        Returns:\n', '            [`xPath`]\n', '        """"""\n']",['type'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:rglob,xPath:rglob,method,2,4,4,39,9.75,0,0,"['self', 'pattern', '**kwargs']","[None, None, None]","[None, None, None]",1375,"['        """"""Rglob function for argument of type :obj:`~pathlib.Path` that supports both local paths end remote URLs.\n', '\n', '        Args:\n', '            pattern (`str`): Pattern that resulting paths must match.\n', '\n', '        Yields:\n', '            [`xPath`]\n', '        """"""\n']",['self.glob'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:stem,xPath:stem,method,2,2,2,56,28.0,0,0,['self'],[None],[None],1405,"['        """"""Stem function for argument of type :obj:`~pathlib.Path` that supports both local paths end remote URLs.\n', '\n', '        Returns:\n', '            `str`\n', '        """"""\n']",['PurePosixPath'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:suffix,xPath:suffix,method,2,2,2,58,29.0,0,0,['self'],[None],[None],1414,"['        """"""Suffix function for argument of type :obj:`~pathlib.Path` that supports both local paths end remote URLs.\n', '\n', '        Returns:\n', '            `str`\n', '        """"""\n']",['PurePosixPath'],1
repos/datasets/src/datasets/utils/file_utils.py:xPath:with_suffix,xPath:with_suffix,method,5,10,9,227,22.7,0,1,"['self', 'suffix']","[None, None]","[None, None]",1448,[],"['str', 'is_local_path', 'type']",3
repos/datasets/src/datasets/utils/info_utils.py:get_size_checksum_dict,get_size_checksum_dict,function,12,28,27,233,8.32,1,1,"['path', 'record_checksum']","[' str', ' bool ']","[None, ' True']",105,"['    """"""Compute the file size and the sha256 checksum of a file""""""\n']","['insecure_hashlib.sha256', 'open', 'iter', 'f.read', 'm.update', 'm.hexdigest']",6
repos/datasets/src/datasets/utils/info_utils.py:is_small_dataset,is_small_dataset,function,4,11,9,106,9.64,0,1,['dataset_size'],[None],[None],118,"['    """"""Check if `dataset_size` is smaller than `config.IN_MEMORY_MAX_SIZE`.\n', '\n', '    Args:\n', '        dataset_size (int): Dataset size in bytes.\n', '\n', '    Returns:\n', '        bool: Whether `dataset_size` is smaller than `config.IN_MEMORY_MAX_SIZE`.\n', '    """"""\n']",[],0
repos/datasets/src/datasets/utils/info_utils.py:verify_checksums,verify_checksums,function,11,73,55,804,11.01,1,5,"['expected_checksums', 'recorded_checksums', 'verification_name']","[' Optional[dict]', ' dict', None]","[None, None, 'None']",52,[],"['logger.info', 'len', 'set', 'ExpectedMoreDownloadedFiles', 'UnexpectedDownloadedFile', 'NonMatchingChecksumError']",6
repos/datasets/src/datasets/utils/info_utils.py:verify_splits,verify_splits,function,10,51,40,619,12.14,1,5,"['expected_splits', 'recorded_splits']","[' Optional[dict]', ' dict']","[None, None]",87,[],"['logger.info', 'len', 'set', 'ExpectedMoreSplits', 'UnexpectedSplits', 'NonMatchingSplitsSizesError']",6
repos/datasets/src/datasets/utils/info_utils.py:ChecksumVerificationException,ChecksumVerificationException,class,0,0,0,0,0.0,0,0,[],[],[],36,[],[],0
repos/datasets/src/datasets/utils/info_utils.py:ExpectedMoreDownloadedFiles,ExpectedMoreDownloadedFiles,class,0,0,0,0,0.0,0,0,[],[],[],44,[],[],0
repos/datasets/src/datasets/utils/info_utils.py:ExpectedMoreSplits,ExpectedMoreSplits,class,0,0,0,0,0.0,0,0,[],[],[],79,[],[],0
repos/datasets/src/datasets/utils/info_utils.py:NonMatchingChecksumError,NonMatchingChecksumError,class,0,0,0,0,0.0,0,0,[],[],[],48,[],[],0
repos/datasets/src/datasets/utils/info_utils.py:NonMatchingSplitsSizesError,NonMatchingSplitsSizesError,class,0,0,0,0,0.0,0,0,[],[],[],83,[],[],0
repos/datasets/src/datasets/utils/info_utils.py:SplitsVerificationException,SplitsVerificationException,class,0,0,0,0,0.0,0,0,[],[],[],71,[],[],0
repos/datasets/src/datasets/utils/info_utils.py:UnexpectedDownloadedFile,UnexpectedDownloadedFile,class,0,0,0,0,0.0,0,0,[],[],[],40,[],[],0
repos/datasets/src/datasets/utils/info_utils.py:UnexpectedSplits,UnexpectedSplits,class,0,0,0,0,0.0,0,0,[],[],[],75,[],[],0
repos/datasets/src/datasets/utils/info_utils.py:VerificationMode,VerificationMode,class,3,6,6,73,12.17,0,0,[],[],[],14,[],[],0
repos/datasets/src/datasets/utils/logging.py:_configure_library_root_logger,_configure_library_root_logger,function,4,4,4,161,40.25,0,0,[],[],[],74,[],"['_get_library_root_logger', 'library_root_logger.addHandler', 'library_root_logger.setLevel']",3
repos/datasets/src/datasets/utils/logging.py:_get_default_logging_level,_get_default_logging_level,function,12,29,26,288,9.93,0,2,[],[],[],49,"['    """"""\n', '    If DATASETS_VERBOSITY env var is set to one of the valid choices return that as the new default level.\n', '    If it is not - fall back to ``_default_log_level``\n', '    """"""\n']","['os.getenv', 'logging.getLogger']",2
repos/datasets/src/datasets/utils/logging.py:_get_library_name,_get_library_name,function,2,2,2,28,14.0,0,0,[],[],[],66,[],['__name__.split'],1
repos/datasets/src/datasets/utils/logging.py:_get_library_root_logger,_get_library_root_logger,function,2,2,2,44,22.0,0,0,[],[],[],70,[],['logging.getLogger'],1
repos/datasets/src/datasets/utils/logging.py:_reset_library_root_logger,_reset_library_root_logger,function,3,3,3,91,30.33,0,0,[],[],[],81,[],"['_get_library_root_logger', 'library_root_logger.setLevel']",2
repos/datasets/src/datasets/utils/logging.py:disable_propagation,disable_propagation,function,1,2,2,42,21.0,0,0,[],[],[],163,"['    """"""Disable propagation of the library log outputs.\n', '    Note that log propagation is disabled by default.\n', '    """"""\n']",['_get_library_root_logger'],1
repos/datasets/src/datasets/utils/logging.py:enable_propagation,enable_propagation,function,1,2,2,41,20.5,0,0,[],[],[],170,"['    """"""Enable propagation of the library log outputs.\n', ""    Please disable the Hugging Face datasets library's default handler to prevent double logging if the root logger has\n"", '    been configured.\n', '    """"""\n']",['_get_library_root_logger'],1
repos/datasets/src/datasets/utils/logging.py:get_logger,get_logger,function,4,8,7,68,8.5,0,1,['name'],[' Optional[str] '],[' None'],86,"['    """"""Return a logger with the specified name.\n', '    This function can be used in dataset scripts.\n', '    """"""\n']","['_get_library_name', 'logging.getLogger']",2
repos/datasets/src/datasets/utils/logging.py:get_verbosity,get_verbosity,function,2,2,2,52,26.0,0,0,[],[],[],95,"['    """"""Return the current level for the HuggingFace datasets library\'s root logger.\n', '    Returns:\n', '        Logging level, e.g., `datasets.logging.DEBUG` and `datasets.logging.INFO`.\n', '\n', '    <Tip>\n', '\n', '        HuggingFace datasets library has following logging levels:\n', '        - `datasets.logging.CRITICAL`, `datasets.logging.FATAL`\n', '        - `datasets.logging.ERROR`\n', '        - `datasets.logging.WARNING`, `datasets.logging.WARN`\n', '        - `datasets.logging.INFO`\n', '        - `datasets.logging.DEBUG`\n', '\n', '    </Tip>\n', '    """"""\n']",['_get_library_root_logger'],1
repos/datasets/src/datasets/utils/logging.py:set_verbosity,set_verbosity,function,1,1,1,46,46.0,0,0,['verbosity'],[' int'],[None],114,"['    """"""Set the level for the Hugging Face Datasets library\'s root logger.\n', '    Args:\n', '        verbosity:\n', '            Logging level, e.g., `datasets.logging.DEBUG` and `datasets.logging.INFO`.\n', '    """"""\n']",['_get_library_root_logger'],1
repos/datasets/src/datasets/utils/logging.py:set_verbosity_debug,set_verbosity_debug,function,2,2,2,26,13.0,0,0,[],[],[],143,"['    """"""Set the level for the Hugging Face datasets library\'s root logger to `DEBUG`.\n', '\n', '    This will display all the logging information and tqdm bars.\n', '\n', '    Shortcut to `datasets.logging.set_verbosity(datasets.logging.DEBUG)`.\n', '    """"""\n']",['set_verbosity'],1
repos/datasets/src/datasets/utils/logging.py:set_verbosity_error,set_verbosity_error,function,2,2,2,26,13.0,0,0,[],[],[],153,"['    """"""Set the level for the Hugging Face datasets library\'s root logger to `ERROR`.\n', '\n', '    This will display only the errors logging information and tqdm bars.\n', '\n', '    Shortcut to `datasets.logging.set_verbosity(datasets.logging.ERROR)`.\n', '    """"""\n']",['set_verbosity'],1
repos/datasets/src/datasets/utils/logging.py:set_verbosity_info,set_verbosity_info,function,2,2,2,25,12.5,0,0,[],[],[],123,"['    """"""Set the level for the Hugging Face datasets library\'s root logger to `INFO`.\n', '\n', '    This will display most of the logging information and tqdm bars.\n', '\n', '    Shortcut to `datasets.logging.set_verbosity(datasets.logging.INFO)`.\n', '    """"""\n']",['set_verbosity'],1
repos/datasets/src/datasets/utils/logging.py:set_verbosity_warning,set_verbosity_warning,function,2,2,2,28,14.0,0,0,[],[],[],133,"['    """"""Set the level for the Hugging Face datasets library\'s root logger to `WARNING`.\n', '\n', '    This will display only the warning and errors logging information and tqdm bars.\n', '\n', '    Shortcut to `datasets.logging.set_verbosity(datasets.logging.WARNING)`.\n', '    """"""\n']",['set_verbosity'],1
repos/datasets/src/datasets/utils/metadata.py:_split_yaml_from_readme,_split_yaml_from_readme,function,9,24,20,285,11.88,0,1,['readme_content'],[' str'],[None],37,[],['list'],1
repos/datasets/src/datasets/utils/metadata.py:DatasetMetadata,DatasetMetadata,class,35,141,89,1401,9.94,2,3,[],[],[],48,[],[],0
repos/datasets/src/datasets/utils/metadata.py:MetadataConfigs,MetadataConfigs,class,8,91,69,696,7.65,0,1,[],[],[],124,[],[],0
repos/datasets/src/datasets/utils/metadata.py:_NoDuplicateSafeLoader,_NoDuplicateSafeLoader,class,16,51,39,490,9.61,2,2,[],[],[],22,[],[],0
repos/datasets/src/datasets/utils/metadata.py:DatasetMetadata:_to_readme,DatasetMetadata:_to_readme,method,7,20,15,211,10.55,0,1,"['self', 'readme_content']","[None, ' Optional[str] ']","[None, ' None']",82,[],"['_split_yaml_from_readme', 'self.to_yaml_string']",2
repos/datasets/src/datasets/utils/metadata.py:DatasetMetadata:from_readme,DatasetMetadata:from_readme,method,9,18,17,185,10.28,0,1,"['cls', 'path', 'str]']","[None, ' Union[Path', None]","[None, None, None]",53,"['        """"""Loads and validates the dataset metadata from its dataset card (README.md)\n', '\n', '        Args:\n', '            path (:obj:`Path`): Path to the dataset card (its README.md file)\n', '\n', '        Returns:\n', ""            :class:`DatasetMetadata`: The dataset's metadata\n"", '\n', '        Raises:\n', ""            :obj:`TypeError`: If the dataset's metadata is invalid\n"", '        """"""\n']","['open', '_split_yaml_from_readme', 'cls.from_yaml_string', 'cls']",4
repos/datasets/src/datasets/utils/metadata.py:DatasetMetadata:from_yaml_string,DatasetMetadata:from_yaml_string,method,7,24,21,227,9.46,1,0,"['cls', 'string']","[None, ' str']","[None, None]",91,"['        """"""Loads and validates the dataset metadata from a YAML string\n', '\n', '        Args:\n', '            string (:obj:`str`): The YAML string\n', '\n', '        Returns:\n', ""            :class:`DatasetMetadata`: The dataset's metadata\n"", '\n', '        Raises:\n', ""            :obj:`TypeError`: If the dataset's metadata is invalid\n"", '        """"""\n']","['yaml.load', 'key.replace', 'metadata_dict.items', 'cls']",4
repos/datasets/src/datasets/utils/metadata.py:DatasetMetadata:to_readme,DatasetMetadata:to_readme,method,8,21,15,269,12.81,0,1,"['self', 'path']","[None, ' Path']","[None, None]",72,[],"['path.exists', 'open', 'readme_file.read', 'self._to_readme', 'readme_file.write']",5
repos/datasets/src/datasets/utils/metadata.py:DatasetMetadata:to_yaml_string,DatasetMetadata:to_yaml_string,method,8,22,21,193,8.77,1,0,['self'],[None],[None],112,[],"['yaml.safe_dump', 'self.items']",2
repos/datasets/src/datasets/utils/metadata.py:MetadataConfigs:_raise_if_data_files_field_not_valid,MetadataConfigs:_raise_if_data_files_field_not_valid,method,4,81,59,543,6.7,0,1,['metadata_config'],[' dict'],[None],130,[],"['metadata_config.get', 'textwrap.dedent']",2
repos/datasets/src/datasets/utils/metadata.py:_NoDuplicateSafeLoader:_check_no_duplicates_on_constructed_node,_NoDuplicateSafeLoader:_check_no_duplicates_on_constructed_node,method,10,38,28,274,7.21,2,2,"['self', 'node']","[None, None]","[None, None]",23,[],"['isinstance', 'Counter', 'TypeError']",3
repos/datasets/src/datasets/utils/metadata.py:_NoDuplicateSafeLoader:construct_mapping,_NoDuplicateSafeLoader:construct_mapping,method,4,6,5,115,19.17,0,0,"['self', 'node', 'deep']","[None, None, None]","[None, None, 'False']",31,[],"['super', 'self._check_no_duplicates_on_constructed_node']",2
repos/datasets/src/datasets/utils/patching.py:_PatchedModuleObj,_PatchedModuleObj,class,11,35,26,267,7.63,1,3,[],[],[],9,[],[],0
repos/datasets/src/datasets/utils/patching.py:patch_submodule,patch_submodule,class,52,173,116,1704,9.85,5,4,[],[],[],21,[],[],0
repos/datasets/src/datasets/utils/patching.py:_PatchedModuleObj:__init__,_PatchedModuleObj:__init__,method,10,31,22,230,7.42,1,3,"['self', 'module', 'attrs']","[None, None, None]","[None, None, 'None']",12,[],"['key.startswith', 'setattr', 'getattr', 'isinstance']",4
repos/datasets/src/datasets/utils/patching.py:patch_submodule:__enter__,patch_submodule:__enter__,method,30,124,81,1225,9.88,4,4,['self'],[None],[None],48,[],"['range', 'import_module', 'getattr', 'isinstance', 'setattr', '_PatchedModuleObj', 'globals', 'RuntimeError']",8
repos/datasets/src/datasets/utils/patching.py:patch_submodule:__exit__,patch_submodule:__exit__,method,2,7,7,76,10.86,1,0,"['self', '*exc_info']","[None, None]","[None, None]",102,[],"['list', 'setattr']",2
repos/datasets/src/datasets/utils/patching.py:patch_submodule:__init__,patch_submodule:__init__,method,12,14,14,112,8.0,0,0,"['self', 'obj', 'target', 'new', 'attrs']","[None, None, ' str', None, None]","[None, None, None, None, 'None']",40,[],['target.split'],1
repos/datasets/src/datasets/utils/patching.py:patch_submodule:start,patch_submodule:start,method,2,2,2,50,25.0,0,0,['self'],[None],[None],106,"['        """"""Activate a patch.""""""\n']",['self.__enter__'],1
repos/datasets/src/datasets/utils/patching.py:patch_submodule:stop,patch_submodule:stop,method,4,8,7,89,11.12,0,0,['self'],[None],[None],111,"['        """"""Stop an active patch.""""""\n']",['self.__exit__'],1
repos/datasets/src/datasets/utils/py_utils.py:_convert_github_url,_convert_github_url,function,18,67,57,627,9.36,0,3,['url_path'],[' str'],[None],567,"['    """"""Convert a link to a file on a github repo in a link to the raw github object.""""""\n']","['urlparse', 'url_path.endswith', 'ValueError', 'url_path.replace', 'github_path.split', 'repo_info.split']",6
repos/datasets/src/datasets/utils/py_utils.py:_get_pool_pid,_get_pool_pid,function,3,6,6,29,4.83,1,0,"['pool', 'multiprocess.pool.Pool]']","[' Union[multiprocessing.pool.Pool', None]","[None, None]",683,[],[],0
repos/datasets/src/datasets/utils/py_utils.py:_single_map_nested,_single_map_nested,function,33,167,94,1280,7.66,1,7,['args'],[None],[None],364,"['    """"""Apply a function recursively to each element of a nested data struct.""""""\n']","['isinstance', 'function', 'all', 'iter_batched', 'logging.get_verbosity', 'logging.set_verbosity_warning', 'any', 'print', 'data_struct.items', 'str', 'hf_tqdm', '_single_map_nested', 'tuple', 'np.array']",14
repos/datasets/src/datasets/utils/py_utils.py:_write_generator_to_queue,_write_generator_to_queue,function,6,8,8,65,8.12,1,0,"['queue', 'func', 'Iterable[Y]]', 'kwargs']","[' queue.Queue', ' Callable[...', None, ' dict']","[None, None, None, None]",677,[],"['enumerate', 'queue.put']",2
repos/datasets/src/datasets/utils/py_utils.py:asdict,asdict,function,17,88,53,757,8.6,1,3,['obj'],[None],[None],192,"['    """"""Convert an object to its dictionary representation recursively.\n', '\n', '    <Added version=""2.4.0""/>\n', '    """"""\n']","['_is_dataclass_instance', 'is_dataclass', 'isinstance', '_asdict_inner', 'fields', 'hasattr', 'type', 'obj.items', 'copy.deepcopy', 'TypeError']",10
repos/datasets/src/datasets/utils/py_utils.py:convert_file_size_to_int,convert_file_size_to_int,function,7,112,50,1001,8.94,0,16,"['size', 'str]']","[' Union[int', None]","[None, None]",96,"['    """"""\n', '    Converts a size expressed as a string with digits an unit (like `""50MB""`) to an integer (in bytes).\n', '\n', '    Args:\n', '        size (`int` or `str`): The size to convert. Will be directly returned if an `int`.\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> convert_file_size_to_int(""1MiB"")\n', '    1048576\n', '    ```\n', '    """"""\n']","['isinstance', 'size.upper', 'int', 'size.endswith', 'ValueError']",5
repos/datasets/src/datasets/utils/py_utils.py:copyfunc,copyfunc,function,5,10,9,161,16.1,0,0,['func'],[None],[None],668,[],['types.FunctionType'],1
repos/datasets/src/datasets/utils/py_utils.py:first_non_null_value,first_non_null_value,function,4,16,13,78,4.88,1,1,['iterable'],[None],[None],314,"['    """"""Return the index and the value of the first non-null value in the iterable. If all values are None, return -1 as index.""""""\n']",['enumerate'],1
repos/datasets/src/datasets/utils/py_utils.py:get_imports,get_imports,function,26,99,65,1156,11.68,1,8,['file_path'],[' str'],[None],595,"['    """"""Find whether we should import or clone additional files for a given processing script.\n', '        And list the import.\n', '\n', '    We allow:\n', '    - library dependencies,\n', '    - local dependencies and\n', '    - external dependencies whose url is specified with a comment starting from ""# From:\' followed by the raw url to a file, an archive or a github repository.\n', '        external dependencies will be downloaded (and extracted if needed in the dataset folder).\n', '        We also add an `__init__.py` to each sub-folder of a downloaded folder so the user can import from them in the script.\n', '\n', '    Note that only direct import in the dataset processing script will be handled\n', ""    We don't recursively explore the additional import to download further files.\n"", '\n', '    Example::\n', '\n', '        import tensorflow\n', '        import .c4_utils\n', '        import .clicr.dataset-code.build_json_dataset  # From: https://raw.githubusercontent.com/clips/clicr/master/dataset-code/build_json_dataset\n', '    """"""\n']","['open', 'lines.extend', 'logger.debug', 're.findall', 'len', 're.match', 'match.group', 'any', '_convert_github_url', 'imports.append']",10
repos/datasets/src/datasets/utils/py_utils.py:glob_pattern_to_regex,glob_pattern_to_regex,function,1,26,26,246,9.46,0,0,['pattern'],[None],[None],140,[],['pattern.replace'],1
repos/datasets/src/datasets/utils/py_utils.py:has_sufficient_disk_space,has_sufficient_disk_space,function,5,11,9,115,10.45,0,0,"['needed_bytes', 'directory']","[None, None]","[None, '"".""']",559,[],['disk_usage'],1
repos/datasets/src/datasets/utils/py_utils.py:iflatmap_unordered,iflatmap_unordered,function,16,78,67,715,9.17,1,4,"['pool', 'multiprocess.pool.Pool]', 'func', 'Iterable[Y]]', '*', 'kwargs_iterable', '']","[' Union[multiprocessing.pool.Pool', None, ' Callable[...', None, None, ' Iterable[dict]', None]","[None, None, None, None, None, None, None]",687,[],"['_get_pool_pid', 'isinstance', 'manager_cls', 'manager.Queue', 'pool.apply_async', 'queue.get', 'all', 'queue.empty', 'RuntimeError']",9
repos/datasets/src/datasets/utils/py_utils.py:iter_batched,iter_batched,function,6,27,20,150,5.56,1,3,"['iterable', 'n']","[' Iterable[T]', ' int']","[None, None]",724,[],"['ValueError', 'batch.append', 'len']",3
repos/datasets/src/datasets/utils/py_utils.py:lock_importable_file,lock_importable_file,function,5,7,6,152,21.71,0,0,['importable_local_file'],[' str'],[None],586,[],"['str', 'FileLock']",2
repos/datasets/src/datasets/utils/py_utils.py:map_nested,map_nested,function,29,226,117,1901,8.41,4,18,"['function', 'Any]', 'data_struct', 'dict_only', 'map_list', 'map_tuple', 'map_numpy', 'num_proc', 'parallel_min_length', 'batched', 'batch_size', 'types', 'disable_tqdm', 'desc', '']","[' Callable[[Any]', None, ' Any', ' bool ', ' bool ', ' bool ', ' bool ', ' Optional[int] ', ' int ', ' bool ', ' Optional[int] ', ' Optional[tuple] ', ' bool ', ' Optional[str] ', None]","[None, None, None, ' False', ' True', ' False', ' False', ' None', ' 2', ' False', ' 1000', ' None', ' True', ' None', None]",408,"['    """"""Apply a function recursively to each element of a nested data struct.\n', '\n', '    Use multiprocessing if num_proc > 1 and the length of data_struct is greater than or equal to\n', '    `parallel_min_length`.\n', '\n', '    <Changed version=""2.5.0"">\n', '\n', '    Before version 2.5.0, multiprocessing was not used if `num_proc` was greater than or equal to ``len(iterable)``.\n', '\n', '    Now, if `num_proc` is greater than or equal to ``len(iterable)``, `num_proc` is set to ``len(iterable)`` and\n', '    multiprocessing is used.\n', '\n', '    </Changed>\n', '\n', '    Args:\n', '        function (`Callable`): Function to be applied to `data_struct`.\n', '        data_struct (`Any`): Data structure to apply `function` to.\n', '        dict_only (`bool`, default `False`): Whether only apply `function` recursively to `dict` values in\n', '            `data_struct`.\n', '        map_list (`bool`, default `True`): Whether also apply `function` recursively to `list` elements (besides `dict`\n', '            values).\n', '        map_tuple (`bool`, default `False`): Whether also apply `function` recursively to `tuple` elements (besides\n', '            `dict` values).\n', '        map_numpy (`bool, default `False`): Whether also apply `function` recursively to `numpy.array` elements (besides\n', '            `dict` values).\n', '        num_proc (`int`, *optional*): Number of processes.\n', '            The level in the data struct used for multiprocessing is the first level that has smaller sub-structs,\n', '            starting from the root.\n', '        parallel_min_length (`int`, default `2`): Minimum length of `data_struct` required for parallel\n', '            processing.\n', '            <Added version=""2.5.0""/>\n', '        batched (`bool`, defaults to `False`):\n', '            Provide batch of items to `function`.\n', '            <Added version=""2.19.0""/>\n', '        batch_size (`int`, *optional*, defaults to `1000`):\n', '            Number of items per batch provided to `function` if `batched=True`.\n', '            If `batch_size <= 0` or `batch_size == None`, provide the full iterable as a single batch to `function`.\n', '            <Added version=""2.19.0""/>\n', '        types (`tuple`, *optional*): Additional types (besides `dict` values) to apply `function` recursively to their\n', '            elements.\n', '        disable_tqdm (`bool`, default `True`): Whether to disable the tqdm progressbar.\n', '        desc (`str`, *optional*): Prefix for the tqdm progressbar.\n', '\n', '    Returns:\n', '        `Any`\n', '    """"""\n']","['types.append', 'tuple', 'isinstance', 'function', 'list', 'any', 'len', 'map_nested', 'max', 'int', '_single_map_nested', 'hf_tqdm', 'warnings.catch_warnings', 'warnings.filterwarnings', 'parallel_map', 'dict', 'np.array']",17
repos/datasets/src/datasets/utils/py_utils.py:no_op_if_value_is_null,no_op_if_value_is_null,function,3,13,12,74,5.69,0,0,['func'],[None],[None],305,"['    """"""If the value is None, return None, else call `func`.""""""\n']","['wrapper', 'func']",2
repos/datasets/src/datasets/utils/py_utils.py:size_str,size_str,function,7,36,31,296,8.22,1,2,['size_in_bytes'],[None],[None],72,"['    """"""Returns a human readable size string.\n', '\n', '    If size_in_bytes is None, then returns ""Unknown size"".\n', '\n', '    For example `size_str(1.5 * datasets.units.GiB) == ""1.50 GiB""`.\n', '\n', '    Args:\n', '        size_in_bytes: `int` or `None`, the size, in bytes, that we want to\n', '            format as a human-readable size string.\n', '    """"""\n']",['float'],1
repos/datasets/src/datasets/utils/py_utils.py:string_to_dict,string_to_dict,function,10,29,26,266,9.17,0,1,"['string', 'pattern']","[' str', ' str']","[None, None]",159,"['    """"""Un-format a string using a python f-string pattern.\n', '    From https://stackoverflow.com/a/36838374\n', '\n', '    Example::\n', '\n', ""        >>> p = 'hello, my name is {name} and I am a {age} year old {what}'\n"", ""        >>> s = p.format(name='cody', age=18, what='quarterback')\n"", '        >>> s\n', ""        'hello, my name is cody and I am a 18 year old quarterback'\n"", '        >>> string_to_dict(s, p)\n', ""        {'age': '18', 'name': 'cody', 'what': 'quarterback'}\n"", '\n', '    Args:\n', '        string (str): input string\n', '        pattern (str): pattern formatted like a python f-string\n', '\n', '    Returns:\n', '        Dict[str, str]: dictionary of variable -> value, retrieved from the input using the pattern\n', '\n', '    Raises:\n', ""        ValueError: if the string doesn't match the pattern\n"", '    """"""\n']","['re.sub', 're.search', 'ValueError', 'list', 're.findall', 'dict']",6
repos/datasets/src/datasets/utils/py_utils.py:temp_seed,temp_seed,function,44,93,72,1225,13.17,0,9,"['seed', 'set_pytorch', 'set_tensorflow']","[' int', None, None]","[None, 'False', 'False']",243,"['    """"""Temporarily set the random seed. This works for python numpy, pytorch and tensorflow.""""""\n']","['tf.executing_eagerly', 'ValueError', 'tfpycontext.context', 'hasattr', 'tf_context._set_global_seed', 'delattr']",6
repos/datasets/src/datasets/utils/py_utils.py:temporary_assignment,temporary_assignment,function,4,13,10,102,7.85,0,0,"['obj', 'attr', 'value']","[None, None, None]","[None, None, None]",232,"['    """"""Temporarily assign obj.attr to value.""""""\n']","['getattr', 'setattr']",2
repos/datasets/src/datasets/utils/py_utils.py:unique_values,unique_values,function,4,14,11,73,5.21,1,1,['values'],[None],[None],296,"['    """"""Iterate over iterable and return only unique values in order.""""""\n']","['set', 'seen.add']",2
repos/datasets/src/datasets/utils/py_utils.py:zip_dict,zip_dict,function,4,16,14,97,6.06,1,0,['*dicts'],[None],[None],322,"['    """"""Iterate over items of dictionaries grouped by their keys.""""""\n']","['unique_values', 'tuple']",2
repos/datasets/src/datasets/utils/py_utils.py:NestedDataStructure,NestedDataStructure,class,9,46,28,312,6.78,2,3,[],[],[],545,[],[],0
repos/datasets/src/datasets/utils/py_utils.py:NonMutableDict,NonMutableDict,class,11,55,45,493,8.96,0,3,[],[],[],329,[],[],0
repos/datasets/src/datasets/utils/py_utils.py:classproperty,classproperty,class,3,7,7,74,10.57,0,0,[],[],[],357,[],[],0
repos/datasets/src/datasets/utils/py_utils.py:NestedDataStructure:__init__,NestedDataStructure:__init__,method,2,9,8,35,3.89,0,1,"['self', 'data']","[None, None]","[None, 'None']",546,[],[],0
repos/datasets/src/datasets/utils/py_utils.py:NestedDataStructure:flatten,NestedDataStructure:flatten,method,7,31,22,219,7.06,2,2,"['self', 'data']","[None, None]","[None, 'None']",549,[],"['isinstance', 'self.flatten']",2
repos/datasets/src/datasets/utils/py_utils.py:NonMutableDict:__init__,NonMutableDict:__init__,method,5,21,21,184,8.76,0,1,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",337,[],"['kwargs.pop', 'ValueError', 'super']",3
repos/datasets/src/datasets/utils/py_utils.py:NonMutableDict:__setitem__,NonMutableDict:__setitem__,method,4,9,9,98,10.89,0,1,"['self', 'key', 'value']","[None, None, None]","[None, None, None]",346,[],"['ValueError', 'super']",2
repos/datasets/src/datasets/utils/py_utils.py:NonMutableDict:update,NonMutableDict:update,method,4,14,13,120,8.57,0,1,"['self', 'other']","[None, None]","[None, None]",351,[],"['any', 'ValueError', 'set', 'super']",4
repos/datasets/src/datasets/utils/py_utils.py:classproperty:__get__,classproperty:__get__,method,2,3,3,39,13.0,0,0,"['self', 'obj', 'objtype']","[None, None, None]","[None, None, 'None']",360,[],[],0
repos/datasets/src/datasets/utils/readme.py:load_yaml_resource,load_yaml_resource,function,4,6,6,120,20.0,0,0,['resource'],[' str'],[None],18,[],"['pkg_resources.read_text', 'yaml.safe_load']",2
repos/datasets/src/datasets/utils/readme.py:ReadMe,ReadMe,class,53,335,186,2871,8.57,1,10,[],[],[],172,[],[],0
repos/datasets/src/datasets/utils/readme.py:Section,Section,class,63,391,209,3423,8.75,3,23,[],[],[],35,[],[],0
repos/datasets/src/datasets/utils/readme.py:ReadMe:__init__,ReadMe:__init__,method,8,30,29,254,8.47,0,1,"['self', 'name', 'lines', 'structure', 'suppress_parsing_errors']","[None, ' str', ' List[str]', ' dict ', ' bool ']","[None, None, None, ' None', ' False']",173,[],"['super', 'self.parse']",2
repos/datasets/src/datasets/utils/readme.py:ReadMe:__str__,ReadMe:__str__,method,2,2,2,25,12.5,0,0,['self'],[None],[None],222,"['        """"""Returns the string of dictionary representation of the ReadMe.""""""\n']",['str'],1
repos/datasets/src/datasets/utils/readme.py:ReadMe:_validate,ReadMe:_validate,method,17,150,86,1201,8.01,0,4,"['self', 'readme_structure']","[None, None]","[None, None]",226,[],"['warning_list.append', 'len', 'error_list.append', 'list', 'start_key.startswith', 'self.to_dict']",6
repos/datasets/src/datasets/utils/readme.py:ReadMe:from_readme,ReadMe:from_readme,method,6,12,12,135,11.25,0,0,"['cls', 'path', 'structure', 'suppress_parsing_errors']","[None, ' Path', ' dict ', ' bool ']","[None, None, ' None', ' False']",193,[],"['open', 'f.readlines', 'cls']",3
repos/datasets/src/datasets/utils/readme.py:ReadMe:from_string,ReadMe:from_string,method,4,7,7,109,15.57,0,0,"['cls', 'string', 'structure', 'root_name', 'suppress_parsing_errors']","[None, ' str', ' dict ', ' str ', ' bool ']","[None, None, ' None', ' ""root""', ' False']",199,[],"['string.split', 'cls']",2
repos/datasets/src/datasets/utils/readme.py:ReadMe:parse,ReadMe:parse,method,15,42,30,334,7.95,1,3,"['self', 'suppress_parsing_errors']","[None, ' bool ']","[None, ' False']",205,[],"['line.strip', 'super']",2
repos/datasets/src/datasets/utils/readme.py:ReadMe:validate,ReadMe:validate,method,10,41,33,361,8.8,0,2,['self'],[None],[None],182,[],"['self._validate', 'ValueError']",2
repos/datasets/src/datasets/utils/readme.py:Section:__init__,Section:__init__,method,12,22,20,240,10.91,0,1,"['self', 'name', 'level', 'lines', 'suppress_parsing_errors']","[None, ' str', ' str', ' List[str] ', ' bool ']","[None, None, None, ' None', ' False']",36,[],['self.parse'],1
repos/datasets/src/datasets/utils/readme.py:Section:parse,Section:parse,method,22,149,84,1344,9.02,1,10,"['self', 'suppress_parsing_errors']","[None, ' bool ']","[None, ' False']",48,[],"['line.strip', 'line.split', 'Section', 'current_lines.append', 'ValueError']",5
repos/datasets/src/datasets/utils/readme.py:Section:to_dict,Section:to_dict,method,1,15,15,145,9.67,0,0,['self'],[None],[None],161,"['        """"""Returns the dictionary representation of a section.""""""\n']",[],0
repos/datasets/src/datasets/utils/readme.py:Section:validate,Section:validate,method,37,178,106,1465,8.23,2,12,"['self', 'structure']","[None, ' dict']","[None, None]",90,"['        """"""Validates a Section class object recursively using the structure provided as a dictionary.\n', '\n', '        Args:\n', '            structute (:obj: `dict`): The dictionary representing expected structure.\n', '\n', '        Returns:\n', '            :obj: `ReadmeValidatorOutput`: The dictionary representation of the section, and the errors.\n', '        """"""\n']","['error_list.append', 'enumerate', 'warning_list.append', 'self.to_dict']",4
repos/datasets/src/datasets/utils/sharding.py:_distribute_shards,_distribute_shards,function,12,32,27,373,11.66,1,2,"['num_shards', 'max_num_jobs']","[' int', ' int']","[None, None]",25,"['    """"""\n', '    Get the range of shard indices per job.\n', '    If num_shards<max_num_jobs, then num_shards jobs are given a range of one shard.\n', '    The shards indices order is preserved: e.g. all the first shards are given the first job.\n', '    Moreover all the jobs are given approximately the same number of shards.\n', '\n', '    Example:\n', '\n', '    ```python\n', '    >>> _distribute_shards(2, max_num_jobs=4)\n', '    [range(0, 1), range(1, 2)]\n', '    >>> _distribute_shards(10, max_num_jobs=3)\n', '    [range(0, 4), range(4, 7), range(7, 10)]\n', '    ```\n', '    """"""\n']","['range', 'shards_indices_per_group.append']",2
repos/datasets/src/datasets/utils/sharding.py:_merge_gen_kwargs,_merge_gen_kwargs,function,9,22,18,174,7.91,3,1,['gen_kwargs_list'],[' List[dict]'],[None],71,[],['isinstance'],1
repos/datasets/src/datasets/utils/sharding.py:_number_of_shards_in_gen_kwargs,_number_of_shards_in_gen_kwargs,function,5,101,81,614,6.08,0,1,['gen_kwargs'],[' dict'],[None],6,"['    """"""Return the number of possible shards according to the input gen_kwargs""""""\n']","['len', 'gen_kwargs.items', 'isinstance', 'RuntimeError', 'lists_lengths.items', 'max']",6
repos/datasets/src/datasets/utils/sharding.py:_shuffle_gen_kwargs,_shuffle_gen_kwargs,function,14,36,26,380,10.56,3,1,"['rng', 'gen_kwargs']","[' np.random.Generator', ' dict']","[None, None]",80,"['    """"""Return a shuffled copy of the input gen_kwargs""""""\n']","['gen_kwargs.values', 'isinstance', 'list', 'rng.shuffle', 'dict', 'shuffled_kwargs.items', 'indices_per_size[len']",7
repos/datasets/src/datasets/utils/sharding.py:_split_gen_kwargs,_split_gen_kwargs,function,11,36,29,397,11.03,2,2,"['gen_kwargs', 'max_num_jobs']","[' dict', ' int']","[None, None]",52,"['    """"""Split the gen_kwargs into `max_num_job` gen_kwargs""""""\n']","['_number_of_shards_in_gen_kwargs', '_distribute_shards', 'isinstance', 'gen_kwargs.items', 'range']",5
repos/datasets/src/datasets/utils/stratify.py:approximate_mode,approximate_mode,function,19,44,38,437,9.93,1,2,"['class_counts', 'n_draws', 'rng']","[None, None, None]","[None, None, None]",4,"['    """"""Computes approximate mode of multivariate hypergeometric.\n', '    This is an approximation to the mode of the multivariate\n', '    hypergeometric given by class_counts and n_draws.\n', ""    It shouldn't be off by more than one.\n"", '    It is the mostly likely outcome of drawing n_draws many\n', '    samples from the population given by class_counts.\n', '    Args\n', '    ----------\n', '    class_counts : ndarray of int\n', '        Population per class.\n', '    n_draws : int\n', '        Number of draws (samples to draw) from the overall population.\n', '    rng : random state\n', '        Used to break ties.\n', '    Returns\n', '    -------\n', '    sampled_classes : ndarray of int\n', '        Number of samples drawn from each class.\n', '        np.sum(sampled_classes) == n_draws\n', '\n', '    """"""\n']","['class_counts.sum', 'np.floor', 'int', 'floored.sum', 'np.sort', 'np.where', 'min', 'rng.choice', 'floored.astype']",9
repos/datasets/src/datasets/utils/stratify.py:stratified_shuffle_split_generate_indices,stratified_shuffle_split_generate_indices,function,29,112,79,1014,9.05,2,3,"['y', 'n_train', 'n_test', 'rng', 'n_splits']","[None, None, None, None, None]","[None, None, None, None, '10']",54,"['    """"""\n', '\n', '    Provides train/test indices to split data in train/test sets.\n', ""    It's reference is taken from StratifiedShuffleSplit implementation\n"", '    of scikit-learn library.\n', '\n', '    Args\n', '    ----------\n', '\n', '    n_train : int,\n', '        represents the absolute number of train samples.\n', '\n', '    n_test : int,\n', '        represents the absolute number of test samples.\n', '\n', '    random_state : int or RandomState instance, default=None\n', '        Controls the randomness of the training and testing indices produced.\n', '        Pass an int for reproducible output across multiple function calls.\n', '\n', '    n_splits : int, default=10\n', '        Number of re-shuffling & splitting iterations.\n', '    """"""\n']","['np.unique', 'np.bincount', 'np.min', 'ValueError', 'np.split', 'np.cumsum', 'range', 'approximate_mode', 'rng.permutation', 'train.extend', 'test.extend']",11
repos/datasets/src/datasets/utils/tf_utils.py:dataset_to_tf,dataset_to_tf,function,40,180,129,2017,11.21,1,7,"['dataset', 'cols_to_retain', 'collate_fn', 'collate_fn_args', 'columns_to_np_types', 'output_signature', 'shuffle', 'batch_size', 'drop_remainder', '']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None]",136,"['    """"""Create a tf.data.Dataset from the underlying Dataset. This is a single-process method - the multiprocess\n', '    equivalent is multiprocess_dataset_to_tf.\n', '\n', '    Args:\n', '        dataset (`Dataset`): Dataset to wrap with tf.data.Dataset.\n', '        cols_to_retain (`List[str]`): Dataset column(s) to load in the\n', '            tf.data.Dataset. It is acceptable to include column names that are created by the `collate_fn` and\n', '            that do not exist in the original dataset.\n', '        collate_fn(`Callable`): A function or callable object (such as a `DataCollator`) that will collate\n', '            lists of samples into a batch.\n', '        collate_fn_args (`Dict`): A  `dict` of keyword arguments to be passed to the\n', '            `collate_fn`. Can be empty.\n', '        columns_to_np_types (`Dict[str, np.dtype]`): A `dict` mapping column names to numpy dtypes.\n', '        output_signature (`Dict[str, tf.TensorSpec]`): A `dict` mapping column names to\n', '            `tf.TensorSpec` objects.\n', '        shuffle(`bool`): Shuffle the dataset order when loading. Recommended True for training, False for\n', '            validation/evaluation.\n', '        batch_size (`int`, default `None`): Size of batches to load from the dataset. Defaults to `None`, which implies that\n', ""            the dataset won't be batched, but the returned dataset can be batched later with `tf_dataset.batch(batch_size)`.\n"", '        drop_remainder(`bool`, default `None`): Drop the last incomplete batch when loading. If not provided,\n', '            defaults to the same setting as shuffle.\n', '\n', '    Returns:\n', '        `tf.data.Dataset`\n', '    """"""\n']","['ImportError', 'hasattr', 'len', 'warnings.warn', 'partial', 'columns_to_np_types.values', 'fetch_function', 'tf.py_function', 'enumerate', 'tf.fill', 'scan_random_index', 'tf.reduce_all', 'random_index_shuffle', 'max_index=len', 'tf_dataset.scan', 'tf_dataset.shuffle', 'tf_dataset.batch', 'tf_dataset.map', 'ensure_shapes', 'tf.ensure_shape', 'input_dict.items']",21
repos/datasets/src/datasets/utils/tf_utils.py:is_numeric_feature,is_numeric_feature,function,10,38,24,432,11.37,0,1,['feature'],[None],[None],70,[],"['isinstance', 'is_numeric_feature', 'is_numeric_pa_type']",3
repos/datasets/src/datasets/utils/tf_utils.py:is_numeric_pa_type,is_numeric_pa_type,function,4,10,8,169,16.9,0,1,['pa_type'],[None],[None],64,[],['is_numeric_pa_type'],1
repos/datasets/src/datasets/utils/tf_utils.py:minimal_tf_collate_fn,minimal_tf_collate_fn,function,18,64,46,460,7.19,1,2,['features'],[None],[None],36,[],"['isinstance', 'ImportError', 'first.items', 'np.stack', 'tf.stack', 'np.array']",6
repos/datasets/src/datasets/utils/tf_utils.py:minimal_tf_collate_fn_with_renaming,minimal_tf_collate_fn_with_renaming,function,5,12,10,116,9.67,0,1,['features'],[None],[None],56,[],['minimal_tf_collate_fn'],1
repos/datasets/src/datasets/utils/tf_utils.py:multiprocess_dataset_to_tf,multiprocess_dataset_to_tf,function,13,45,42,747,16.6,0,2,"['dataset', 'cols_to_retain', 'collate_fn', 'collate_fn_args', 'columns_to_np_types', 'output_signature', 'shuffle', 'batch_size', 'drop_remainder', 'num_workers', '']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None]",521,"['    """"""Create a tf.data.Dataset from the underlying Dataset. This is a multi-process method - the single-process\n', '    equivalent is dataset_to_tf.\n', '\n', '    Args:\n', '        dataset (`Dataset`): Dataset to wrap with tf.data.Dataset.\n', '        cols_to_retain (`List[str]`): Dataset column(s) to load in the\n', '            tf.data.Dataset. It is acceptable to include column names that are created by the `collate_fn` and\n', '            that do not exist in the original dataset.\n', '        collate_fn(`Callable`): A function or callable object (such as a `DataCollator`) that will collate\n', '            lists of samples into a batch.\n', '        collate_fn_args (`Dict`): A  `dict` of keyword arguments to be passed to the\n', '            `collate_fn`. Can be empty.\n', '        columns_to_np_types (`Dict[str, np.dtype]`): A `dict` mapping column names to numpy dtypes.\n', '        output_signature (`Dict[str, tf.TensorSpec]`): A `dict` mapping column names to\n', '            `tf.TensorSpec` objects.\n', '        shuffle(`bool`): Shuffle the dataset order when loading. Recommended True for training, False for\n', '            validation/evaluation.\n', '        batch_size (`int`, default `None`): Size of batches to load from the dataset. Defaults to `None`, which implies that\n', ""            the dataset won't be batched, but the returned dataset can be batched later with `tf_dataset.batch(batch_size)`.\n"", '        drop_remainder(`bool`, default `None`): Drop the last incomplete batch when loading. If not provided,\n', '            defaults to the same setting as shuffle.\n', '        num_workers (`int`): Number of workers to use for loading the dataset. Should be >= 1.\n', '\n', '    Returns:\n', '        `tf.data.Dataset`\n', '    """"""\n']","['ImportError', 'NumpyMultiprocessingGenerator', 'int', 'tf_dataset.apply']",4
repos/datasets/src/datasets/utils/tf_utils.py:np_get_batch,np_get_batch,function,31,121,77,1034,8.55,4,6,"['indices', 'dataset', 'cols_to_retain', 'collate_fn', 'collate_fn_args', 'columns_to_np_types', 'return_dict']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, 'False']",88,[],"['isinstance', 'indices.numpy', 'np.all', 'RuntimeError', 'batch.items', 'len', 'range', 'collate_fn', 'columns_to_np_types.items', 'np.array', 'array.astype', 'out_batch.append']",12
repos/datasets/src/datasets/utils/tf_utils.py:NumpyMultiprocessingGenerator,NumpyMultiprocessingGenerator,class,133,484,306,5759,11.9,15,10,[],[],[],281,[],[],0
repos/datasets/src/datasets/utils/tf_utils.py:SharedMemoryContext,SharedMemoryContext,class,17,57,42,587,10.3,2,1,[],[],[],249,[],[],0
repos/datasets/src/datasets/utils/tf_utils.py:NumpyMultiprocessingGenerator:__call__,NumpyMultiprocessingGenerator:__call__,method,1,2,2,10,5.0,0,0,['self'],[None],[None],413,[],[],0
repos/datasets/src/datasets/utils/tf_utils.py:NumpyMultiprocessingGenerator:__init__,NumpyMultiprocessingGenerator:__init__,method,28,65,43,639,9.83,3,1,"['self', 'dataset', 'cols_to_retain', 'collate_fn', 'collate_fn_args', 'columns_to_np_types', 'output_signature', 'shuffle', 'batch_size', 'drop_remainder', 'num_workers', '']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None]",282,[],"['columns_to_np_types.items', 'np.dtype', 'int', 'output_signature.items']",4
repos/datasets/src/datasets/utils/tf_utils.py:NumpyMultiprocessingGenerator:__iter__,NumpyMultiprocessingGenerator:__iter__,method,57,188,137,2283,12.14,7,3,['self'],[None],[None],316,[],"['min', 'int', 'self.distribute_batches', 'get_context', 'range', 'SharedMemoryContext', 'str', 'names.append', 'shm_ctx.get_array', 'shape_arrays.append', 'ctx.Process', 'worker.start', 'workers.append', 'TimeoutError', 'any', 'array_shapes.values', 'batch_shm_ctx.get_array', 'array_shapes.items', 'np.copy', 'arrays.items', 'worker.join']",21
repos/datasets/src/datasets/utils/tf_utils.py:NumpyMultiprocessingGenerator:distribute_batches,NumpyMultiprocessingGenerator:distribute_batches,method,24,75,53,1018,13.57,1,3,"['dataset', 'batch_size', 'drop_remainder', 'num_workers', 'shuffle']","[None, None, None, None, None]","[None, None, None, None, None]",489,[],"['np.arange', 'len', 'np.split', 'indices.reshape', 'range', 'np.concatenate']",6
repos/datasets/src/datasets/utils/tf_utils.py:NumpyMultiprocessingGenerator:worker_loop,NumpyMultiprocessingGenerator:worker_loop,method,37,113,92,1294,11.45,4,3,"['dataset', 'cols_to_retain', 'collate_fn', 'collate_fn_args', 'columns_to_np_types', 'columns_to_ranks', 'string_columns', 'indices', 'extra_batch', 'worker_name', 'array_ready_event', 'array_loaded_event', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None, None]",417,[],"['ImportError', 'send_batch_to_parent', 'np_get_batch', 'SharedMemoryContext', 'columns_to_np_types.items', 'array.view', 'batch_shm_ctx.get_array', 'array_ready_event.set', 'array_loaded_event.wait', 'array_loaded_event.clear', 'shm_ctx.get_array', 'columns_to_ranks.items', 'shape_arrays.items']",13
repos/datasets/src/datasets/utils/tf_utils.py:SharedMemoryContext:__enter__,SharedMemoryContext:__enter__,method,1,2,2,10,5.0,0,0,['self'],[None],[None],270,[],[],0
repos/datasets/src/datasets/utils/tf_utils.py:SharedMemoryContext:__exit__,SharedMemoryContext:__exit__,method,5,11,8,89,8.09,2,0,"['self', 'exc_type', 'exc_value', 'traceback']","[None, None, None, None]","[None, None, None, None]",273,[],"['shm.close', 'shm.unlink']",2
repos/datasets/src/datasets/utils/tf_utils.py:SharedMemoryContext:__init__,SharedMemoryContext:__init__,method,2,4,4,40,10.0,0,0,['self'],[None],[None],252,[],[],0
repos/datasets/src/datasets/utils/tf_utils.py:SharedMemoryContext:get_array,SharedMemoryContext:get_array,method,4,9,9,137,15.22,0,0,"['self', 'name', 'shape', 'dtype', 'create']","[None, None, None, None, None]","[None, None, None, None, None]",266,[],"['self.get_shm', 'np.dtype', 'np.ndarray']",3
repos/datasets/src/datasets/utils/tf_utils.py:SharedMemoryContext:get_shm,SharedMemoryContext:get_shm,method,6,11,10,141,12.82,0,1,"['self', 'name', 'size', 'create']","[None, None, None, None]","[None, None, None, None]",256,[],['SharedMemory'],1
repos/datasets/src/datasets/utils/tqdm.py:are_progress_bars_disabled,are_progress_bars_disabled,function,2,4,4,83,20.75,0,0,[],[],[],94,"['    """"""Return whether progress bars are globally disabled or not.\n', '\n', '    Progress bars used in `datasets` can be enable or disabled globally using [`~utils.enable_progress_bars`]\n', '    and [`~utils.disable_progress_bars`] or by setting `HF_DATASETS_DISABLE_PROGRESS_BAR` as environment variable.\n', '    """"""\n']",[],0
repos/datasets/src/datasets/utils/tqdm.py:disable_progress_bars,disable_progress_bars,function,4,24,23,257,10.71,0,1,[],[],[],60,"['    """"""\n', '    Disable globally progress bars used in `datasets` except if `HF_DATASETS_DISABLE_PROGRESS_BAR` environment\n', '    variable has been set.\n', '\n', '    Use [`~utils.enable_progress_bars`] to re-enable them.\n', '    """"""\n']",['warnings.warn'],1
repos/datasets/src/datasets/utils/tqdm.py:enable_progress_bars,enable_progress_bars,function,4,24,23,256,10.67,0,1,[],[],[],77,"['    """"""\n', '    Enable globally progress bars used in `datasets` except if `HF_DATASETS_DISABLE_PROGRESS_BAR` environment\n', '    variable has been set.\n', '\n', '    Use [`~utils.disable_progress_bars`] to disable them.\n', '    """"""\n']",['warnings.warn'],1
repos/datasets/src/datasets/utils/tqdm.py:is_progress_bar_enabled,is_progress_bar_enabled,function,2,3,3,37,12.33,0,0,[],[],[],130,[],['are_progress_bars_disabled'],1
repos/datasets/src/datasets/utils/tqdm.py:tqdm,tqdm,class,7,24,22,234,9.75,0,2,[],[],[],104,[],[],0
repos/datasets/src/datasets/utils/tqdm.py:tqdm:__delattr__,tqdm:__delattr__,method,3,8,8,75,9.38,0,1,"['self', 'attr']","[None, ' str']","[None, None]",116,"['        """"""Fix for https://github.com/huggingface/datasets/issues/6066""""""\n']",['super'],1
repos/datasets/src/datasets/utils/tqdm.py:tqdm:__init__,tqdm:__init__,method,3,6,6,87,14.5,0,1,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",111,[],"['are_progress_bars_disabled', 'super']",2
repos/datasets/src/datasets/utils/track.py:TrackedIterable,TrackedIterable,class,6,19,15,197,10.37,0,1,[],[],[],40,[],[],0
repos/datasets/src/datasets/utils/track.py:tracked_list,tracked_list,class,8,37,26,331,8.95,1,1,[],[],[],22,[],[],0
repos/datasets/src/datasets/utils/track.py:tracked_str,tracked_str,class,9,36,27,393,10.92,0,2,[],[],[],5,[],[],0
repos/datasets/src/datasets/utils/track.py:TrackedIterable:__init__,TrackedIterable:__init__,method,2,3,3,38,12.67,0,0,['self'],[None],[None],41,[],['super'],1
repos/datasets/src/datasets/utils/track.py:TrackedIterable:__repr__,TrackedIterable:__repr__,method,4,8,8,109,13.62,0,1,['self'],[None],[None],45,[],['super'],1
repos/datasets/src/datasets/utils/track.py:tracked_list:__init__,tracked_list:__init__,method,2,4,4,52,13.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",23,[],['super'],1
repos/datasets/src/datasets/utils/track.py:tracked_list:__iter__,tracked_list:__iter__,method,3,10,8,69,6.9,1,0,['self'],[None],[None],27,[],['super'],1
repos/datasets/src/datasets/utils/track.py:tracked_list:__repr__,tracked_list:__repr__,method,4,9,8,115,12.78,0,1,['self'],[None],[None],33,[],['super'],1
repos/datasets/src/datasets/utils/track.py:tracked_str:__repr__,tracked_str:__repr__,method,3,14,13,173,12.36,0,1,['self'],[None],[None],15,[],['super'],1
repos/datasets/src/datasets/utils/track.py:tracked_str:get_origin,tracked_str:get_origin,method,2,3,3,52,17.33,0,0,['self'],[None],[None],12,[],['str'],1
repos/datasets/src/datasets/utils/track.py:tracked_str:set_origin,tracked_str:set_origin,method,2,7,7,78,11.14,0,1,"['self', 'origin']","[None, ' str']","[None, None]",8,[],['super'],1
repos/datasets/src/datasets/utils/version.py:_str_to_version_tuple,_str_to_version_tuple,function,5,25,25,224,8.96,0,1,['version_str'],[None],[None],96,"['    """"""Return the tuple (major, minor, patch) version extracted from the str.""""""\n']","['_VERSION_REG.match', 'ValueError', 'tuple', 'res.group']",4
repos/datasets/src/datasets/utils/version.py:_version_tuple_to_str,_version_tuple_to_str,function,1,6,6,41,6.83,0,0,['version_tuple'],[None],[None],104,"['    """"""Return the str version from the version tuple (major, minor, patch).""""""\n']",[],0
repos/datasets/src/datasets/utils/version.py:Version,Version,class,36,112,74,1076,9.61,1,1,[],[],[],30,[],[],0
repos/datasets/src/datasets/utils/version.py:Version:__eq__,Version:__eq__,method,5,12,11,118,9.83,0,0,"['self', 'other']","[None, None]","[None, None]",72,[],['self._validate_operand'],1
repos/datasets/src/datasets/utils/version.py:Version:__hash__,Version:__hash__,method,2,2,2,45,22.5,0,0,['self'],[None],[None],84,[],['hash'],1
repos/datasets/src/datasets/utils/version.py:Version:__lt__,Version:__lt__,method,5,6,6,64,10.67,0,0,"['self', 'other']","[None, None]","[None, None]",80,[],['self._validate_operand'],1
repos/datasets/src/datasets/utils/version.py:Version:__post_init__,Version:__post_init__,method,4,4,4,72,18.0,0,0,['self'],[None],[None],55,[],['_str_to_version_tuple'],1
repos/datasets/src/datasets/utils/version.py:Version:__repr__,Version:__repr__,method,1,2,2,56,28.0,0,0,['self'],[None],[None],58,[],[],0
repos/datasets/src/datasets/utils/version.py:Version:_to_yaml_string,Version:_to_yaml_string,method,2,2,2,22,11.0,0,0,['self'],[None],[None],92,[],[],0
repos/datasets/src/datasets/utils/version.py:Version:_validate_operand,Version:_validate_operand,method,5,19,17,160,8.42,0,1,"['self', 'other']","[None, None]","[None, None]",65,[],"['isinstance', 'Version', 'TypeError']",3
repos/datasets/src/datasets/utils/version.py:Version:from_dict,Version:from_dict,method,5,18,14,103,5.72,1,0,"['cls', 'dic']","[None, None]","[None, None]",88,[],"['dataclasses.fields', 'cls', 'dic.items']",3
repos/datasets/src/datasets/utils/version.py:Version:tuple,Version:tuple,method,4,4,4,38,9.5,0,0,['self'],[None],[None],62,[],[],0
repos/datasets/templates/new_dataset_script.py:NewDataset,NewDataset,class,32,213,134,2053,9.64,2,2,[],[],[],58,[],[],0
repos/datasets/templates/new_dataset_script.py:NewDataset:_generate_examples,NewDataset:_generate_examples,method,8,46,34,386,8.39,1,1,"['self', 'filepath', 'split']","[None, None, None]","[None, None, None]",154,[],"['open', 'enumerate', 'json.loads']",3
repos/datasets/templates/new_dataset_script.py:NewDataset:_info,NewDataset:_info,method,14,76,57,687,9.04,1,1,['self'],[None],[None],81,[],"['datasets.Features', 'datasets.Value', 'datasets.DatasetInfo']",3
repos/datasets/templates/new_dataset_script.py:NewDataset:_split_generators,NewDataset:_split_generators,method,5,37,23,493,13.32,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",117,[],"['dl_manager.download_and_extract', 'datasets.SplitGenerator']",2
repos/datasets/tests/commands/conftest.py:dataset_loading_script_code,dataset_loading_script_code,function,2,2,2,33,16.5,0,0,[],[],[],62,[],[],0
repos/datasets/tests/commands/conftest.py:dataset_loading_script_dir,dataset_loading_script_dir,function,11,18,17,245,13.61,0,0,"['dataset_loading_script_name', 'dataset_loading_script_code', 'tmp_path']","[None, None, None]","[None, None, None]",67,[],"['script_dir.mkdir', 'open', 'f.write', 'str']",4
repos/datasets/tests/commands/conftest.py:dataset_loading_script_name,dataset_loading_script_name,function,2,2,2,33,16.5,0,0,[],[],[],57,[],[],0
repos/datasets/tests/commands/conftest.py:__DummyDataset1__,__DummyDataset1__,class,17,60,51,789,13.15,1,0,[],[],[],17,[],[],0
repos/datasets/tests/commands/conftest.py:__DummyDataset1__:_generate_examples,__DummyDataset1__:_generate_examples,method,6,14,13,92,6.57,1,0,"['self', 'filepath']","[None, None]","[None, None]",49,[],"['open', 'enumerate', 'json.loads']",3
repos/datasets/tests/commands/conftest.py:__DummyDataset1__:_info,__DummyDataset1__:_info,method,5,27,24,373,13.81,0,0,['self'],[None],[None],19,[],"['datasets.Features', 'datasets.Sequence', 'datasets.DatasetInfo']",3
repos/datasets/tests/commands/conftest.py:__DummyDataset1__:_split_generators,__DummyDataset1__:_split_generators,method,4,11,10,229,20.82,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",42,[],"['dl_manager.download', 'datasets.SplitGenerator']",2
repos/datasets/tests/commands/test_test.py:is_1percent_close,is_1percent_close,function,1,6,5,38,6.33,0,0,"['source', 'target']","[None, None]","[None, None]",29,[],[],0
repos/datasets/tests/commands/test_test.py:test_test_command,test_test_command,function,29,103,80,1362,13.22,2,1,['dataset_loading_script_dir'],[None],[None],34,[],"['_TestCommandArgs', 'TestCommand', 'test_command.run', 'DatasetInfosDict.from_directory', 'DatasetInfosDict', 'DatasetInfo', 'features=Features', 'Sequence', 'ClassLabel', 'dataset_infos.keys', 'expected_dataset_infos.keys', 'getattr', 'is_1percent_close', 'list']",14
repos/datasets/tests/conftest.py:disable_tqdm_output,disable_tqdm_output,function,1,1,1,31,31.0,0,0,[],[],[],40,[],['datasets.disable_progress_bar'],1
repos/datasets/tests/conftest.py:pytest_collection_modifyitems,pytest_collection_modifyitems,function,5,15,12,121,8.07,1,1,"['config', 'items']","[None, None]","[None, None]",11,[],"['any', 'item.add_marker']",2
repos/datasets/tests/conftest.py:pytest_configure,pytest_configure,function,1,8,8,88,11.0,0,0,['config'],[None],[None],19,[],['config.addinivalue_line'],1
repos/datasets/tests/conftest.py:set_sqlalchemy_silence_uber_warning,set_sqlalchemy_silence_uber_warning,function,2,6,6,109,18.17,0,0,['monkeypatch'],[None],[None],51,[],['monkeypatch.setattr'],1
repos/datasets/tests/conftest.py:set_test_cache_config,set_test_cache_config,function,8,29,24,800,27.59,0,0,"['tmp_path_factory', 'monkeypatch']","[None, None]","[None, None]",24,[],"['tmp_path_factory.getbasetemp', 'monkeypatch.setattr', 'str']",3
repos/datasets/tests/conftest.py:set_update_download_counts_to_false,set_update_download_counts_to_false,function,1,2,2,70,35.0,0,0,['monkeypatch'],[None],[None],45,[],['monkeypatch.setattr'],1
repos/datasets/tests/conftest.py:zero_time_out_for_remote_code,zero_time_out_for_remote_code,function,1,2,2,38,19.0,0,0,[],[],[],61,[],[],0
repos/datasets/tests/distributed_scripts/run_torch_distributed.py:gen,gen,function,4,13,11,78,6.0,2,0,['shards'],[' List[str]'],[None],19,[],['range'],1
repos/datasets/tests/distributed_scripts/run_torch_distributed.py:main,main,function,27,69,60,926,13.42,1,2,[],[],[],25,[],"['int', 'ArgumentParser', 'parser.add_argument', 'parser.parse_args', 'range', 'IterableDataset.from_generator', 'Dataset.from_list', 'split_dataset_by_node', 'sum', 'FailedTestError']",10
repos/datasets/tests/distributed_scripts/run_torch_distributed.py:FailedTestError,FailedTestError,class,0,1,1,4,4.0,0,0,[],[],[],15,[],[],0
repos/datasets/tests/features/test_array_xd.py:generate_examples,generate_examples,function,20,56,42,567,10.12,3,2,"['features', 'num_examples', 'seq_shapes']","[' dict', None, None]","[None, '100', 'None']",34,[],"['range', 'enumerate', 'isinstance', 'random.randint', 'dummy_data.append']",5
repos/datasets/tests/features/test_array_xd.py:get_array_feature_types,get_array_feature_types,function,7,32,32,233,7.28,1,0,[],[],[],129,[],"['tuple', 'zip']",2
repos/datasets/tests/features/test_array_xd.py:test_array_xd_numpy_arrow_extractor,test_array_xd_numpy_arrow_extractor,function,8,21,20,333,15.86,0,0,"['dtype', 'dummy_value']","[None, None]","[None, None]",371,[],"['datasets.Features', 'datasets.Array2D', 'NumpyArrowExtractor', 'isinstance', 'np.array']",5
repos/datasets/tests/features/test_array_xd.py:test_array_xd_with_none,test_array_xd_with_none,function,13,91,54,1042,11.45,0,0,[],[],[],379,[],"['datasets.Features', 'datasets.Array2D', 'np.array', 'NumpyArrowExtractor', 'isinstance', 'np.allclose', 'np.all', 'np.isnan']",8
repos/datasets/tests/features/test_array_xd.py:test_array_xd_with_np,test_array_xd_with_np,function,11,34,26,440,12.94,0,1,"['seq_type', 'dtype', 'shape', 'feature_class']","[None, None, None, None]","[None, None, None, None]",419,[],"['feature_class', 'np.zeros', 'data.tolist', 'datasets.Sequence']",4
repos/datasets/tests/features/test_array_xd.py:test_dataset_map,test_dataset_map,function,22,96,85,740,7.71,1,2,['with_none'],[None],[None],436,[],"['process_data', 'np.array', 'datasets.Features', 'Array3D', 'ds.map', 'processed_ds.with_format', 'enumerate', 'isinstance', 'np.all']",9
repos/datasets/tests/features/test_array_xd.py:test_table_to_pandas,test_table_to_pandas,function,10,23,22,347,15.09,0,0,"['dtype', 'dummy_value']","[None, None]","[None, None]",361,[],"['datasets.Features', 'datasets.Array2D', 'type', 'np.array']",4
repos/datasets/tests/features/test_array_xd.py:ArrayXDDynamicTest,ArrayXDDynamicTest,class,36,277,94,4013,14.49,7,0,[],[],[],244,[],[],0
repos/datasets/tests/features/test_array_xd.py:ArrayXDTest,ArrayXDTest,class,47,231,114,3937,17.04,1,0,[],[],[],144,[],[],0
repos/datasets/tests/features/test_array_xd.py:ExtensionTypeCompatibilityTest,ExtensionTypeCompatibilityTest,class,40,232,108,2883,12.43,4,0,[],[],[],59,[],[],0
repos/datasets/tests/features/test_array_xd.py:ArrayXDDynamicTest:get_one_col_dataset,ArrayXDDynamicTest:get_one_col_dataset,method,6,18,17,267,14.83,0,0,"['self', 'first_dim_list', 'fixed_shape']","[None, None, None]","[None, None, None]",245,[],"['datasets.Features', 'Array3D']",2
repos/datasets/tests/features/test_array_xd.py:ArrayXDDynamicTest:get_two_col_datasset,ArrayXDDynamicTest:get_two_col_datasset,method,6,30,25,332,11.07,0,0,"['self', 'first_dim_list', 'fixed_shape']","[None, None, None]","[None, None, None]",251,[],"['datasets.Features', 'Array3D', 'Value']",3
repos/datasets/tests/features/test_array_xd.py:ArrayXDDynamicTest:test_iter_dataset,ArrayXDDynamicTest:test_iter_dataset,method,11,23,23,293,12.74,1,0,['self'],[None],[None],307,[],"['self.get_one_col_dataset', 'zip', 'self.assertIsInstance', 'self.assertTupleEqual']",4
repos/datasets/tests/features/test_array_xd.py:ArrayXDDynamicTest:test_map_dataset,ArrayXDDynamicTest:test_map_dataset,method,12,31,30,378,12.19,1,0,['self'],[None],[None],346,[],"['self.get_one_col_dataset', 'dataset.map', 'np.concatenate', 'zip', 'self.assertIsInstance', 'self.assertTupleEqual']",6
repos/datasets/tests/features/test_array_xd.py:ArrayXDDynamicTest:test_to_numpy,ArrayXDDynamicTest:test_to_numpy,method,16,71,38,1155,16.27,2,0,['self'],[None],[None],274,[],"['self.get_one_col_dataset', 'SimpleArrowExtractor', 'self.assertIsInstance', 'arr_xd.to_numpy', 'self.assertEqual', 'zip', 'self.assertTupleEqual', 'self.assertNotEqual']",8
repos/datasets/tests/features/test_array_xd.py:ArrayXDDynamicTest:test_to_pandas,ArrayXDDynamicTest:test_to_pandas,method,15,59,35,927,15.71,2,0,['self'],[None],[None],317,[],"['self.get_one_col_dataset', 'dataset.to_pandas', 'self.assertEqual', 'self.assertIsInstance', 'zip', 'self.assertTupleEqual', 'self.assertNotEqual']",7
repos/datasets/tests/features/test_array_xd.py:ArrayXDDynamicTest:test_to_pylist,ArrayXDDynamicTest:test_to_pylist,method,13,27,27,411,15.22,1,0,['self'],[None],[None],262,[],"['self.get_one_col_dataset', 'SimpleArrowExtractor', 'self.assertIsInstance', 'arr_xd.to_pylist', 'zip', 'self.assertTupleEqual']",6
repos/datasets/tests/features/test_array_xd.py:ArrayXDTest:_check_getitem_output_type,ArrayXDTest:_check_getitem_output_type,method,12,59,37,1535,26.02,0,0,"['self', 'dataset', 'shape_1', 'shape_2', 'first_matrix']","[None, None, None, None, None]","[None, None, None, None, None]",175,[],"['self.assertIsInstance', 'self.assertTupleEqual', 'self.assertEqual', 'np.array', 'dataset.formatted_as']",5
repos/datasets/tests/features/test_array_xd.py:ArrayXDTest:get_dict_example_0,ArrayXDTest:get_dict_example_0,method,2,9,9,130,14.44,0,0,"['self', 'shape_1', 'shape_2']","[None, None, None]","[None, None, None]",154,[],[],0
repos/datasets/tests/features/test_array_xd.py:ArrayXDTest:get_dict_example_1,ArrayXDTest:get_dict_example_1,method,2,9,9,130,14.44,0,0,"['self', 'shape_1', 'shape_2']","[None, None, None]","[None, None, None]",161,[],[],0
repos/datasets/tests/features/test_array_xd.py:ArrayXDTest:get_dict_examples,ArrayXDTest:get_dict_examples,method,2,12,11,160,13.33,0,0,"['self', 'shape_1', 'shape_2']","[None, None, None]","[None, None, None]",168,[],[],0
repos/datasets/tests/features/test_array_xd.py:ArrayXDTest:get_features,ArrayXDTest:get_features,method,4,13,12,153,11.77,0,0,"['self', 'array_feature', 'shape_1', 'shape_2']","[None, None, None, None]","[None, None, None, None]",145,[],"['datasets.Features', 'array_feature', 'Value']",3
repos/datasets/tests/features/test_array_xd.py:ArrayXDTest:test_from_dict,ArrayXDTest:test_from_dict,method,6,16,13,258,16.12,0,0,"['self', 'array_feature', 'shape_1', 'shape_2']","[None, None, None, None]","[None, None, None, None]",235,[],"['self.get_dict_examples', 'self._check_getitem_output_type']",2
repos/datasets/tests/features/test_array_xd.py:ArrayXDTest:test_write,ArrayXDTest:test_write,method,18,43,38,600,13.95,1,0,"['self', 'array_feature', 'shape_1', 'shape_2']","[None, None, None, None]","[None, None, None, None]",207,[],"['tempfile.TemporaryDirectory', 'self.get_features', 'self.get_dict_example_0', 'self.get_dict_example_1', 'ArrowWriter', 'my_features.encode_example', 'writer.write', 'writer.finalize', 'self._check_getitem_output_type']",9
repos/datasets/tests/features/test_array_xd.py:ArrayXDTest:test_write_batch,ArrayXDTest:test_write_batch,method,16,32,26,540,16.88,0,0,"['self', 'array_feature', 'shape_1', 'shape_2']","[None, None, None, None]","[None, None, None, None]",223,[],"['tempfile.TemporaryDirectory', 'self.get_features', 'self.get_dict_examples', 'my_features.encode_batch', 'ArrowWriter', 'writer.write_batch', 'writer.finalize', 'self._check_getitem_output_type']",8
repos/datasets/tests/features/test_array_xd.py:ExtensionTypeCompatibilityTest:test_array2d_nonspecific_shape,ExtensionTypeCompatibilityTest:test_array2d_nonspecific_shape,method,26,70,62,789,11.27,1,0,['self'],[None],[None],60,[],"['tempfile.TemporaryDirectory', 'DEFAULT_FEATURES.copy', 'ArrowWriter', 'generate_examples', 'my_features.encode_example', 'writer.write', 'writer.finalize', 'dataset.set_format', 'self.assertTrue', 'self.assertEqual', 'len', 'self.assertNotEqual']",12
repos/datasets/tests/features/test_array_xd.py:ExtensionTypeCompatibilityTest:test_compatability_with_string_values,ExtensionTypeCompatibilityTest:test_compatability_with_string_values,method,20,40,37,552,13.8,1,0,['self'],[None],[None],100,[],"['tempfile.TemporaryDirectory', 'DEFAULT_FEATURES.copy', 'datasets.Value', 'ArrowWriter', 'generate_examples', 'my_features.encode_example', 'writer.write', 'writer.finalize', 'self.assertIsInstance']",9
repos/datasets/tests/features/test_array_xd.py:ExtensionTypeCompatibilityTest:test_extension_indexing,ExtensionTypeCompatibilityTest:test_extension_indexing,method,23,43,40,625,14.53,1,0,['self'],[None],[None],113,[],"['tempfile.TemporaryDirectory', 'DEFAULT_FEATURES.copy', 'Array2D', 'ArrowWriter', 'generate_examples', 'my_features.encode_example', 'writer.write', 'writer.finalize', 'dataset.set_format', 'self.assertIsInstance']",10
repos/datasets/tests/features/test_array_xd.py:ExtensionTypeCompatibilityTest:test_multiple_extensions_same_row,ExtensionTypeCompatibilityTest:test_multiple_extensions_same_row,method,24,71,48,747,10.52,1,0,['self'],[None],[None],81,[],"['tempfile.TemporaryDirectory', 'DEFAULT_FEATURES.copy', 'ArrowWriter', 'generate_examples', 'my_features.encode_example', 'writer.write', 'writer.finalize', 'dataset.set_format', 'len', 'self.assertEqual']",10
repos/datasets/tests/features/test_audio.py:iter_archive,iter_archive,function,9,15,13,134,8.93,1,0,['archive_path'],[None],[None],33,[],"['tarfile.open', 'tar.extractfile']",2
repos/datasets/tests/features/test_audio.py:jsonl_audio_dataset_path,jsonl_audio_dataset_path,function,11,27,26,257,9.52,1,0,"['shared_datadir', 'tmp_path_factory']","[None, None]","[None, None]",576,[],"['str', 'open', 'f.write']",3
repos/datasets/tests/features/test_audio.py:tar_mp3_path,tar_mp3_path,function,8,15,14,211,14.07,0,0,"['shared_datadir', 'tmp_path_factory']","[None, None]","[None, None]",25,[],"['str', 'tmp_path_factory.mktemp', 'tarfile.TarFile', 'f.add']",4
repos/datasets/tests/features/test_audio.py:tar_wav_path,tar_wav_path,function,8,15,14,211,14.07,0,0,"['shared_datadir', 'tmp_path_factory']","[None, None]","[None, None]",16,[],"['str', 'tmp_path_factory.mktemp', 'tarfile.TarFile', 'f.add']",4
repos/datasets/tests/features/test_audio.py:test_audio_decode_example,test_audio_decode_example,function,11,24,21,417,17.38,0,0,['shared_datadir'],[None],[None],105,[],"['str', 'Audio', 'audio.decode_example', 'decoded_example.keys', 'pytest.raises']",5
repos/datasets/tests/features/test_audio.py:test_audio_decode_example_mp3,test_audio_decode_example_mp3,function,9,21,18,337,16.05,0,0,['shared_datadir'],[None],[None],130,[],"['str', 'Audio', 'audio.decode_example', 'decoded_example.keys']",4
repos/datasets/tests/features/test_audio.py:test_audio_decode_example_opus,test_audio_decode_example_opus,function,9,21,18,337,16.05,0,0,['shared_datadir'],[None],[None],141,[],"['str', 'Audio', 'audio.decode_example', 'decoded_example.keys']",4
repos/datasets/tests/features/test_audio.py:test_audio_decode_example_pcm,test_audio_decode_example_pcm,function,10,30,26,443,14.77,0,0,"['shared_datadir', 'sampling_rate']","[None, None]","[None, None]",152,[],"['str', 'Audio', 'audio.decode_example', 'decoded_example.keys']",4
repos/datasets/tests/features/test_audio.py:test_audio_embed_storage,test_audio_embed_storage,function,10,25,21,369,14.76,0,0,['shared_datadir'],[None],[None],679,[],"['str', 'pa.array', 'pa.binary', 'pa.string', 'Audio', 'embedded_storage.to_pylist', 'open']",7
repos/datasets/tests/features/test_audio.py:test_audio_feature_encode_example,test_audio_feature_encode_example,function,12,31,26,408,13.16,0,0,"['shared_datadir', 'build_example']","[None, None]","[None, None]",73,[],"['str', 'Audio', 'audio.encode_example', 'isinstance', 'encoded_example.keys', 'audio.decode_example', 'decoded_example.keys']",7
repos/datasets/tests/features/test_audio.py:test_audio_feature_encode_example_pcm,test_audio_feature_encode_example_pcm,function,12,31,26,428,13.81,0,0,"['shared_datadir', 'build_example']","[None, None]","[None, None]",93,[],"['str', 'Audio', 'audio.encode_example', 'isinstance', 'encoded_example.keys', 'audio.decode_example', 'decoded_example.keys']",7
repos/datasets/tests/features/test_audio.py:test_audio_feature_type_to_arrow,test_audio_feature_type_to_arrow,function,4,23,17,427,18.57,0,0,[],[],[],51,[],"['Features', 'Audio', 'pa.schema', 'pa.struct', 'Sequence', 'pa.list_']",6
repos/datasets/tests/features/test_audio.py:test_audio_instantiation,test_audio_instantiation,function,9,26,18,215,8.27,0,0,[],[],[],41,[],"['Audio', 'pa.struct', 'pa.binary', 'pa.string']",4
repos/datasets/tests/features/test_audio.py:test_audio_resampling,test_audio_resampling,function,9,21,18,355,16.9,0,0,['shared_datadir'],[None],[None],119,[],"['str', 'Audio', 'audio.decode_example', 'decoded_example.keys']",4
repos/datasets/tests/features/test_audio.py:test_audio_resampling_mp3_different_sampling_rates,test_audio_resampling_mp3_different_sampling_rates,function,10,40,24,683,17.07,0,0,['shared_datadir'],[None],[None],164,[],"['str', 'Audio', 'audio.decode_example', 'decoded_example.keys']",4
repos/datasets/tests/features/test_audio.py:test_dataset_cast_to_audio_features,test_dataset_cast_to_audio_features,function,11,29,20,379,13.07,0,0,"['shared_datadir', 'build_data']","[None, None]","[None, None]",429,[],"['str', 'build_data', 'Dataset.from_dict', 'dset.cast', 'Audio', 'item.keys', 'dset.cast_column']",7
repos/datasets/tests/features/test_audio.py:test_dataset_concatenate_audio_features,test_dataset_concatenate_audio_features,function,12,32,27,563,17.59,0,0,['shared_datadir'],[None],[None],441,[],"['str', 'Dataset.from_dict', 'features=Features', 'Audio', 'open', 'concatenate_datasets', 'len']",7
repos/datasets/tests/features/test_audio.py:test_dataset_concatenate_nested_audio_features,test_dataset_concatenate_nested_audio_features,function,12,40,34,796,19.9,0,0,['shared_datadir'],[None],[None],454,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'open', 'concatenate_datasets', 'len']",7
repos/datasets/tests/features/test_audio.py:test_dataset_with_audio_feature,test_dataset_with_audio_feature,function,24,72,44,896,12.44,0,0,['shared_datadir'],[None],[None],183,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'item.keys', 'batch.keys', 'len']",7
repos/datasets/tests/features/test_audio.py:test_dataset_with_audio_feature_loaded_from_cache,test_dataset_with_audio_feature_loaded_from_cache,function,3,10,8,179,17.9,0,0,[],[],[],605,[],"['load_dataset', 'isinstance']",2
repos/datasets/tests/features/test_audio.py:test_dataset_with_audio_feature_map_is_decoded,test_dataset_with_audio_feature_map_is_decoded,function,23,68,49,1022,15.03,3,0,['shared_datadir'],[None],[None],497,[],"['str', 'Features', 'Audio', 'Value', 'Dataset.from_dict', 'process_audio_sampling_rate_by_example', 'dset.map', 'decoded_dset.cast_column', 'item.keys', 'process_audio_sampling_rate_by_batch', 'double_sampling_rates.append']",11
repos/datasets/tests/features/test_audio.py:test_dataset_with_audio_feature_map_is_not_decoded,test_dataset_with_audio_feature_map_is_not_decoded,function,19,59,40,683,11.58,2,0,['shared_datadir'],[None],[None],475,[],"['str', 'Features', 'Audio', 'Value', 'Dataset.from_dict', 'features.encode_batch', 'dset.cast_column', 'item.keys', 'process_text', 'dset.map', 'processed_dset.cast_column']",11
repos/datasets/tests/features/test_audio.py:test_dataset_with_audio_feature_map_undecoded,test_dataset_with_audio_feature_map_undecoded,function,13,35,28,483,13.8,1,0,['shared_datadir'],[None],[None],661,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'assert_audio_example_undecoded', 'dset.map', 'assert_audio_batch_undecoded']",7
repos/datasets/tests/features/test_audio.py:test_dataset_with_audio_feature_tar_mp3,test_dataset_with_audio_feature_tar_mp3,function,28,81,52,1004,12.4,1,0,['tar_mp3_path'],[None],[None],240,[],"['iter_archive', 'file_obj.read', 'Features', 'Audio', 'Dataset.from_dict', 'item.keys', 'batch.keys', 'len']",8
repos/datasets/tests/features/test_audio.py:test_dataset_with_audio_feature_tar_wav,test_dataset_with_audio_feature_tar_wav,function,28,81,52,1004,12.4,1,0,['tar_wav_path'],[None],[None],210,[],"['iter_archive', 'file_obj.read', 'Features', 'Audio', 'Dataset.from_dict', 'item.keys', 'batch.keys', 'len']",8
repos/datasets/tests/features/test_audio.py:test_dataset_with_audio_feature_undecoded,test_dataset_with_audio_feature_undecoded,function,18,48,33,497,10.35,0,0,['shared_datadir'],[None],[None],613,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'item.keys', 'batch.keys', 'len']",7
repos/datasets/tests/features/test_audio.py:test_dataset_with_audio_feature_with_none,test_dataset_with_audio_feature_with_none,function,17,98,48,866,8.84,0,0,[],[],[],270,[],"['Features', 'Audio', 'Dataset.from_dict', 'item.keys', 'len', 'batch.keys', 'isinstance', 'all', 'Sequence']",9
repos/datasets/tests/features/test_audio.py:test_formatted_dataset_with_audio_feature,test_formatted_dataset_with_audio_feature,function,32,142,60,1737,12.23,0,0,['shared_datadir'],[None],[None],526,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'dset.formatted_as', 'item.keys', 'batch.keys', 'len']",8
repos/datasets/tests/features/test_audio.py:test_formatted_dataset_with_audio_feature_undecoded,test_formatted_dataset_with_audio_feature_undecoded,function,24,93,44,907,9.75,0,0,['shared_datadir'],[None],[None],630,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'dset.formatted_as', 'item.keys', 'batch.keys', 'len']",8
repos/datasets/tests/features/test_audio.py:test_load_dataset_with_audio_feature,test_load_dataset_with_audio_feature,function,16,41,37,523,12.76,0,1,"['streaming', 'jsonl_audio_dataset_path', 'shared_datadir']","[None, None, None]","[None, None, None]",590,[],"['str', 'Features', 'Audio', 'Value', 'load_dataset', 'next', 'item.keys']",7
repos/datasets/tests/features/test_audio.py:test_resampling_after_loading_dataset_with_audio_feature,test_resampling_after_loading_dataset_with_audio_feature,function,25,80,47,1008,12.6,0,0,['shared_datadir'],[None],[None],358,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'dset.cast_column', 'item.keys', 'batch.keys', 'len']",8
repos/datasets/tests/features/test_audio.py:test_resampling_after_loading_dataset_with_audio_feature_mp3,test_resampling_after_loading_dataset_with_audio_feature_mp3,function,25,80,47,1008,12.6,0,0,['shared_datadir'],[None],[None],388,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'dset.cast_column', 'item.keys', 'batch.keys', 'len']",8
repos/datasets/tests/features/test_audio.py:test_resampling_at_loading_dataset_with_audio_feature,test_resampling_at_loading_dataset_with_audio_feature,function,24,72,44,912,12.67,0,0,['shared_datadir'],[None],[None],304,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'item.keys', 'batch.keys', 'len']",7
repos/datasets/tests/features/test_audio.py:test_resampling_at_loading_dataset_with_audio_feature_mp3,test_resampling_at_loading_dataset_with_audio_feature_mp3,function,24,72,44,912,12.67,0,0,['shared_datadir'],[None],[None],331,[],"['str', 'Features', 'Audio', 'Dataset.from_dict', 'item.keys', 'batch.keys', 'len']",7
repos/datasets/tests/features/test_features.py:dict_diff,dict_diff,function,6,24,22,190,7.92,1,1,"['d1', 'd2']","[' dict', ' dict']","[None, None]",467,[],"['zip', 'isinstance', 'dict_diff', 'iternumpy']",4
repos/datasets/tests/features/test_features.py:iternumpy,iternumpy,function,6,25,23,165,6.6,1,1,"['key1', 'value1', 'value2']","[None, None, None]","[None, None, None]",460,[],['AssertionError'],1
repos/datasets/tests/features/test_features.py:test_class_label_to_and_from_dict,test_class_label_to_and_from_dict,function,11,28,24,414,14.79,0,1,"['class_label_arg', 'tmp_path_factory']","[None, None]","[None, None]",370,[],"['str', 'open', 'f.write', 'ClassLabel', 'generate_from_dict']",5
repos/datasets/tests/features/test_features.py:test_classlabel_cast_storage,test_classlabel_cast_storage,function,12,93,34,1051,11.3,0,0,[],[],[],330,[],"['ClassLabel', 'pa.array', 'classlabel.cast_storage', 'pa.int64', 'result.to_pylist', 'pytest.raises', 'pa.string']",7
repos/datasets/tests/features/test_features.py:test_classlabel_init,test_classlabel_init,function,12,69,37,953,13.81,0,0,['tmp_path_factory'],[None],[None],281,[],"['str', 'open', 'f.write', 'ClassLabel', 'len', 'range', 'pytest.raises']",7
repos/datasets/tests/features/test_features.py:test_classlabel_int2str,test_classlabel_int2str,function,8,21,17,299,14.24,1,0,[],[],[],317,[],"['ClassLabel', 'range', 'classlabel.int2str', 'pytest.raises']",4
repos/datasets/tests/features/test_features.py:test_classlabel_str2int,test_classlabel_str2int,function,7,21,17,314,14.95,1,0,[],[],[],304,[],"['ClassLabel', 'classlabel.str2int', 'names.index', 'pytest.raises']",4
repos/datasets/tests/features/test_features.py:test_dataset_feature_with_none,test_dataset_feature_with_none,function,17,98,48,834,8.51,0,0,['feature'],[None],[None],427,[],"['Features', 'Dataset.from_dict', 'item.keys', 'len', 'batch.keys', 'isinstance', 'all', 'Sequence']",8
repos/datasets/tests/features/test_features.py:test_encode_batch_with_example_with_empty_first_elem,test_encode_batch_with_example_with_empty_first_elem,function,4,27,20,203,7.52,0,0,[],[],[],391,[],"['Features', 'Sequence', 'features.encode_batch']",3
repos/datasets/tests/features/test_features.py:test_encode_column_dict_with_none,test_encode_column_dict_with_none,function,4,25,21,191,7.64,0,0,[],[],[],408,[],"['Features', 'ClassLabel', 'Value', 'features.encode_column']",4
repos/datasets/tests/features/test_features.py:test_encode_nested_example_sequence_with_none,test_encode_nested_example_sequence_with_none,function,5,11,10,96,8.73,0,0,['inner_type'],[None],[None],384,[],"['Sequence', 'encode_nested_example']",2
repos/datasets/tests/features/test_features.py:test_features_alignment,test_features_alignment,function,5,16,15,146,9.12,0,0,"['features', 'Features]']","[' Tuple[List[Features]', None]","[None, None]",688,[],"['_check_if_features_can_be_aligned', '_align_features']",2
repos/datasets/tests/features/test_features.py:test_features_to_arrow_schema,test_features_to_arrow_schema,function,6,10,8,150,15.0,0,0,['features'],[' Features'],[None],648,[],"['isinstance', 'Features.from_arrow_schema']",2
repos/datasets/tests/features/test_features.py:test_features_to_dict,test_features_to_dict,function,6,10,8,137,13.7,0,0,['features'],[' Features'],[None],632,[],"['features.to_dict', 'isinstance', 'Features.from_dict']",3
repos/datasets/tests/features/test_features.py:test_features_to_yaml_list,test_features_to_yaml_list,function,6,10,8,164,16.4,0,0,['features'],[' Features'],[None],640,[],"['features._to_yaml_list', 'isinstance', 'Features._from_yaml_list']",3
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest,CastToPythonObjectsTest,class,37,385,102,3788,9.84,0,0,[],[],[],483,[],[],0
repos/datasets/tests/features/test_features.py:FeaturesTest,FeaturesTest,class,59,490,219,6644,13.56,4,0,[],[],[],31,[],[],0
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_objects_dataframe,CastToPythonObjectsTest:test_cast_to_python_objects_dataframe,method,6,36,23,255,7.08,0,0,['self'],[None],[None],514,[],"['pd.DataFrame', 'cast_to_python_objects', 'self.assertDictEqual']",3
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_objects_jax,CastToPythonObjectsTest:test_cast_to_python_objects_jax,method,11,45,36,382,8.49,0,0,['self'],[None],[None],571,[],"['jnp.array', 'np.array', 'cast_to_python_objects', 'dict_diff']",4
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_objects_list,CastToPythonObjectsTest:test_cast_to_python_objects_list,method,5,36,21,241,6.69,0,0,['self'],[None],[None],484,[],"['cast_to_python_objects', 'self.assertDictEqual']",2
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_objects_pandas_timedelta,CastToPythonObjectsTest:test_cast_to_python_objects_pandas_timedelta,method,9,18,16,357,19.83,0,0,['self'],[None],[None],530,[],"['pd.Timedelta', 'obj.to_pytimedelta', 'cast_to_python_objects', 'self.assertEqual', 'self.assertListEqual', 'self.assertDictEqual']",6
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_objects_pandas_timestamp,CastToPythonObjectsTest:test_cast_to_python_objects_pandas_timestamp,method,9,20,18,355,17.75,0,0,['self'],[None],[None],520,[],"['pd.Timestamp', 'obj.to_pydatetime', 'cast_to_python_objects', 'self.assertEqual', 'self.assertListEqual', 'self.assertDictEqual']",6
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_objects_series,CastToPythonObjectsTest:test_cast_to_python_objects_series,method,6,38,28,267,7.03,0,0,['self'],[None],[None],505,[],"['pd.Series', 'cast_to_python_objects', 'self.assertDictEqual']",3
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_objects_tf,CastToPythonObjectsTest:test_cast_to_python_objects_tf,method,10,40,31,320,8.0,0,0,['self'],[None],[None],556,[],"['tf.constant', 'np.array', 'cast_to_python_objects', 'dict_diff']",4
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_objects_torch,CastToPythonObjectsTest:test_cast_to_python_objects_torch,method,9,38,29,313,8.24,0,0,['self'],[None],[None],541,[],"['torch.tensor', 'np.array', 'cast_to_python_objects', 'dict_diff']",4
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_objects_tuple,CastToPythonObjectsTest:test_cast_to_python_objects_tuple,method,5,36,21,241,6.69,0,0,['self'],[None],[None],490,[],"['cast_to_python_objects', 'self.assertDictEqual']",2
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_cast_to_python_or_numpy,CastToPythonObjectsTest:test_cast_to_python_or_numpy,method,7,34,29,269,7.91,0,0,['self'],[None],[None],496,[],"['np.arange', 'np.array', 'cast_to_python_objects', 'dict_diff']",4
repos/datasets/tests/features/test_features.py:CastToPythonObjectsTest:test_dont_iterate_over_each_element_in_a_list,CastToPythonObjectsTest:test_dont_iterate_over_each_element_in_a_list,method,3,16,15,117,7.31,0,0,"['self', 'mocked_cast']","[None, None]","[None, None]",587,[],"['cast_to_python_objects', 'self.assertEqual']",2
repos/datasets/tests/features/test_features.py:FeaturesTest:test_class_label_feature_with_no_labels,FeaturesTest:test_class_label_feature_with_no_labels,method,6,10,8,188,18.8,0,0,['self'],[None],[None],115,"['        """"""reference: issue #4681""""""\n']","['Features', 'ClassLabel', 'DatasetInfo', 'Features.from_dict']",4
repos/datasets/tests/features/test_features.py:FeaturesTest:test_feature_named_self_as_kwarg,FeaturesTest:test_feature_named_self_as_kwarg,method,6,9,7,178,19.78,0,0,['self'],[None],[None],108,"['        """"""reference: issue #5641""""""\n']","['Features', 'DatasetInfo', 'Features.from_dict']",3
repos/datasets/tests/features/test_features.py:FeaturesTest:test_feature_named_type,FeaturesTest:test_feature_named_type,method,6,10,8,183,18.3,0,0,['self'],[None],[None],101,"['        """"""reference: issue #1110""""""\n']","['Features', 'Value', 'DatasetInfo', 'Features.from_dict']",4
repos/datasets/tests/features/test_features.py:FeaturesTest:test_features_dicts_are_synced,FeaturesTest:test_features_dicts_are_synced,method,14,35,29,783,22.37,0,0,['self'],[None],[None],256,[],"['assert_features_dicts_are_synced', 'hasattr', 'features.keys', 'Features', 'Sequence', 'Value', 'Image', 'features.update', 'features.pop', 'features.popitem', 'features.setdefault', 'features.clear']",12
repos/datasets/tests/features/test_features.py:FeaturesTest:test_flatten,FeaturesTest:test_flatten,method,8,27,23,311,11.52,0,0,['self'],[None],[None],242,[],"['Features', 'Value', 'features.copy', 'features.flatten']",4
repos/datasets/tests/features/test_features.py:FeaturesTest:test_flatten_with_sequence,FeaturesTest:test_flatten_with_sequence,method,8,24,21,279,11.62,0,0,['self'],[None],[None],249,[],"['Features', 'Sequence', 'Value', 'features.copy', 'features.flatten']",5
repos/datasets/tests/features/test_features.py:FeaturesTest:test_from_arrow_schema_simple,FeaturesTest:test_from_arrow_schema_simple,method,10,30,27,411,13.7,0,0,['self'],[None],[None],32,[],"['Features', 'Value', 'Dataset.from_dict', 'self.assertEqual', 'self.assertDictEqual']",5
repos/datasets/tests/features/test_features.py:FeaturesTest:test_from_arrow_schema_with_sequence,FeaturesTest:test_from_arrow_schema_with_sequence,method,10,30,28,423,14.1,0,0,['self'],[None],[None],42,[],"['Features', 'Sequence', 'Value', 'Dataset.from_dict', 'self.assertEqual', 'self.assertDictEqual']",6
repos/datasets/tests/features/test_features.py:FeaturesTest:test_reorder_fields_as,FeaturesTest:test_reorder_fields_as,method,12,218,63,2377,10.9,0,0,['self'],[None],[None],122,[],"['Features', 'Value', 'Sequence', 'ClassLabel', 'features.reorder_fields_as', 'self.assertDictEqual', 'self.assertEqual', 'self.assertNotEqual']",8
repos/datasets/tests/features/test_features.py:FeaturesTest:test_string_to_arrow_bijection_for_primitive_types,FeaturesTest:test_string_to_arrow_bijection_for_primitive_types,method,10,77,61,1093,14.19,4,0,['self'],[None],[None],52,[],"['pa.time32', 'pa.time64', 'pa.timestamp', 'pa.date32', 'pa.date64', 'pa.duration', 'pa.decimal128', 'pa.decimal256', 'pa.string', 'pa.int32', 'pa.float64', 'pa.array', 'DataType', 'self.assertEqual', 'string_to_arrow', 'self.assertRaises', '_arrow_to_datasets_dtype']",17
repos/datasets/tests/features/test_image.py:data_dir,data_dir,function,9,18,18,209,11.61,0,0,"['shared_datadir', 'tmp_path']","[None, None]","[None, None]",591,[],"['data_dir.mkdir', 'str', 'open', 'f.write']",4
repos/datasets/tests/features/test_image.py:dataset_loading_script_dir,dataset_loading_script_dir,function,11,17,16,222,13.06,0,0,['tmp_path'],[None],[None],601,[],"['script_dir.mkdir', 'open', 'f.write', 'str']",4
repos/datasets/tests/features/test_image.py:iter_archive,iter_archive,function,9,15,13,134,8.93,1,0,['archive_path'],[None],[None],26,[],"['tarfile.open', 'tar.extractfile']",2
repos/datasets/tests/features/test_image.py:tar_jpg_path,tar_jpg_path,function,8,15,14,209,13.93,0,0,"['shared_datadir', 'tmp_path_factory']","[None, None]","[None, None]",18,[],"['str', 'tmp_path_factory.mktemp', 'tarfile.TarFile', 'f.add']",4
repos/datasets/tests/features/test_image.py:test_dataset_cast_to_image_features,test_dataset_cast_to_image_features,function,13,27,20,367,13.59,0,0,"['shared_datadir', 'build_data']","[None, None]","[None, None]",306,[],"['str', 'build_data', 'Dataset.from_dict', 'dset.cast', 'Image', 'item.keys', 'isinstance', 'dset.cast_column']",8
repos/datasets/tests/features/test_image.py:test_dataset_concatenate_image_features,test_dataset_concatenate_image_features,function,12,32,27,501,15.66,0,0,['shared_datadir'],[None],[None],321,[],"['str', 'Dataset.from_dict', 'features=Features', 'Image', 'open', 'concatenate_datasets', 'len']",7
repos/datasets/tests/features/test_image.py:test_dataset_concatenate_nested_image_features,test_dataset_concatenate_nested_image_features,function,12,40,34,734,18.35,0,0,['shared_datadir'],[None],[None],335,[],"['str', 'Features', 'Image', 'Dataset.from_dict', 'open', 'concatenate_datasets', 'len']",7
repos/datasets/tests/features/test_image.py:test_dataset_with_image_feature,test_dataset_with_image_feature,function,26,94,57,1045,11.12,0,0,['shared_datadir'],[None],[None],127,[],"['str', 'Features', 'Image', 'Dataset.from_dict', 'item.keys', 'isinstance', 'len', 'batch.keys', 'all']",9
repos/datasets/tests/features/test_image.py:test_dataset_with_image_feature_from_np_array,test_dataset_with_image_feature_from_np_array,function,26,99,60,1111,11.22,0,0,[],[],[],192,[],"['np.arange', 'Features', 'Image', 'Dataset.from_dict', 'item.keys', 'isinstance', 'len', 'batch.keys', 'all']",9
repos/datasets/tests/features/test_image.py:test_dataset_with_image_feature_from_pil_image,test_dataset_with_image_feature_from_pil_image,function,26,99,62,1087,10.98,0,0,"['infer_feature', 'shared_datadir']","[None, None]","[None, None]",160,[],"['str', 'Features', 'Image', 'Dataset.from_dict', 'item.keys', 'isinstance', 'len', 'batch.keys', 'all']",9
repos/datasets/tests/features/test_image.py:test_dataset_with_image_feature_map,test_dataset_with_image_feature_map,function,25,127,66,1463,11.52,4,0,['shared_datadir'],[None],[None],356,[],"['str', 'Features', 'Image', 'Value', 'Dataset.from_dict', 'dset.cast_column', 'item.keys', 'process_caption', 'dset.map', 'processed_dset.cast_column', 'process_image_by_example', 'decoded_dset.cast_column', 'process_image_by_batch']",13
repos/datasets/tests/features/test_image.py:test_dataset_with_image_feature_map_change_image,test_dataset_with_image_feature_map_change_image,function,29,156,67,1974,12.65,5,0,['shared_datadir'],[None],[None],440,[],"['str', 'Image', 'Features', 'Dataset.from_dict', 'dset.cast_column', 'item.keys', 'process_image_resize_by_example', 'dset.map', 'decoded_dset.cast_column', 'image_to_bytes', 'process_image_resize_by_batch', 'process_image_resize_by_example_return_np_array', 'np.array', 'process_image_resize_by_batch_return_np_array']",14
repos/datasets/tests/features/test_image.py:test_dataset_with_image_feature_map_undecoded,test_dataset_with_image_feature_map_undecoded,function,13,35,28,481,13.74,1,0,['shared_datadir'],[None],[None],678,[],"['str', 'Features', 'Image', 'Dataset.from_dict', 'assert_image_example_undecoded', 'dset.map', 'assert_image_batch_undecoded']",7
repos/datasets/tests/features/test_image.py:test_dataset_with_image_feature_tar_jpg,test_dataset_with_image_feature_tar_jpg,function,29,101,61,1030,10.2,1,0,['tar_jpg_path'],[None],[None],225,[],"['iter_archive', 'file_obj.read', 'Features', 'Image', 'Dataset.from_dict', 'item.keys', 'isinstance', 'len', 'batch.keys', 'all']",10
repos/datasets/tests/features/test_image.py:test_dataset_with_image_feature_undecoded,test_dataset_with_image_feature_undecoded,function,18,48,33,495,10.31,0,0,['shared_datadir'],[None],[None],628,[],"['str', 'Features', 'Image', 'Dataset.from_dict', 'item.keys', 'batch.keys', 'len']",7
repos/datasets/tests/features/test_image.py:test_dataset_with_image_feature_with_none,test_dataset_with_image_feature_with_none,function,17,98,51,870,8.88,0,0,[],[],[],260,[],"['Features', 'Image', 'Dataset.from_dict', 'item.keys', 'len', 'batch.keys', 'isinstance', 'all', 'Sequence']",9
repos/datasets/tests/features/test_image.py:test_encode_np_array,test_encode_np_array,function,24,59,48,823,13.95,0,1,"['array', 'dtype_cast', 'expected_image_format']","[None, None, None]","[None, None, None]",719,[],"['dtype_cast.startswith', 'dtype_cast.split', 'np.dtype', 'pytest.warns', 'Image', 'pytest.raises', 'warnings.catch_warnings', 'warnings.simplefilter', 'isinstance', 'encoded_image.keys']",10
repos/datasets/tests/features/test_image.py:test_formatted_dataset_with_image_feature,test_formatted_dataset_with_image_feature,function,35,150,82,1566,10.44,0,0,['shared_datadir'],[None],[None],510,[],"['str', 'Features', 'Image', 'Dataset.from_dict', 'dset.formatted_as', 'item.keys', 'isinstance', 'batch.keys', 'len', 'all']",10
repos/datasets/tests/features/test_image.py:test_formatted_dataset_with_image_feature_map,test_formatted_dataset_with_image_feature_map,function,25,102,60,1308,12.82,3,0,['shared_datadir'],[None],[None],405,[],"['str', 'Image', 'Features', 'Value', 'Dataset.from_dict', 'dset.cast_column', 'item.keys', 'process_image_by_example', 'dset.with_format', 'decoded_dset.cast_column', 'encode_np_array', 'process_image_by_batch']",12
repos/datasets/tests/features/test_image.py:test_formatted_dataset_with_image_feature_undecoded,test_formatted_dataset_with_image_feature_undecoded,function,24,93,44,905,9.73,0,0,['shared_datadir'],[None],[None],646,[],"['str', 'Features', 'Image', 'Dataset.from_dict', 'dset.formatted_as', 'item.keys', 'batch.keys', 'len']",8
repos/datasets/tests/features/test_image.py:test_image_change_mode,test_image_change_mode,function,12,33,29,365,11.06,0,0,['shared_datadir'],[None],[None],113,[],"['str', 'Image', 'image.decode_example', 'isinstance', 'hasattr']",5
repos/datasets/tests/features/test_image.py:test_image_decode_example,test_image_decode_example,function,14,28,25,414,14.79,0,0,['shared_datadir'],[None],[None],78,[],"['str', 'Image', 'image.decode_example', 'isinstance', 'pytest.raises']",5
repos/datasets/tests/features/test_image.py:test_image_decode_example_with_exif_orientation_tag,test_image_decode_example_with_exif_orientation_tag,function,19,39,35,467,11.97,1,0,['shared_datadir'],[None],[None],95,[],"['str', 'BytesIO', 'Image', 'image.decode_example', 'buffer.getvalue', 'isinstance']",6
repos/datasets/tests/features/test_image.py:test_image_embed_storage,test_image_embed_storage,function,10,25,21,365,14.6,0,0,['shared_datadir'],[None],[None],697,[],"['str', 'pa.array', 'pa.binary', 'pa.string', 'Image', 'embedded_storage.to_pylist', 'open']",7
repos/datasets/tests/features/test_image.py:test_image_feature_encode_example,test_image_feature_encode_example,function,13,31,26,409,13.19,0,0,"['shared_datadir', 'build_example']","[None, None]","[None, None]",64,[],"['str', 'Image', 'image.encode_example', 'isinstance', 'encoded_example.keys', 'image.decode_example']",6
repos/datasets/tests/features/test_image.py:test_image_feature_type_to_arrow,test_image_feature_type_to_arrow,function,4,23,17,427,18.57,0,0,[],[],[],42,[],"['Features', 'Image', 'pa.schema', 'pa.struct', 'Sequence', 'pa.list_']",6
repos/datasets/tests/features/test_image.py:test_image_instantiation,test_image_instantiation,function,7,18,15,171,9.5,0,0,[],[],[],34,[],"['Image', 'pa.struct', 'pa.binary', 'pa.string']",4
repos/datasets/tests/features/test_image.py:test_load_dataset_with_image_feature,test_load_dataset_with_image_feature,function,15,37,32,460,12.43,0,1,"['shared_datadir', 'data_dir', 'dataset_loading_script_dir', 'streaming']","[None, None, None, None]","[None, None, None, None]",613,[],"['str', 'load_dataset', 'next', 'item.keys', 'isinstance']",5
repos/datasets/tests/features/test_image.py:__DummyDataset__,__DummyDataset__,class,14,42,37,469,11.17,1,0,[],[],[],572,[],[],0
repos/datasets/tests/features/test_image.py:__DummyDataset__:_generate_examples,__DummyDataset__:_generate_examples,method,9,19,18,161,8.47,1,0,"['self', 'filepath', '**kwargs']","[None, None, None]","[None, None, None]",582,[],"['open', 'enumerate', 'line.split', 'image_path.strip', 'caption.strip']",5
repos/datasets/tests/features/test_image.py:__DummyDataset__:_info,__DummyDataset__:_info,method,2,5,5,81,16.2,0,0,['self'],[None],[None],574,[],"['DatasetInfo', 'Image', 'Value']",3
repos/datasets/tests/features/test_image.py:__DummyDataset__:_split_generators,__DummyDataset__:_split_generators,method,1,7,7,110,15.71,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",577,[],['SplitGenerator'],1
repos/datasets/tests/fixtures/files.py:arrow_file,arrow_file,function,4,6,5,111,18.5,0,0,"['tmp_path_factory', 'dataset']","[None, None]","[None, None]",51,[],"['str', 'dataset.map']",2
repos/datasets/tests/fixtures/files.py:arrow_path,arrow_path,function,6,8,7,157,19.62,0,0,['tmp_path_factory'],[None],[None],242,[],"['str', 'dataset.map']",2
repos/datasets/tests/fixtures/files.py:audio_file,audio_file,function,2,5,5,68,13.6,0,0,[],[],[],550,[],[],0
repos/datasets/tests/fixtures/files.py:bz2_csv_path,bz2_csv_path,function,11,20,16,159,7.95,0,0,"['csv_path', 'tmp_path_factory']","[None, None]","[None, None]",284,[],"['tmp_path_factory.mktemp', 'open', 'f.read', 'bz2.open', 'f.write']",5
repos/datasets/tests/fixtures/files.py:bz2_file,bz2_file,function,10,16,15,147,9.19,0,0,['tmp_path_factory'],[None],[None],80,[],"['tmp_path_factory.mktemp', 'bytes', 'bz2.open', 'f.write']",4
repos/datasets/tests/fixtures/files.py:csv2_path,csv2_path,function,11,22,21,221,10.05,1,0,['tmp_path_factory'],[None],[None],273,[],"['str', 'open', 'csv.DictWriter', 'writer.writeheader', 'writer.writerow']",5
repos/datasets/tests/fixtures/files.py:csv_path,csv_path,function,11,22,21,220,10.0,1,0,['tmp_path_factory'],[None],[None],262,[],"['str', 'open', 'csv.DictWriter', 'writer.writeheader', 'writer.writerow']",5
repos/datasets/tests/fixtures/files.py:data_dir_with_hidden_files,data_dir_with_hidden_files,function,6,53,19,456,8.6,0,0,['tmp_path_factory'],[None],[None],564,[],"['tmp_path_factory.mktemp', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:dataset,dataset,function,9,47,35,506,10.77,0,0,[],[],[],23,[],"['datasets.Features', 'datasets.Sequence', 'datasets.Value', 'list']",4
repos/datasets/tests/fixtures/files.py:dataset_dict,dataset_dict,function,2,2,2,24,12.0,0,0,[],[],[],237,[],[],0
repos/datasets/tests/fixtures/files.py:geoparquet_path,geoparquet_path,function,6,8,7,206,25.75,0,0,['tmp_path_factory'],[None],[None],342,[],"['pd.read_parquet', 'str', 'df.to_parquet']",3
repos/datasets/tests/fixtures/files.py:gz_file,gz_file,function,10,16,15,153,9.56,0,0,['tmp_path_factory'],[None],[None],91,[],"['str', 'bytes', 'gzip.open', 'f.write']",4
repos/datasets/tests/fixtures/files.py:image_file,image_file,function,2,5,5,66,13.2,0,0,[],[],[],545,[],[],0
repos/datasets/tests/fixtures/files.py:json_dict_of_lists_path,json_dict_of_lists_path,function,7,15,14,141,9.4,0,0,['tmp_path_factory'],[None],[None],359,[],"['str', 'open', 'json.dump']",3
repos/datasets/tests/fixtures/files.py:json_list_of_dicts_path,json_list_of_dicts_path,function,7,15,14,127,8.47,0,0,['tmp_path_factory'],[None],[None],350,[],"['str', 'open', 'json.dump']",3
repos/datasets/tests/fixtures/files.py:jsonl2_path,jsonl2_path,function,8,16,15,138,8.62,1,0,['tmp_path_factory'],[None],[None],377,[],"['str', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:jsonl_312_path,jsonl_312_path,function,8,16,15,145,9.06,1,0,['tmp_path_factory'],[None],[None],386,[],"['str', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:jsonl_gz_path,jsonl_gz_path,function,9,18,15,193,10.72,0,0,"['tmp_path_factory', 'jsonl_path']","[None, None]","[None, None]",415,[],"['str', 'open', 'gzip.open', 'zipped_file.writelines']",4
repos/datasets/tests/fixtures/files.py:jsonl_path,jsonl_path,function,8,16,15,137,8.56,1,0,['tmp_path_factory'],[None],[None],368,[],"['str', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:jsonl_str_path,jsonl_str_path,function,8,16,15,145,9.06,1,0,['tmp_path_factory'],[None],[None],395,[],"['str', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:lz4_file,lz4_file,function,11,18,17,192,10.67,0,1,['tmp_path_factory'],[None],[None],102,[],"['tmp_path_factory.mktemp', 'bytes', 'f.write']",3
repos/datasets/tests/fixtures/files.py:parquet_path,parquet_path,function,15,38,34,363,9.55,0,0,['tmp_path_factory'],[None],[None],324,[],"['str', 'pa.schema', 'pa.string', 'pa.int64', 'pa.float64', 'open', 'pq.ParquetWriter', 'range', 'writer.write_table', 'writer.close']",10
repos/datasets/tests/fixtures/files.py:seven_zip_file,seven_zip_file,function,9,16,15,212,13.25,0,1,"['tmp_path_factory', 'text_file']","[None, None]","[None, None]",114,[],"['tmp_path_factory.mktemp', 'py7zr.SevenZipFile', 'archive.write']",3
repos/datasets/tests/fixtures/files.py:sqlite_path,sqlite_path,function,11,34,33,316,9.29,1,0,['tmp_path_factory'],[None],[None],250,[],"['str', 'contextlib.closing', 'con.cursor', 'cur.execute', 'dataset', 'tuple', 'con.commit']",7
repos/datasets/tests/fixtures/files.py:tar_file,tar_file,function,8,14,13,163,11.64,0,0,"['tmp_path_factory', 'text_file']","[None, None]","[None, None]",125,[],"['tmp_path_factory.mktemp', 'tarfile.TarFile', 'f.add']",3
repos/datasets/tests/fixtures/files.py:tar_jsonl_path,tar_jsonl_path,function,6,14,13,213,15.21,0,0,"['jsonl_path', 'jsonl2_path', 'tmp_path_factory']","[None, None, None]","[None, None, None]",452,[],"['tmp_path_factory.mktemp', 'tarfile.TarFile', 'f.add']",3
repos/datasets/tests/fixtures/files.py:tar_nested_jsonl_path,tar_nested_jsonl_path,function,6,13,12,194,14.92,0,0,"['tar_jsonl_path', 'jsonl_path', 'jsonl2_path', 'tmp_path_factory']","[None, None, None, None]","[None, None, None, None]",461,[],"['tmp_path_factory.mktemp', 'tarfile.TarFile', 'f.add']",3
repos/datasets/tests/fixtures/files.py:text2_path,text2_path,function,8,21,20,147,7.0,1,0,['tmp_path_factory'],[None],[None],479,[],"['str', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:text_dir,text_dir,function,9,21,21,157,7.48,1,0,['tmp_path_factory'],[None],[None],489,[],"['tmp_path_factory.mktemp', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:text_dir_with_unsupported_extension,text_dir_with_unsupported_extension,function,8,21,20,141,6.71,1,0,['tmp_path_factory'],[None],[None],499,[],"['tmp_path_factory.mktemp', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:text_file,text_file,function,8,13,12,125,9.62,0,0,['tmp_path_factory'],[None],[None],71,[],"['tmp_path_factory.mktemp', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:text_file_content,text_file_content,function,2,2,2,18,9.0,0,0,[],[],[],66,[],[],0
repos/datasets/tests/fixtures/files.py:text_gz_path,text_gz_path,function,9,18,15,190,10.56,0,0,"['tmp_path_factory', 'text_path']","[None, None]","[None, None]",404,[],"['str', 'open', 'gzip.open', 'zipped_file.writelines']",4
repos/datasets/tests/fixtures/files.py:text_path,text_path,function,8,21,20,146,6.95,1,0,['tmp_path_factory'],[None],[None],469,[],"['str', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:text_path_with_unicode_new_lines,text_path_with_unicode_new_lines,function,7,19,18,210,11.05,0,0,['tmp_path_factory'],[None],[None],536,[],"['str', 'open', 'f.write']",3
repos/datasets/tests/fixtures/files.py:xml_file,xml_file,function,8,14,13,131,9.36,0,0,['tmp_path_factory'],[None],[None],171,[],"['tmp_path_factory.mktemp', 'textwrap.dedent', 'open', 'f.write']",4
repos/datasets/tests/fixtures/files.py:xz_file,xz_file,function,10,16,15,148,9.25,0,0,['tmp_path_factory'],[None],[None],135,[],"['tmp_path_factory.mktemp', 'bytes', 'lzma.open', 'f.write']",4
repos/datasets/tests/fixtures/files.py:zip_csv_path,zip_csv_path,function,6,14,13,215,15.36,0,0,"['csv_path', 'csv2_path', 'tmp_path_factory']","[None, None, None]","[None, None, None]",297,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_csv_with_dir_path,zip_csv_with_dir_path,function,6,16,14,266,16.62,0,0,"['csv_path', 'csv2_path', 'tmp_path_factory']","[None, None, None]","[None, None, None]",315,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_file,zip_file,function,8,14,13,165,11.79,0,0,"['tmp_path_factory', 'text_file']","[None, None]","[None, None]",146,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_image_path,zip_image_path,function,6,15,13,237,15.8,0,0,"['image_file', 'tmp_path_factory']","[None, None]","[None, None]",555,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_jsonl_path,zip_jsonl_path,function,6,14,13,217,15.5,0,0,"['jsonl_path', 'jsonl2_path', 'tmp_path_factory']","[None, None, None]","[None, None, None]",426,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_jsonl_with_dir_path,zip_jsonl_with_dir_path,function,6,16,14,276,17.25,0,0,"['jsonl_path', 'jsonl2_path', 'tmp_path_factory']","[None, None, None]","[None, None, None]",443,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_nested_jsonl_path,zip_nested_jsonl_path,function,6,13,12,196,15.08,0,0,"['zip_jsonl_path', 'jsonl_path', 'jsonl2_path', 'tmp_path_factory']","[None, None, None, None]","[None, None, None, None]",435,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_text_path,zip_text_path,function,6,14,13,212,15.14,0,0,"['text_path', 'text2_path', 'tmp_path_factory']","[None, None, None]","[None, None, None]",509,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_text_with_dir_path,zip_text_with_dir_path,function,6,16,14,271,16.94,0,0,"['text_path', 'text2_path', 'tmp_path_factory']","[None, None, None]","[None, None, None]",518,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_unsupported_ext_path,zip_unsupported_ext_path,function,6,14,13,228,16.29,0,0,"['text_path', 'text2_path', 'tmp_path_factory']","[None, None, None]","[None, None, None]",527,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zip_uppercase_csv_path,zip_uppercase_csv_path,function,6,16,14,253,15.81,0,0,"['csv_path', 'csv2_path', 'tmp_path_factory']","[None, None, None]","[None, None, None]",306,[],"['tmp_path_factory.mktemp', 'zipfile.ZipFile', 'f.write']",3
repos/datasets/tests/fixtures/files.py:zstd_file,zstd_file,function,12,20,18,199,9.95,0,1,['tmp_path_factory'],[None],[None],156,[],"['tmp_path_factory.mktemp', 'bytes', 'zstd.open', 'f.write']",4
repos/datasets/tests/fixtures/fsspec.py:mock_fsspec,mock_fsspec,function,2,9,7,141,15.67,0,0,[],[],[],94,[],[],0
repos/datasets/tests/fixtures/fsspec.py:mockfs,mockfs,function,4,5,5,112,22.4,0,0,"['tmp_path_factory', 'mock_fsspec']","[None, None]","[None, None]",103,[],"['tmp_path_factory.mktemp', 'MockFileSystem']",2
repos/datasets/tests/fixtures/fsspec.py:tmpfs,tmpfs,function,6,9,9,164,18.22,0,0,"['tmp_path_factory', 'mock_fsspec']","[None, None]","[None, None]",109,[],"['tmp_path_factory.mktemp', 'patch.object', 'TmpDirFileSystem', 'TmpDirFileSystem.clear_instance_cache']",4
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem,MockFileSystem,class,42,177,75,2159,12.2,0,2,[],[],[],10,[],[],0
repos/datasets/tests/fixtures/fsspec.py:TmpDirFileSystem,TmpDirFileSystem,class,12,33,28,324,9.82,0,1,[],[],[],77,[],[],0
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:__init__,MockFileSystem:__init__,method,5,7,7,125,17.86,0,0,"['self', '*args', 'local_root_dir', '**kwargs']","[None, None, None, None]","[None, None, None, None]",13,[],"['super', 'LocalFileSystem', 'Path']",3
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:_open,MockFileSystem:_open,method,4,7,7,109,15.57,0,0,"['self', 'path', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",57,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:_strip_protocol,MockFileSystem:_strip_protocol,method,5,8,6,80,10.0,0,1,"['cls', 'path']","[None, None]","[None, None]",70,[],"['stringify_path', 'path.startswith']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:cp_file,MockFileSystem:cp_file,method,5,11,10,190,17.27,0,0,"['self', 'path1', 'path2', '*args', '**kwargs']","[None, None, None, None, None]","[None, None, None, None, None]",44,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:created,MockFileSystem:created,method,4,5,5,96,19.2,0,0,"['self', 'path']","[None, None]","[None, None]",61,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:info,MockFileSystem:info,method,6,12,11,173,14.42,0,0,"['self', 'path', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",38,[],"['posixpath.join', 'self._strip_protocol', 'dict']",3
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:ls,MockFileSystem:ls,method,7,27,24,262,9.7,0,1,"['self', 'path', 'detail', '*args', '**kwargs']","[None, None, None, None, None]","[None, None, 'True', None, None]",30,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:makedirs,MockFileSystem:makedirs,method,4,7,7,112,16.0,0,0,"['self', 'path', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",22,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:mkdir,MockFileSystem:mkdir,method,4,7,7,109,15.57,0,0,"['self', 'path', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",18,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:modified,MockFileSystem:modified,method,4,5,5,97,19.4,0,0,"['self', 'path']","[None, None]","[None, None]",65,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:rm,MockFileSystem:rm,method,4,7,7,106,15.14,0,0,"['self', 'path', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",53,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:rm_file,MockFileSystem:rm_file,method,4,7,7,111,15.86,0,0,"['self', 'path', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",49,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:MockFileSystem:rmdir,MockFileSystem:rmdir,method,4,5,5,94,18.8,0,0,"['self', 'path']","[None, None]","[None, None]",26,[],"['posixpath.join', 'self._strip_protocol']",2
repos/datasets/tests/fixtures/fsspec.py:TmpDirFileSystem:__init__,TmpDirFileSystem:__init__,method,3,13,11,139,10.69,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",81,[],['super'],1
repos/datasets/tests/fixtures/fsspec.py:TmpDirFileSystem:_strip_protocol,TmpDirFileSystem:_strip_protocol,method,5,8,6,79,9.88,0,1,"['cls', 'path']","[None, None]","[None, None]",86,[],"['stringify_path', 'path.startswith']",2
repos/datasets/tests/fixtures/hub.py:ci_hfh_hf_hub_url,ci_hfh_hf_hub_url,function,1,4,4,117,29.25,0,0,['monkeypatch'],[None],[None],22,[],['monkeypatch.setattr'],1
repos/datasets/tests/fixtures/hub.py:ci_hub_config,ci_hub_config,function,1,4,4,142,35.5,0,0,['monkeypatch'],[None],[None],29,[],['monkeypatch.setattr'],1
repos/datasets/tests/fixtures/hub.py:cleanup_repo,cleanup_repo,function,3,7,7,118,16.86,0,0,['hf_api'],[None],[None],54,[],"['_cleanup_repo', 'hf_api.delete_repo']",2
repos/datasets/tests/fixtures/hub.py:hf_api,hf_api,function,2,2,2,37,18.5,0,0,[],[],[],44,[],['HfApi'],1
repos/datasets/tests/fixtures/hub.py:hf_private_dataset_repo_txt_data,hf_private_dataset_repo_txt_data,function,2,2,2,39,19.5,0,0,"['hf_private_dataset_repo_txt_data_', 'ci_hub_config', 'ci_hfh_hf_hub_url']","[None, None, None]","[None, None, None]",97,[],[],0
repos/datasets/tests/fixtures/hub.py:hf_private_dataset_repo_txt_data_,hf_private_dataset_repo_txt_data_,function,9,34,33,489,14.38,0,0,"['hf_api', 'hf_token', 'text_file_content']","[' HfApi', None, None]","[None, None, None]",78,[],"['hf_api.create_repo', 'hf_api.upload_file', 'hf_api.delete_repo']",3
repos/datasets/tests/fixtures/hub.py:hf_private_dataset_repo_zipped_img_data,hf_private_dataset_repo_zipped_img_data,function,2,2,2,46,23.0,0,0,"['hf_private_dataset_repo_zipped_img_data_', 'ci_hub_config', 'ci_hfh_hf_hub_url']","[None, None, None]","[None, None, None]",147,[],[],0
repos/datasets/tests/fixtures/hub.py:hf_private_dataset_repo_zipped_img_data_,hf_private_dataset_repo_zipped_img_data_,function,9,34,33,479,14.09,0,0,"['hf_api', 'hf_token', 'zip_image_path']","[' HfApi', None, None]","[None, None, None]",128,[],"['hf_api.create_repo', 'hf_api.upload_file', 'path_or_fileobj=str', 'hf_api.delete_repo']",4
repos/datasets/tests/fixtures/hub.py:hf_private_dataset_repo_zipped_txt_data,hf_private_dataset_repo_zipped_txt_data,function,2,2,2,46,23.0,0,0,"['hf_private_dataset_repo_zipped_txt_data_', 'ci_hub_config', 'ci_hfh_hf_hub_url']","[None, None, None]","[None, None, None]",121,[],[],0
repos/datasets/tests/fixtures/hub.py:hf_private_dataset_repo_zipped_txt_data_,hf_private_dataset_repo_zipped_txt_data_,function,9,34,33,486,14.29,0,0,"['hf_api', 'hf_token', 'zip_csv_with_dir_path']","[' HfApi', None, None]","[None, None, None]",102,[],"['hf_api.create_repo', 'hf_api.upload_file', 'path_or_fileobj=str', 'hf_api.delete_repo']",4
repos/datasets/tests/fixtures/hub.py:hf_token,hf_token,function,1,2,2,22,11.0,0,0,[],[],[],49,[],[],0
repos/datasets/tests/fixtures/hub.py:set_ci_hub_access_token,set_ci_hub_access_token,function,4,7,7,125,17.86,0,0,['ci_hub_config'],[None],[None],35,[],['dict'],1
repos/datasets/tests/fixtures/hub.py:temporary_repo,temporary_repo,function,8,21,19,269,12.81,0,0,['cleanup_repo'],[None],[None],62,[],"['_temporary_repo', 'cleanup_repo']",2
repos/datasets/tests/io/test_csv.py:_check_csv_dataset,_check_csv_dataset,function,8,22,17,250,11.36,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",13,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/io/test_csv.py:_check_csv_datasetdict,_check_csv_datasetdict,function,12,28,21,305,10.89,2,0,"['dataset_dict', 'expected_features', 'splits', '']","[None, None, None, None]","[None, None, '(""train""', None]",74,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/io/test_csv.py:iter_csv_file,iter_csv_file,function,4,8,8,74,9.25,0,0,['csv_path'],[None],[None],129,[],"['open', 'csv.reader']",2
repos/datasets/tests/io/test_csv.py:test_csv_datasetdict_reader_features,test_csv_datasetdict_reader_features,function,9,40,35,424,10.6,0,0,"['features', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",104,[],"['features.copy', 'Features', 'Value', 'features.items', 'CsvDatasetReader', '_check_csv_datasetdict']",6
repos/datasets/tests/io/test_csv.py:test_csv_datasetdict_reader_keep_in_memory,test_csv_datasetdict_reader_keep_in_memory,function,8,23,22,342,14.87,0,0,"['keep_in_memory', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",86,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'CsvDatasetReader', '_check_csv_datasetdict']",4
repos/datasets/tests/io/test_csv.py:test_csv_datasetdict_reader_split,test_csv_datasetdict_reader_split,function,10,34,30,366,10.76,0,1,"['split', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",117,[],"['CsvDatasetReader', '_check_csv_datasetdict', 'splits=list', 'all', 'path.keys']",5
repos/datasets/tests/io/test_csv.py:test_dataset_from_csv_features,test_dataset_from_csv_features,function,9,39,34,410,10.51,0,0,"['features', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",41,[],"['features.copy', 'Features', 'Value', 'features.items', 'CsvDatasetReader', '_check_csv_dataset']",6
repos/datasets/tests/io/test_csv.py:test_dataset_from_csv_keep_in_memory,test_dataset_from_csv_keep_in_memory,function,8,22,21,328,14.91,0,0,"['keep_in_memory', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",23,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'CsvDatasetReader', '_check_csv_dataset']",4
repos/datasets/tests/io/test_csv.py:test_dataset_from_csv_path_type,test_dataset_from_csv_path_type,function,9,25,22,291,11.64,0,1,"['path_type', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",63,[],"['issubclass', 'CsvDatasetReader', '_check_csv_dataset']",3
repos/datasets/tests/io/test_csv.py:test_dataset_from_csv_split,test_dataset_from_csv_split,function,8,23,21,261,11.35,0,1,"['split', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",54,[],"['CsvDatasetReader', '_check_csv_dataset']",2
repos/datasets/tests/io/test_csv.py:test_dataset_to_csv,test_dataset_to_csv,function,14,26,25,347,13.35,1,0,"['csv_path', 'tmp_path']","[None, None]","[None, None]",134,[],"['CsvDatasetReader', 'CsvDatasetWriter', 'iter_csv_file', 'zip']",4
repos/datasets/tests/io/test_csv.py:test_dataset_to_csv_fsspec,test_dataset_to_csv_fsspec,function,8,20,18,257,12.85,0,0,"['dataset', 'mockfs']","[None, None]","[None, None]",168,[],"['CsvDatasetWriter', 'writer.write', 'mockfs.isfile', 'fsspec.open', 'f.read']",5
repos/datasets/tests/io/test_csv.py:test_dataset_to_csv_invalidproc,test_dataset_to_csv_invalidproc,function,9,15,15,231,15.4,0,0,"['csv_path', 'tmp_path']","[None, None]","[None, None]",160,[],"['CsvDatasetReader', 'pytest.raises', 'CsvDatasetWriter']",3
repos/datasets/tests/io/test_csv.py:test_dataset_to_csv_multiproc,test_dataset_to_csv_multiproc,function,14,26,25,347,13.35,1,0,"['csv_path', 'tmp_path']","[None, None]","[None, None]",147,[],"['CsvDatasetReader', 'CsvDatasetWriter', 'iter_csv_file', 'zip']",4
repos/datasets/tests/io/test_json.py:_check_json_dataset,_check_json_dataset,function,8,22,17,250,11.36,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",13,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/io/test_json.py:_check_json_datasetdict,_check_json_datasetdict,function,12,28,21,305,10.89,2,0,"['dataset_dict', 'expected_features', 'splits', '']","[None, None, None, None]","[None, None, '(""train""', None]",113,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/io/test_json.py:load_json,load_json,function,2,2,2,23,11.5,0,0,['buffer'],[None],[None],168,[],['json.load'],1
repos/datasets/tests/io/test_json.py:load_json_lines,load_json_lines,function,1,6,6,39,6.5,0,0,['buffer'],[None],[None],172,[],[],0
repos/datasets/tests/io/test_json.py:test_dataset_from_json_features,test_dataset_from_json_features,function,9,39,35,415,10.64,0,0,"['features', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",41,[],"['features.copy', 'Features', 'Value', 'features.items', 'JsonDatasetReader', '_check_json_dataset']",6
repos/datasets/tests/io/test_json.py:test_dataset_from_json_keep_in_memory,test_dataset_from_json_keep_in_memory,function,8,22,22,333,15.14,0,0,"['keep_in_memory', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",23,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'JsonDatasetReader', '_check_json_dataset']",4
repos/datasets/tests/io/test_json.py:test_dataset_from_json_path_type,test_dataset_from_json_path_type,function,9,25,23,298,11.92,0,1,"['path_type', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",102,[],"['issubclass', 'JsonDatasetReader', '_check_json_dataset']",3
repos/datasets/tests/io/test_json.py:test_dataset_from_json_split,test_dataset_from_json_split,function,8,23,22,266,11.57,0,1,"['split', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",93,[],"['JsonDatasetReader', '_check_json_dataset']",2
repos/datasets/tests/io/test_json.py:test_dataset_from_json_with_mismatched_features,test_dataset_from_json_with_mismatched_features,function,15,55,45,567,10.31,1,0,"['jsonl_312_path', 'tmp_path']","[None, None]","[None, None]",75,[],"['features.copy', 'Features', 'Value', 'features.items', 'JsonDatasetReader', 'isinstance', 'expected_features.items']",7
repos/datasets/tests/io/test_json.py:test_dataset_from_json_with_unsorted_column_names,test_dataset_from_json_with_unsorted_column_names,function,16,59,47,623,10.56,1,0,"['features', 'jsonl_312_path', 'tmp_path']","[None, None, None]","[None, None, None]",59,[],"['features.copy', 'Features', 'Value', 'features.items', 'JsonDatasetReader', 'isinstance', 'expected_features.items']",7
repos/datasets/tests/io/test_json.py:test_datasetdict_from_json_features,test_datasetdict_from_json_features,function,9,40,36,429,10.72,0,0,"['features', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",143,[],"['features.copy', 'Features', 'Value', 'features.items', 'JsonDatasetReader', '_check_json_datasetdict']",6
repos/datasets/tests/io/test_json.py:test_datasetdict_from_json_keep_in_memory,test_datasetdict_from_json_keep_in_memory,function,8,23,23,347,15.09,0,0,"['keep_in_memory', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",125,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'JsonDatasetReader', '_check_json_datasetdict']",4
repos/datasets/tests/io/test_json.py:test_datasetdict_from_json_splits,test_datasetdict_from_json_splits,function,10,36,32,389,10.81,0,1,"['split', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",155,[],"['JsonDatasetReader', '_check_json_datasetdict', 'splits=list', 'all', 'path.keys']",5
repos/datasets/tests/io/test_json.py:TestJsonDatasetWriter,TestJsonDatasetWriter,class,33,325,132,3711,11.42,0,6,[],[],[],176,[],[],0
repos/datasets/tests/io/test_json.py:TestJsonDatasetWriter:test_dataset_to_json_compression,TestJsonDatasetWriter:test_dataset_to_json_compression,method,10,28,20,393,14.04,0,0,"['self', 'shared_datadir', 'tmp_path_factory', 'extension', 'compression', 'dataset']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",261,[],"['tmp_path_factory.mktemp', 'str', 'JsonDatasetWriter', 'fsspec.open', 'f.read']",5
repos/datasets/tests/io/test_json.py:TestJsonDatasetWriter:test_dataset_to_json_fsspec,TestJsonDatasetWriter:test_dataset_to_json_fsspec,method,8,20,18,259,12.95,0,0,"['self', 'dataset', 'mockfs']","[None, None, None]","[None, None, None]",272,[],"['JsonDatasetWriter', 'writer.write', 'mockfs.isfile', 'fsspec.open', 'f.read']",5
repos/datasets/tests/io/test_json.py:TestJsonDatasetWriter:test_dataset_to_json_lines,TestJsonDatasetWriter:test_dataset_to_json_lines,method,7,19,17,253,13.32,0,0,"['self', 'lines', 'load_json_function', 'dataset']","[None, None, None, None]","[None, None, None, None]",178,[],"['io.BytesIO', 'JsonDatasetWriter', 'buffer.seek', 'load_json_function', 'isinstance', 'len']",6
repos/datasets/tests/io/test_json.py:TestJsonDatasetWriter:test_dataset_to_json_lines_multiproc,TestJsonDatasetWriter:test_dataset_to_json_lines_multiproc,method,7,20,18,264,13.2,0,0,"['self', 'lines', 'load_json_function', 'dataset']","[None, None, None, None]","[None, None, None, None]",217,[],"['io.BytesIO', 'JsonDatasetWriter', 'buffer.seek', 'load_json_function', 'isinstance', 'len']",6
repos/datasets/tests/io/test_json.py:TestJsonDatasetWriter:test_dataset_to_json_orient,TestJsonDatasetWriter:test_dataset_to_json_orient,method,13,45,34,473,10.51,0,3,"['self', 'orient', 'container', 'keys', 'len_at', 'dataset']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",198,[],"['io.BytesIO', 'JsonDatasetWriter', 'buffer.seek', 'load_json', 'isinstance', 'exported_content.keys', 'hasattr', 'len']",8
repos/datasets/tests/io/test_json.py:TestJsonDatasetWriter:test_dataset_to_json_orient_invalidproc,TestJsonDatasetWriter:test_dataset_to_json_orient_invalidproc,method,4,9,8,101,11.22,0,0,"['self', 'dataset']","[None, None]","[None, None]",255,[],"['pytest.raises', 'io.BytesIO', 'JsonDatasetWriter']",3
repos/datasets/tests/io/test_json.py:TestJsonDatasetWriter:test_dataset_to_json_orient_multiproc,TestJsonDatasetWriter:test_dataset_to_json_orient_multiproc,method,13,46,35,484,10.52,0,3,"['self', 'orient', 'container', 'keys', 'len_at', 'dataset']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",237,[],"['io.BytesIO', 'JsonDatasetWriter', 'buffer.seek', 'load_json', 'isinstance', 'exported_content.keys', 'hasattr', 'len']",8
repos/datasets/tests/io/test_parquet.py:_check_parquet_dataset,_check_parquet_dataset,function,8,22,17,250,11.36,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",13,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/io/test_parquet.py:_check_parquet_datasetdict,_check_parquet_datasetdict,function,10,29,22,330,11.38,2,0,"['dataset_dict', 'expected_features', 'splits', '']","[None, None, None, None]","[None, None, '(""train""', None]",92,[],"['isinstance', 'len', 'set', 'expected_features.items']",4
repos/datasets/tests/io/test_parquet.py:test_dataset_from_parquet_features,test_dataset_from_parquet_features,function,9,39,35,423,10.85,0,0,"['features', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",41,[],"['features.copy', 'Features', 'Value', 'features.items', 'ParquetDatasetReader', '_check_parquet_dataset']",6
repos/datasets/tests/io/test_parquet.py:test_dataset_from_parquet_keep_in_memory,test_dataset_from_parquet_keep_in_memory,function,8,22,22,341,15.5,0,0,"['keep_in_memory', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",23,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'ParquetDatasetReader', '_check_parquet_dataset']",4
repos/datasets/tests/io/test_parquet.py:test_dataset_from_parquet_path_type,test_dataset_from_parquet_path_type,function,9,25,23,308,12.32,0,1,"['path_type', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",62,[],"['issubclass', 'ParquetDatasetReader', '_check_parquet_dataset']",3
repos/datasets/tests/io/test_parquet.py:test_dataset_from_parquet_split,test_dataset_from_parquet_split,function,8,23,22,274,11.91,0,1,"['split', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",53,[],"['ParquetDatasetReader', '_check_parquet_dataset']",2
repos/datasets/tests/io/test_parquet.py:test_dataset_to_parquet_fsspec,test_dataset_to_parquet_fsspec,function,8,20,18,261,13.05,0,0,"['dataset', 'mockfs']","[None, None]","[None, None]",219,[],"['ParquetDatasetWriter', 'writer.write', 'mockfs.isfile', 'fsspec.open', 'f.read']",5
repos/datasets/tests/io/test_parquet.py:test_dataset_to_parquet_keeps_features,test_dataset_to_parquet_keeps_features,function,17,33,30,523,15.85,0,0,"['shared_datadir', 'tmp_path']","[None, None]","[None, None]",192,[],"['str', 'Features', 'Image', 'Dataset.from_dict', 'ParquetDatasetWriter', 'writer.write', 'Dataset.from_parquet', 'ParquetDatasetReader']",8
repos/datasets/tests/io/test_parquet.py:test_get_writer_batch_size,test_get_writer_batch_size,function,1,3,3,46,15.33,0,0,"['feature', 'expected']","[None, None]","[None, None]",215,[],['get_writer_batch_size'],1
repos/datasets/tests/io/test_parquet.py:test_parquet_datasetdict_reader_columns,test_parquet_datasetdict_reader_columns,function,10,63,47,671,10.65,0,1,"['streaming', 'columns', 'pass_features', 'pass_info', 'parquet_path', 'tmp_path']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",142,[],"['DatasetInfo', 'Value', 'default_expected_features.items', 'Features', 'expected_features.items', 'ParquetDatasetReader', '_check_parquet_datasetdict']",7
repos/datasets/tests/io/test_parquet.py:test_parquet_datasetdict_reader_features,test_parquet_datasetdict_reader_features,function,9,43,39,459,10.67,0,0,"['streaming', 'features', 'parquet_path', 'tmp_path']","[None, None, None, None]","[None, None, None, None]",125,[],"['features.copy', 'Features', 'Value', 'features.items', 'ParquetDatasetReader', '_check_parquet_datasetdict']",6
repos/datasets/tests/io/test_parquet.py:test_parquet_datasetdict_reader_keep_in_memory,test_parquet_datasetdict_reader_keep_in_memory,function,8,25,25,357,14.28,0,0,"['keep_in_memory', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",104,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'ParquetDatasetReader', '_check_parquet_datasetdict']",4
repos/datasets/tests/io/test_parquet.py:test_parquet_datasetdict_reader_split,test_parquet_datasetdict_reader_split,function,10,36,32,401,11.14,0,1,"['split', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",171,[],"['ParquetDatasetReader', '_check_parquet_datasetdict', 'splits=list', 'all', 'path.keys']",5
repos/datasets/tests/io/test_parquet.py:test_parquet_read_geoparquet,test_parquet_read_geoparquet,function,13,44,38,527,11.98,1,0,"['geoparquet_path', 'tmp_path']","[None, None]","[None, None]",73,[],"['ParquetDatasetReader', 'isinstance', 'expected_features.items']",3
repos/datasets/tests/io/test_parquet.py:test_parquet_write,test_parquet_write,function,8,16,13,186,11.62,0,0,"['dataset', 'tmp_path']","[None, None]","[None, None]",184,[],"['ParquetDatasetWriter', 'writer.write', 'pq.ParquetFile', 'pf.read']",4
repos/datasets/tests/io/test_sql.py:_check_sql_dataset,_check_sql_dataset,function,8,22,17,250,11.36,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",13,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/io/test_sql.py:iter_sql_file,iter_sql_file,function,6,15,14,131,8.73,1,0,['sqlite_path'],[None],[None],56,[],"['contextlib.closing', 'con.cursor', 'cur.execute']",3
repos/datasets/tests/io/test_sql.py:test_dataset_from_sql_features,test_dataset_from_sql_features,function,9,41,37,437,10.66,0,0,"['features', 'sqlite_path', 'tmp_path', 'set_sqlalchemy_silence_uber_warning']","[None, None, None, None]","[None, None, None, None]",45,[],"['features.copy', 'Features', 'Value', 'features.items', 'SqlDatasetReader', '_check_sql_dataset']",6
repos/datasets/tests/io/test_sql.py:test_dataset_from_sql_keep_in_memory,test_dataset_from_sql_keep_in_memory,function,8,26,26,357,13.73,0,0,"['keep_in_memory', 'sqlite_path', 'tmp_path', 'set_sqlalchemy_silence_uber_warning']","[None, None, None, None]","[None, None, None, None]",24,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'SqlDatasetReader', '_check_sql_dataset']",4
repos/datasets/tests/io/test_sql.py:test_dataset_to_sql,test_dataset_to_sql,function,14,29,27,404,13.93,1,0,"['sqlite_path', 'tmp_path', 'set_sqlalchemy_silence_uber_warning']","[None, None, None]","[None, None, None]",65,[],"['SqlDatasetReader', 'SqlDatasetWriter', 'iter_sql_file', 'zip']",4
repos/datasets/tests/io/test_sql.py:test_dataset_to_sql_invalidproc,test_dataset_to_sql_invalidproc,function,9,18,17,285,15.83,0,0,"['sqlite_path', 'tmp_path', 'set_sqlalchemy_silence_uber_warning']","[None, None, None]","[None, None, None]",93,[],"['SqlDatasetReader', 'pytest.raises', 'SqlDatasetWriter']",3
repos/datasets/tests/io/test_sql.py:test_dataset_to_sql_multiproc,test_dataset_to_sql_multiproc,function,14,29,27,404,13.93,1,0,"['sqlite_path', 'tmp_path', 'set_sqlalchemy_silence_uber_warning']","[None, None, None]","[None, None, None]",79,[],"['SqlDatasetReader', 'SqlDatasetWriter', 'iter_sql_file', 'zip']",4
repos/datasets/tests/io/test_text.py:_check_text_dataset,_check_text_dataset,function,8,20,15,233,11.65,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",9,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/io/test_text.py:_check_text_datasetdict,_check_text_datasetdict,function,12,26,19,288,11.08,2,0,"['dataset_dict', 'expected_features', 'splits', '']","[None, None, None, None]","[None, None, '(""train""', None]",68,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/io/test_text.py:test_dataset_from_text_features,test_dataset_from_text_features,function,9,35,31,379,10.83,0,0,"['features', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",36,[],"['features.copy', 'Features', 'Value', 'features.items', 'TextDatasetReader', '_check_text_dataset']",6
repos/datasets/tests/io/test_text.py:test_dataset_from_text_keep_in_memory,test_dataset_from_text_keep_in_memory,function,8,18,18,297,16.5,0,0,"['keep_in_memory', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",19,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'TextDatasetReader', '_check_text_dataset']",4
repos/datasets/tests/io/test_text.py:test_dataset_from_text_path_type,test_dataset_from_text_path_type,function,9,21,19,261,12.43,0,1,"['path_type', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",57,[],"['issubclass', 'TextDatasetReader', '_check_text_dataset']",3
repos/datasets/tests/io/test_text.py:test_dataset_from_text_split,test_dataset_from_text_split,function,8,19,18,230,12.11,0,1,"['split', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",48,[],"['TextDatasetReader', '_check_text_dataset']",2
repos/datasets/tests/io/test_text.py:test_datasetdict_from_text_features,test_datasetdict_from_text_features,function,9,36,32,393,10.92,0,0,"['features', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",97,[],"['features.copy', 'Features', 'Value', 'features.items', 'TextDatasetReader', '_check_text_datasetdict']",6
repos/datasets/tests/io/test_text.py:test_datasetdict_from_text_keep_in_memory,test_datasetdict_from_text_keep_in_memory,function,8,19,19,311,16.37,0,0,"['keep_in_memory', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",80,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'TextDatasetReader', '_check_text_datasetdict']",4
repos/datasets/tests/io/test_text.py:test_datasetdict_from_text_split,test_datasetdict_from_text_split,function,10,32,28,351,10.97,0,1,"['split', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",110,[],"['TextDatasetReader', '_check_text_datasetdict', 'splits=list', 'all', 'path.keys']",5
repos/datasets/tests/packaged_modules/test_audiofolder.py:audio_file_with_metadata,audio_file_with_metadata,function,11,21,20,305,14.52,0,0,"['tmp_path', 'audio_file']","[None, None]","[None, None]",71,[],"['shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'str']",5
repos/datasets/tests/packaged_modules/test_audiofolder.py:audio_files_with_labels_and_duplicated_label_key_in_metadata,audio_files_with_labels_and_duplicated_label_key_in_metadata,function,18,43,34,698,16.23,0,0,"['tmp_path', 'audio_file']","[None, None]","[None, None]",44,[],"['data_dir.mkdir', 'subdir_class_0.mkdir', 'subdir_class_1.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'str']",8
repos/datasets/tests/packaged_modules/test_audiofolder.py:audio_files_with_metadata_that_misses_one_audio,audio_files_with_metadata_that_misses_one_audio,function,12,27,24,413,15.3,0,0,"['tmp_path', 'audio_file']","[None, None]","[None, None]",86,[],"['shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'str']",5
repos/datasets/tests/packaged_modules/test_audiofolder.py:cache_dir,cache_dir,function,2,3,3,43,14.33,0,0,['tmp_path'],[None],[None],18,[],['str'],1
repos/datasets/tests/packaged_modules/test_audiofolder.py:data_files_with_labels_no_metadata,data_files_with_labels_no_metadata,function,13,32,24,591,18.47,0,0,"['tmp_path', 'audio_file']","[None, None]","[None, None]",23,[],"['data_dir.mkdir', 'subdir_class_0.mkdir', 'subdir_class_1.mkdir', 'shutil.copyfile', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix']",7
repos/datasets/tests/packaged_modules/test_audiofolder.py:data_files_with_one_split_and_metadata,data_files_with_one_split_and_metadata,function,18,54,43,878,16.26,0,0,"['tmp_path', 'audio_file']","[None, None]","[None, None]",103,[],"['data_dir.mkdir', 'subdir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix', 'len']",10
repos/datasets/tests/packaged_modules/test_audiofolder.py:data_files_with_two_splits_and_metadata,data_files_with_two_splits_and_metadata,function,22,97,55,1360,14.02,0,2,"['request', 'tmp_path', 'audio_file']","[None, None, None]","[None, None, None]",135,[],"['data_dir.mkdir', 'train_dir.mkdir', 'test_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix', 'len']",11
repos/datasets/tests/packaged_modules/test_audiofolder.py:data_files_with_zip_archives,data_files_with_zip_archives,function,25,67,55,1028,15.34,0,0,"['tmp_path', 'audio_file']","[None, None]","[None, None]",196,[],"['data_dir.mkdir', 'archive_dir.mkdir', 'subdir.mkdir', 'shutil.copyfile', 'librosa.load', 'sf.write', 'textwrap.dedent', 'open', 'f.write', 'shutil.make_archive', 'shutil.rmtree', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'len']",14
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_data_files_with_metadata_and_archives,test_data_files_with_metadata_and_archives,function,13,63,47,703,11.16,1,0,"['streaming', 'cache_dir', 'data_files_with_zip_archives']","[None, None, None]","[None, None, None]",414,[],"['AudioFolder', 'audiofolder.download_and_prepare', 'audiofolder.as_streaming_dataset', 'audiofolder.as_dataset', 'data_files_with_zip_archives.items', 'len', 'list', 'sum', 'all']",9
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_data_files_with_metadata_and_multiple_splits,test_data_files_with_metadata_and_multiple_splits,function,13,58,41,646,11.14,1,0,"['streaming', 'cache_dir', 'data_files_with_two_splits_and_metadata']","[None, None, None]","[None, None, None]",396,[],"['AudioFolder', 'audiofolder.download_and_prepare', 'audiofolder.as_streaming_dataset', 'audiofolder.as_dataset', 'data_files.items', 'len', 'list', 'all']",8
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_data_files_with_metadata_and_single_split,test_data_files_with_metadata_and_single_split,function,13,58,41,645,11.12,1,0,"['streaming', 'cache_dir', 'data_files_with_one_split_and_metadata']","[None, None, None]","[None, None, None]",378,[],"['AudioFolder', 'audiofolder.download_and_prepare', 'audiofolder.as_streaming_dataset', 'audiofolder.as_dataset', 'data_files.items', 'len', 'list', 'all']",8
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_data_files_with_with_metadata_in_different_formats,test_data_files_with_with_metadata_in_different_formats,function,22,53,40,869,16.4,0,0,"['cache_dir', 'tmp_path', 'audio_file']","[None, None, None]","[None, None, None]",477,[],"['data_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'AudioFolder', 'pytest.raises', 'audiofolder.download_and_prepare', 'str']",11
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_data_files_with_wrong_audio_file_name_column_in_metadata_file,test_data_files_with_wrong_audio_file_name_column_in_metadata_file,function,20,47,42,685,14.57,0,0,"['cache_dir', 'tmp_path', 'audio_file']","[None, None, None]","[None, None, None]",456,[],"['data_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'AudioFolder', 'pytest.raises', 'audiofolder.download_and_prepare', 'str']",11
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_data_files_with_wrong_metadata_file_name,test_data_files_with_wrong_metadata_file_name,function,17,38,36,637,16.76,0,0,"['cache_dir', 'tmp_path', 'audio_file']","[None, None, None]","[None, None, None]",434,[],"['data_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'AudioFolder', 'audiofolder.download_and_prepare', 'audiofolder.as_dataset']",10
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_generate_examples_drop_labels,test_generate_examples_drop_labels,function,13,71,42,637,8.97,2,1,"['data_files_with_labels_no_metadata', 'drop_metadata', 'drop_labels']","[None, None, None]","[None, None, None]",284,[],"['AudioFolder', 'audiofolder._split_generators', 'bool', 'audiofolder._generate_examples', 'all', 'example.keys', 'example.values']",7
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_generate_examples_drop_metadata,test_generate_examples_drop_metadata,function,23,58,46,758,13.07,2,2,"['audio_file_with_metadata', 'drop_metadata', 'drop_labels']","[None, None, None]","[None, None, None]",308,[],"['AudioFolder', 'audiofolder._split_generators', 'bool', 'audiofolder._generate_examples', 'expected_columns.add', 'len', 'example.keys']",7
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_generate_examples_duplicated_label_key,test_generate_examples_duplicated_label_key,function,19,85,52,1054,12.4,0,3,"['audio_files_with_labels_and_duplicated_label_key_in_metadata', 'drop_metadata', 'drop_labels', 'cache_dir', 'caplog']","[None, None, None, None, None]","[None, None, None, None, None]",250,[],"['AudioFolder', 'audiofolder.download_and_prepare', 'any', 'audiofolder.as_dataset', 'ClassLabel', 'all', 'Value', 'Features', 'Audio']",9
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_generate_examples_with_labels,test_generate_examples_with_labels,function,11,22,20,451,20.5,0,0,"['data_files_with_labels_no_metadata', 'cache_dir']","[None, None]","[None, None]",235,[],"['AudioFolder', 'audiofolder.download_and_prepare', 'Features', 'Audio', 'ClassLabel', 'list']",6
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_generate_examples_with_metadata_in_wrong_location,test_generate_examples_with_metadata_in_wrong_location,function,14,38,34,471,12.39,1,1,"['audio_file', 'audio_file_with_metadata', 'drop_metadata']","[None, None, None]","[None, None, None]",334,[],"['AudioFolder', 'audiofolder._split_generators', 'audiofolder._generate_examples', 'pytest.raises', 'list', 'all', 'example.keys', 'example.values']",8
repos/datasets/tests/packaged_modules/test_audiofolder.py:test_generate_examples_with_metadata_that_misses_one_audio,test_generate_examples_with_metadata_that_misses_one_audio,function,19,56,46,675,12.05,1,2,"['audio_files_with_metadata_that_misses_one_audio', 'drop_metadata']","[None, None]","[None, None]",351,[],"['Features', 'Audio', 'Value', 'AudioFolder', 'audiofolder._split_generators', 'audiofolder._generate_examples', 'pytest.raises', 'list', 'all', 'example.keys', 'example.values']",11
repos/datasets/tests/packaged_modules/test_cache.py:test_cache,test_cache,function,10,20,19,330,16.5,0,0,"['text_dir', 'tmp_path']","[' Path', ' Path']","[None, None]",14,[],"['load_dataset', 'cache_dir=str', 'Path', 'Cache', 'cache.as_dataset', 'list']",6
repos/datasets/tests/packaged_modules/test_cache.py:test_cache_auto_hash,test_cache_auto_hash,function,8,19,18,297,15.63,0,0,"['text_dir', 'tmp_path']","[' Path', ' Path']","[None, None]",34,[],"['load_dataset', 'cache_dir=str', 'Cache', 'cache.as_dataset', 'list']",5
repos/datasets/tests/packaged_modules/test_cache.py:test_cache_auto_hash_with_custom_config,test_cache_auto_hash_with_custom_config,function,14,44,35,781,17.75,0,0,"['text_dir', 'tmp_path']","[' Path', ' Path']","[None, None]",43,[],"['load_dataset', 'cache_dir=str', 'Cache', 'cache.as_dataset', 'another_cache.as_dataset', 'list']",6
repos/datasets/tests/packaged_modules/test_cache.py:test_cache_capital_letters,test_cache_capital_letters,function,16,40,31,613,15.32,0,0,['tmp_path'],[' Path'],[None],140,[],"['repo_id.split', 'load_dataset', 'cache_dir=str', 'Cache', 'cache.as_dataset', 'list', 'len']",7
repos/datasets/tests/packaged_modules/test_cache.py:test_cache_missing,test_cache_missing,function,6,29,18,629,21.69,0,0,"['text_dir', 'tmp_path']","[' Path', ' Path']","[None, None]",61,[],"['load_dataset', 'cache_dir=str', 'Cache', 'pytest.raises']",4
repos/datasets/tests/packaged_modules/test_cache.py:test_cache_multi_configs,test_cache_multi_configs,function,22,46,36,660,14.35,0,0,['tmp_path'],[' Path'],[None],76,[],"['repo_id.split', 'load_dataset', 'cache_dir=str', 'Cache', 'cache.as_dataset', 'list', 'len', 'pytest.raises', 'str']",9
repos/datasets/tests/packaged_modules/test_cache.py:test_cache_single_config,test_cache_single_config,function,22,59,40,868,14.71,0,0,['tmp_path'],[' Path'],[None],106,[],"['repo_id.split', 'load_dataset', 'cache_dir=str', 'Cache', 'cache.as_dataset', 'list', 'len', 'pytest.raises', 'str']",9
repos/datasets/tests/packaged_modules/test_cache.py:test_cache_streaming,test_cache_streaming,function,10,20,19,350,17.5,0,0,"['text_dir', 'tmp_path']","[' Path', ' Path']","[None, None]",24,[],"['load_dataset', 'cache_dir=str', 'Path', 'Cache', 'cache.as_streaming_dataset', 'list']",6
repos/datasets/tests/packaged_modules/test_csv.py:csv_file,csv_file,function,2,3,3,28,9.33,0,0,['tmp_path'],[None],[None],14,[],[],0
repos/datasets/tests/packaged_modules/test_csv.py:csv_file_with_image,csv_file_with_image,function,9,17,17,148,8.71,0,0,"['tmp_path', 'image_file']","[None, None]","[None, None]",44,[],"['textwrap.dedent', 'open', 'f.write', 'str']",4
repos/datasets/tests/packaged_modules/test_csv.py:csv_file_with_int_list,csv_file_with_int_list,function,9,14,14,126,9.0,0,0,['tmp_path'],[None],[None],74,[],"['textwrap.dedent', 'open', 'f.write', 'str']",4
repos/datasets/tests/packaged_modules/test_csv.py:malformed_csv_file,malformed_csv_file,function,9,14,14,123,8.79,0,0,['tmp_path'],[None],[None],29,[],"['textwrap.dedent', 'open', 'f.write', 'str']",4
repos/datasets/tests/packaged_modules/test_csv.py:test_csv_cast_image,test_csv_cast_image,function,13,31,29,422,13.61,0,0,['csv_file_with_image'],[None],[None],104,[],"['open', 'f.read', 'Csv', 'features=Features', 'Image', 'csv._generate_tables', 'pa.concat_tables', 'pa_table.to_pydict']",8
repos/datasets/tests/packaged_modules/test_csv.py:test_csv_cast_label,test_csv_cast_label,function,13,35,30,500,14.29,0,0,['csv_file_with_label'],[None],[None],115,[],"['open', 'f.read', 'Csv', 'features=Features', 'ClassLabel', 'csv._generate_tables', 'pa.concat_tables', 'pa_table.to_pydict']",8
repos/datasets/tests/packaged_modules/test_csv.py:test_csv_convert_int_list,test_csv_convert_int_list,function,9,35,31,369,10.54,0,0,['csv_file_with_int_list'],[None],[None],126,[],"['Csv', 'x.split', 'csv._generate_tables', 'pa.concat_tables', 'pa_table.to_pydict']",5
repos/datasets/tests/packaged_modules/test_csv.py:test_csv_generate_tables_raises_error_with_malformed_csv,test_csv_generate_tables_raises_error_with_malformed_csv,function,10,35,29,313,8.94,2,0,"['csv_file', 'malformed_csv_file', 'caplog']","[None, None, None]","[None, None, None]",89,[],"['Csv', 'csv._generate_tables', 'pytest.raises', 'any']",4
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:auto_text_file,auto_text_file,function,2,2,2,20,10.0,0,0,['text_file'],[None],[None],39,[],['str'],1
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:cache_dir,cache_dir,function,2,3,3,42,14.0,0,0,['tmp_path'],[None],[None],34,[],['str'],1
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:data_files_with_different_levels_no_metadata,data_files_with_different_levels_no_metadata,function,13,33,25,580,17.58,0,0,"['tmp_path', 'auto_text_file']","[None, None]","[None, None]",65,[],"['data_dir.mkdir', 'subdir_class_0.mkdir', 'subdir_class_1.mkdir', 'shutil.copyfile', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix']",7
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:data_files_with_labels_no_metadata,data_files_with_labels_no_metadata,function,13,32,24,577,18.03,0,0,"['tmp_path', 'auto_text_file']","[None, None]","[None, None]",44,[],"['data_dir.mkdir', 'subdir_class_0.mkdir', 'subdir_class_1.mkdir', 'shutil.copyfile', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix']",7
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:data_files_with_one_label_no_metadata,data_files_with_one_label_no_metadata,function,9,20,16,372,18.6,0,0,"['tmp_path', 'auto_text_file']","[None, None]","[None, None]",86,[],"['data_dir.mkdir', 'shutil.copyfile', 'DataFilesDict.from_patterns', 'data_dir.as_posix']",4
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:data_files_with_one_split_and_metadata,data_files_with_one_split_and_metadata,function,18,54,43,821,15.2,0,0,"['tmp_path', 'auto_text_file']","[None, None]","[None, None]",161,[],"['data_dir.mkdir', 'subdir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix', 'len']",10
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:data_files_with_two_splits_and_metadata,data_files_with_two_splits_and_metadata,function,22,78,51,1161,14.88,0,0,"['tmp_path', 'auto_text_file']","[None, None]","[None, None]",193,[],"['data_dir.mkdir', 'train_dir.mkdir', 'test_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix', 'len']",11
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:data_files_with_zip_archives,data_files_with_zip_archives,function,21,56,45,869,15.52,0,0,"['tmp_path', 'auto_text_file']","[None, None]","[None, None]",235,[],"['data_dir.mkdir', 'archive_dir.mkdir', 'subdir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'shutil.make_archive', 'shutil.rmtree', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'len']",12
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:file_with_metadata,file_with_metadata,function,11,21,20,250,11.9,0,0,"['tmp_path', 'text_file']","[None, None]","[None, None]",129,[],"['shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'str']",5
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:files_with_labels_and_duplicated_label_key_in_metadata,files_with_labels_and_duplicated_label_key_in_metadata,function,18,43,34,648,15.07,0,0,"['tmp_path', 'auto_text_file']","[None, None]","[None, None]",102,[],"['data_dir.mkdir', 'subdir_class_0.mkdir', 'subdir_class_1.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'str']",8
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:files_with_metadata_that_misses_one_sample,files_with_metadata_that_misses_one_sample,function,12,27,24,343,12.7,0,0,"['tmp_path', 'auto_text_file']","[None, None]","[None, None]",144,[],"['shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'str']",5
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_data_files_with_different_levels_no_metadata,test_data_files_with_different_levels_no_metadata,function,13,51,36,625,12.25,0,2,"['data_files_with_different_levels_no_metadata', 'drop_labels', 'remote', 'cache_dir']","[None, None, None, None]","[None, None, None, None]",387,[],"['DummyFolderBasedBuilder', 'autofolder._split_generators', 'autofolder._generate_examples', 'all', 'isinstance']",5
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_data_files_with_metadata_and_archives,test_data_files_with_metadata_and_archives,function,12,57,40,844,14.81,1,0,"['streaming', 'cache_dir', 'data_files_with_zip_archives']","[None, None, None]","[None, None, None]",478,[],"['DummyFolderBasedBuilder', 'StreamingDownloadManager', 'DownloadManager', 'autofolder._split_generators', 'zip', 'len', 'list', 'all']",8
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_data_files_with_metadata_and_splits,test_data_files_with_metadata_and_splits,function,15,64,45,886,13.84,1,1,"['streaming', 'cache_dir', 'n_splits', 'data_files_with_one_split_and_metadata', 'data_files_with_two_splits_and_metadata']","[None, None, None, None, None]","[None, None, None, None, None]",457,[],"['DummyFolderBasedBuilder', 'StreamingDownloadManager', 'DownloadManager', 'autofolder._split_generators', 'zip', 'len', 'list', 'all']",8
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_data_files_with_metadata_that_misses_one_sample,test_data_files_with_metadata_that_misses_one_sample,function,16,55,45,656,11.93,0,2,"['files_with_metadata_that_misses_one_sample', 'drop_metadata', 'cache_dir']","[None, None, None]","[None, None, None]",430,[],"['Features', 'Value', 'DummyFolderBasedBuilder', 'autofolder._split_generators', 'autofolder._generate_examples', 'pytest.raises', 'list', 'all', 'example.keys', 'example.values']",10
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_data_files_with_one_label_no_metadata,test_data_files_with_one_label_no_metadata,function,13,51,36,622,12.2,0,2,"['data_files_with_one_label_no_metadata', 'drop_labels', 'remote', 'cache_dir']","[None, None, None, None]","[None, None, None, None]",410,[],"['DummyFolderBasedBuilder', 'autofolder._split_generators', 'autofolder._generate_examples', 'all', 'isinstance']",5
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_data_files_with_wrong_file_name_column_in_metadata_file,test_data_files_with_wrong_file_name_column_in_metadata_file,function,21,48,43,708,14.75,0,0,"['cache_dir', 'tmp_path', 'auto_text_file']","[None, None, None]","[None, None, None]",513,[],"['data_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'DummyFolderBasedBuilder', 'pytest.raises', 'autofolder._split_generators', 'str']",11
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_data_files_with_wrong_metadata_file_name,test_data_files_with_wrong_metadata_file_name,function,19,44,40,706,16.05,0,0,"['cache_dir', 'tmp_path', 'auto_text_file']","[None, None, None]","[None, None, None]",493,[],"['data_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'DummyFolderBasedBuilder', 'autofolder._split_generators', 'autofolder._generate_examples', 'all']",11
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_default_folder_builder_not_usable,test_default_folder_builder_not_usable,function,4,7,7,126,18.0,0,0,"['data_files_with_labels_no_metadata', 'cache_dir']","[None, None]","[None, None]",278,[],"['pytest.raises', 'FolderBasedBuilder']",2
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_generate_examples_drop_labels,test_generate_examples_drop_labels,function,13,63,38,631,10.02,1,1,"['data_files_with_labels_no_metadata', 'auto_text_file', 'drop_metadata', 'drop_labels', 'cache_dir']","[None, None, None, None, None]","[None, None, None, None, None]",331,[],"['DummyFolderBasedBuilder', 'autofolder._split_generators', 'bool', 'autofolder._generate_examples', 'all', 'example.keys', 'example.values']",7
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_generate_examples_drop_metadata,test_generate_examples_drop_metadata,function,23,58,46,764,13.17,2,2,"['file_with_metadata', 'drop_metadata', 'drop_labels', 'cache_dir']","[None, None, None, None]","[None, None, None, None]",358,[],"['DummyFolderBasedBuilder', 'autofolder._split_generators', 'bool', 'autofolder._generate_examples', 'expected_columns.add', 'len', 'example.keys']",7
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_generate_examples_duplicated_label_key,test_generate_examples_duplicated_label_key,function,20,87,55,1008,11.59,0,3,"['files_with_labels_and_duplicated_label_key_in_metadata', 'drop_metadata', 'drop_labels', 'cache_dir', 'caplog']","[None, None, None, None, None]","[None, None, None, None, None]",299,[],"['DummyFolderBasedBuilder', 'autofolder._split_generators', 'autofolder._generate_examples', 'any', 'ClassLabel', 'all', 'Value', 'Features']",8
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_inferring_labels_from_data_dirs,test_inferring_labels_from_data_dirs,function,9,27,25,429,15.89,0,0,"['data_files_with_labels_no_metadata', 'cache_dir']","[None, None]","[None, None]",268,[],"['DummyFolderBasedBuilder', 'autofolder._split_generators', 'Features', 'ClassLabel', 'autofolder._generate_examples', 'all']",6
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:test_streaming_patched,test_streaming_patched,function,6,9,8,172,19.11,0,0,[],[],[],290,[],"['DummyFolderBasedBuilder', 'importlib.import_module', 'hasattr']",3
repos/datasets/tests/packaged_modules/test_folder_based_builder.py:DummyFolderBasedBuilder,DummyFolderBasedBuilder,class,7,11,11,187,17.0,0,0,[],[],[],25,[],[],0
repos/datasets/tests/packaged_modules/test_imagefolder.py:cache_dir,cache_dir,function,2,3,3,43,14.33,0,0,['tmp_path'],[None],[None],16,[],['str'],1
repos/datasets/tests/packaged_modules/test_imagefolder.py:data_files_with_labels_no_metadata,data_files_with_labels_no_metadata,function,13,32,24,595,18.59,0,0,"['tmp_path', 'image_file']","[None, None]","[None, None]",21,[],"['data_dir.mkdir', 'subdir_class_0.mkdir', 'subdir_class_1.mkdir', 'shutil.copyfile', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix']",7
repos/datasets/tests/packaged_modules/test_imagefolder.py:data_files_with_one_split_and_metadata,data_files_with_one_split_and_metadata,function,19,62,48,948,15.29,0,1,"['request', 'tmp_path', 'image_file']","[None, None, None]","[None, None, None]",101,[],"['data_dir.mkdir', 'subdir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix', 'len']",10
repos/datasets/tests/packaged_modules/test_imagefolder.py:data_files_with_two_splits_and_metadata,data_files_with_two_splits_and_metadata,function,22,97,55,1368,14.1,0,2,"['request', 'tmp_path', 'image_file']","[None, None, None]","[None, None, None]",144,[],"['data_dir.mkdir', 'train_dir.mkdir', 'test_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'get_data_patterns', 'data_dir.as_posix', 'len']",11
repos/datasets/tests/packaged_modules/test_imagefolder.py:data_files_with_zip_archives,data_files_with_zip_archives,function,27,60,50,964,16.07,0,0,"['tmp_path', 'image_file']","[None, None]","[None, None]",205,[],"['data_dir.mkdir', 'archive_dir.mkdir', 'subdir.mkdir', 'shutil.copyfile', 'ImageOps.flip', 'textwrap.dedent', 'open', 'f.write', 'shutil.make_archive', 'shutil.rmtree', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'len']",13
repos/datasets/tests/packaged_modules/test_imagefolder.py:image_file_with_metadata,image_file_with_metadata,function,11,21,20,304,14.48,0,0,"['tmp_path', 'image_file']","[None, None]","[None, None]",69,[],"['shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'str']",5
repos/datasets/tests/packaged_modules/test_imagefolder.py:image_files_with_labels_and_duplicated_label_key_in_metadata,image_files_with_labels_and_duplicated_label_key_in_metadata,function,18,43,34,702,16.33,0,0,"['tmp_path', 'image_file']","[None, None]","[None, None]",42,[],"['data_dir.mkdir', 'subdir_class_0.mkdir', 'subdir_class_1.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'str']",8
repos/datasets/tests/packaged_modules/test_imagefolder.py:image_files_with_metadata_that_misses_one_image,image_files_with_metadata_that_misses_one_image,function,12,27,24,411,15.22,0,0,"['tmp_path', 'image_file']","[None, None]","[None, None]",84,[],"['shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'str']",5
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_data_files_with_metadata_and_archives,test_data_files_with_metadata_and_archives,function,13,62,44,686,11.06,1,0,"['streaming', 'cache_dir', 'data_files_with_zip_archives']","[None, None, None]","[None, None, None]",423,[],"['ImageFolder', 'imagefolder.download_and_prepare', 'imagefolder.as_streaming_dataset', 'imagefolder.as_dataset', 'data_files_with_zip_archives.items', 'len', 'list', 'all']",8
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_data_files_with_metadata_and_multiple_splits,test_data_files_with_metadata_and_multiple_splits,function,13,58,41,653,11.26,1,0,"['streaming', 'cache_dir', 'data_files_with_two_splits_and_metadata']","[None, None, None]","[None, None, None]",405,[],"['ImageFolder', 'imagefolder.download_and_prepare', 'imagefolder.as_streaming_dataset', 'imagefolder.as_dataset', 'data_files.items', 'len', 'list', 'all']",8
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_data_files_with_metadata_and_single_split,test_data_files_with_metadata_and_single_split,function,13,58,41,652,11.24,1,0,"['streaming', 'cache_dir', 'data_files_with_one_split_and_metadata']","[None, None, None]","[None, None, None]",387,[],"['ImageFolder', 'imagefolder.download_and_prepare', 'imagefolder.as_streaming_dataset', 'imagefolder.as_dataset', 'data_files.items', 'len', 'list', 'all']",8
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_data_files_with_with_metadata_in_different_formats,test_data_files_with_with_metadata_in_different_formats,function,22,53,40,868,16.38,0,0,"['cache_dir', 'tmp_path', 'image_file']","[None, None, None]","[None, None, None]",483,[],"['data_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'ImageFolder', 'pytest.raises', 'imagefolder.download_and_prepare', 'str']",11
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_data_files_with_wrong_image_file_name_column_in_metadata_file,test_data_files_with_wrong_image_file_name_column_in_metadata_file,function,20,47,42,684,14.55,0,0,"['cache_dir', 'tmp_path', 'image_file']","[None, None, None]","[None, None, None]",462,[],"['data_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'ImageFolder', 'pytest.raises', 'imagefolder.download_and_prepare', 'str']",11
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_data_files_with_wrong_metadata_file_name,test_data_files_with_wrong_metadata_file_name,function,17,38,36,639,16.82,0,0,"['cache_dir', 'tmp_path', 'image_file']","[None, None, None]","[None, None, None]",440,[],"['data_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent', 'open', 'f.write', 'DataFilesDict.from_patterns', 'data_dir.as_posix', 'ImageFolder', 'imagefolder.download_and_prepare', 'imagefolder.as_dataset']",10
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_generate_examples_drop_labels,test_generate_examples_drop_labels,function,13,62,36,601,9.69,2,1,"['data_files_with_labels_no_metadata', 'drop_metadata', 'drop_labels']","[None, None, None]","[None, None, None]",293,[],"['ImageFolder', 'imagefolder._split_generators', 'bool', 'imagefolder._generate_examples', 'all', 'example.keys', 'example.values']",7
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_generate_examples_drop_metadata,test_generate_examples_drop_metadata,function,23,58,46,761,13.12,2,2,"['image_file_with_metadata', 'drop_metadata', 'drop_labels']","[None, None, None]","[None, None, None]",317,[],"['ImageFolder', 'imagefolder._split_generators', 'bool', 'imagefolder._generate_examples', 'expected_columns.add', 'len', 'example.keys']",7
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_generate_examples_duplicated_label_key,test_generate_examples_duplicated_label_key,function,19,85,52,1062,12.49,0,3,"['image_files_with_labels_and_duplicated_label_key_in_metadata', 'drop_metadata', 'drop_labels', 'cache_dir', 'caplog']","[None, None, None, None, None]","[None, None, None, None, None]",259,[],"['ImageFolder', 'imagefolder.download_and_prepare', 'any', 'imagefolder.as_dataset', 'ClassLabel', 'all', 'Value', 'Features', 'Image']",9
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_generate_examples_with_labels,test_generate_examples_with_labels,function,10,22,20,455,20.68,0,0,"['data_files_with_labels_no_metadata', 'cache_dir']","[None, None]","[None, None]",244,[],"['ImageFolder', 'imagefolder.download_and_prepare', 'Features', 'Image', 'ClassLabel', 'list']",6
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_generate_examples_with_metadata_in_wrong_location,test_generate_examples_with_metadata_in_wrong_location,function,14,38,34,471,12.39,1,1,"['image_file', 'image_file_with_metadata', 'drop_metadata']","[None, None, None]","[None, None, None]",343,[],"['ImageFolder', 'imagefolder._split_generators', 'imagefolder._generate_examples', 'pytest.raises', 'list', 'all', 'example.keys', 'example.values']",8
repos/datasets/tests/packaged_modules/test_imagefolder.py:test_generate_examples_with_metadata_that_misses_one_image,test_generate_examples_with_metadata_that_misses_one_image,function,18,55,45,676,12.29,1,2,"['image_files_with_metadata_that_misses_one_image', 'drop_metadata']","[None, None]","[None, None]",360,[],"['Features', 'Image', 'Value', 'ImageFolder', 'imagefolder._split_generators', 'imagefolder._generate_examples', 'pytest.raises', 'list', 'all', 'example.keys', 'example.values']",11
repos/datasets/tests/packaged_modules/test_json.py:json_file_with_list_of_dicts,json_file_with_list_of_dicts,function,9,14,14,133,9.5,0,0,['tmp_path'],[None],[None],41,[],"['textwrap.dedent', 'open', 'f.write', 'str']",4
repos/datasets/tests/packaged_modules/test_json.py:json_file_with_list_of_dicts_field,json_file_with_list_of_dicts_field,function,9,14,14,139,9.93,0,0,['tmp_path'],[None],[None],75,[],"['textwrap.dedent', 'open', 'f.write', 'str']",4
repos/datasets/tests/packaged_modules/test_json.py:json_file_with_list_of_strings,json_file_with_list_of_strings,function,9,14,14,135,9.64,0,0,['tmp_path'],[None],[None],58,[],"['textwrap.dedent', 'open', 'f.write', 'str']",4
repos/datasets/tests/packaged_modules/test_json.py:jsonl_file,jsonl_file,function,9,14,14,115,8.21,0,0,['tmp_path'],[None],[None],11,[],"['textwrap.dedent', 'open', 'f.write', 'str']",4
repos/datasets/tests/packaged_modules/test_json.py:jsonl_file_utf16_encoded,jsonl_file_utf16_encoded,function,9,15,15,147,9.8,0,0,['tmp_path'],[None],[None],26,[],"['textwrap.dedent', 'open', 'f.write', 'str']",4
repos/datasets/tests/packaged_modules/test_json.py:test_json_generate_tables,test_json_generate_tables,function,9,35,32,359,10.26,0,1,"['file_fixture', 'config_kwargs', 'request']","[None, None, None]","[None, None, None]",105,[],"['Json', 'json._generate_tables', 'pa.concat_tables', 'pa_table.to_pydict']",4
repos/datasets/tests/packaged_modules/test_json.py:test_json_generate_tables_with_missing_features,test_json_generate_tables_with_missing_features,function,7,25,24,256,10.24,0,0,"['file_fixture', 'config_kwargs', 'request']","[None, None, None]","[None, None, None]",138,[],"['Json', 'json._generate_tables', 'pa.concat_tables', 'pa_table.to_pydict']",4
repos/datasets/tests/packaged_modules/test_spark.py:_get_expected_row_ids_and_row_dicts_for_partition_order,_get_expected_row_ids_and_row_dicts_for_partition_order,function,10,18,15,278,15.44,2,0,"['df', 'partition_order']","[None, None]","[None, None]",17,[],"['df.where', 'enumerate', 'expected_row_ids_and_row_dicts.append', 'row.asDict']",4
repos/datasets/tests/packaged_modules/test_spark.py:test_generate_iterable_examples,test_generate_iterable_examples,function,16,32,30,517,16.16,1,0,[],[],[],41,[],"['spark.range', '_generate_iterable_examples', '_get_expected_row_ids_and_row_dicts_for_partition_order', 'enumerate']",4
repos/datasets/tests/packaged_modules/test_spark.py:test_repartition_df_if_needed,test_repartition_df_if_needed,function,8,10,10,258,25.8,0,0,[],[],[],28,[],"['spark.range', 'Spark', 'spark_builder._repartition_df_if_needed']",3
repos/datasets/tests/packaged_modules/test_spark.py:test_repartition_df_if_needed_max_num_df_rows,test_repartition_df_if_needed_max_num_df_rows,function,8,10,10,258,25.8,0,0,[],[],[],111,[],"['spark.range', 'Spark', 'spark_builder._repartition_df_if_needed']",3
repos/datasets/tests/packaged_modules/test_spark.py:test_spark_examples_iterable,test_spark_examples_iterable,function,10,22,20,261,11.86,1,0,[],[],[],56,[],"['spark.range', 'SparkExamplesIterable', 'enumerate']",3
repos/datasets/tests/packaged_modules/test_spark.py:test_spark_examples_iterable_shard,test_spark_examples_iterable_shard,function,20,54,35,908,16.81,2,0,[],[],[],86,[],"['spark.range', 'SparkExamplesIterable', '_get_expected_row_ids_and_row_dicts_for_partition_order', 'enumerate']",4
repos/datasets/tests/packaged_modules/test_spark.py:test_spark_examples_iterable_shuffle,test_spark_examples_iterable_shuffle,function,22,37,34,616,16.65,1,0,[],[],[],68,[],"['spark.range', 'patch', 'x.reverse', '_get_expected_row_ids_and_row_dicts_for_partition_order', 'SparkExamplesIterable', 'enumerate']",6
repos/datasets/tests/packaged_modules/test_text.py:test_text_cast_image,test_text_cast_image,function,13,31,29,427,13.77,0,0,['text_file_with_image'],[None],[None],53,[],"['open', 'f.read', 'Text', 'features=Features', 'Image', 'text._generate_tables', 'pa.concat_tables', 'pa_table.to_pydict']",8
repos/datasets/tests/packaged_modules/test_text.py:test_text_linebreaks,test_text_linebreaks,function,10,22,20,337,15.32,0,0,"['text_file', 'keep_linebreaks']","[None, None]","[None, None]",43,[],"['open', 'f.read', 'Text', 'text._generate_tables', 'pa.concat_tables']",5
repos/datasets/tests/packaged_modules/test_text.py:test_text_sample_by,test_text_sample_by,function,13,38,30,509,13.39,0,1,"['sample_by', 'text_file']","[None, None]","[None, None]",65,[],"['open', 'f.read', 'expected_content.splitlines', 'expected_content.split', 'Text', 'text._generate_tables', 'pa.concat_tables']",7
repos/datasets/tests/packaged_modules/test_text.py:text_file,text_file,function,9,15,15,130,8.67,0,0,['tmp_path'],[None],[None],13,[],"['textwrap.dedent', 'open', 'f.write', 'str']",4
repos/datasets/tests/packaged_modules/test_text.py:text_file_with_image,text_file_with_image,function,7,12,12,123,10.25,0,0,"['tmp_path', 'image_file']","[None, None]","[None, None]",35,[],"['open', 'f.write', 'str']",3
repos/datasets/tests/packaged_modules/test_webdataset.py:audio_wds_file,audio_wds_file,function,13,33,29,358,10.85,1,0,"['tmp_path', 'audio_file']","[None, None]","[None, None]",28,[],"['json_file.open', 'f.write', 'tarfile.open', 'range', 'f.add', 'str']",6
repos/datasets/tests/packaged_modules/test_webdataset.py:bad_wds_file,bad_wds_file,function,10,25,21,252,10.08,0,0,"['tmp_path', 'image_file', 'text_file']","[None, None, None]","[None, None, None]",42,[],"['json_file.open', 'f.write', 'tarfile.open', 'f.add', 'str']",5
repos/datasets/tests/packaged_modules/test_webdataset.py:image_wds_file,image_wds_file,function,13,33,29,351,10.64,1,0,"['tmp_path', 'image_file']","[None, None]","[None, None]",14,[],"['json_file.open', 'f.write', 'tarfile.open', 'range', 'f.add', 'str']",6
repos/datasets/tests/packaged_modules/test_webdataset.py:test_audio_webdataset,test_audio_webdataset,function,22,73,56,1054,14.44,0,0,['audio_wds_file'],[None],[None],85,[],"['WebDataset', 'webdataset._split_generators', 'Features', 'Value', 'Audio', 'len', 'webdataset._generate_examples', 'zip', 'isinstance']",9
repos/datasets/tests/packaged_modules/test_webdataset.py:test_image_webdataset,test_image_webdataset,function,24,69,56,967,14.01,0,0,['image_wds_file'],[None],[None],54,[],"['WebDataset', 'webdataset._split_generators', 'Features', 'Value', 'Image', 'len', 'webdataset._generate_examples', 'zip', 'isinstance']",9
repos/datasets/tests/packaged_modules/test_webdataset.py:test_webdataset_errors_on_bad_file,test_webdataset_errors_on_bad_file,function,6,8,8,159,19.88,0,0,['bad_wds_file'],[None],[None],115,[],"['WebDataset', 'pytest.raises', 'webdataset._split_generators']",3
repos/datasets/tests/packaged_modules/test_webdataset.py:test_webdataset_with_features,test_webdataset_with_features,function,26,56,50,859,15.34,0,0,['image_wds_file'],[None],[None],123,[],"['Features', 'Value', 'Image', 'WebDataset', 'webdataset._split_generators', 'webdataset._generate_examples', 'next', 'isinstance']",8
repos/datasets/tests/test_arrow_dataset.py:_check_csv_dataset,_check_csv_dataset,function,8,22,17,250,11.36,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",3606,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_arrow_dataset.py:_check_generator_dataset,_check_generator_dataset,function,8,22,17,250,11.36,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",3870,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_arrow_dataset.py:_check_json_dataset,_check_json_dataset,function,8,22,17,250,11.36,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",3667,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_arrow_dataset.py:_check_parquet_dataset,_check_parquet_dataset,function,8,22,17,250,11.36,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",3736,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_arrow_dataset.py:_check_sql_dataset,_check_sql_dataset,function,8,22,17,250,11.36,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",3972,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_arrow_dataset.py:_check_text_dataset,_check_text_dataset,function,8,20,15,233,11.65,1,0,"['dataset', 'expected_features']","[None, None]","[None, None]",3796,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_arrow_dataset.py:assert_arrow_metadata_are_synced_with_dataset_features,assert_arrow_metadata_are_synced_with_dataset_features,function,7,34,19,464,13.65,0,0,['dataset'],[' Dataset'],[None],109,[],"['json.loads', 'DatasetInfo.from_dict', 'Features.from_arrow_schema', 'list']",4
repos/datasets/tests/test_arrow_dataset.py:data_generator,data_generator,function,4,37,28,203,5.49,1,0,[],[],[],3856,[],['_gen'],1
repos/datasets/tests/test_arrow_dataset.py:picklable_filter_function,picklable_filter_function,function,1,4,4,42,10.5,0,0,['x'],[None],[None],101,[],['int'],1
repos/datasets/tests/test_arrow_dataset.py:picklable_filter_function_with_rank,picklable_filter_function_with_rank,function,2,3,3,10,3.33,0,0,"['x', 'r']","[None, None]","[None, None]",105,[],[],0
repos/datasets/tests/test_arrow_dataset.py:picklable_map_function,picklable_map_function,function,1,3,3,46,15.33,0,0,['x'],[None],[None],85,[],['int'],1
repos/datasets/tests/test_arrow_dataset.py:picklable_map_function_with_indices,picklable_map_function_with_indices,function,1,3,3,14,4.67,0,0,"['x', 'i']","[None, None]","[None, None]",89,[],[],0
repos/datasets/tests/test_arrow_dataset.py:picklable_map_function_with_indices_and_rank,picklable_map_function_with_indices_and_rank,function,1,5,5,23,4.6,0,0,"['x', 'i', 'r']","[None, None, None]","[None, None, None]",97,[],[],0
repos/datasets/tests/test_arrow_dataset.py:picklable_map_function_with_rank,picklable_map_function_with_rank,function,1,3,3,16,5.33,0,0,"['x', 'r']","[None, None]","[None, None]",93,[],[],0
repos/datasets/tests/test_arrow_dataset.py:test_build_local_temp_path,test_build_local_temp_path,function,9,29,23,565,19.48,0,0,['uri_or_path'],[None],[None],4117,[],"['strip_protocol', 'Dataset._build_local_temp_path', 'Path', 'local_temp_path.startswith', 'local_temp_path.endswith']",5
repos/datasets/tests/test_arrow_dataset.py:test_cast_with_sliced_list,test_cast_with_sliced_list,function,9,34,31,372,10.94,0,0,[],[],[],3240,[],"['Features', 'Sequence', 'Dataset.from_dict', 'range', 'dataset.cast']",5
repos/datasets/tests/test_arrow_dataset.py:test_class_encode_column_with_none,test_class_encode_column_with_none,function,6,33,26,380,11.52,0,1,['include_nulls'],[None],[None],3250,[],"['Dataset.from_dict', 'dataset.class_encode_column', 'isinstance', 'set', 'dataset.unique']",5
repos/datasets/tests/test_arrow_dataset.py:test_concatenate_datasets,test_concatenate_datasets,function,16,59,45,601,10.19,0,2,"['dataset_type', 'axis', 'expected_shape', 'dataset_dict', 'arrow_path']","[None, None, None, None, None]","[None, None, None, None, None]",3289,[],"['InMemoryTable.from_pydict', 'MemoryMappedTable.from_file', 'concatenate_datasets', 'assert_arrow_metadata_are_synced_with_dataset_features']",4
repos/datasets/tests/test_arrow_dataset.py:test_concatenate_datasets_complex_features,test_concatenate_datasets_complex_features,function,11,38,33,456,12.0,0,1,['axis'],[None],[None],3325,[],"['Dataset.from_dict', 'list', 'features=Features', 'Value', 'ClassLabel', 'dataset1.rename_columns', 'Features', 'concatenate_datasets']",8
repos/datasets/tests/test_arrow_dataset.py:test_concatenate_datasets_duplicate_columns,test_concatenate_datasets_duplicate_columns,function,4,11,11,125,11.36,0,0,['dataset'],[None],[None],3386,[],"['pytest.raises', 'concatenate_datasets', 'str']",3
repos/datasets/tests/test_arrow_dataset.py:test_concatenate_datasets_new_columns,test_concatenate_datasets_new_columns,function,10,89,48,777,8.73,0,0,[],[],[],3306,[],"['Dataset.from_dict', 'concatenate_datasets', 'Features', 'Value']",4
repos/datasets/tests/test_arrow_dataset.py:test_concatenate_datasets_with_concatenation_tables,test_concatenate_datasets_with_concatenation_tables,function,23,104,65,1190,11.44,1,3,"['axis', 'expected_shape', 'other_dataset_type', 'dataset_dict', 'arrow_path']","[None, None, None, None, None]","[None, None, None, None, None]",3342,[],"['_create_concatenation_table', 'ConcatenationTable.from_blocks', 'InMemoryTable.from_pydict', 'MemoryMappedTable.from_file', 'reversed', 'concatenate_datasets']",6
repos/datasets/tests/test_arrow_dataset.py:test_dataset_add_column,test_dataset_add_column,function,38,126,90,1559,12.37,2,4,"['column', 'expected_dtype', 'in_memory', 'transform', 'dataset_dict', 'arrow_path']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",3489,[],"['Dataset', 'getattr', 'original_dataset.add_column', 'list', 'expected_features.items', 'len', 'dataset.reset_format', 'original_dataset.reset_format', 'all', 'set', 'assert_arrow_metadata_are_synced_with_dataset_features']",11
repos/datasets/tests/test_arrow_dataset.py:test_dataset_add_item,test_dataset_add_item,function,30,94,69,1175,12.5,1,3,"['item', 'in_memory', 'dataset_dict', 'arrow_path', 'transform']","[None, None, None, None, None]","[None, None, None, None, None]",3538,[],"['Dataset', 'getattr', 'dataset_to_test.add_item', 'sorted', 'expected_features.items', 'len', 'dataset.reset_format', 'dataset_to_test.reset_format', 'int', 'item.items']",10
repos/datasets/tests/test_arrow_dataset.py:test_dataset_add_item_introduce_feature_type,test_dataset_add_item_introduce_feature_type,function,7,23,17,234,10.17,0,0,[],[],[],3582,[],"['Dataset.from_dict', 'dataset.add_item', 'Features', 'Value']",4
repos/datasets/tests/test_arrow_dataset.py:test_dataset_add_item_new_columns,test_dataset_add_item_new_columns,function,7,71,39,615,8.66,0,0,[],[],[],3566,[],"['Dataset.from_dict', 'features=Features', 'Value', 'dataset.add_item', 'Features']",5
repos/datasets/tests/test_arrow_dataset.py:test_dataset_estimate_nbytes,test_dataset_estimate_nbytes,function,4,91,28,627,6.89,0,0,[],[],[],4653,[],"['Dataset.from_dict', 'ds._estimate_nbytes', 'concatenate_datasets']",3
repos/datasets/tests/test_arrow_dataset.py:test_dataset_filter_batched_indices,test_dataset_filter_batched_indices,function,4,24,21,144,6.0,0,0,[],[],[],3590,[],"['Dataset.from_dict', 'ds.filter', 'all']",3
repos/datasets/tests/test_arrow_dataset.py:test_dataset_format_with_unformatted_image,test_dataset_format_with_unformatted_image,function,7,29,27,289,9.97,0,0,[],[],[],4687,[],"['Dataset.from_dict', 'Features', 'Image', 'Sequence', 'ds.set_format', 'isinstance']",6
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_csv_features,test_dataset_from_csv_features,function,9,39,34,403,10.33,0,0,"['features', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",3634,[],"['features.copy', 'Features', 'Value', 'features.items', 'Dataset.from_csv', '_check_csv_dataset']",6
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_csv_keep_in_memory,test_dataset_from_csv_keep_in_memory,function,8,22,21,321,14.59,0,0,"['keep_in_memory', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",3616,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'Dataset.from_csv', '_check_csv_dataset']",4
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_csv_path_type,test_dataset_from_csv_path_type,function,9,25,22,284,11.36,0,1,"['path_type', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",3656,[],"['issubclass', 'Dataset.from_csv', '_check_csv_dataset']",3
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_csv_split,test_dataset_from_csv_split,function,8,23,21,254,11.04,0,1,"['split', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",3647,[],"['Dataset.from_csv', '_check_csv_dataset']",2
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_file,test_dataset_from_file,function,11,26,21,369,14.19,0,0,"['in_memory', 'dataset', 'arrow_file']","[None, None, None]","[None, None, None]",3597,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'Dataset.from_file']",3
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_generator_features,test_dataset_from_generator_features,function,9,39,35,422,10.82,0,0,"['features', 'data_generator', 'tmp_path']","[None, None, None]","[None, None, None]",3898,[],"['features.copy', 'Features', 'Value', 'features.items', 'Dataset.from_generator', '_check_generator_dataset']",6
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_generator_keep_in_memory,test_dataset_from_generator_keep_in_memory,function,8,22,22,340,15.45,0,0,"['keep_in_memory', 'data_generator', 'tmp_path']","[None, None, None]","[None, None, None]",3880,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'Dataset.from_generator', '_check_generator_dataset']",4
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_json_features,test_dataset_from_json_features,function,9,39,35,408,10.46,0,0,"['features', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",3695,[],"['features.copy', 'Features', 'Value', 'features.items', 'Dataset.from_json', '_check_json_dataset']",6
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_json_keep_in_memory,test_dataset_from_json_keep_in_memory,function,8,22,22,326,14.82,0,0,"['keep_in_memory', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",3677,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'Dataset.from_json', '_check_json_dataset']",4
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_json_path_type,test_dataset_from_json_path_type,function,9,25,23,291,11.64,0,1,"['path_type', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",3725,[],"['issubclass', 'Dataset.from_json', '_check_json_dataset']",3
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_json_split,test_dataset_from_json_split,function,8,23,22,259,11.26,0,1,"['split', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",3716,[],"['Dataset.from_json', '_check_json_dataset']",2
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_json_with_class_label_feature,test_dataset_from_json_with_class_label_feature,function,7,22,22,272,12.36,0,0,"['jsonl_str_path', 'tmp_path']","[None, None]","[None, None]",3706,[],"['Features', 'ClassLabel', 'Value', 'Dataset.from_json']",4
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_parquet_features,test_dataset_from_parquet_features,function,9,39,35,416,10.67,0,0,"['features', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",3764,[],"['features.copy', 'Features', 'Value', 'features.items', 'Dataset.from_parquet', '_check_parquet_dataset']",6
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_parquet_keep_in_memory,test_dataset_from_parquet_keep_in_memory,function,8,22,22,334,15.18,0,0,"['keep_in_memory', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",3746,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'Dataset.from_parquet', '_check_parquet_dataset']",4
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_parquet_path_type,test_dataset_from_parquet_path_type,function,9,25,23,301,12.04,0,1,"['path_type', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",3785,[],"['issubclass', 'Dataset.from_parquet', '_check_parquet_dataset']",3
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_parquet_split,test_dataset_from_parquet_split,function,8,23,22,267,11.61,0,1,"['split', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",3776,[],"['Dataset.from_parquet', '_check_parquet_dataset']",2
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_sql_con_type,test_dataset_from_sql_con_type,function,13,62,41,613,9.89,0,2,"['con_type', 'sqlite_path', 'tmp_path', 'set_sqlalchemy_silence_uber_warning', 'caplog']","[None, None, None, None, None]","[None, None, None, None, None]",3983,[],"['sqlalchemy.create_engine', 'caplog.at_level', 'logger=get_logger', 'Dataset.from_sql', '_check_sql_dataset']",5
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_sql_features,test_dataset_from_sql_features,function,9,41,37,430,10.49,0,0,"['features', 'sqlite_path', 'tmp_path', 'set_sqlalchemy_silence_uber_warning']","[None, None, None, None]","[None, None, None, None]",4021,[],"['features.copy', 'Features', 'Value', 'features.items', 'Dataset.from_sql', '_check_sql_dataset']",6
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_sql_keep_in_memory,test_dataset_from_sql_keep_in_memory,function,8,26,26,350,13.46,0,0,"['keep_in_memory', 'sqlite_path', 'tmp_path', 'set_sqlalchemy_silence_uber_warning']","[None, None, None, None]","[None, None, None, None]",4034,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'Dataset.from_sql', '_check_sql_dataset']",4
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_text_features,test_dataset_from_text_features,function,9,35,31,372,10.63,0,0,"['features', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",3823,[],"['features.copy', 'Features', 'Value', 'features.items', 'Dataset.from_text', '_check_text_dataset']",6
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_text_keep_in_memory,test_dataset_from_text_keep_in_memory,function,8,18,18,290,16.11,0,0,"['keep_in_memory', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",3806,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'Dataset.from_text', '_check_text_dataset']",4
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_text_path_type,test_dataset_from_text_path_type,function,9,21,19,254,12.1,0,1,"['path_type', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",3844,[],"['issubclass', 'Dataset.from_text', '_check_text_dataset']",3
repos/datasets/tests/test_arrow_dataset.py:test_dataset_from_text_split,test_dataset_from_text_split,function,8,19,18,223,11.74,0,1,"['split', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",3835,[],"['Dataset.from_text', '_check_text_dataset']",2
repos/datasets/tests/test_arrow_dataset.py:test_dataset_getitem_raises,test_dataset_getitem_raises,function,6,12,10,125,10.42,0,0,[],[],[],4823,[],"['Dataset.from_dict', 'pytest.raises', 'ds._getitem']",3
repos/datasets/tests/test_arrow_dataset.py:test_dataset_iter_batch,test_dataset_iter_batch,function,13,45,31,437,9.71,1,1,"['batch_size', 'drop_last_batch']","[None, None]","[None, None]",3459,[],"['Dataset.from_dict', 'list', 'enumerate', 'batches.append', 'all', 'len']",6
repos/datasets/tests/test_arrow_dataset.py:test_dataset_to_iterable_dataset,test_dataset_to_iterable_dataset,function,9,32,18,607,18.97,0,0,['dataset'],[' Dataset'],[None],4670,[],"['dataset.to_iterable_dataset', 'isinstance', 'list', 'pytest.raises', 'dataset.with_format']",5
repos/datasets/tests/test_arrow_dataset.py:test_dataset_to_json,test_dataset_to_json,function,10,20,16,297,14.85,0,0,"['dataset', 'tmp_path']","[None, None]","[None, None]",4044,[],"['dataset.to_json', 'file_path.is_file', 'file_path.stat', 'pd.read_json', 'list']",5
repos/datasets/tests/test_arrow_dataset.py:test_dataset_with_torch_dataloader,test_dataset_with_torch_dataloader,function,14,44,32,456,10.36,0,1,"['dataset', 'batch_size']","[None, None]","[None, None]",4701,[],"['DataLoader', 'patch.object', 'list', 'len', 'int', 'version.parse']",6
repos/datasets/tests/test_arrow_dataset.py:test_dummy_dataset_serialize_fs,test_dummy_dataset_serialize_fs,function,9,21,16,381,18.14,0,0,"['dataset', 'mockfs']","[None, None]","[None, None]",4096,[],"['dataset.save_to_disk', 'mockfs.isdir', 'mockfs.glob', 'dataset.load_from_disk', 'len', 'reloaded.to_dict', 'dataset.to_dict']",7
repos/datasets/tests/test_arrow_dataset.py:test_from_spark,test_from_spark,function,13,43,40,407,9.47,0,0,[],[],[],3912,[],"['spark.createDataFrame', 'Dataset.from_spark', 'isinstance']",3
repos/datasets/tests/test_arrow_dataset.py:test_from_spark_different_cache,test_from_spark_different_cache,function,13,34,26,468,13.76,0,0,[],[],[],3957,[],"['spark.createDataFrame', 'Dataset.from_spark', 'isinstance']",3
repos/datasets/tests/test_arrow_dataset.py:test_from_spark_features,test_from_spark_features,function,18,49,43,639,13.04,0,0,[],[],[],3933,[],"['np.arange', 'spark.createDataFrame', 'Features', 'Value', 'Image', 'Dataset.from_spark', 'isinstance', 'assert_arrow_metadata_are_synced_with_dataset_features']",8
repos/datasets/tests/test_arrow_dataset.py:test_interleave_datasets,test_interleave_datasets,function,14,49,40,460,9.39,1,0,[],[],[],3392,[],"['Dataset.from_dict', 'interleave_datasets', 'min', 'len', 'itertools.chain', 'isinstance']",6
repos/datasets/tests/test_arrow_dataset.py:test_interleave_datasets_oversampling_strategy,test_interleave_datasets_oversampling_strategy,function,13,58,44,526,9.07,0,0,[],[],[],3422,[],"['Dataset.from_dict', 'interleave_datasets', 'max', 'len', 'isinstance']",5
repos/datasets/tests/test_arrow_dataset.py:test_interleave_datasets_probabilities,test_interleave_datasets_probabilities,function,13,62,46,543,8.76,0,0,[],[],[],3405,[],"['Dataset.from_dict', 'interleave_datasets', 'isinstance', 'len']",4
repos/datasets/tests/test_arrow_dataset.py:test_interleave_datasets_probabilities_oversampling_strategy,test_interleave_datasets_probabilities_oversampling_strategy,function,13,77,52,642,8.34,0,0,[],[],[],3435,[],"['Dataset.from_dict', 'interleave_datasets', 'isinstance', 'len']",4
repos/datasets/tests/test_arrow_dataset.py:test_map_cases,test_map_cases,function,16,372,82,2143,5.76,0,13,['return_lazy_dict'],[None],[None],4717,[],"['f', 'dict', 'Dataset.from_dict', 'ds.map', 'ds.with_format', 'Features', 'Array2D']",7
repos/datasets/tests/test_arrow_dataset.py:test_pickle_dataset_after_transforming_the_table,test_pickle_dataset_after_transforming_the_table,function,20,38,34,438,11.53,0,1,"['in_memory', 'method_and_params', 'arrow_file']","[None, None, None]","[None, None, None]",4082,[],"['Dataset.from_file', 'getattr', 'pickle.dumps', 'pickle.loads']",4
repos/datasets/tests/test_arrow_dataset.py:test_sort_with_none,test_sort_with_none,function,5,31,21,321,10.35,0,1,['null_placement'],[None],[None],3262,[],"['Dataset.from_dict', 'dataset.sort']",2
repos/datasets/tests/test_arrow_dataset.py:test_update_metadata_with_features,test_update_metadata_with_features,function,17,32,25,507,15.84,0,0,['dataset_dict'],[None],[None],3271,[],"['Features.from_arrow_schema', 'features1.copy', 'ClassLabel', 'update_metadata_with_features', 'json.loads', 'Features.from_dict', 'Dataset']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest,BaseDatasetTest,class,416,7765,1881,112555,14.5,34,15,[],[],[],128,[],[],0
repos/datasets/tests/test_arrow_dataset.py:MiscellaneousDatasetTest,MiscellaneousDatasetTest,class,49,465,172,6977,15.0,2,0,[],[],[],3093,[],[],0
repos/datasets/tests/test_arrow_dataset.py:PickableMagicMock,PickableMagicMock,class,3,5,5,39,7.8,0,0,[],[],[],71,[],[],0
repos/datasets/tests/test_arrow_dataset.py:StratifiedTest,StratifiedTest,class,24,248,91,2015,8.12,2,1,[],[],[],4587,[],[],0
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest,TaskTemplatesTest,class,82,1084,319,13594,12.54,5,0,[],[],[],4132,[],[],0
repos/datasets/tests/test_arrow_dataset.py:Unpicklable,Unpicklable,class,7,15,13,132,8.8,1,0,[],[],[],76,[],[],0
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:_create_dummy_dataset,BaseDatasetTest:_create_dummy_dataset,method,16,126,91,962,7.63,1,2,"['self', 'in_memory', 'tmp_dir', 'multiple_columns', 'array_features', 'nested_features']","[None, ' bool', ' str', None, None, None]","[None, None, None, 'False', 'False', 'False']",133,[],"['int', 'Dataset.from_dict', 'Features', 'Array2D', 'Array3D', 'Sequence', 'range', 'Value', 'str', 'np.arange', 'self._to']",11
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:_to,BaseDatasetTest:_to,method,5,36,29,338,9.39,1,2,"['self', 'in_memory', 'tmp_dir', '*datasets']","[None, None, None, None]","[None, None, None, None]",164,[],"['dataset.map', 'enumerate', 'len']",3
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:inject_fixtures,BaseDatasetTest:inject_fixtures,method,2,2,2,19,9.5,0,0,"['self', 'caplog', 'set_sqlalchemy_silence_uber_warning']","[None, None, None]","[None, None, None]",130,[],[],0
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_cast,BaseDatasetTest:test_cast,method,15,38,33,635,16.71,0,0,"['self', 'in_memory']","[None, None]","[None, None]",540,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'Value', 'Features', 'list', 'dset.cast', 'self.assertEqual', 'self.assertIsInstance', 'self.assertNotEqual', 'assert_arrow_metadata_are_synced_with_dataset_features']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_class_encode_column,BaseDatasetTest:test_class_encode_column,method,11,89,55,1624,18.25,1,0,"['self', 'in_memory']","[None, None]","[None, None]",556,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'self.assertRaises', 'dset.class_encode_column', 'self.assertIsInstance', 'self.assertListEqual', 'self.assertNotEqual', 'assert_arrow_metadata_are_synced_with_dataset_features']",8
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_concatenate,BaseDatasetTest:test_concatenate,method,18,77,60,782,10.16,0,0,"['self', 'in_memory']","[None, None]","[None, None]",697,[],"['DatasetInfo', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'concatenate_datasets', 'self.assertTupleEqual', 'len', 'self.assertEqual', 'self.assertListEqual']",9
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_concatenate_formatted,BaseDatasetTest:test_concatenate_formatted,method,19,63,43,696,11.05,0,0,"['self', 'in_memory']","[None, None]","[None, None]",717,[],"['DatasetInfo', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset1.set_format', 'concatenate_datasets', 'self.assertEqual', 'dset2.set_format', 'dset3.set_format']",9
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_concatenate_pickle,BaseDatasetTest:test_concatenate_pickle,method,32,164,104,1715,10.46,0,2,"['self', 'in_memory']","[None, None]","[None, None]",825,[],"['DatasetInfo', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset1.select', 'dset2.select', 'dset3.select', 'dset3.rename_column', 'dset3.remove_columns', 'Unpicklable', 'concatenate_datasets', 'pickle.loads', 'self.assertTupleEqual', 'len', 'self.assertEqual', 'self.assertListEqual']",16
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_concatenate_with_indices,BaseDatasetTest:test_concatenate_with_indices,method,26,157,92,1809,11.52,0,0,"['self', 'in_memory']","[None, None]","[None, None]",738,[],"['DatasetInfo', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset1.select', 'dset2.select', 'concatenate_datasets', 'self.assertTupleEqual', 'len', 'self.assertEqual', 'self.assertListEqual', 'dset1.rename_columns', 'dset2.rename_columns', 'dset3.rename_columns', 'self.assertIsNone', 'self.assertTrue']",16
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_concatenate_with_indices_from_disk,BaseDatasetTest:test_concatenate_with_indices_from_disk,method,20,99,71,1048,10.59,0,0,"['self', 'in_memory']","[None, None]","[None, None]",794,[],"['DatasetInfo', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset1.select', 'dset2.select', 'dset3.select', 'concatenate_datasets', 'self.assertTupleEqual', 'len', 'self.assertEqual', 'self.assertListEqual']",12
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_dataset_getitem,BaseDatasetTest:test_dataset_getitem,method,6,52,35,1171,22.52,0,0,"['self', 'in_memory']","[None, None]","[None, None]",207,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'self.assertEqual', 'self.assertListEqual', 'dset.select']",5
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_dummy_dataset,BaseDatasetTest:test_dummy_dataset,method,7,81,51,1032,12.74,0,0,"['self', 'in_memory']","[None, None]","[None, None]",177,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'self.assertDictEqual', 'Features', 'Value', 'self.assertEqual', 'Array2D', 'Array3D', 'Sequence']",9
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_dummy_dataset_deepcopy,BaseDatasetTest:test_dummy_dataset_deepcopy,method,9,24,19,418,17.42,0,0,"['self', 'in_memory']","[None, None]","[None, None]",231,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'assert_arrow_memory_doesnt_increase', 'copy.deepcopy', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value']",8
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_dummy_dataset_load_from_disk,BaseDatasetTest:test_dummy_dataset_load_from_disk,method,9,26,21,456,17.54,0,0,"['self', 'in_memory']","[None, None]","[None, None]",364,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.save_to_disk', 'load_from_disk', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value']",8
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_dummy_dataset_pickle,BaseDatasetTest:test_dummy_dataset_pickle,method,16,82,40,1021,12.45,0,1,"['self', 'in_memory']","[None, None]","[None, None]",243,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'open', 'pickle.dump', 'pickle.load', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'range', 'Unpicklable']",11
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_dummy_dataset_serialize,BaseDatasetTest:test_dummy_dataset_serialize,method,17,215,67,3596,16.73,0,0,"['self', 'in_memory']","[None, None]","[None, None]",274,[],"['tempfile.TemporaryDirectory', 'set_current_working_directory_to_temp_dir', 'self._create_dummy_dataset', 'dset.save_to_disk', 'Dataset.load_from_disk', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'dset.to_dict', 'range', 'assert_arrow_memory_doesnt_increase', 'dset._estimate_nbytes']",13
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_export,BaseDatasetTest:test_export,method,23,81,68,1029,12.7,1,0,"['self', 'in_memory']","[None, None]","[None, None]",2226,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'formatted_dset.flatten', 'formatted_dset.set_format', 'formatted_dset.export', 'tf_dset.map', 'enumerate', 'self.assertEqual', 'len']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_filter,BaseDatasetTest:test_filter,method,10,64,38,1065,16.64,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1705,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.filter', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'self.assertNotEqual', 'dset.set_format']",9
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_filter_batched,BaseDatasetTest:test_filter_batched,method,7,46,29,335,7.28,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1753,[],"['tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.filter', 'self.assertListEqual']",5
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_filter_caching,BaseDatasetTest:test_filter_caching,method,9,51,37,626,12.27,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1815,[],"['tempfile.TemporaryDirectory', 'logger=get_logger', 'self._create_dummy_dataset', 'dset.filter', 'list', 'self.assertEqual', 'self.assertTrue']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_filter_empty,BaseDatasetTest:test_filter_empty,method,10,39,26,508,13.03,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1738,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'self.assertIsNone', 'dset.filter', 'self.assertEqual', 'self.assertIsNotNone']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_filter_fn_kwargs,BaseDatasetTest:test_filter_fn_kwargs,method,6,67,37,589,8.79,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1771,[],"['tempfile.TemporaryDirectory', 'Dataset.from_dict', 'range', 'self._to', 'dset.filter', 'len']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_filter_input_columns,BaseDatasetTest:test_filter_input_columns,method,7,36,32,393,10.92,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1762,[],"['tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.filter', 'self.assertListEqual']",5
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_filter_multiprocessing,BaseDatasetTest:test_filter_multiprocessing,method,9,72,44,1200,16.67,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1792,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.filter', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'self.assertNotEqual', 'min', 'len']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_filter_with_indices_mapping,BaseDatasetTest:test_filter_with_indices_mapping,method,7,36,24,291,8.08,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1729,[],"['tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.filter', 'self.assertListEqual']",5
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_flatten,BaseDatasetTest:test_flatten,method,14,176,77,2154,12.24,0,0,"['self', 'in_memory']","[None, None]","[None, None]",877,[],"['tempfile.TemporaryDirectory', 'Dataset.from_dict', 'features=Features', 'Sequence', 'Value', 'self._to', 'dset.flatten', 'self.assertListEqual', 'self.assertDictEqual', 'Features', 'self.assertNotEqual', 'assert_arrow_metadata_are_synced_with_dataset_features', 'Translation', 'TranslationVariableLanguages']",14
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_flatten_complex_image,BaseDatasetTest:test_flatten_complex_image,method,13,218,63,2689,12.33,0,0,"['self', 'in_memory']","[None, None]","[None, None]",937,[],"['tempfile.TemporaryDirectory', 'Dataset.from_dict', 'features=Features', 'Image', 'Value', 'self._to', 'dset.flatten', 'self.assertListEqual', 'self.assertDictEqual', 'Features', 'self.assertNotEqual', 'assert_arrow_metadata_are_synced_with_dataset_features', 'np.arange']",13
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_flatten_indices,BaseDatasetTest:test_flatten_indices,method,18,91,41,1558,17.12,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2637,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'self.assertIsNone', 'dset.select', 'self.assertEqual', 'self.assertIsNotNone', 'dset.set_format', 'dset.flatten_indices', 'len', 'self.assertNotEqual', 'dset.unique', 'assert_arrow_metadata_are_synced_with_dataset_features', 'dset.filter']",13
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_format_nested,BaseDatasetTest:test_format_nested,method,16,68,45,1237,18.19,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2800,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'np.ones', 'len', 'self.assertDictEqual', 'Features', 'Value', 'Sequence', 'dset.set_format', 'self.assertIsNotNone', 'self.assertIsInstance']",12
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_format_pandas,BaseDatasetTest:test_format_pandas,method,8,21,17,301,14.33,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2833,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.set_format', 'self.assertIsInstance']",4
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_format_polars,BaseDatasetTest:test_format_polars,method,8,21,17,301,14.33,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2844,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.set_format', 'self.assertIsInstance']",4
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_format_ragged_vectors,BaseDatasetTest:test_format_ragged_vectors,method,21,121,74,2015,16.65,2,0,"['self', 'in_memory']","[None, None]","[None, None]",2740,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'np.ones', 'self.assertIsNotNone', 'self.assertIsInstance', 'self.assertDictEqual', 'Features', 'Value', 'Sequence', 'dset.set_format', 'self.assertListEqual', 'self.assertTupleEqual']",13
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_format_vectors,BaseDatasetTest:test_format_vectors,method,20,108,67,1778,16.46,2,0,"['self', 'in_memory']","[None, None]","[None, None]",2687,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'np.ones', 'self.assertIsNotNone', 'self.assertIsInstance', 'self.assertDictEqual', 'Features', 'Value', 'Sequence', 'dset.set_format', 'self.assertTupleEqual']",12
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_keep_features_after_loading_from_cache,BaseDatasetTest:test_keep_features_after_loading_from_cache,method,17,62,50,777,12.53,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1899,[],"['Features', 'Sequence', 'invert_labels', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.map', 'self.assertGreater', 'self.assertEqual', 'self.assertDictEqual']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_keep_features_after_transform_specified,BaseDatasetTest:test_keep_features_after_transform_specified,method,12,52,42,619,11.9,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1827,[],"['Features', 'Sequence', 'invert_labels', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'assert_arrow_metadata_are_synced_with_dataset_features']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_keep_features_after_transform_to_file,BaseDatasetTest:test_keep_features_after_transform_to_file,method,14,55,45,628,11.42,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1863,[],"['Features', 'Sequence', 'invert_labels', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.map', 'Dataset.from_file', 'self.assertEqual', 'self.assertDictEqual']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_keep_features_after_transform_to_memory,BaseDatasetTest:test_keep_features_after_transform_to_memory,method,11,51,41,551,10.8,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1882,[],"['Features', 'Sequence', 'invert_labels', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.map', 'self.assertEqual', 'self.assertDictEqual']",9
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_keep_features_after_transform_unspecified,BaseDatasetTest:test_keep_features_after_transform_unspecified,method,12,51,41,601,11.78,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1845,[],"['Features', 'Sequence', 'invert_labels', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'assert_arrow_metadata_are_synced_with_dataset_features']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_keep_features_with_new_features,BaseDatasetTest:test_keep_features_with_new_features,method,14,65,50,809,12.45,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1922,[],"['Features', 'Sequence', 'invert_labels', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'assert_arrow_metadata_are_synced_with_dataset_features']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map,BaseDatasetTest:test_map,method,23,209,90,3346,16.01,0,1,"['self', 'in_memory']","[None, None]","[None, None]",1006,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'self.assertDictEqual', 'Features', 'Value', 'dset.map', 'int', 'self.assertEqual', 'self.assertListEqual', 'list', 'self.assertNotEqual', 'assert_arrow_metadata_are_synced_with_dataset_features', 'func', 'KeyboardInterrupt', 'self.assertRaises', 'self.assertFalse', 'self.assertTrue', 'dset.set_format', 'self.assertIsInstance']",19
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_batched,BaseDatasetTest:test_map_batched,method,17,187,67,3051,16.32,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1233,[],"['map_batched', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'assert_arrow_metadata_are_synced_with_dataset_features', 'dset.formatted_as', 'map_batched_with_indices', 'str', 'map_batched_modifying_inputs_inplace']",13
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_caching,BaseDatasetTest:test_map_caching,method,18,253,75,3443,13.61,0,1,"['self', 'in_memory']","[None, None]","[None, None]",1346,[],"['tempfile.TemporaryDirectory', 'logger=get_logger', 'self._create_dummy_dataset', 'patch', 'dset.map', 'list', 'self.assertEqual', 'int', 'self.assertTrue', 'self.assertNotIn', 'datasets.disable_caching', 'self.assertNotEqual', 'self.assertIn', 'Path', 'datasets.enable_caching']",15
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_crash_subprocess,BaseDatasetTest:test_map_crash_subprocess,method,12,43,38,361,8.4,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1686,[],"['do_crash', 'os.kill', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'pytest.raises', 'dset.map', 'str']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_fn_kwargs,BaseDatasetTest:test_map_fn_kwargs,method,7,70,38,654,9.34,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1325,[],"['tempfile.TemporaryDirectory', 'Dataset.from_dict', 'range', 'self._to', 'dset.map', 'list']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_input_columns,BaseDatasetTest:test_map_input_columns,method,6,40,35,431,10.78,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1610,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertEqual', 'Features', 'Value']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_jax,BaseDatasetTest:test_map_jax,method,13,37,31,419,11.32,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1563,[],"['func', 'jnp.asarray', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'Sequence', 'self.assertListEqual']",11
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_multiprocessing,BaseDatasetTest:test_map_multiprocessing,method,22,360,108,5590,15.53,1,1,"['self', 'in_memory']","[None, None]","[None, None]",1097,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'self.assertDictEqual', 'Features', 'Value', 'dset.map', 'self.assertEqual', 'self.assertIn', 'self.assertListEqual', 'list', 'self.assertNotEqual', 'assert_arrow_metadata_are_synced_with_dataset_features', 'dset.select', 'self.assertRaises', 'sorted', 'enumerate', 'int']",17
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_nested,BaseDatasetTest:test_map_nested,method,6,39,24,384,9.85,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1310,[],"['tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.map', 'self.assertEqual']",5
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_new_features,BaseDatasetTest:test_map_new_features,method,9,36,32,494,13.72,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1219,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'Features', 'Value', 'ClassLabel', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'assert_arrow_metadata_are_synced_with_dataset_features']",9
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_numpy,BaseDatasetTest:test_map_numpy,method,10,33,28,395,11.97,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1579,[],"['func', 'np.array', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'Sequence', 'self.assertListEqual']",11
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_remove_columns,BaseDatasetTest:test_map_remove_columns,method,15,96,58,1403,14.61,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1627,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertTrue', 'self.assertDictEqual', 'Features', 'Value', 'assert_arrow_metadata_are_synced_with_dataset_features', 'mapped_dset.with_format', 'mapped_dset.map', 'dset.select', 'self.assertEqual', 'empty_dset.map', 'self.assertListEqual']",14
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_return_example_as_dict_value,BaseDatasetTest:test_map_return_example_as_dict_value,method,6,36,24,296,8.22,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1318,[],"['tempfile.TemporaryDirectory', 'Dataset.from_dict', 'self._to', 'dset.map', 'self.assertEqual']",5
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_return_pa_table,BaseDatasetTest:test_map_return_pa_table,method,21,127,57,1851,14.57,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1432,[],"['func_return_single_row_pa_table', 'pa.table', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'func_return_single_row_pa_table_batched', 'len', 'func_return_multi_row_pa_table', 'self.assertRaises', 'func_return_table_from_expression', 'pds.dataset', 'pds.field', 'dset.with_format']",17
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_return_pd_dataframe,BaseDatasetTest:test_map_return_pd_dataframe,method,13,92,45,1286,13.98,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1489,[],"['func_return_single_row_pd_dataframe', 'pd.DataFrame', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'func_return_single_row_pd_dataframe_batched', 'len', 'func_return_multi_row_pd_dataframe', 'self.assertRaises']",13
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_stateful_callable,BaseDatasetTest:test_map_stateful_callable,method,12,40,31,442,11.05,0,1,"['self', 'in_memory']","[None, None]","[None, None]",1659,[],"['__init__', '__call__', 'len', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'ExampleCounter', 'dset.map', 'self.assertEqual']",8
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_tensor_batched,BaseDatasetTest:test_map_tensor_batched,method,12,37,32,447,12.08,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1594,[],"['func', 'torch.tensor', 'len', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'Sequence', 'self.assertListEqual']",12
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_tf,BaseDatasetTest:test_map_tf,method,13,37,31,419,11.32,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1546,[],"['func', 'tf.constant', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'Sequence', 'self.assertListEqual']",11
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_map_torch,BaseDatasetTest:test_map_torch,method,12,35,30,411,11.74,0,0,"['self', 'in_memory']","[None, None]","[None, None]",1529,[],"['func', 'torch.tensor', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.map', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'Sequence', 'self.assertListEqual']",11
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_pickle_after_many_transforms_on_disk,BaseDatasetTest:test_pickle_after_many_transforms_on_disk,method,16,92,65,1065,11.58,0,2,"['self', 'in_memory']","[None, None]","[None, None]",2061,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'self.assertEqual', 'dset.rename_column', 'self.assertListEqual', 'dset.select', 'dset.map', 'int', 'self.assertIn', 'Unpicklable', 'pickle.dumps', 'pickle.loads']",12
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_remove_columns,BaseDatasetTest:test_remove_columns,method,11,66,30,1232,18.67,0,0,"['self', 'in_memory']","[None, None]","[None, None]",592,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.remove_columns', 'self.assertEqual', 'self.assertListEqual', 'self.assertNotEqual', 'assert_arrow_metadata_are_synced_with_dataset_features']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_rename_column,BaseDatasetTest:test_rename_column,method,10,30,24,555,18.5,0,0,"['self', 'in_memory']","[None, None]","[None, None]",617,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.rename_column', 'self.assertEqual', 'self.assertListEqual', 'self.assertNotEqual', 'assert_arrow_metadata_are_synced_with_dataset_features']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_rename_columns,BaseDatasetTest:test_rename_columns,method,10,62,31,1030,16.61,0,0,"['self', 'in_memory']","[None, None]","[None, None]",628,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.rename_columns', 'self.assertEqual', 'self.assertListEqual', 'self.assertNotEqual', 'self.assertRaises']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_restore_saved_format,BaseDatasetTest:test_restore_saved_format,method,9,23,19,370,16.09,0,0,"['self', 'in_memory']","[None, None]","[None, None]",376,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.set_format', 'dset.save_to_disk', 'load_from_disk', 'self.assertEqual']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_select,BaseDatasetTest:test_select,method,32,223,95,3573,16.02,2,0,"['self', 'in_memory']","[None, None]","[None, None]",1948,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'list', 'len', 'dset.select', 'self.assertIsNotNone', 'self.assertTrue', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'self.assertNotEqual', 'self.assertIsNone', 'self.assertListEqual', 'range', 'self.assertRaises', 'self.assertFalse', 'iter', 'reversed', 'dset.set_format', 'enumerate']",21
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_select_columns,BaseDatasetTest:test_select_columns,method,11,107,37,2050,19.16,0,0,"['self', 'in_memory']","[None, None]","[None, None]",656,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.select_columns', 'self.assertEqual', 'self.assertListEqual', 'self.assertNotEqual', 'assert_arrow_metadata_are_synced_with_dataset_features']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_select_then_map,BaseDatasetTest:test_select_then_map,method,7,74,26,811,10.96,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2042,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.select', 'd1.map', 'int', 'self.assertEqual', 'd2.map']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_set_format_numpy_multiple_columns,BaseDatasetTest:test_set_format_numpy_multiple_columns,method,13,76,52,1489,19.59,0,0,"['self', 'in_memory']","[None, None]","[None, None]",386,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.set_format', 'self.assertEqual', 'self.assertIsInstance', 'self.assertListEqual', 'np.array', 'self.assertNotEqual', 'dset.reset_format', 'dset.formatted_as']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_set_format_pandas,BaseDatasetTest:test_set_format_pandas,method,7,28,24,492,17.57,0,0,"['self', 'in_memory']","[None, None]","[None, None]",474,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.set_format', 'self.assertEqual', 'self.assertIsInstance', 'self.assertListEqual']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_set_format_polars,BaseDatasetTest:test_set_format_polars,method,10,32,27,509,15.91,0,0,"['self', 'in_memory']","[None, None]","[None, None]",488,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.set_format', 'self.assertEqual', 'self.assertIsInstance', 'self.assertListEqual']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_set_format_tf,BaseDatasetTest:test_set_format_tf,method,10,40,32,735,18.38,0,0,"['self', 'in_memory']","[None, None]","[None, None]",454,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.set_format', 'self.assertEqual', 'self.assertIsInstance', 'self.assertListEqual']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_set_format_torch,BaseDatasetTest:test_set_format_torch,method,9,54,32,1108,20.52,0,0,"['self', 'in_memory']","[None, None]","[None, None]",424,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.set_format', 'self.assertEqual', 'self.assertIsInstance', 'self.assertListEqual']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_set_transform,BaseDatasetTest:test_set_transform,method,10,50,42,716,14.32,0,0,"['self', 'in_memory']","[None, None]","[None, None]",503,[],"['transform', 'batch.items', 'tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.set_transform', 'self.assertEqual', 'dset.set_format']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_shard,BaseDatasetTest:test_shard,method,16,93,70,1448,15.57,1,0,"['self', 'in_memory']","[None, None]","[None, None]",2602,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.select', 'self.assertEqual', 'dset.shard', 'len', 'self.assertDictEqual', 'Features', 'Value', 'self.assertNotEqual', 'str', 'range', 'dset.set_format']",13
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_shuffle,BaseDatasetTest:test_shuffle,method,13,83,44,1599,19.27,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2091,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.shuffle', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'self.assertNotEqual', 'self.assertListEqual', 'temp_seed']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_sort,BaseDatasetTest:test_sort,method,19,217,82,3007,13.86,4,0,"['self', 'in_memory']","[None, None]","[None, None]",2126,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.select', 'dset.shuffle', 'self.assertEqual', 'dset.sort', 'enumerate', 'self.assertDictEqual', 'Features', 'Value', 'self.assertNotEqual', 'len', 'dset.set_format', 'pytest.raises']",14
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_tf_dataset_conversion,BaseDatasetTest:test_tf_dataset_conversion,method,35,134,83,1651,12.32,1,1,"['self', 'in_memory']","[None, None]","[None, None]",2910,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.to_tf_dataset', 'next', 'self.assertEqual', 'dset.with_transform', 'transform_dset.to_tf_dataset']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_tf_dataset_options,BaseDatasetTest:test_tf_dataset_options,method,26,173,115,2659,15.37,1,1,"['self', 'in_memory']","[None, None]","[None, None]",3035,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.to_tf_dataset', 'next', 'self.assertEqual', 'tf_dataset.batch', 'tf_dataset_batch.unbatch', 'tf_dataset_batch.cardinality', 'zip', 'self.assertListEqual', 'batch_2.numpy', 'self.assertRaisesRegex']",12
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_tf_index_reshuffling,BaseDatasetTest:test_tf_index_reshuffling,method,16,61,41,777,12.74,4,0,"['self', 'in_memory']","[None, None]","[None, None]",2948,[],"['list', 'Dataset.from_dict', 'dset.to_tf_dataset', 'indices.append', 'np.concatenate', 'second_indices.append', 'self.assertFalse', 'self.assertEqual', 'len', 'enumerate']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_tf_label_renaming,BaseDatasetTest:test_tf_label_renaming,method,21,121,59,1708,14.12,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2975,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.rename_columns', 'new_dset.to_tf_dataset', 'next', 'self.assertTrue', 'self.assertEqual', 'isinstance']",8
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_to_csv,BaseDatasetTest:test_to_csv,method,15,93,33,1816,19.53,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2263,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.to_csv', 'self.assertTrue', 'self.assertEqual', 'pd.read_csv', 'self.assertListEqual', 'list', 'open', 'dset.select', 'len']",11
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_to_dict,BaseDatasetTest:test_to_dict,method,12,44,29,713,16.2,2,0,"['self', 'in_memory']","[None, None]","[None, None]",2315,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.to_dict', 'self.assertIsInstance', 'self.assertListEqual', 'sorted', 'self.assertLessEqual', 'len', 'dset.select', 'self.assertEqual']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_to_list,BaseDatasetTest:test_to_list,method,10,42,26,572,13.62,2,0,"['self', 'in_memory']","[None, None]","[None, None]",2337,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.to_list', 'self.assertIsInstance', 'self.assertListEqual', 'sorted', 'dset.select', 'self.assertEqual']",8
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_to_pandas,BaseDatasetTest:test_to_pandas,method,16,62,38,1029,16.6,4,0,"['self', 'in_memory']","[None, None]","[None, None]",2355,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.to_pandas', 'self.assertIsInstance', 'self.assertListEqual', 'sorted', 'self.assertLessEqual', 'self.assertEqual', 'len', 'dset.select']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_to_parquet,BaseDatasetTest:test_to_parquet,method,14,81,30,1620,20.0,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2417,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.to_parquet', 'self.assertTrue', 'pd.read_parquet', 'self.assertEqual', 'self.assertListEqual', 'list', 'open', 'dset.select', 'len']",11
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_to_polars,BaseDatasetTest:test_to_polars,method,17,64,40,1095,17.11,4,0,"['self', 'in_memory']","[None, None]","[None, None]",2386,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.to_polars', 'self.assertIsInstance', 'self.assertListEqual', 'sorted', 'self.assertLessEqual', 'self.assertEqual', 'len', 'dset.select']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_to_sql,BaseDatasetTest:test_to_sql,method,17,128,36,2150,16.8,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2470,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.to_sql', 'self.assertTrue', 'pd.read_sql', 'self.assertEqual', 'self.assertListEqual', 'list', 'contextlib.closing', 'dset.select', 'len']",11
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_train_test_split,BaseDatasetTest:test_train_test_split,method,19,152,61,3480,22.89,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2531,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.train_test_split', 'self.assertListEqual', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'self.assertNotEqual', 'dset.set_format']",10
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_transmit_format,BaseDatasetTest:test_transmit_format,method,7,31,21,667,21.52,0,0,"['self', 'in_memory']","[None, None]","[None, None]",524,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'self.assertEqual', 'transform', 'dset.set_format']",5
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_transmit_format_dict,BaseDatasetTest:test_transmit_format_dict,method,13,37,34,527,14.24,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2869,[],"['my_split_transform', 'DatasetDict', 'return_factory', 'tempfile.TemporaryDirectory', 'partial', 'dset.set_format', 'self.assertDictEqual']",7
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_transmit_format_single,BaseDatasetTest:test_transmit_format_single,method,10,34,30,471,13.85,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2854,[],"['my_single_transform', 'return_factory', 'tempfile.TemporaryDirectory', 'partial', 'dset.set_format', 'self.assertDictEqual']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_with_format,BaseDatasetTest:test_with_format,method,7,21,17,323,15.38,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2886,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.with_format', 'dset.set_format', 'self.assertDictEqual', 'self.assertEqual']",6
repos/datasets/tests/test_arrow_dataset.py:BaseDatasetTest:test_with_transform,BaseDatasetTest:test_with_transform,method,12,34,29,504,14.82,0,0,"['self', 'in_memory']","[None, None]","[None, None]",2897,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset', 'dset.with_transform', 'dset.set_transform', 'self.assertDictEqual', 'self.assertEqual', 'dset.reset_format', 'self.assertNotEqual']",8
repos/datasets/tests/test_arrow_dataset.py:MiscellaneousDatasetTest:test_concatenate_mixed_memory_and_disk,MiscellaneousDatasetTest:test_concatenate_mixed_memory_and_disk,method,15,54,43,645,11.94,0,0,['self'],[None],[None],3194,[],"['DatasetInfo', 'tempfile.TemporaryDirectory', 'Dataset.from_dict', 'concatenate_datasets', 'self.assertEqual', 'len', 'self.assertListEqual']",7
repos/datasets/tests/test_arrow_dataset.py:MiscellaneousDatasetTest:test_from_dict,MiscellaneousDatasetTest:test_from_dict,method,9,152,53,2208,14.53,0,0,['self'],[None],[None],3149,[],"['pa.array', 'Dataset.from_dict', 'self.assertListEqual', 'self.assertDictEqual', 'Features', 'Value', 'info=DatasetInfo', 'self.assertRaises']",8
repos/datasets/tests/test_arrow_dataset.py:MiscellaneousDatasetTest:test_from_pandas,MiscellaneousDatasetTest:test_from_pandas,method,10,83,38,1330,16.02,0,0,['self'],[None],[None],3094,[],"['Dataset.from_pandas', 'self.assertListEqual', 'self.assertDictEqual', 'Features', 'Value', 'info=DatasetInfo', 'Sequence', 'self.assertRaises']",8
repos/datasets/tests/test_arrow_dataset.py:MiscellaneousDatasetTest:test_from_polars,MiscellaneousDatasetTest:test_from_polars,method,13,87,41,1373,15.78,0,0,['self'],[None],[None],3121,[],"['pl.from_dict', 'Dataset.from_polars', 'self.assertListEqual', 'self.assertDictEqual', 'Features', 'Value', 'info=DatasetInfo', 'Sequence', 'self.assertRaises']",9
repos/datasets/tests/test_arrow_dataset.py:MiscellaneousDatasetTest:test_set_format_encode,MiscellaneousDatasetTest:test_set_format_encode,method,12,25,23,355,14.2,0,0,['self'],[None],[None],3210,[],"['BertTokenizer.from_pretrained', 'encode', 'tokenizer', 'Dataset.from_dict', 'dset.set_transform', 'self.assertEqual', 'str']",7
repos/datasets/tests/test_arrow_dataset.py:MiscellaneousDatasetTest:test_tf_string_encoding,MiscellaneousDatasetTest:test_tf_string_encoding,method,10,48,38,791,16.48,2,0,['self'],[None],[None],3223,[],"['Dataset.from_dict', 'dset.to_tf_dataset', 'zip', 'self.assertEqual', 'tf_dset_wo_batch.batch']",5
repos/datasets/tests/test_arrow_dataset.py:PickableMagicMock:__reduce__,PickableMagicMock:__reduce__,method,2,3,3,18,6.0,0,0,['self'],[None],[None],72,[],[],0
repos/datasets/tests/test_arrow_dataset.py:StratifiedTest:test_errors_train_test_split_stratify,StratifiedTest:test_errors_train_test_split_stratify,method,11,119,47,879,7.39,1,1,['self'],[None],[None],4588,[],"['np.array', 'range', 'Features', 'Value', 'ClassLabel', 'np.ones', 'Dataset.from_dict', 'self.assertRaises']",8
repos/datasets/tests/test_arrow_dataset.py:StratifiedTest:test_train_test_split_startify,StratifiedTest:test_train_test_split_startify,method,19,125,66,1046,8.37,1,0,['self'],[None],[None],4622,[],"['np.array', 'Features', 'Value', 'ClassLabel', 'np.ones', 'Dataset.from_dict', 'd1.train_test_split', 'np.asanyarray', 'np.ceil', 'len', 'npt.assert_array_equal', 'np.unique', 'np.bincount', 'float', 'npt.assert_array_almost_equal']",15
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_align_labels_with_mapping_classification,TaskTemplatesTest:test_align_labels_with_mapping_classification,method,19,75,58,738,9.84,2,0,['self'],[None],[None],4370,[],"['Features', 'Value', 'ClassLabel', 'label2id.items', 'Dataset.from_dict', 'dset.align_labels_with_mapping', 'self.assertListEqual']",7
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_align_labels_with_mapping_ner,TaskTemplatesTest:test_align_labels_with_mapping_ner,method,21,83,65,747,9.0,3,0,['self'],[None],[None],4388,[],"['Features', 'Value', 'Sequence', 'ClassLabel', 'label2id.items', 'Dataset.from_dict', 'dset.align_labels_with_mapping', 'self.assertListEqual']",8
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_concatenate_with_equal_task_templates,TaskTemplatesTest:test_concatenate_with_equal_task_templates,method,14,45,37,581,12.91,0,0,['self'],[None],[None],4425,[],"['TextClassification', 'DatasetInfo', 'features=Features', 'Value', 'ClassLabel', 'task_templates=TextClassification', 'Dataset.from_dict', 'concatenate_datasets', 'self.assertListEqual']",9
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_concatenate_with_mixed_task_templates_in_common,TaskTemplatesTest:test_concatenate_with_mixed_task_templates_in_common,method,18,113,65,1220,10.8,0,0,['self'],[None],[None],4440,[],"['TextClassification', 'QuestionAnsweringExtractive', 'DatasetInfo', 'features=Features', 'Value', 'ClassLabel', 'Sequence', 'Dataset.from_dict', 'concatenate_datasets', 'self.assertListEqual']",10
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_concatenate_with_no_mixed_task_templates_in_common,TaskTemplatesTest:test_concatenate_with_no_mixed_task_templates_in_common,method,23,155,72,1737,11.21,0,0,['self'],[None],[None],4492,[],"['TextClassification', 'QuestionAnsweringExtractive', 'DatasetInfo', 'features=Features', 'Value', 'ClassLabel', 'Sequence', 'Dataset.from_dict', 'concatenate_datasets', 'self.assertEqual']",10
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_concatenate_with_no_task_templates,TaskTemplatesTest:test_concatenate_with_no_task_templates,method,10,32,25,332,10.38,0,0,['self'],[None],[None],4416,[],"['DatasetInfo', 'Dataset.from_dict', 'concatenate_datasets', 'self.assertEqual']",4
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_task_automatic_speech_recognition,TaskTemplatesTest:test_task_automatic_speech_recognition,method,15,72,48,1047,14.54,0,0,['self'],[None],[None],4271,[],"['Features', 'Audio', 'Value', 'AutomaticSpeechRecognition', 'DatasetInfo', 'Dataset.from_dict', 'dset.prepare_for_task', 'self.assertSetEqual', 'set', 'self.assertDictEqual']",10
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_task_question_answering,TaskTemplatesTest:test_task_question_answering,method,16,114,63,1511,13.25,0,0,['self'],[None],[None],4183,[],"['Features', 'Value', 'Sequence', 'QuestionAnsweringExtractive', 'DatasetInfo', 'Dataset.from_dict', 'self.assertSetEqual', 'set', 'self.assertDictEqual', 'dset.prepare_for_task']",10
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_task_summarization,TaskTemplatesTest:test_task_summarization,method,14,81,54,950,11.73,0,0,['self'],[None],[None],4240,[],"['Features', 'Value', 'Summarization', 'DatasetInfo', 'Dataset.from_dict', 'dset.prepare_for_task', 'self.assertSetEqual', 'set', 'self.assertDictEqual']",9
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_task_templates_empty_after_preparation,TaskTemplatesTest:test_task_templates_empty_after_preparation,method,12,35,30,452,12.91,0,0,['self'],[None],[None],4353,[],"['Features', 'Value', 'ClassLabel', 'TextClassification', 'DatasetInfo', 'Dataset.from_dict', 'dset.prepare_for_task', 'self.assertIsNone']",8
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_task_text_classification,TaskTemplatesTest:test_task_text_classification,method,19,98,50,1607,16.4,0,0,['self'],[None],[None],4133,[],"['sorted', 'Features', 'Value', 'ClassLabel', 'TextClassification', 'DatasetInfo', 'Dataset.from_dict', 'self.assertSetEqual', 'set', 'self.assertDictEqual', 'dset.prepare_for_task']",11
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_task_text_classification_when_columns_removed,TaskTemplatesTest:test_task_text_classification_when_columns_removed,method,15,45,40,584,12.98,0,0,['self'],[None],[None],4566,[],"['sorted', 'Features', 'Value', 'ClassLabel', 'TextClassification', 'DatasetInfo', 'Dataset.from_dict', 'dset.map', 'self.assertDictEqual']",9
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_task_with_incompatible_templates,TaskTemplatesTest:test_task_with_incompatible_templates,method,13,38,34,502,13.21,0,0,['self'],[None],[None],4312,[],"['sorted', 'Features', 'Value', 'ClassLabel', 'TextClassification', 'DatasetInfo', 'Dataset.from_dict', 'self.assertRaises']",8
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_task_with_multiple_compatible_task_templates,TaskTemplatesTest:test_task_with_multiple_compatible_task_templates,method,14,54,42,641,11.87,0,0,['self'],[None],[None],4332,[],"['Features', 'Value', 'LanguageModeling', 'DatasetInfo', 'Dataset.from_dict', 'self.assertRaises', 'dset.prepare_for_task', 'self.assertEqual']",8
repos/datasets/tests/test_arrow_dataset.py:TaskTemplatesTest:test_task_with_no_template,TaskTemplatesTest:test_task_with_no_template,method,5,14,13,176,12.57,0,0,['self'],[None],[None],4306,[],"['Dataset.from_dict', 'self.assertRaises', 'dset.prepare_for_task']",3
repos/datasets/tests/test_arrow_dataset.py:Unpicklable:__getstate__,Unpicklable:__getstate__,method,1,2,2,27,13.5,0,0,['self'],[None],[None],81,[],['pickle.PicklingError'],1
repos/datasets/tests/test_arrow_dataset.py:Unpicklable:__init__,Unpicklable:__init__,method,4,8,7,53,6.62,1,0,"['self', '**kwargs']","[None, None]","[None, None]",77,[],"['kwargs.items', 'setattr']",2
repos/datasets/tests/test_arrow_reader.py:test_make_file_instructions,test_make_file_instructions,function,21,159,75,1681,10.57,1,3,"['split_name', 'instruction', 'shard_lengths', 'read_range']","[None, None, None, None]","[None, None, None, None]",200,[],"['SplitInfo', 'isinstance', 'sum', 'make_file_instructions', 'enumerate', 'file_instructions_list.append']",6
repos/datasets/tests/test_arrow_reader.py:test_make_file_instructions_basic,test_make_file_instructions_basic,function,10,83,42,1158,13.95,0,0,[],[],[],161,[],"['make_file_instructions', 'isinstance']",2
repos/datasets/tests/test_arrow_reader.py:test_make_file_instructions_raises,test_make_file_instructions_raises,function,8,17,17,246,14.47,0,0,"['name', 'expected_exception']","[None, None]","[None, None]",263,[],"['pytest.raises', 'make_file_instructions']",2
repos/datasets/tests/test_arrow_reader.py:test_read_files,test_read_files,function,13,35,32,500,14.29,0,0,"['in_memory', 'dataset', 'arrow_file']","[None, None, None]","[None, None, None]",126,[],"['ArrowReader', 'assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'reader.read_files', 'dataset_kwargs.keys', 'set', 'dict']",7
repos/datasets/tests/test_arrow_reader.py:test_read_instruction_spec,test_read_instruction_spec,function,7,41,29,947,23.1,0,0,[],[],[],138,[],"['ReadInstruction', 'ReadInstruction.from_spec']",2
repos/datasets/tests/test_arrow_reader.py:test_read_table,test_read_table,function,8,24,22,357,14.88,0,0,"['in_memory', 'dataset', 'arrow_file']","[None, None, None]","[None, None, None]",116,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'ArrowReader.read_table', 'set', 'dict']",5
repos/datasets/tests/test_arrow_reader.py:BaseReaderTest,BaseReaderTest,class,32,167,100,2592,15.52,1,0,[],[],[],39,[],[],0
repos/datasets/tests/test_arrow_reader.py:ReaderTest,ReaderTest,class,11,51,37,483,9.47,0,2,[],[],[],17,[],[],0
repos/datasets/tests/test_arrow_reader.py:BaseReaderTest:test_read,BaseReaderTest:test_read,method,28,85,72,1371,16.13,1,0,['self'],[None],[None],40,[],"['SplitInfo', 'SplitDict', 'split_dict.add', 'DatasetInfo', 'tempfile.TemporaryDirectory', 'ReaderTest', 'Dataset', 'self.assertEqual', 'ReadInstruction.from_spec', 'self.assertIsInstance']",10
repos/datasets/tests/test_arrow_reader.py:BaseReaderTest:test_read_files,BaseReaderTest:test_read_files,method,17,41,37,570,13.9,0,0,['self'],[None],[None],94,[],"['SplitInfo', 'SplitDict', 'split_dict.add', 'DatasetInfo', 'tempfile.TemporaryDirectory', 'ReaderTest', 'Dataset', 'self.assertEqual']",8
repos/datasets/tests/test_arrow_reader.py:BaseReaderTest:test_read_sharded,BaseReaderTest:test_read_sharded,method,17,35,35,575,16.43,0,0,['self'],[None],[None],76,[],"['SplitInfo', 'SplitDict', 'split_dict.add', 'DatasetInfo', 'tempfile.TemporaryDirectory', 'ReaderTest', 'Dataset', 'self.assertEqual']",8
repos/datasets/tests/test_arrow_reader.py:ReaderTest:_get_table_from_filename,ReaderTest:_get_table_from_filename,method,10,47,33,413,8.79,0,2,"['self', 'filename_skip_take', 'in_memory']","[None, None, None]","[None, None, 'False']",23,"['        """"""Returns a Dataset instance from given (filename, skip, take).""""""\n']","['open', 'len', 'pa_table.slice']",3
repos/datasets/tests/test_arrow_writer.py:_check_output,_check_output,function,9,25,24,277,11.08,0,0,"['output', 'expected_num_chunks']","[None, ' int']","[None, None]",79,[],"['pa.BufferReader', 'isinstance', 'pa.memory_map', 'f.read_all', 'len', 'pa_table.to_pydict']",6
repos/datasets/tests/test_arrow_writer.py:change_first_primitive_element_in_list,change_first_primitive_element_in_list,function,4,8,8,98,12.25,0,1,"['lst', 'value']","[None, None]","[None, None]",256,[],"['isinstance', 'change_first_primitive_element_in_list']",2
repos/datasets/tests/test_arrow_writer.py:get_base_dtype,get_base_dtype,function,4,7,6,92,13.14,0,1,['arr_type'],[None],[None],249,[],['get_base_dtype'],1
repos/datasets/tests/test_arrow_writer.py:test_always_nullable,test_always_nullable,function,8,15,14,271,18.07,0,0,[],[],[],362,[],"['pa.schema', 'pa.string', 'pa.BufferOutputStream', 'ArrowWriter', 'writer._build_writer']",5
repos/datasets/tests/test_arrow_writer.py:test_arrow_writer_closes_stream,test_arrow_writer_closes_stream,function,9,20,20,223,11.15,0,1,"['raise_exception', 'tmp_path']","[None, None]","[None, None]",297,[],"['str', 'ArrowWriter']",2
repos/datasets/tests/test_arrow_writer.py:test_arrow_writer_with_filesystem,test_arrow_writer_with_filesystem,function,12,33,26,394,11.94,0,0,['mockfs'],[None],[None],311,[],"['ArrowWriter', 'isinstance', 'type', 'writer.write', 'writer.finalize', 'mockfs.exists']",6
repos/datasets/tests/test_arrow_writer.py:test_duplicate_keys,test_duplicate_keys,function,9,26,22,332,12.77,0,0,['writer_batch_size'],[None],[None],149,[],"['pa.BufferOutputStream', 'ArrowWriter', 'pytest.raises', 'writer.write', 'writer.finalize']",5
repos/datasets/tests/test_arrow_writer.py:test_key_datatype,test_key_datatype,function,9,22,21,284,12.91,0,0,['writer_batch_size'],[None],[None],135,[],"['pa.BufferOutputStream', 'ArrowWriter', 'pytest.raises', 'writer.write', 'writer.finalize']",5
repos/datasets/tests/test_arrow_writer.py:test_optimized_int_type_for_typed_sequence,test_optimized_int_type_for_typed_sequence,function,3,6,6,122,20.33,0,0,"['sequence', 'optimized_int_type', 'expected_dtype']","[None, None, None]","[None, None, None]",265,[],"['pa.array', 'get_base_dtype']",2
repos/datasets/tests/test_arrow_writer.py:test_optimized_typed_sequence,test_optimized_typed_sequence,function,9,22,17,358,16.27,0,1,"['sequence', 'col', 'expected_dtype']","[None, None, None]","[None, None, None]",281,[],"['pa.array', 'get_base_dtype', 'copy.deepcopy', 'np.iinfo', 'change_first_primitive_element_in_list', 'pa.int64']",6
repos/datasets/tests/test_arrow_writer.py:test_parquet_writer_write,test_parquet_writer_write,function,15,37,31,381,10.3,0,0,[],[],[],324,[],"['pa.BufferOutputStream', 'ParquetWriter', 'writer.write', 'writer.finalize', 'pa.BufferReader', 'pq.read_table', 'pa_table.to_pydict']",7
repos/datasets/tests/test_arrow_writer.py:test_write,test_write,function,15,51,41,549,10.76,0,1,"['fields', 'writer_batch_size']","[None, None]","[None, None]",98,[],"['pa.BufferOutputStream', 'pa.schema', 'ArrowWriter', 'writer.write', 'writer.finalize', 'pa.string', 'pa.int64', '_check_output']",8
repos/datasets/tests/test_arrow_writer.py:test_write_batch,test_write_batch,function,15,53,43,571,10.77,0,1,"['fields', 'writer_batch_size']","[None, None]","[None, None]",184,[],"['pa.BufferOutputStream', 'pa.schema', 'ArrowWriter', 'writer.write_batch', 'writer.finalize', 'pa.string', 'pa.int64', '_check_output']",8
repos/datasets/tests/test_arrow_writer.py:test_write_file,test_write_file,function,15,39,33,432,11.08,0,0,[],[],[],236,[],"['tempfile.TemporaryDirectory', 'pa.string', 'pa.int64', 'ArrowWriter', 'writer.write_batch', 'writer.finalize', 'pa.schema', '_check_output']",8
repos/datasets/tests/test_arrow_writer.py:test_write_row,test_write_row,function,15,51,41,609,11.94,0,1,"['fields', 'writer_batch_size']","[None, None]","[None, None]",221,[],"['pa.BufferOutputStream', 'pa.schema', 'ArrowWriter', 'writer.write_row', 'writer.finalize', 'pa.string', 'pa.int64', '_check_output']",8
repos/datasets/tests/test_arrow_writer.py:test_write_table,test_write_table,function,15,49,41,549,11.2,0,1,"['fields', 'writer_batch_size']","[None, None]","[None, None]",203,[],"['pa.BufferOutputStream', 'pa.schema', 'ArrowWriter', 'writer.write_table', 'writer.finalize', 'pa.string', 'pa.int64', '_check_output']",8
repos/datasets/tests/test_arrow_writer.py:test_write_with_features,test_write_with_features,function,27,52,38,684,13.15,0,0,[],[],[],113,[],"['pa.BufferOutputStream', 'Features', 'ClassLabel', 'ArrowWriter', 'writer.write', 'writer.finalize', 'pa.BufferReader', 'f.read_all', 'Features.from_arrow_schema']",9
repos/datasets/tests/test_arrow_writer.py:test_write_with_keys,test_write_with_keys,function,10,38,34,423,11.13,0,0,['writer_batch_size'],[None],[None],164,[],"['pa.BufferOutputStream', 'ArrowWriter', 'writer.write', 'writer.finalize', '_check_output']",5
repos/datasets/tests/test_arrow_writer.py:test_writer_embed_local_files,test_writer_embed_local_files,function,25,51,45,674,13.22,0,1,"['tmp_path', 'embed_local_files']","[None, None]","[None, None]",339,[],"['str', 'pa.BufferOutputStream', 'ParquetWriter', 'features=Features', 'Image', 'writer.write', 'writer.finalize', 'pa.BufferReader', 'pq.read_table', 'pa_table.to_pydict', 'isinstance', 'open', 'f.read']",13
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest,TypedSequenceTest,class,28,138,69,2113,15.31,0,0,[],[],[],20,[],[],0
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_array_type_forbidden,TypedSequenceTest:test_array_type_forbidden,method,4,7,7,85,12.14,0,0,['self'],[None],[None],25,[],"['self.assertRaises', 'pa.array']",2
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_compatible_extension_type,TypedSequenceTest:test_compatible_extension_type,method,3,11,10,131,11.91,0,0,['self'],[None],[None],49,[],"['pa.array', 'type=Array2D', 'self.assertEqual', 'Array2DExtensionType']",4
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_compatible_type,TypedSequenceTest:test_compatible_type,method,3,7,7,94,13.43,0,0,['self'],[None],[None],33,[],"['pa.array', 'type=Value', 'self.assertEqual', 'pa.int32']",4
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_exhaustive_cast,TypedSequenceTest:test_exhaustive_cast,method,13,26,26,457,17.58,0,0,['self'],[None],[None],66,[],"['patch', 'pa.array', 'type=Image', 'self.assertIn', 'self.assertFalse']",5
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_incompatible_extension_type,TypedSequenceTest:test_incompatible_extension_type,method,4,9,9,124,13.78,0,0,['self'],[None],[None],53,[],"['self.assertRaises', 'pa.array', 'type=Array2D']",3
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_incompatible_type,TypedSequenceTest:test_incompatible_type,method,4,7,7,116,16.57,0,0,['self'],[None],[None],37,[],"['self.assertRaises', 'pa.array', 'type=Value']",3
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_no_type,TypedSequenceTest:test_no_type,method,3,6,6,74,12.33,0,0,['self'],[None],[None],21,[],"['pa.array', 'self.assertEqual', 'pa.int64']",3
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_try_compatible_extension_type,TypedSequenceTest:test_try_compatible_extension_type,method,3,11,10,135,12.27,0,0,['self'],[None],[None],57,[],"['pa.array', 'try_type=Array2D', 'self.assertEqual', 'Array2DExtensionType']",4
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_try_compatible_type,TypedSequenceTest:test_try_compatible_type,method,3,7,7,98,14.0,0,0,['self'],[None],[None],41,[],"['pa.array', 'try_type=Value', 'self.assertEqual', 'pa.int32']",4
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_try_incompatible_extension_type,TypedSequenceTest:test_try_incompatible_extension_type,method,3,8,8,113,14.12,0,0,['self'],[None],[None],61,[],"['pa.array', 'try_type=Array2D', 'self.assertEqual', 'pa.string']",4
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_try_incompatible_type,TypedSequenceTest:test_try_incompatible_type,method,3,6,6,105,17.5,0,0,['self'],[None],[None],45,[],"['pa.array', 'try_type=Value', 'self.assertEqual', 'pa.string']",4
repos/datasets/tests/test_arrow_writer.py:TypedSequenceTest:test_try_type_and_type_forbidden,TypedSequenceTest:test_try_type_and_type_forbidden,method,4,8,8,112,14.0,0,0,['self'],[None],[None],29,[],"['self.assertRaises', 'pa.array', 'try_type=Value', 'type=Value']",4
repos/datasets/tests/test_beam.py:get_test_dummy_examples,get_test_dummy_examples,function,1,11,11,78,7.09,0,0,[],[],[],53,[],['enumerate'],1
repos/datasets/tests/test_beam.py:get_test_nested_examples,get_test_nested_examples,function,1,12,12,80,6.67,0,0,[],[],[],57,[],['enumerate'],1
repos/datasets/tests/test_beam.py:BeamBuilderTest,BeamBuilderTest,class,27,180,72,3466,19.26,0,0,[],[],[],61,[],[],0
repos/datasets/tests/test_beam.py:DummyBeamDataset,DummyBeamDataset,class,12,31,27,414,13.35,0,0,[],[],[],13,[],[],0
repos/datasets/tests/test_beam.py:NestedBeamDataset,NestedBeamDataset,class,12,34,30,436,12.82,0,0,[],[],[],32,[],[],0
repos/datasets/tests/test_beam.py:BeamBuilderTest:test_download_and_prepare,BeamBuilderTest:test_download_and_prepare,method,12,45,36,923,20.51,0,0,['self'],[None],[None],63,[],"['len', 'tempfile.TemporaryDirectory', 'DummyBeamDataset', 'builder.download_and_prepare', 'self.assertTrue', 'self.assertDictEqual', 'datasets.Features', 'datasets.Value', 'builder.as_dataset', 'self.assertEqual', 'get_test_dummy_examples']",11
repos/datasets/tests/test_beam.py:BeamBuilderTest:test_download_and_prepare_sharded,BeamBuilderTest:test_download_and_prepare_sharded,method,21,67,45,1211,18.07,0,0,['self'],[None],[None],87,[],"['len', 'tempfile.TemporaryDirectory', 'DummyBeamDataset', 'patch', 'partial', 'builder.download_and_prepare', 'self.assertTrue', 'self.assertDictEqual', 'datasets.Features', 'datasets.Value', 'builder.as_dataset', 'self.assertEqual', 'self.assertListEqual', 'sorted']",14
repos/datasets/tests/test_beam.py:BeamBuilderTest:test_nested_features,BeamBuilderTest:test_nested_features,method,12,48,37,948,19.75,0,0,['self'],[None],[None],130,[],"['len', 'tempfile.TemporaryDirectory', 'NestedBeamDataset', 'builder.download_and_prepare', 'self.assertTrue', 'self.assertDictEqual', 'datasets.Features', 'datasets.Sequence', 'datasets.Value', 'builder.as_dataset', 'self.assertEqual', 'get_test_nested_examples']",12
repos/datasets/tests/test_beam.py:BeamBuilderTest:test_no_beam_options,BeamBuilderTest:test_no_beam_options,method,5,8,8,183,22.88,0,0,['self'],[None],[None],124,[],"['tempfile.TemporaryDirectory', 'DummyBeamDataset', 'self.assertRaises']",3
repos/datasets/tests/test_beam.py:DummyBeamDataset:_build_pcollection,DummyBeamDataset:_build_pcollection,method,7,11,11,76,6.91,0,0,"['self', 'pipeline', 'examples']","[None, None, None]","[None, None, None]",26,[],['beam.Create'],1
repos/datasets/tests/test_beam.py:DummyBeamDataset:_info,DummyBeamDataset:_info,method,3,6,6,117,19.5,0,0,['self'],[None],[None],16,[],"['datasets.DatasetInfo', 'datasets.Value']",2
repos/datasets/tests/test_beam.py:DummyBeamDataset:_split_generators,DummyBeamDataset:_split_generators,method,1,4,4,108,27.0,0,0,"['self', 'dl_manager', 'pipeline']","[None, None, None]","[None, None, None]",23,[],['get_test_dummy_examples'],1
repos/datasets/tests/test_beam.py:NestedBeamDataset:_build_pcollection,NestedBeamDataset:_build_pcollection,method,7,11,11,76,6.91,0,0,"['self', 'pipeline', 'examples']","[None, None, None]","[None, None, None]",26,[],['beam.Create'],1
repos/datasets/tests/test_beam.py:NestedBeamDataset:_info,NestedBeamDataset:_info,method,3,7,7,136,19.43,0,0,['self'],[None],[None],16,[],"['datasets.DatasetInfo', 'datasets.Sequence', 'datasets.Value']",3
repos/datasets/tests/test_beam.py:NestedBeamDataset:_split_generators,NestedBeamDataset:_split_generators,method,1,6,6,111,18.5,0,0,"['self', 'dl_manager', 'pipeline']","[None, None, None]","[None, None, None]",23,"['    """"""Dummy beam dataset.""""""\n']","['datasets.SplitGenerator', 'get_test_nested_examples']",2
repos/datasets/tests/test_builder.py:_run_concurrent_download_and_prepare,_run_concurrent_download_and_prepare,function,4,5,4,134,26.8,0,0,['tmp_dir'],[None],[None],244,[],"['DummyBuilder', 'builder.download_and_prepare']",2
repos/datasets/tests/test_builder.py:_run_test_builder_streaming_works_in_subprocesses,_run_test_builder_streaming_works_in_subprocesses,function,4,9,8,139,15.44,0,0,['builder'],[None],[None],973,[],"['check_streaming', 'builder.as_streaming_dataset', 'isinstance', 'len']",4
repos/datasets/tests/test_builder.py:check_streaming,check_streaming,function,5,8,7,146,18.25,0,0,['builder'],[None],[None],250,[],['importlib.import_module'],1
repos/datasets/tests/test_builder.py:test_arrow_based_builder_download_and_prepare_as_parquet,test_arrow_based_builder_download_and_prepare_as_parquet,function,8,21,19,351,16.71,0,0,['tmp_path'],[None],[None],1185,[],"['DummyArrowBasedBuilder', 'builder.download_and_prepare', 'pq.ParquetFile']",3
repos/datasets/tests/test_builder.py:test_arrow_based_builder_download_and_prepare_sharded,test_arrow_based_builder_download_and_prepare_sharded,function,12,46,39,716,15.57,0,0,['tmp_path'],[None],[None],1196,[],"['DummyArrowBasedBuilder', 'patch', 'builder.download_and_prepare', 'pq.ParquetFile', 'Path', 'len', 'sum']",7
repos/datasets/tests/test_builder.py:test_arrow_based_builder_download_and_prepare_with_ambiguous_shards,test_arrow_based_builder_download_and_prepare_with_ambiguous_shards,function,5,5,5,134,26.8,0,0,"['num_proc', 'expectation', 'tmp_path']","[None, None, None]","[None, None, None]",1268,[],"['DummyArrowBasedBuilderWithAmbiguousShards', 'builder.download_and_prepare']",2
repos/datasets/tests/test_builder.py:test_arrow_based_builder_download_and_prepare_with_max_shard_size,test_arrow_based_builder_download_and_prepare_with_max_shard_size,function,10,44,37,686,15.59,0,0,['tmp_path'],[None],[None],1220,[],"['DummyArrowBasedBuilder', 'builder.download_and_prepare', 'pq.ParquetFile', 'Path', 'len', 'sum']",6
repos/datasets/tests/test_builder.py:test_arrow_based_builder_download_and_prepare_with_num_proc,test_arrow_based_builder_download_and_prepare_with_num_proc,function,14,51,36,595,11.67,2,0,['tmp_path'],[None],[None],1243,[],"['DummyArrowBasedBuilderWithShards', 'builder.download_and_prepare', 'builder.as_dataset', 'len', 'ds.to_dict', 'range']",6
repos/datasets/tests/test_builder.py:test_arrow_based_download_and_prepare,test_arrow_based_download_and_prepare,function,7,26,22,432,16.62,0,0,['tmp_path'],[None],[None],839,[],"['DummyArrowBasedBuilder', 'builder.download_and_prepare', 'Features', 'Value']",4
repos/datasets/tests/test_builder.py:test_beam_based_as_dataset,test_beam_based_as_dataset,function,6,15,13,223,14.87,0,0,['tmp_path'],[None],[None],875,[],"['DummyBeamBasedBuilder', 'builder.download_and_prepare', 'builder.as_dataset', 'isinstance', 'len']",5
repos/datasets/tests/test_builder.py:test_beam_based_builder_as_streaming_dataset,test_beam_based_builder_as_streaming_dataset,function,6,6,6,148,24.67,0,0,['tmp_path'],[None],[None],966,[],"['DummyBeamBasedBuilder', 'check_streaming', 'pytest.raises', 'builder.as_streaming_dataset']",4
repos/datasets/tests/test_builder.py:test_beam_based_builder_download_and_prepare_as_parquet,test_beam_based_builder_download_and_prepare_as_parquet,function,8,22,20,377,17.14,0,0,['tmp_path'],[None],[None],1275,[],"['DummyBeamBasedBuilder', 'builder.download_and_prepare', 'pq.ParquetFile']",3
repos/datasets/tests/test_builder.py:test_beam_based_download_and_prepare,test_beam_based_download_and_prepare,function,7,27,23,458,16.96,0,0,['tmp_path'],[None],[None],857,[],"['DummyBeamBasedBuilder', 'builder.download_and_prepare', 'Features', 'Value']",4
repos/datasets/tests/test_builder.py:test_builder_as_dataset,test_builder_as_dataset,function,29,77,62,1138,14.78,2,1,"['split', 'expected_dataset_class', 'expected_dataset_length', 'in_memory', 'tmp_path']","[None, None, None, None, None]","[None, None, None, None, None]",893,[],"['str', 'DummyBuilder', 'os.makedirs', 'SplitDict', 'ArrowWriter', 'features=Features', 'Value', 'writer.write_batch', 'writer.finalize', 'assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'builder.as_dataset', 'isinstance', 'list', 'dataset.values', 'zip', 'len', 'Features']",18
repos/datasets/tests/test_builder.py:test_builder_as_streaming_dataset,test_builder_as_streaming_dataset,function,7,22,17,390,17.73,0,0,['tmp_path'],[None],[None],953,[],"['DummyGeneratorBasedBuilder', 'check_streaming', 'dummy_builder.as_streaming_dataset', 'isinstance', 'len']",5
repos/datasets/tests/test_builder.py:test_builder_config_version,test_builder_config_version,function,5,8,8,113,14.12,0,0,"['builder_class', 'kwargs', 'tmp_path']","[None, None, None]","[None, None, None]",1044,[],"['str', 'builder_class']",2
repos/datasets/tests/test_builder.py:test_builder_download_and_prepare_with_absolute_output_dir,test_builder_download_and_prepare_with_absolute_output_dir,function,7,17,13,386,22.71,0,0,['tmp_path'],[None],[None],1050,[],"['DummyGeneratorBasedBuilder', 'str', 'builder.download_and_prepare']",3
repos/datasets/tests/test_builder.py:test_builder_download_and_prepare_with_relative_output_dir,test_builder_download_and_prepare_with_relative_output_dir,function,8,19,15,467,24.58,0,0,[],[],[],1060,[],"['set_current_working_directory_to_temp_dir', 'DummyGeneratorBasedBuilder', 'builder.download_and_prepare', 'Path']",4
repos/datasets/tests/test_builder.py:test_builder_streaming_works_in_subprocess,test_builder_streaming_works_in_subprocess,function,6,7,7,174,24.86,0,0,['tmp_path'],[None],[None],980,[],"['DummyGeneratorBasedBuilder', 'Process', 'p.start', 'p.join']",4
repos/datasets/tests/test_builder.py:test_builder_with_filesystem_download_and_prepare,test_builder_with_filesystem_download_and_prepare,function,9,21,15,523,24.9,0,0,"['tmp_path', 'mockfs']","[None, None]","[None, None]",1071,[],"['DummyGeneratorBasedBuilder', 'builder.download_and_prepare', 'is_local_path', 'isinstance', 'type', 'mockfs.exists']",6
repos/datasets/tests/test_builder.py:test_builder_with_filesystem_download_and_prepare_reload,test_builder_with_filesystem_download_and_prepare_reload,function,9,18,17,436,24.22,0,0,"['tmp_path', 'mockfs', 'caplog']","[None, None, None]","[None, None, None]",1083,[],"['DummyGeneratorBasedBuilder', 'mockfs.makedirs', 'DatasetInfo', 'mockfs.touch', 'caplog.clear', 'caplog.at_level', 'logger=get_logger', 'builder.download_and_prepare']",8
repos/datasets/tests/test_builder.py:test_custom_writer_batch_size,test_custom_writer_batch_size,function,10,20,19,465,23.25,0,1,"['tmp_path', 'writer_batch_size', 'default_writer_batch_size', 'expected_chunks']","[None, None, None, None]","[None, None, None, None]",942,[],"['str', 'DummyGeneratorBasedBuilder', 'builder.download_and_prepare', 'builder.as_dataset', 'len']",5
repos/datasets/tests/test_builder.py:test_generator_based_builder_as_dataset,test_generator_based_builder_as_dataset,function,12,23,22,396,17.22,0,0,"['in_memory', 'tmp_path']","[None, None]","[None, None]",928,[],"['cache_dir.mkdir', 'str', 'DummyGeneratorBasedBuilder', 'builder.download_and_prepare', 'assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'builder.as_dataset']",7
repos/datasets/tests/test_builder.py:test_generator_based_builder_download_and_prepare_as_parquet,test_generator_based_builder_download_and_prepare_as_parquet,function,8,21,19,355,16.9,0,0,['tmp_path'],[None],[None],1094,[],"['DummyGeneratorBasedBuilder', 'builder.download_and_prepare', 'pq.ParquetFile']",3
repos/datasets/tests/test_builder.py:test_generator_based_builder_download_and_prepare_sharded,test_generator_based_builder_download_and_prepare_sharded,function,13,51,43,797,15.63,0,0,['tmp_path'],[None],[None],1105,[],"['DummyGeneratorBasedBuilder', 'patch', 'builder.download_and_prepare', 'pq.ParquetFile', 'Path', 'len', 'sum']",7
repos/datasets/tests/test_builder.py:test_generator_based_builder_download_and_prepare_with_ambiguous_shards,test_generator_based_builder_download_and_prepare_with_ambiguous_shards,function,5,5,5,138,27.6,0,0,"['num_proc', 'expectation', 'tmp_path']","[None, None, None]","[None, None, None]",1179,[],"['DummyGeneratorBasedBuilderWithAmbiguousShards', 'builder.download_and_prepare']",2
repos/datasets/tests/test_builder.py:test_generator_based_builder_download_and_prepare_with_max_shard_size,test_generator_based_builder_download_and_prepare_with_max_shard_size,function,11,49,41,767,15.65,0,0,['tmp_path'],[None],[None],1130,[],"['DummyGeneratorBasedBuilder', 'builder.download_and_prepare', 'pq.ParquetFile', 'Path', 'len', 'sum']",6
repos/datasets/tests/test_builder.py:test_generator_based_builder_download_and_prepare_with_num_proc,test_generator_based_builder_download_and_prepare_with_num_proc,function,14,51,36,599,11.75,2,0,['tmp_path'],[None],[None],1154,[],"['DummyGeneratorBasedBuilderWithShards', 'builder.download_and_prepare', 'builder.as_dataset', 'len', 'ds.to_dict', 'range']",6
repos/datasets/tests/test_builder.py:BuilderTest,BuilderTest,class,104,1203,342,22884,19.02,4,2,[],[],[],256,[],[],0
repos/datasets/tests/test_builder.py:CustomBuilderConfig,CustomBuilderConfig,class,7,15,15,184,12.27,0,0,[],[],[],1013,[],[],0
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilder,DummyArrowBasedBuilder,class,9,21,18,239,11.38,1,0,[],[],[],70,[],[],0
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilderWithAmbiguousShards,DummyArrowBasedBuilderWithAmbiguousShards,class,12,58,45,517,8.91,2,0,[],[],[],198,[],[],0
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilderWithShards,DummyArrowBasedBuilderWithShards,class,12,45,35,400,8.89,2,0,[],[],[],168,[],[],0
repos/datasets/tests/test_builder.py:DummyBeamBasedBuilder,DummyBeamBasedBuilder,class,13,38,31,383,10.08,1,0,[],[],[],82,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilder,DummyBuilder,class,15,32,28,544,17.0,0,0,[],[],[],42,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithBuilderConfigs,DummyBuilderWithBuilderConfigs,class,6,15,13,219,14.6,0,0,[],[],[],1000,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithCustomBuilderConfigs,DummyBuilderWithCustomBuilderConfigs,class,8,17,15,266,15.65,0,0,[],[],[],1021,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithDefaultConfig,DummyBuilderWithDefaultConfig,class,1,2,2,23,11.5,0,0,[],[],[],139,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithDownload,DummyBuilderWithDownload,class,9,41,29,466,11.37,0,2,[],[],[],143,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithManualDownload,DummyBuilderWithManualDownload,class,19,35,31,328,9.37,0,1,[],[],[],157,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithMultipleConfigs,DummyBuilderWithMultipleConfigs,class,1,5,5,107,21.4,0,0,[],[],[],132,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithVersion,DummyBuilderWithVersion,class,6,14,12,172,12.29,0,0,[],[],[],987,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilder,DummyGeneratorBasedBuilder,class,8,20,17,227,11.35,1,0,[],[],[],58,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderConfig,DummyGeneratorBasedBuilderConfig,class,6,12,12,126,10.5,0,0,[],[],[],111,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithAmbiguousShards,DummyGeneratorBasedBuilderWithAmbiguousShards,class,11,53,40,486,9.17,2,0,[],[],[],221,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithConfig,DummyGeneratorBasedBuilderWithConfig,class,10,23,20,313,13.61,1,0,[],[],[],118,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithIntegers,DummyGeneratorBasedBuilderWithIntegers,class,8,20,17,217,10.85,1,0,[],[],[],99,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithShards,DummyGeneratorBasedBuilderWithShards,class,11,40,30,369,9.22,2,0,[],[],[],183,[],[],0
repos/datasets/tests/test_builder.py:BuilderTest:test_as_dataset_with_post_process,BuilderTest:test_as_dataset_with_post_process,method,34,279,108,4912,17.61,2,0,['self'],[None],[None],348,[],"['_post_process', 'char_tokenize', 'list', 'dataset.map', '_post_processing_resources', 'tempfile.TemporaryDirectory', 'DummyBuilder', 'PostProcessedInfo', 'features=Features', 'Value', 'types.MethodType', 'os.makedirs', 'SplitDict', 'ArrowWriter', 'writer.write_batch', 'writer.finalize', 'builder.as_dataset', 'self.assertIsInstance', 'self.assertListEqual', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'self.assertGreater', 'dataset.select']",24
repos/datasets/tests/test_builder.py:BuilderTest:test_as_dataset_with_post_process_with_index,BuilderTest:test_as_dataset_with_post_process_with_index,method,34,141,95,2761,19.58,1,1,['self'],[None],[None],484,[],"['_post_process', 'dataset.load_faiss_index', 'dataset.add_faiss_index_from_external_arrays', 'dataset.save_faiss_index', '_post_processing_resources', 'tempfile.TemporaryDirectory', 'DummyBuilder', 'types.MethodType', 'os.makedirs', 'SplitDict', 'ArrowWriter', 'features=Features', 'Value', 'writer.write_batch', 'writer.finalize', 'builder.as_dataset', 'self.assertIsInstance', 'self.assertListEqual', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'self.assertGreater']",22
repos/datasets/tests/test_builder.py:BuilderTest:test_cache_dir_for_config_kwargs,BuilderTest:test_cache_dir_for_config_kwargs,method,9,62,21,1391,22.44,0,0,['self'],[None],[None],773,[],"['tempfile.TemporaryDirectory', 'DummyGeneratorBasedBuilderWithConfig', 'self.assertEqual', 'self.assertIn', 'self.assertNotEqual', 'DummyBuilderWithMultipleConfigs']",6
repos/datasets/tests/test_builder.py:BuilderTest:test_cache_dir_for_configured_builder,BuilderTest:test_cache_dir_for_configured_builder,method,10,27,21,539,19.96,0,0,['self'],[None],[None],824,[],"['tempfile.TemporaryDirectory', 'configure_builder_class', 'builder_cls', 'self.assertEqual', 'self.assertNotEqual']",5
repos/datasets/tests/test_builder.py:BuilderTest:test_cache_dir_for_data_dir,BuilderTest:test_cache_dir_for_data_dir,method,7,23,15,493,21.43,0,0,['self'],[None],[None],816,[],"['tempfile.TemporaryDirectory', 'DummyBuilderWithManualDownload', 'self.assertEqual', 'self.assertNotEqual']",4
repos/datasets/tests/test_builder.py:BuilderTest:test_cache_dir_for_data_files,BuilderTest:test_cache_dir_for_data_files,method,12,144,49,2970,20.62,0,0,['self'],[None],[None],707,[],"['tempfile.TemporaryDirectory', 'open', 'f.writelines', 'DummyGeneratorBasedBuilder', 'self.assertEqual', 'self.assertNotEqual']",6
repos/datasets/tests/test_builder.py:BuilderTest:test_cache_dir_for_features,BuilderTest:test_cache_dir_for_features,method,10,23,18,481,20.91,0,0,['self'],[None],[None],763,[],"['tempfile.TemporaryDirectory', 'Features', 'Value', 'DummyGeneratorBasedBuilderWithIntegers', 'self.assertEqual', 'self.assertNotEqual']",6
repos/datasets/tests/test_builder.py:BuilderTest:test_cache_dir_no_args,BuilderTest:test_cache_dir_no_args,method,7,14,14,282,20.14,0,0,['self'],[None],[None],701,[],"['tempfile.TemporaryDirectory', 'DummyGeneratorBasedBuilder', 'Path', 'self.assertTupleEqual']",4
repos/datasets/tests/test_builder.py:BuilderTest:test_concurrent_download_and_prepare,BuilderTest:test_concurrent_download_and_prepare,method,10,55,44,663,12.05,1,0,['self'],[None],[None],290,[],"['tempfile.TemporaryDirectory', 'Pool', 'pool.apply_async', 'range', 'self.assertTrue', 'self.assertDictEqual', 'Features', 'Value', 'self.assertEqual']",9
repos/datasets/tests/test_builder.py:BuilderTest:test_config_names,BuilderTest:test_config_names,method,8,36,27,677,18.81,0,0,['self'],[None],[None],798,[],"['tempfile.TemporaryDirectory', 'self.assertRaises', 'DummyBuilderWithMultipleConfigs', 'self.assertIn', 'str', 'self.assertEqual', 'DummyBuilderWithDefaultConfig']",7
repos/datasets/tests/test_builder.py:BuilderTest:test_download_and_prepare,BuilderTest:test_download_and_prepare,method,8,30,24,554,18.47,0,0,['self'],[None],[None],257,[],"['tempfile.TemporaryDirectory', 'DummyBuilder', 'builder.download_and_prepare', 'self.assertTrue', 'self.assertDictEqual', 'Features', 'Value', 'self.assertEqual']",8
repos/datasets/tests/test_builder.py:BuilderTest:test_download_and_prepare_checksum_computation,BuilderTest:test_download_and_prepare_checksum_computation,method,8,34,24,608,17.88,0,0,['self'],[None],[None],274,[],"['tempfile.TemporaryDirectory', 'DummyBuilder', 'builder_no_verification.download_and_prepare', 'self.assertTrue', 'all', 'builder_with_verification.download_and_prepare']",6
repos/datasets/tests/test_builder.py:BuilderTest:test_download_and_prepare_with_base_path,BuilderTest:test_download_and_prepare_with_base_path,method,11,45,31,902,20.04,0,0,['self'],[None],[None],319,[],"['tempfile.TemporaryDirectory', 'DummyBuilderWithDownload', 'self.assertRaises', 'builder.download_and_prepare', 'open', 'self.assertTrue']",6
repos/datasets/tests/test_builder.py:BuilderTest:test_download_and_prepare_with_post_process,BuilderTest:test_download_and_prepare_with_post_process,method,25,175,71,3240,18.51,0,1,['self'],[None],[None],557,[],"['_post_process', 'char_tokenize', 'list', 'dataset.map', '_post_processing_resources', 'tempfile.TemporaryDirectory', 'DummyBuilder', 'PostProcessedInfo', 'features=Features', 'Value', 'types.MethodType', 'builder.download_and_prepare', 'self.assertTrue', 'self.assertDictEqual', 'Features', 'self.assertEqual', 'dataset.select', 'self.assertIsNone', 'dataset.load_faiss_index', 'dataset.add_faiss_index_from_external_arrays', 'dataset.save_faiss_index']",21
repos/datasets/tests/test_builder.py:BuilderTest:test_error_download_and_prepare,BuilderTest:test_error_download_and_prepare,method,9,22,22,379,17.23,0,0,['self'],[None],[None],646,[],"['_prepare_split', 'ValueError', 'tempfile.TemporaryDirectory', 'DummyBuilder', 'types.MethodType', 'self.assertRaises']",6
repos/datasets/tests/test_builder.py:BuilderTest:test_generator_based_download_and_prepare,BuilderTest:test_generator_based_download_and_prepare,method,15,60,40,1296,21.6,0,0,['self'],[None],[None],660,[],"['tempfile.TemporaryDirectory', 'DummyGeneratorBasedBuilder', 'builder.download_and_prepare', 'self.assertTrue', 'self.assertDictEqual', 'Features', 'Value', 'self.assertEqual', 'patch', 'mock_arrow_writer.assert_called_once', 'self.assertFalse', 'mock_arrow_writer.reset_mock']",12
repos/datasets/tests/test_builder.py:CustomBuilderConfig:__init__,CustomBuilderConfig:__init__,method,6,9,9,116,12.89,0,0,"['self', 'date', 'language', 'version', '**kwargs']","[None, None, None, None, None]","[None, 'None', 'None', '""2.0.0""', None]",1014,[],['super'],1
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilder:_generate_tables,DummyArrowBasedBuilder:_generate_tables,method,4,9,9,53,5.89,1,0,['self'],[None],[None],77,[],"['range', 'pa.table']",2
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilder:_info,DummyArrowBasedBuilder:_info,method,2,3,3,62,20.67,0,0,['self'],[None],[None],43,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilder:_split_generators,DummyArrowBasedBuilder:_split_generators,method,1,2,2,40,20.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",46,[],[],0
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilderWithAmbiguousShards:_generate_tables,DummyArrowBasedBuilderWithAmbiguousShards:_generate_tables,method,7,24,21,125,5.21,2,0,"['self', 'filepaths', 'dummy_kwarg_with_different_length']","[None, None, None]","[None, None, None]",213,[],"['range', 'pa.table']",2
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilderWithAmbiguousShards:_info,DummyArrowBasedBuilderWithAmbiguousShards:_info,method,2,5,5,85,17.0,0,0,['self'],[None],[None],43,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilderWithAmbiguousShards:_split_generators,DummyArrowBasedBuilderWithAmbiguousShards:_split_generators,method,1,20,17,179,8.95,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",46,[],"['SplitGenerator', 'range']",2
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilderWithShards:_generate_tables,DummyArrowBasedBuilderWithShards:_generate_tables,method,7,24,21,125,5.21,2,0,"['self', 'filepaths']","[None, None]","[None, None]",175,[],"['range', 'pa.table']",2
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilderWithShards:_info,DummyArrowBasedBuilderWithShards:_info,method,2,5,5,85,17.0,0,0,['self'],[None],[None],43,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyArrowBasedBuilderWithShards:_split_generators,DummyArrowBasedBuilderWithShards:_split_generators,method,1,8,8,96,12.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",46,[],['range'],1
repos/datasets/tests/test_builder.py:DummyBeamBasedBuilder:_build_pcollection,DummyBeamBasedBuilder:_build_pcollection,method,9,25,23,186,7.44,1,0,"['self', 'pipeline']","[None, None]","[None, None]",89,[],"['_process', 'range', 'beam.Create', 'beam.FlatMap']",4
repos/datasets/tests/test_builder.py:DummyBeamBasedBuilder:_info,DummyBeamBasedBuilder:_info,method,2,3,3,62,20.67,0,0,['self'],[None],[None],43,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyBeamBasedBuilder:_split_generators,DummyBeamBasedBuilder:_split_generators,method,1,2,2,40,20.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",46,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilder:_info,DummyBuilder:_info,method,2,3,3,62,20.67,0,0,['self'],[None],[None],43,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyBuilder:_prepare_split,DummyBuilder:_prepare_split,method,10,18,17,335,18.61,0,0,"['self', 'split_generator', '**kwargs']","[None, None, None]","[None, None, None]",49,[],"['ArrowWriter', 'writer.write_batch', 'writer.finalize']",3
repos/datasets/tests/test_builder.py:DummyBuilder:_split_generators,DummyBuilder:_split_generators,method,1,2,2,40,20.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",46,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithBuilderConfigs:_generate_examples,DummyBuilderWithBuilderConfigs:_generate_examples,method,0,1,1,4,4.0,0,0,['self'],[None],[None],1009,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithBuilderConfigs:_info,DummyBuilderWithBuilderConfigs:_info,method,2,3,3,62,20.67,0,0,['self'],[None],[None],1003,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyBuilderWithBuilderConfigs:_split_generators,DummyBuilderWithBuilderConfigs:_split_generators,method,0,1,1,4,4.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",1006,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithCustomBuilderConfigs:_generate_examples,DummyBuilderWithCustomBuilderConfigs:_generate_examples,method,0,1,1,4,4.0,0,0,['self'],[None],[None],1031,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithCustomBuilderConfigs:_info,DummyBuilderWithCustomBuilderConfigs:_info,method,2,3,3,62,20.67,0,0,['self'],[None],[None],1025,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyBuilderWithCustomBuilderConfigs:_split_generators,DummyBuilderWithCustomBuilderConfigs:_split_generators,method,0,1,1,4,4.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",1028,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithDownload:__init__,DummyBuilderWithDownload:__init__,method,5,6,6,80,13.33,0,0,"['self', '*args', 'rel_path', 'abs_path', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",144,[],['super'],1
repos/datasets/tests/test_builder.py:DummyBuilderWithDownload:_split_generators,DummyBuilderWithDownload:_split_generators,method,4,26,17,284,10.92,0,2,"['self', 'dl_manager']","[None, None]","[None, None]",149,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithManualDownload:_split_generators,DummyBuilderWithManualDownload:_split_generators,method,3,10,10,154,15.4,0,1,"['self', 'dl_manager']","[None, None]","[None, None]",162,[],['FileNotFoundError'],1
repos/datasets/tests/test_builder.py:DummyBuilderWithManualDownload:manual_download_instructions,DummyBuilderWithManualDownload:manual_download_instructions,method,15,19,17,85,4.47,0,0,['self'],[None],[None],159,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithVersion:_generate_examples,DummyBuilderWithVersion:_generate_examples,method,0,1,1,4,4.0,0,0,['self'],[None],[None],996,[],[],0
repos/datasets/tests/test_builder.py:DummyBuilderWithVersion:_info,DummyBuilderWithVersion:_info,method,2,3,3,62,20.67,0,0,['self'],[None],[None],990,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyBuilderWithVersion:_split_generators,DummyBuilderWithVersion:_split_generators,method,0,1,1,4,4.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",993,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilder:_generate_examples,DummyGeneratorBasedBuilder:_generate_examples,method,3,8,8,39,4.88,1,0,['self'],[None],[None],65,[],['range'],1
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilder:_info,DummyGeneratorBasedBuilder:_info,method,2,3,3,62,20.67,0,0,['self'],[None],[None],43,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilder:_split_generators,DummyGeneratorBasedBuilder:_split_generators,method,1,2,2,40,20.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",46,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderConfig:__init__,DummyGeneratorBasedBuilderConfig:__init__,method,5,6,6,70,11.67,0,0,"['self', 'content', 'times', '*args', '**kwargs']","[None, None, None, None, None]","[None, '""foo""', '2', None, None]",112,[],['super'],1
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithAmbiguousShards:_generate_examples,DummyGeneratorBasedBuilderWithAmbiguousShards:_generate_examples,method,6,19,16,92,4.84,2,0,"['self', 'filepaths', 'dummy_kwarg_with_different_length']","[None, None, None]","[None, None, None]",236,[],['range'],1
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithAmbiguousShards:_info,DummyGeneratorBasedBuilderWithAmbiguousShards:_info,method,2,5,5,85,17.0,0,0,['self'],[None],[None],43,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithAmbiguousShards:_split_generators,DummyGeneratorBasedBuilderWithAmbiguousShards:_split_generators,method,1,20,17,179,8.95,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",46,[],"['SplitGenerator', 'range']",2
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithConfig:_generate_examples,DummyGeneratorBasedBuilderWithConfig:_generate_examples,method,3,9,9,71,7.89,1,0,['self'],[None],[None],127,[],['range'],1
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithConfig:_info,DummyGeneratorBasedBuilderWithConfig:_info,method,2,3,3,62,20.67,0,0,['self'],[None],[None],121,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithConfig:_split_generators,DummyGeneratorBasedBuilderWithConfig:_split_generators,method,1,2,2,40,20.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",124,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithIntegers:_generate_examples,DummyGeneratorBasedBuilderWithIntegers:_generate_examples,method,3,8,8,33,4.12,1,0,['self'],[None],[None],65,[],['range'],1
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithIntegers:_info,DummyGeneratorBasedBuilderWithIntegers:_info,method,2,3,3,58,19.33,0,0,['self'],[None],[None],43,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithIntegers:_split_generators,DummyGeneratorBasedBuilderWithIntegers:_split_generators,method,1,2,2,40,20.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",46,[],[],0
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithShards:_generate_examples,DummyGeneratorBasedBuilderWithShards:_generate_examples,method,6,19,16,92,4.84,2,0,"['self', 'filepaths']","[None, None]","[None, None]",190,[],['range'],1
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithShards:_info,DummyGeneratorBasedBuilderWithShards:_info,method,2,5,5,85,17.0,0,0,['self'],[None],[None],43,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_builder.py:DummyGeneratorBasedBuilderWithShards:_split_generators,DummyGeneratorBasedBuilderWithShards:_split_generators,method,1,8,8,96,12.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",46,[],['range'],1
repos/datasets/tests/test_data_files.py:complex_data_dir,complex_data_dir,function,8,110,39,871,7.92,0,0,['tmp_path'],[None],[None],45,[],"['data_dir.mkdir', 'open', 'f.write', 'str']",4
repos/datasets/tests/test_data_files.py:dummy_fs,dummy_fs,function,4,12,9,183,15.25,0,0,[],[],[],355,[],['mock_fs'],1
repos/datasets/tests/test_data_files.py:hub_dataset_repo_path,hub_dataset_repo_path,function,6,14,14,173,12.36,1,1,"['tmpfs', 'complex_data_dir']","[None, None]","[None, None]",117,[],"['Path', 'path.is_file', 'tmpfs.open', 'f.write']",4
repos/datasets/tests/test_data_files.py:hub_dataset_repo_patterns_results,hub_dataset_repo_patterns_results,function,3,16,14,157,9.81,1,0,"['hub_dataset_repo_path', 'complex_data_dir', 'pattern_results']","[None, None, None]","[None, None, None]",126,[],['Path'],1
repos/datasets/tests/test_data_files.py:is_relative_to,is_relative_to,function,3,8,7,70,8.75,0,0,"['path', '*other']","[None, None]","[None, None]",79,[],['path.relative_to'],1
repos/datasets/tests/test_data_files.py:mock_fs,mock_fs,function,21,93,66,805,8.66,1,4,['file_paths'],[' List[str]'],[None],513,"['    """"""\n', '    Set up a mock filesystem for fsspec containing the provided files\n', '\n', '    Example:\n', '\n', '    ```py\n', '    >>> DummyTestFS = mock_fs([""data/train.txt"", ""data.test.txt""])\n', '    >>> fs = DummyTestFS()\n', '    >>> assert fsspec.get_filesystem_class(""mock"").__name__ == ""DummyTestFS""\n', '    >>> assert type(fs).__name__ == ""DummyTestFS""\n', '    >>> print(fs.glob(""**""))\n', '    [""data"", ""data/train.txt"", ""data.test.txt""]\n', '    ```\n', '    """"""\n']","['range', 'DummyTestFS', 'ls', 'kwargs.pop', 'self._strip_protocol', 'self._ls_from_cache', 'self._parent', 'files.sort']",8
repos/datasets/tests/test_data_files.py:pattern_results,pattern_results,function,9,34,26,344,10.12,2,1,['complex_data_dir'],[None],[None],89,[],"['sorted', 'Path', 'fsspec.filesystem', 'any', 'is_relative_to']",5
repos/datasets/tests/test_data_files.py:test_DataFilesDict_from_patterns_in_dataset_repository,test_DataFilesDict_from_patterns_in_dataset_repository,function,6,22,20,363,16.5,0,0,"['hub_dataset_repo_path', 'hub_dataset_repo_patterns_results', 'pattern']","[None, None, None]","[None, None, None]",400,[],"['DataFilesDict.from_patterns', 'all', 'data_files.values', 'sorted', 'len']",5
repos/datasets/tests/test_data_files.py:test_DataFilesDict_from_patterns_in_dataset_repository_hashing,test_DataFilesDict_from_patterns_in_dataset_repository_hashing,function,8,29,18,566,19.52,0,0,['hub_dataset_repo_path'],[None],[None],444,[],"['DataFilesDict.from_patterns', 'Hasher.hash', 'DataFilesDict', 'patch']",4
repos/datasets/tests/test_data_files.py:test_DataFilesDict_from_patterns_in_dataset_repository_with_base_path,test_DataFilesDict_from_patterns_in_dataset_repository_with_base_path,function,8,21,21,259,12.33,0,1,"['hub_dataset_repo_path', 'pattern', 'size', 'base_path', 'split_name']","[None, None, None, None, None]","[None, None, None, None, None]",421,[],"['DataFilesDict.from_patterns', 'len', 'pytest.raises', 'resolve_pattern']",4
repos/datasets/tests/test_data_files.py:test_DataFilesDict_from_patterns_locally,test_DataFilesDict_from_patterns_locally,function,6,22,20,322,14.64,0,0,"['complex_data_dir', 'pattern_results', 'pattern']","[None, None, None]","[None, None, None]",434,[],"['DataFilesDict.from_patterns', 'all', 'data_files.values', 'sorted', 'len']",5
repos/datasets/tests/test_data_files.py:test_DataFilesDict_from_patterns_locally_or_remote_hashing,test_DataFilesDict_from_patterns_locally_or_remote_hashing,function,9,44,22,831,18.89,0,0,['text_file'],[None],[None],460,[],"['DataFilesDict.from_patterns', 'Hasher.hash', 'DataFilesDict', 'patch']",4
repos/datasets/tests/test_data_files.py:test_DataFilesList_from_patterns_in_dataset_repository_,test_DataFilesList_from_patterns_in_dataset_repository_,function,4,15,13,302,20.13,0,0,"['hub_dataset_repo_path', 'hub_dataset_repo_patterns_results', 'pattern']","[None, None, None]","[None, None, None]",370,[],"['DataFilesList.from_patterns', 'sorted', 'len']",3
repos/datasets/tests/test_data_files.py:test_DataFilesList_from_patterns_locally_with_extra_files,test_DataFilesList_from_patterns_locally_with_extra_files,function,2,11,10,202,18.36,0,0,"['complex_data_dir', 'text_file']","[None, None]","[None, None]",381,[],"['DataFilesList.from_patterns', 'text_file.as_posix', 'list', 'len']",4
repos/datasets/tests/test_data_files.py:test_DataFilesList_from_patterns_raises_FileNotFoundError,test_DataFilesList_from_patterns_raises_FileNotFoundError,function,3,4,4,114,28.5,0,0,['complex_data_dir'],[None],[None],387,[],"['pytest.raises', 'DataFilesList.from_patterns']",2
repos/datasets/tests/test_data_files.py:test_DataFilesPatternsDict,test_DataFilesPatternsDict,function,5,18,14,349,19.39,0,0,['text_file'],[None],[None],503,[],"['DataFilesPatternsDict', 'DataFilesPatternsList', 'data_files_patterns_dict.resolve', 'isinstance']",4
repos/datasets/tests/test_data_files.py:test_DataFilesPatternsList,test_DataFilesPatternsList,function,7,48,18,1076,22.42,0,0,['text_file'],[None],[None],482,[],"['DataFilesPatternsList', 'data_files_patterns.resolve', 'isinstance', 'pytest.raises']",4
repos/datasets/tests/test_data_files.py:test_fail_resolve_pattern_in_dataset_repository,test_fail_resolve_pattern_in_dataset_repository,function,3,4,4,88,22.0,0,0,['hub_dataset_repo_path'],[None],[None],312,[],"['pytest.raises', 'resolve_pattern']",2
repos/datasets/tests/test_data_files.py:test_fail_resolve_pattern_locally,test_fail_resolve_pattern_locally,function,3,4,4,85,21.25,0,0,['complex_data_dir'],[None],[None],256,[],"['pytest.raises', 'resolve_pattern']",2
repos/datasets/tests/test_data_files.py:test_get_data_files_patterns,test_get_data_files_patterns,function,24,106,62,994,9.38,7,2,"['base_path', 'data_file_per_split']","[None, None]","[None, None]",642,[],"['isinstance', 'data_file_per_split.items', 'sum', 'mock_fs', 'DummyTestFS', 'resolver', 'file_path[len', 'fs.glob', 'fs.isfile', '_get_data_files_patterns', 'list', 'patterns_per_split.items', 'fs._strip_protocol']",13
repos/datasets/tests/test_data_files.py:test_get_data_patterns_from_directory_with_the_word_data_twice,test_get_data_patterns_from_directory_with_the_word_data_twice,function,8,24,21,387,16.12,0,0,['tmp_path'],[None],[None],697,[],"['data_dir.mkdir', 'data_file.touch', 'get_data_patterns']",3
repos/datasets/tests/test_data_files.py:test_get_metadata_files_patterns,test_get_metadata_files_patterns,function,12,29,22,309,10.66,3,0,['metadata_files'],[None],[None],685,[],"['mock_fs', 'DummyTestFS', 'resolver', 'fs.glob', 'fs.isfile', '_get_metadata_files_patterns', 'sorted']",7
repos/datasets/tests/test_data_files.py:test_is_inside_unrequested_special_dir,test_is_inside_unrequested_special_dir,function,7,46,26,499,10.85,1,1,"['complex_data_dir', 'pattern_results']","[None, None]","[None, None]",136,[],"['pattern_results.items', 'str', '_is_inside_unrequested_special_dir', 'f']",4
repos/datasets/tests/test_data_files.py:test_is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir,test_is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir,function,8,66,27,760,11.52,1,1,"['complex_data_dir', 'pattern_results']","[None, None]","[None, None]",152,[],"['pattern_results.items', 'str', '_is_inside_unrequested_special_dir', 'f']",4
repos/datasets/tests/test_data_files.py:test_pattern_results_fixture,test_pattern_results_fixture,function,1,9,8,131,14.56,0,0,"['pattern_results', 'pattern']","[None, None]","[None, None]",173,[],"['len', 'all']",2
repos/datasets/tests/test_data_files.py:test_resolve_pattern_fs,test_resolve_pattern_fs,function,2,6,5,116,19.33,0,0,['dummy_fs'],[None],[None],364,[],['resolve_pattern'],1
repos/datasets/tests/test_data_files.py:test_resolve_pattern_in_dataset_repository,test_resolve_pattern_in_dataset_repository,function,4,16,15,243,15.19,0,0,"['hub_dataset_repo_path', 'pattern', 'hub_dataset_repo_patterns_results']","[None, None, None]","[None, None, None]",281,[],"['resolve_pattern', 'sorted', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_in_dataset_repository_hidden_base_path,test_resolve_pattern_in_dataset_repository_hidden_base_path,function,3,7,7,129,18.43,0,0,['tmpfs'],[None],[None],324,[],"['tmpfs.touch', 'resolve_pattern', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_in_dataset_repository_returns_hidden_dir_only_if_requested,test_resolve_pattern_in_dataset_repository_returns_hidden_dir_only_if_requested,function,4,16,11,352,22.0,0,0,['hub_dataset_repo_path'],[None],[None],330,[],"['pytest.raises', 'resolve_pattern', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_in_dataset_repository_returns_hidden_file_only_if_requested,test_resolve_pattern_in_dataset_repository_returns_hidden_file_only_if_requested,function,4,10,9,187,18.7,0,0,['hub_dataset_repo_path'],[None],[None],317,[],"['pytest.raises', 'resolve_pattern', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_in_dataset_repository_returns_special_dir_only_if_requested,test_resolve_pattern_in_dataset_repository_returns_special_dir_only_if_requested,function,4,16,11,352,22.0,0,0,['hub_dataset_repo_path'],[None],[None],339,[],"['pytest.raises', 'resolve_pattern', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_in_dataset_repository_special_base_path,test_resolve_pattern_in_dataset_repository_special_base_path,function,3,7,7,137,19.57,0,0,['tmpfs'],[None],[None],348,[],"['tmpfs.touch', 'resolve_pattern', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_in_dataset_repository_with_base_path,test_resolve_pattern_in_dataset_repository_with_base_path,function,7,20,19,228,11.4,0,1,"['hub_dataset_repo_path', 'pattern', 'size', 'base_path']","[None, None, None, None]","[None, None, None, None]",292,[],"['resolve_pattern', 'len', 'pytest.raises']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_in_dataset_repository_with_extensions,test_resolve_pattern_in_dataset_repository_with_extensions,function,5,18,15,284,15.78,0,1,"['hub_dataset_repo_path', 'pattern', 'size', 'extensions']","[None, None, None, None]","[None, None, None, None]",303,[],"['resolve_pattern', 'len', 'pytest.raises']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally,test_resolve_pattern_locally,function,4,16,15,202,12.62,0,0,"['complex_data_dir', 'pattern', 'pattern_results']","[None, None, None]","[None, None, None]",179,[],"['resolve_pattern', 'sorted', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_does_not_resolve_symbolic_links,test_resolve_pattern_locally_does_not_resolve_symbolic_links,function,3,14,13,277,19.79,0,0,"['tmp_path', 'complex_data_dir']","[None, None]","[None, None]",262,[],"['resolve_pattern', 'str', 'len', 'Path']",4
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_hidden_base_path,test_resolve_pattern_locally_hidden_base_path,function,5,13,13,194,14.92,0,0,['tmp_path'],[None],[None],212,[],"['hidden.mkdir', 'resolve_pattern', 'str', 'len']",4
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_returns_hidden_file_only_if_requested,test_resolve_pattern_locally_returns_hidden_file_only_if_requested,function,4,10,9,177,17.7,0,0,['complex_data_dir'],[None],[None],205,[],"['pytest.raises', 'resolve_pattern', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_returns_special_dir_only_if_requested,test_resolve_pattern_locally_returns_special_dir_only_if_requested,function,4,16,11,337,21.06,0,0,['complex_data_dir'],[None],[None],229,[],"['pytest.raises', 'resolve_pattern', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_sorted_files,test_resolve_pattern_locally_sorted_files,function,10,27,24,335,12.41,1,0,['tmp_path_factory'],[None],[None],269,[],"['str', 'open', 'resolve_pattern', 'sorted']",4
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_special_base_path,test_resolve_pattern_locally_special_base_path,function,5,13,13,205,15.77,0,0,['tmp_path'],[None],[None],238,[],"['special.mkdir', 'resolve_pattern', 'str', 'len']",4
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_with_absolute_path,test_resolve_pattern_locally_with_absolute_path,function,4,11,11,161,14.64,0,0,"['tmp_path', 'complex_data_dir']","[None, None]","[None, None]",193,[],"['resolve_pattern', 'str', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_with_dot_in_base_path,test_resolve_pattern_locally_with_dot_in_base_path,function,4,11,11,207,18.82,0,0,['complex_data_dir'],[None],[None],187,[],"['resolve_pattern', 'len']",2
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_with_double_dots,test_resolve_pattern_locally_with_double_dots,function,4,13,13,201,15.46,0,0,"['tmp_path', 'complex_data_dir']","[None, None]","[None, None]",199,[],"['resolve_pattern', 'str', 'len']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locally_with_extensions,test_resolve_pattern_locally_with_extensions,function,5,17,15,254,14.94,0,1,"['complex_data_dir', 'pattern', 'size', 'extensions']","[None, None, None, None]","[None, None, None, None]",247,[],"['resolve_pattern', 'len', 'pytest.raises']",3
repos/datasets/tests/test_data_files.py:test_resolve_pattern_locallyreturns_hidden_dir_only_if_requested,test_resolve_pattern_locallyreturns_hidden_dir_only_if_requested,function,4,16,11,337,21.06,0,0,['complex_data_dir'],[None],[None],220,[],"['pytest.raises', 'resolve_pattern', 'len']",3
repos/datasets/tests/test_data_files.py:TestDataFilesDict,TestDataFilesDict,class,5,18,18,232,12.89,0,0,[],[],[],392,[],[],0
repos/datasets/tests/test_data_files.py:TestDataFilesDict:test_key_order_after_copy,TestDataFilesDict:test_key_order_after_copy,method,4,16,16,196,12.25,0,0,['self'],[None],[None],393,[],"['DataFilesDict', 'copy.deepcopy', 'list']",3
repos/datasets/tests/test_dataset_dict.py:_check_csv_datasetdict,_check_csv_datasetdict,function,12,28,21,305,10.89,2,0,"['dataset_dict', 'expected_features', 'splits', '']","[None, None, None, None]","[None, None, '(""train""', None]",604,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_dataset_dict.py:_check_json_datasetdict,_check_json_datasetdict,function,12,28,21,305,10.89,2,0,"['dataset_dict', 'expected_features', 'splits', '']","[None, None, None, None]","[None, None, '(""train""', None]",660,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_dataset_dict.py:_check_parquet_datasetdict,_check_parquet_datasetdict,function,12,28,21,305,10.89,2,0,"['dataset_dict', 'expected_features', 'splits', '']","[None, None, None, None]","[None, None, '(""train""', None]",715,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_dataset_dict.py:_check_text_datasetdict,_check_text_datasetdict,function,12,26,19,288,11.08,2,0,"['dataset_dict', 'expected_features', 'splits', '']","[None, None, None, None]","[None, None, '(""train""', None]",770,[],"['isinstance', 'expected_features.items']",2
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_csv_features,test_datasetdict_from_csv_features,function,9,40,35,421,10.53,0,0,"['features', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",634,[],"['features.copy', 'Features', 'Value', 'features.items', 'DatasetDict.from_csv', '_check_csv_datasetdict']",6
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_csv_keep_in_memory,test_datasetdict_from_csv_keep_in_memory,function,8,23,22,339,14.74,0,0,"['keep_in_memory', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",616,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'DatasetDict.from_csv', '_check_csv_datasetdict']",4
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_csv_split,test_datasetdict_from_csv_split,function,10,36,31,377,10.47,0,1,"['split', 'csv_path', 'tmp_path']","[None, None, None]","[None, None, None]",647,[],"['DatasetDict.from_csv', '_check_csv_datasetdict', 'splits=list', 'all', 'path.keys']",5
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_json_features,test_datasetdict_from_json_features,function,9,40,36,426,10.65,0,0,"['features', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",690,[],"['features.copy', 'Features', 'Value', 'features.items', 'DatasetDict.from_json', '_check_json_datasetdict']",6
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_json_keep_in_memory,test_datasetdict_from_json_keep_in_memory,function,8,23,23,344,14.96,0,0,"['keep_in_memory', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",672,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'DatasetDict.from_json', '_check_json_datasetdict']",4
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_json_splits,test_datasetdict_from_json_splits,function,10,36,32,386,10.72,0,1,"['split', 'jsonl_path', 'tmp_path']","[None, None, None]","[None, None, None]",702,[],"['DatasetDict.from_json', '_check_json_datasetdict', 'splits=list', 'all', 'path.keys']",5
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_parquet_features,test_datasetdict_from_parquet_features,function,9,40,36,434,10.85,0,0,"['features', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",745,[],"['features.copy', 'Features', 'Value', 'features.items', 'DatasetDict.from_parquet', '_check_parquet_datasetdict']",6
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_parquet_keep_in_memory,test_datasetdict_from_parquet_keep_in_memory,function,8,23,23,352,15.3,0,0,"['keep_in_memory', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",727,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'DatasetDict.from_parquet', '_check_parquet_datasetdict']",4
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_parquet_split,test_datasetdict_from_parquet_split,function,10,36,32,398,11.06,0,1,"['split', 'parquet_path', 'tmp_path']","[None, None, None]","[None, None, None]",757,[],"['DatasetDict.from_parquet', '_check_parquet_datasetdict', 'splits=list', 'all', 'path.keys']",5
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_text_features,test_datasetdict_from_text_features,function,9,36,32,390,10.83,0,0,"['features', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",799,[],"['features.copy', 'Features', 'Value', 'features.items', 'DatasetDict.from_text', '_check_text_datasetdict']",6
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_text_keep_in_memory,test_datasetdict_from_text_keep_in_memory,function,8,19,19,308,16.21,0,0,"['keep_in_memory', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",782,[],"['assert_arrow_memory_increases', 'assert_arrow_memory_doesnt_increase', 'DatasetDict.from_text', '_check_text_datasetdict']",4
repos/datasets/tests/test_dataset_dict.py:test_datasetdict_from_text_split,test_datasetdict_from_text_split,function,10,32,28,348,10.88,0,1,"['split', 'text_path', 'tmp_path']","[None, None, None]","[None, None, None]",811,[],"['DatasetDict.from_text', '_check_text_datasetdict', 'splits=list', 'all', 'path.keys']",5
repos/datasets/tests/test_dataset_dict.py:test_dummy_datasetdict_serialize_fs,test_dummy_datasetdict_serialize_fs,function,13,36,30,559,15.53,1,0,['mockfs'],[None],[None],586,[],"['DatasetDict', 'Dataset.from_dict', 'range', 'dataset_dict.save_to_disk', 'mockfs.isdir', 'mockfs.glob', 'dataset_dict.load_from_disk', 'list']",8
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest,DatasetDictTest,class,165,1294,517,20600,15.92,36,2,[],[],[],25,[],[],0
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:_create_dummy_dataset,DatasetDictTest:_create_dummy_dataset,method,5,29,27,216,7.45,0,1,"['self', 'multiple_columns']","[None, None]","[None, 'False']",26,[],"['Dataset.from_dict', 'np.arange']",2
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:_create_dummy_dataset_dict,DatasetDictTest:_create_dummy_dataset_dict,method,3,9,8,165,18.33,0,0,"['self', 'multiple_columns']","[None, None]","[None, 'False']",36,[],"['DatasetDict', 'self._create_dummy_dataset']",2
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:_create_dummy_iterable_dataset,DatasetDictTest:_create_dummy_iterable_dataset,method,10,38,32,267,7.03,2,1,"['self', 'multiple_columns']","[None, None]","[None, 'False']",44,[],"['gen', 'zip', 'range', 'IterableDataset.from_generator']",4
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:_create_dummy_iterable_dataset_dict,DatasetDictTest:_create_dummy_iterable_dataset_dict,method,3,9,8,191,21.22,0,0,"['self', 'multiple_columns']","[None, None]","[None, 'False']",56,[],"['IterableDatasetDict', 'self._create_dummy_iterable_dataset']",2
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_align_labels_with_mapping,DatasetDictTest:test_align_labels_with_mapping,method,26,133,79,1513,11.38,3,0,['self'],[None],[None],546,[],"['Features', 'Value', 'ClassLabel', 'label2id.items', 'DatasetDict', 'Dataset.from_dict', 'dsets.align_labels_with_mapping', 'self.assertListEqual']",8
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_cast,DatasetDictTest:test_cast,method,12,20,18,348,17.4,1,0,['self'],[None],[None],240,[],"['self._create_dummy_dataset_dict', 'Value', 'dset.cast', 'dset.values', 'self.assertEqual', 'self.assertIsInstance']",6
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_check_values_type,DatasetDictTest:test_check_values_type,method,5,21,15,270,12.86,0,0,['self'],[None],[None],480,[],"['self._create_dummy_dataset_dict', 'self.assertRaises']",2
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_filter,DatasetDictTest:test_filter,method,18,63,50,974,15.46,0,0,['self'],[None],[None],339,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset_dict', 'dsets.filter', 'int', 'self.assertListEqual', 'list', 'self.assertEqual', 'filtered_dsets_1.filter']",8
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_flatten,DatasetDictTest:test_flatten,method,8,43,35,526,12.23,0,0,['self'],[None],[None],64,[],"['Dataset.from_dict', 'features=Features', 'Sequence', 'Value', 'DatasetDict', 'dset.flatten', 'self.assertDictEqual', 'self.assertListEqual', 'Features']",9
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_flatten_indices,DatasetDictTest:test_flatten_indices,method,15,31,29,600,19.35,0,0,['self'],[None],[None],458,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset_dict', 'dsets.shuffle', 'self.assertIsNotNone', 'dsets_shuffled.flatten_indices', 'self.assertIsNone']",6
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_iterable_filter,DatasetDictTest:test_iterable_filter,method,13,32,32,453,14.16,0,0,['self'],[None],[None],364,[],"['self._create_dummy_iterable_dataset_dict', 'next', 'dsets.filter', 'int', 'self.assertListEqual', 'list', 'self.assertEqual']",7
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_iterable_map,DatasetDictTest:test_iterable_map,method,14,27,27,385,14.26,0,0,['self'],[None],[None],326,[],"['self._create_dummy_iterable_dataset_dict', 'dsets.map', 'len', 'next', 'self.assertListEqual', 'sorted', 'self.assertLessEqual']",7
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_load_from_disk,DatasetDictTest:test_load_from_disk,method,9,24,19,418,17.42,0,0,['self'],[None],[None],533,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset_dict', 'dsets.save_to_disk', 'load_from_disk', 'self.assertListEqual', 'self.assertEqual']",6
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_map,DatasetDictTest:test_map,method,15,49,44,784,16.0,0,0,['self'],[None],[None],307,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset_dict', 'dsets.map', 'len', 'self.assertListEqual', 'list', 'mapped_dsets_1.map', 'sorted']",8
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_remove_columns,DatasetDictTest:test_remove_columns,method,9,46,20,817,17.76,4,0,['self'],[None],[None],251,[],"['self._create_dummy_dataset_dict', 'dset.remove_columns', 'dset.values', 'self.assertEqual', 'self.assertListEqual']",5
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_rename_column,DatasetDictTest:test_rename_column,method,8,16,14,294,18.38,1,0,['self'],[None],[None],273,[],"['self._create_dummy_dataset_dict', 'dset.rename_column', 'dset.values', 'self.assertEqual', 'self.assertListEqual']",5
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_select_columns,DatasetDictTest:test_select_columns,method,8,54,22,984,18.22,5,0,['self'],[None],[None],281,[],"['self._create_dummy_dataset_dict', 'dset.select_columns', 'dset.values', 'self.assertEqual', 'self.assertListEqual']",5
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_serialization,DatasetDictTest:test_serialization,method,12,85,33,1948,22.92,0,0,['self'],[None],[None],489,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset_dict', 'dsets.save_to_disk', 'DatasetDict.load_from_disk', 'self.assertListEqual', 'self.assertEqual']",6
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_set_format_numpy,DatasetDictTest:test_set_format_numpy,method,11,68,38,1275,18.75,5,0,['self'],[None],[None],78,[],"['self._create_dummy_dataset_dict', 'dset.set_format', 'dset.values', 'self.assertEqual', 'self.assertIsInstance', 'dset.reset_format', 'dset.formatted_as']",7
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_set_format_pandas,DatasetDictTest:test_set_format_pandas,method,9,30,23,533,17.77,2,0,['self'],[None],[None],164,[],"['self._create_dummy_dataset_dict', 'dset.set_format', 'dset.values', 'self.assertEqual', 'self.assertIsInstance', 'self.assertListEqual']",6
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_set_format_polars,DatasetDictTest:test_set_format_polars,method,11,34,27,540,15.88,2,0,['self'],[None],[None],180,[],"['self._create_dummy_dataset_dict', 'dset.set_format', 'dset.values', 'self.assertEqual', 'self.assertIsInstance']",5
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_set_format_tf,DatasetDictTest:test_set_format_tf,method,12,46,31,824,17.91,3,0,['self'],[None],[None],141,[],"['self._create_dummy_dataset_dict', 'dset.set_format', 'dset.values', 'self.assertEqual', 'self.assertIsInstance', 'self.assertListEqual']",6
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_set_format_torch,DatasetDictTest:test_set_format_torch,method,11,50,27,964,19.28,3,0,['self'],[None],[None],113,[],"['self._create_dummy_dataset_dict', 'dset.set_format', 'dset.values', 'self.assertEqual', 'self.assertIsInstance', 'self.assertListEqual']",6
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_set_transform,DatasetDictTest:test_set_transform,method,12,56,39,833,14.88,3,0,['self'],[None],[None],197,[],"['transform', 'batch.items', 'self._create_dummy_dataset_dict', 'dset.set_transform', 'dset.values', 'self.assertEqual', 'dset[list', 'dset_split.set_format']",8
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_shuffle,DatasetDictTest:test_shuffle,method,26,104,64,1959,18.84,0,0,['self'],[None],[None],401,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset_dict', 'dsets.shuffle', 'self.assertListEqual', 'self.assertEqual', 'self.assertDictEqual', 'Features', 'Value', 'self.assertNotEqual']",9
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_sort,DatasetDictTest:test_sort,method,16,58,43,806,13.9,0,0,['self'],[None],[None],376,[],"['tempfile.TemporaryDirectory', 'self._create_dummy_dataset_dict', 'dsets.sort', 'self.assertListEqual', 'list', 'sorted', 'range', 'sorted_dsets_1.sort']",8
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_with_format,DatasetDictTest:test_with_format,method,11,18,16,287,15.94,1,0,['self'],[None],[None],221,[],"['self._create_dummy_dataset_dict', 'dset.with_format', 'dset.set_format', 'zip', 'dset2.values', 'self.assertDictEqual']",6
repos/datasets/tests/test_dataset_dict.py:DatasetDictTest:test_with_transform,DatasetDictTest:test_with_transform,method,13,32,26,373,11.66,1,0,['self'],[None],[None],229,[],"['transform', 'batch.items', 'self._create_dummy_dataset_dict', 'dset.with_transform', 'dset.set_transform', 'zip', 'dset2.values', 'self.assertDictEqual']",8
repos/datasets/tests/test_dataset_list.py:DatasetListTest,DatasetListTest,class,23,133,93,1382,10.39,1,0,[],[],[],7,[],[],0
repos/datasets/tests/test_dataset_list.py:DatasetListTest:_create_example_dict,DatasetListTest:_create_example_dict,method,3,13,13,80,6.15,0,0,['self'],[None],[None],16,[],['Dataset.from_dict'],1
repos/datasets/tests/test_dataset_list.py:DatasetListTest:_create_example_records,DatasetListTest:_create_example_records,method,1,19,13,109,5.74,0,0,['self'],[None],[None],8,[],[],0
repos/datasets/tests/test_dataset_list.py:DatasetListTest:test_create,DatasetListTest:test_create,method,9,14,14,212,15.14,1,0,['self'],[None],[None],20,[],"['self._create_example_records', 'Dataset.from_list', 'self.assertListEqual', 'enumerate', 'self.assertDictEqual']",5
repos/datasets/tests/test_dataset_list.py:DatasetListTest:test_create_empty,DatasetListTest:test_create_empty,method,4,6,6,99,16.5,0,0,['self'],[None],[None],44,[],"['Dataset.from_list', 'self.assertEqual', 'self.assertListEqual']",3
repos/datasets/tests/test_dataset_list.py:DatasetListTest:test_list_dict_equivalent,DatasetListTest:test_list_dict_equivalent,method,7,17,15,224,13.18,0,0,['self'],[None],[None],27,[],"['self._create_example_records', 'Dataset.from_list', 'Dataset.from_dict', 'self.assertEqual']",4
repos/datasets/tests/test_dataset_list.py:DatasetListTest:test_uneven_records,DatasetListTest:test_uneven_records,method,4,6,6,99,16.5,0,0,['self'],[None],[None],33,[],"['Dataset.from_list', 'self.assertEqual', 'self.assertListEqual']",3
repos/datasets/tests/test_dataset_list.py:DatasetListTest:test_variable_list_records,DatasetListTest:test_variable_list_records,method,4,6,6,99,16.5,0,0,['self'],[None],[None],39,[],"['Dataset.from_list', 'self.assertEqual', 'self.assertListEqual']",3
repos/datasets/tests/test_distributed.py:test_distributed_shuffle_iterable,test_distributed_shuffle_iterable,function,10,45,31,606,13.47,0,0,[],[],[],58,[],"['gen', 'range', 'IterableDataset.from_generator', 'len', 'split_dataset_by_node', 'pytest.raises']",6
repos/datasets/tests/test_distributed.py:test_split_dataset_by_node_iterable,test_split_dataset_by_node_iterable,function,8,43,31,371,8.63,0,0,[],[],[],24,[],"['gen', 'range', 'IterableDataset.from_generator', 'len', 'split_dataset_by_node', 'sum']",6
repos/datasets/tests/test_distributed.py:test_split_dataset_by_node_iterable_sharded,test_split_dataset_by_node_iterable_sharded,function,18,73,50,677,9.27,3,0,['shards_per_node'],[None],[None],39,[],"['gen', 'range', 'IterableDataset.from_generator', 'len', 'split_dataset_by_node', 'sum']",6
repos/datasets/tests/test_distributed.py:test_split_dataset_by_node_map_style,test_split_dataset_by_node_map_style,function,6,35,25,317,9.06,0,0,[],[],[],13,[],"['Dataset.from_dict', 'range', 'len', 'split_dataset_by_node', 'sum']",5
repos/datasets/tests/test_distributed.py:test_torch_distributed_run,test_torch_distributed_run,function,2,8,8,140,17.5,0,0,['streaming'],[None],[None],81,[],[],0
repos/datasets/tests/test_download_manager.py:_test_jsonl,_test_jsonl,function,8,18,16,175,9.72,1,0,"['path', 'file']","[None, None]","[None, None]",144,[],"['path.endswith', 'enumerate', 'json.loads', 'item.keys']",4
repos/datasets/tests/test_download_manager.py:mock_request,mock_request,function,2,2,2,20,10.0,0,0,"['*args', '**kwargs']","[None, None]","[None, None]",28,[],['MockResponse'],1
repos/datasets/tests/test_download_manager.py:test_download_manager_delete_extracted_files,test_download_manager_delete_extracted_files,function,19,36,30,590,16.39,0,0,['xz_file'],[None],[None],123,[],"['DownloadConfig', 'DownloadManager', 'dl_manager.extract', 'Path', 'hash_url_to_filename', 'extracted_path.exists', 'dl_manager.delete_extracted_files']",7
repos/datasets/tests/test_download_manager.py:test_download_manager_download,test_download_manager_download,function,41,99,84,1389,14.03,1,2,"['urls_type', 'tmp_path', 'monkeypatch']","[None, None, None]","[None, None, None]",33,[],"['monkeypatch.setattr', 'DownloadConfig', 'DownloadManager', 'dl_manager.download', 'isinstance', 'type', 'len', 'downloaded_paths.keys', 'urls.keys', 'list', 'zip', 'NestedDataStructure', 'Path', 'downloaded_path.exists', 'downloaded_path.read_text', 'downloaded_path.with_suffix', 'metadata_downloaded_path.exists', 'json.loads']",18
repos/datasets/tests/test_download_manager.py:test_download_manager_extract,test_download_manager_extract,function,38,103,73,1451,14.09,2,3,"['paths_type', 'xz_file', 'text_file', 'extract_on_the_fly']","[None, None, None, None]","[None, None, None, None]",77,[],"['str', 'issubclass', 'DownloadConfig', 'DownloadManager', 'dl_manager.extract', 'isinstance', 'extracted_paths.keys', 'extracted_paths.values', 'paths.values', 'zip', 'Path', 'hash_url_to_filename', 'extracted_path.exists', 'extracted_path.read_text', 'text_file.read_text', 'StreamingDownloadManager', 'xopen']",17
repos/datasets/tests/test_download_manager.py:test_iter_archive_file,test_iter_archive_file,function,9,26,22,342,13.15,2,0,"['archive_nested_jsonl', 'request']","[None, None]","[None, None]",162,[],"['request.getfixturevalue', 'DownloadManager', 'enumerate', '_test_jsonl']",4
repos/datasets/tests/test_download_manager.py:test_iter_archive_path,test_iter_archive_path,function,7,16,16,218,13.62,1,0,"['archive_jsonl', 'request']","[None, None]","[None, None]",153,[],"['request.getfixturevalue', 'DownloadManager', 'enumerate', '_test_jsonl']",4
repos/datasets/tests/test_download_manager.py:test_iter_files,test_iter_files,function,7,19,17,205,10.79,1,0,['data_dir_with_hidden_files'],[None],[None],172,[],"['DownloadManager', 'enumerate']",2
repos/datasets/tests/test_download_manager.py:MockResponse,MockResponse,class,5,13,13,122,9.38,0,0,[],[],[],19,[],[],0
repos/datasets/tests/test_download_manager.py:MockResponse:iter_content,MockResponse:iter_content,method,1,3,3,30,10.0,0,0,"['self', '**kwargs']","[None, None]","[None, None]",24,[],[],0
repos/datasets/tests/test_experimental.py:dummy_function,dummy_function,function,1,2,2,15,7.5,0,0,[],[],[],8,[],[],0
repos/datasets/tests/test_experimental.py:TestExperimentalFlag,TestExperimentalFlag,class,5,11,11,184,16.73,0,0,[],[],[],12,[],[],0
repos/datasets/tests/test_experimental.py:TestExperimentalFlag:test_experimental_warning,TestExperimentalFlag:test_experimental_warning,method,4,9,9,148,16.44,0,0,['self'],[None],[None],13,[],"['warnings.catch_warnings', 'warnings.simplefilter', 'self.assertEqual']",3
repos/datasets/tests/test_extract.py:tar_file_with_dot_dot,tar_file_with_dot_dot,function,10,19,17,212,11.16,0,0,"['tmp_path', 'text_file']","[None, None]","[None, None]",145,[],"['directory.mkdir', 'tarfile.TarFile', 'f.add']",3
repos/datasets/tests/test_extract.py:tar_file_with_sym_link,tar_file_with_sym_link,function,11,34,31,311,9.15,0,0,['tmp_path'],[None],[None],157,[],"['directory.mkdir', 'os.symlink', 'tarfile.TarFile', 'f.add']",4
repos/datasets/tests/test_extract.py:test_base_extractors,test_base_extractors,function,26,90,73,1197,13.3,1,3,"['compression_format', 'is_archive', 'bz2_file', 'gz_file', 'lz4_file', 'seven_zip_file', 'tar_file', 'xz_file', 'zip_file', 'zstd_file', 'tmp_path', 'text_file', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None, None]",34,[],"['pytest.skip', 'base_extractor.is_extractable', 'base_extractor.extract', 'output_path.is_dir', 'output_path.iterdir', 'file_path.read_text', 'output_path.read_text', 'text_file.read_text']",8
repos/datasets/tests/test_extract.py:test_extractor,test_extractor,function,25,87,67,1072,12.32,1,3,"['compression_format', 'is_archive', 'bz2_file', 'gz_file', 'lz4_file', 'seven_zip_file', 'tar_file', 'xz_file', 'zip_file', 'zstd_file', 'tmp_path', 'text_file', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None, None]",95,[],"['pytest.skip', 'Extractor.infer_extractor_format', 'Extractor.extract', 'output_path.is_dir', 'output_path.iterdir', 'file_path.read_text', 'output_path.read_text', 'text_file.read_text']",8
repos/datasets/tests/test_extract.py:test_is_zipfile_false_positive,test_is_zipfile_false_positive,function,8,31,29,475,15.32,0,0,['tmpdir'],[None],[None],189,[],"['not_a_zip_file.open', 'f.write', 'zipfile.is_zipfile', 'ZipExtractor.is_extractable']",4
repos/datasets/tests/test_extract.py:test_tar_extract_insecure_files,test_tar_extract_insecure_files,function,12,27,24,350,12.96,1,0,"['insecure_tar_file', 'error_log', 'tar_file_with_dot_dot', 'tar_file_with_sym_link', 'tmp_path', 'caplog']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",173,[],['TarExtractor.extract'],1
repos/datasets/tests/test_file_utils.py:_readd_double_slash_removed_by_path,_readd_double_slash_removed_by_path,function,2,4,4,62,15.5,0,0,['path_as_posix'],[' str'],[None],268,"['    """"""Path(...) on an url path like zip://file.txt::http://host.com/data.zip\n', '    converts the :// to :/\n', '    This function readds the ://\n', '\n', '    It handles cases like:\n', '\n', '    - https://host.com/data.zip\n', '    - C://data.zip\n', '    - zip://file.txt::https://host.com/data.zip\n', '    - zip://file.txt::/Users/username/data.zip\n', '    - zip://file.txt::C://data.zip\n', '\n', '    Args:\n', '        path_as_posix (str): output of Path(...).as_posix()\n', '\n', '    Returns:\n', '        str: the url path with :// instead of :/\n', '    """"""\n']",['re.sub'],1
repos/datasets/tests/test_file_utils.py:mock_fsspec2,mock_fsspec2,function,2,4,4,62,15.5,0,0,[],[],[],262,"['    """"""Path(...) on an url path like zip://file.txt::http://host.com/data.zip\n', '    converts the :// to :/\n', '    This function readds the ://\n', '\n', '    It handles cases like:\n', '\n', '    - https://host.com/data.zip\n', '    - C://data.zip\n', '    - zip://file.txt::https://host.com/data.zip\n', '    - zip://file.txt::/Users/username/data.zip\n', '    - zip://file.txt::C://data.zip\n', '\n', '    Args:\n', '        path_as_posix (str): output of Path(...).as_posix()\n', '\n', '    Returns:\n', '        str: the url path with :// instead of :/\n', '    """"""\n']",['re.sub'],1
repos/datasets/tests/test_file_utils.py:test_cached_path_extract,test_cached_path_extract,function,14,33,27,448,13.58,0,0,"['compression_format', 'gz_file', 'xz_file', 'zstd_path', 'tmp_path', 'text_file']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",81,[],"['DownloadConfig', 'cached_path', 'open', 'f.read']",4
repos/datasets/tests/test_file_utils.py:test_cached_path_local,test_cached_path_local,function,6,14,11,314,22.43,0,0,['text_file'],[None],[None],117,[],['str'],1
repos/datasets/tests/test_file_utils.py:test_cached_path_missing_local,test_cached_path_missing_local,function,5,11,8,225,20.45,0,0,['tmp_path'],[None],[None],128,[],"['str', 'pytest.raises', 'cached_path']",3
repos/datasets/tests/test_file_utils.py:test_cached_path_offline,test_cached_path_offline,function,3,3,3,78,26.0,0,0,[],[],[],147,[],"['pytest.raises', 'cached_path']",2
repos/datasets/tests/test_file_utils.py:test_extracted_datasets_path,test_extracted_datasets_path,function,16,46,36,829,18.02,0,3,"['default_extracted', 'default_cache_dir', 'xz_file', 'tmp_path', 'monkeypatch']","[None, None, None, None, None]","[None, None, None, None, None]",96,[],"['monkeypatch.setattr', 'str', 'DownloadConfig', 'cached_path', 'Path']",5
repos/datasets/tests/test_file_utils.py:test_fsspec_offline,test_fsspec_offline,function,6,10,8,222,22.2,0,0,['tmp_path_factory'],[None],[None],171,[],"['tmp_path_factory.mktemp', 'pytest.raises', 'fsspec_get', 'fsspec_head']",4
repos/datasets/tests/test_file_utils.py:test_ftp_offline,test_ftp_offline,function,6,10,8,218,21.8,0,0,['tmp_path_factory'],[None],[None],162,[],"['tmp_path_factory.mktemp', 'pytest.raises', 'ftp_get', 'ftp_head']",4
repos/datasets/tests/test_file_utils.py:test_get_extraction_protocol,test_get_extraction_protocol,function,1,3,3,58,19.33,0,0,"['urlpath', 'expected_protocol']","[None, None]","[None, None]",862,[],['_get_extraction_protocol'],1
repos/datasets/tests/test_file_utils.py:test_get_extraction_protocol_gg_drive,test_get_extraction_protocol_gg_drive,function,1,3,3,58,19.33,0,0,"['urlpath', 'expected_protocol']","[None, None]","[None, None]",874,[],['_get_extraction_protocol'],1
repos/datasets/tests/test_file_utils.py:test_get_from_cache_fsspec,test_get_from_cache_fsspec,function,7,11,10,144,13.09,0,0,['tmpfs_file'],[None],[None],139,[],"['get_from_cache', 'open', 'f.read']",3
repos/datasets/tests/test_file_utils.py:test_http_offline,test_http_offline,function,6,10,8,224,22.4,0,0,['tmp_path_factory'],[None],[None],153,[],"['tmp_path_factory.mktemp', 'pytest.raises', 'http_get', 'http_head']",4
repos/datasets/tests/test_file_utils.py:test_streaming_gg_drive,test_streaming_gg_drive,function,3,7,7,70,10.0,0,0,[],[],[],880,[],"['xopen', 'f.read']",2
repos/datasets/tests/test_file_utils.py:test_xdirname,test_xdirname,function,3,7,5,197,28.14,0,0,"['input_path', 'expected_path']","[None, None]","[None, None]",342,[],"['xdirname', '_readd_double_slash_removed_by_path']",2
repos/datasets/tests/test_file_utils.py:test_xexists,test_xexists,function,4,12,12,176,14.67,0,1,"['input_path', 'exists', 'tmp_path', 'mock_fsspec2']","[None, None, None, None]","[None, None, None, None]",357,[],"['input_path.startswith', 'input_path.replace', 'str', 'xexists']",4
repos/datasets/tests/test_file_utils.py:test_xexists_private,test_xexists_private,function,5,14,12,272,19.43,0,0,"['hf_private_dataset_repo_txt_data', 'hf_token']","[None, None]","[None, None]",365,[],"['hf_dataset_url', 'DownloadConfig', 'xexists']",3
repos/datasets/tests/test_file_utils.py:test_xgetsize,test_xgetsize,function,4,14,13,219,15.64,0,1,"['input_path', 'size', 'tmp_path', 'mock_fsspec2']","[None, None, None, None]","[None, None, None, None]",520,[],"['input_path.startswith', 'input_path.replace', 'str', 'xgetsize']",4
repos/datasets/tests/test_file_utils.py:test_xgetsize_private,test_xgetsize_private,function,7,15,13,291,19.4,0,0,"['hf_private_dataset_repo_txt_data', 'hf_token']","[None, None]","[None, None]",529,[],"['hf_dataset_url', 'DownloadConfig', 'xgetsize', 'pytest.raises']",4
repos/datasets/tests/test_file_utils.py:test_xglob,test_xglob,function,7,26,21,317,12.19,1,1,"['input_path', 'expected_paths', 'tmp_path', 'mock_fsspec2']","[None, None, None, None]","[None, None, None, None]",562,[],"['input_path.startswith', 'input_path.replace', 'str', 'sorted']",4
repos/datasets/tests/test_file_utils.py:test_xglob_private,test_xglob_private,function,4,15,12,282,18.8,0,0,"['hf_private_dataset_repo_zipped_txt_data', 'hf_token']","[None, None]","[None, None]",573,[],"['hf_dataset_url', 'DownloadConfig', 'len']",3
repos/datasets/tests/test_file_utils.py:test_xisdir,test_xisdir,function,4,11,11,174,15.82,0,1,"['input_path', 'isdir', 'tmp_path', 'mock_fsspec2']","[None, None, None, None]","[None, None, None, None]",471,[],"['input_path.startswith', 'input_path.replace', 'str', 'xisdir']",4
repos/datasets/tests/test_file_utils.py:test_xisdir_private,test_xisdir_private,function,5,28,16,419,14.96,0,0,"['hf_private_dataset_repo_zipped_txt_data', 'hf_token']","[None, None]","[None, None]",479,[],"['hf_dataset_url', 'DownloadConfig', 'xisdir']",3
repos/datasets/tests/test_file_utils.py:test_xisfile,test_xisfile,function,4,11,11,176,16.0,0,1,"['input_path', 'isfile', 'tmp_path', 'mock_fsspec2']","[None, None, None, None]","[None, None, None, None]",497,[],"['input_path.startswith', 'input_path.replace', 'str', 'xisfile']",4
repos/datasets/tests/test_file_utils.py:test_xisfile_private,test_xisfile_private,function,5,17,13,266,15.65,0,0,"['hf_private_dataset_repo_txt_data', 'hf_token']","[None, None]","[None, None]",505,[],"['hf_dataset_url', 'DownloadConfig', 'xisfile']",3
repos/datasets/tests/test_file_utils.py:test_xjoin,test_xjoin,function,4,11,7,172,15.64,0,0,"['input_path', 'paths_to_join', 'expected_path']","[None, None, None]","[None, None, None]",320,[],"['xjoin', 'xPath']",2
repos/datasets/tests/test_file_utils.py:test_xlistdir,test_xlistdir,function,7,18,17,249,13.83,1,1,"['input_path', 'expected_paths', 'tmp_path', 'mock_fsspec2']","[None, None, None, None]","[None, None, None, None]",440,[],"['input_path.startswith', 'input_path.replace', 'str', 'sorted']",4
repos/datasets/tests/test_file_utils.py:test_xlistdir_private,test_xlistdir_private,function,7,24,18,481,20.04,0,0,"['hf_private_dataset_repo_zipped_txt_data', 'hf_token']","[None, None]","[None, None]",450,[],"['hf_dataset_url', 'DownloadConfig', 'len', 'pytest.raises', 'xlistdir']",5
repos/datasets/tests/test_file_utils.py:test_xnumpy_load,test_xnumpy_load,function,15,30,24,294,9.8,0,0,['tmp_path'],[None],[None],885,[],"['np.arange', 'np.save', 'xnumpy_load', 'np.array_equal', 'np.savez']",5
repos/datasets/tests/test_file_utils.py:test_xopen_local,test_xopen_local,function,3,25,13,263,10.52,0,0,['text_path'],[None],[None],416,[],"['xopen', 'open', 'list', 'xPath']",4
repos/datasets/tests/test_file_utils.py:test_xopen_remote,test_xopen_remote,function,3,17,11,211,12.41,0,0,[],[],[],424,[],"['xopen', 'list', 'TEST_URL_CONTENT.splitlines', 'xPath']",4
repos/datasets/tests/test_file_utils.py:test_xrelpath,test_xrelpath,function,3,6,5,82,13.67,0,0,"['input_path', 'start_path', 'expected_path']","[None, None, None]","[None, None, None]",638,[],['xrelpath'],1
repos/datasets/tests/test_file_utils.py:test_xsplit,test_xsplit,function,9,16,12,305,19.06,0,0,"['input_path', 'expected_head_and_tail']","[None, None]","[None, None]",385,[],"['xsplit', '_readd_double_slash_removed_by_path']",2
repos/datasets/tests/test_file_utils.py:test_xsplitext,test_xsplitext,function,9,16,12,303,18.94,0,0,"['input_path', 'expected_path_and_ext']","[None, None]","[None, None]",407,[],"['xsplitext', '_readd_double_slash_removed_by_path']",2
repos/datasets/tests/test_file_utils.py:test_xwalk,test_xwalk,function,10,44,33,493,11.2,2,1,"['input_path', 'expected_outputs', 'tmp_path', 'mock_fsspec2']","[None, None, None, None]","[None, None, None, None]",595,[],"['input_path.startswith', 'input_path.replace', 'str', 'sorted']",4
repos/datasets/tests/test_file_utils.py:test_xwalk_private,test_xwalk_private,function,4,20,14,377,18.85,0,0,"['hf_private_dataset_repo_zipped_txt_data', 'hf_token']","[None, None]","[None, None]",612,[],"['hf_dataset_url', 'DownloadConfig', 'len']",3
repos/datasets/tests/test_file_utils.py:tmpfs_file,tmpfs_file,function,5,9,9,100,11.11,0,0,['tmpfs'],[None],[None],74,[],"['open', 'f.write']",2
repos/datasets/tests/test_file_utils.py:zstd_path,zstd_path,function,8,15,14,143,9.53,0,0,['tmp_path_factory'],[None],[None],65,[],"['tmp_path_factory.mktemp', 'bytes', 'zstd.open', 'f.write']",4
repos/datasets/tests/test_file_utils.py:DummyTestFS,DummyTestFS,class,25,202,97,2058,10.19,2,5,[],[],[],179,[],[],0
repos/datasets/tests/test_file_utils.py:TestxPath,TestxPath,class,24,373,143,6222,16.68,4,3,[],[],[],643,[],[],0
repos/datasets/tests/test_file_utils.py:DummyTestFS:__getitem__,DummyTestFS:__getitem__,method,5,13,13,96,7.38,1,1,"['self', 'name']","[None, None]","[None, None]",221,[],['IndexError'],1
repos/datasets/tests/test_file_utils.py:DummyTestFS:_open,DummyTestFS:_open,method,2,10,10,106,10.6,0,0,"['self', 'path', 'mode', 'block_size', 'autocommit', 'cache_options', '**kwargs', '']","[None, None, None, None, None, None, None, None]","[None, None, '""rb""', 'None', 'True', 'None', None, None]",241,[],['self._file_class'],1
repos/datasets/tests/test_file_utils.py:DummyTestFS:ls,DummyTestFS:ls,method,14,37,26,330,8.92,1,4,"['self', 'path', 'detail', 'refresh', '**kwargs']","[None, None, None, None, None]","[None, None, 'True', 'True', None]",227,[],"['kwargs.pop', 'self._strip_protocol', 'self._ls_from_cache', 'self._parent', 'files.sort']",5
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_as_posix,TestxPath:test_xpath_as_posix,method,1,3,3,49,16.33,0,0,"['self', 'input_path', 'expected_path']","[None, None, None]","[None, None, None]",667,[],['xPath'],1
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_exists,TestxPath:test_xpath_exists,method,4,12,12,176,14.67,0,1,"['self', 'input_path', 'exists', 'tmp_path', 'mock_fsspec2']","[None, None, None, None, None]","[None, None, None, None, None]",679,[],"['input_path.startswith', 'input_path.replace', 'str', 'xexists']",4
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_glob,TestxPath:test_xpath_glob,method,7,32,20,316,9.88,2,1,"['self', 'input_path', 'pattern', 'expected_paths', 'tmp_path', 'mock_fsspec2']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",712,[],['sorted'],1
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_name,TestxPath:test_xpath_name,method,1,3,3,38,12.67,0,0,"['self', 'input_path', 'expected']","[None, None, None]","[None, None, None]",803,[],['xPath'],1
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_parent,TestxPath:test_xpath_parent,method,1,3,3,52,17.33,0,0,"['self', 'input_path', 'expected_path']","[None, None, None]","[None, None, None]",790,[],['xPath'],1
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_rglob,TestxPath:test_xpath_rglob,method,9,36,24,358,9.94,2,1,"['self', 'input_path', 'pattern', 'expected_paths', 'tmp_path', 'mock_fsspec2']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",767,[],"['dir_path.mkdir', 'sorted']",2
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_stem,TestxPath:test_xpath_stem,method,1,3,3,38,12.67,0,0,"['self', 'input_path', 'expected']","[None, None, None]","[None, None, None]",816,[],['xPath'],1
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_str,TestxPath:test_xpath_str,method,1,3,3,40,13.33,0,0,"['self', 'input_path']","[None, None]","[None, None]",654,[],['str'],1
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_suffix,TestxPath:test_xpath_suffix,method,1,3,3,40,13.33,0,0,"['self', 'input_path', 'expected']","[None, None, None]","[None, None, None]",829,[],['xPath'],1
repos/datasets/tests/test_file_utils.py:TestxPath:test_xpath_with_suffix,TestxPath:test_xpath_with_suffix,method,1,3,3,60,20.0,0,0,"['self', 'input_path', 'suffix', 'expected']","[None, None, None, None]","[None, None, None, None]",846,[],['xPath'],1
repos/datasets/tests/test_filelock.py:test_long_path,test_long_path,function,4,15,13,196,13.07,0,0,['tmpdir'],[None],[None],6,[],"['FileLock', 'len']",2
repos/datasets/tests/test_filesystem.py:test_compression_filesystems,test_compression_filesystems,function,19,63,52,816,12.95,0,2,"['compression_fs_class', 'gz_file', 'bz2_file', 'lz4_file', 'zstd_file', 'xz_file', 'text_file']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",47,[],"['pytest.skip', 'fsspec.filesystem', 'isinstance', 'expected_filename.rindex', 'fs.glob', 'fs.open', 'open', 'f.read', 'expected_file.read']",9
repos/datasets/tests/test_filesystem.py:test_extract_path_from_uri,test_extract_path_from_uri,function,5,17,11,276,16.24,0,0,[],[],[],25,[],"['extract_path_from_uri', 'dataset_path.startswith']",2
repos/datasets/tests/test_filesystem.py:test_fs_isfile,test_fs_isfile,function,8,20,19,324,16.2,0,0,"['protocol', 'zip_jsonl_path', 'jsonl_gz_path']","[None, None, None]","[None, None, None]",67,[],"['url_to_fs', 'fs.isfile']",2
repos/datasets/tests/test_filesystem.py:test_fs_overwrites,test_fs_overwrites,function,7,31,29,315,10.16,0,0,[],[],[],77,[],"['register_implementation', 'pytest.warns', 'importlib.reload', 'len', 'str']",5
repos/datasets/tests/test_filesystem.py:test_is_remote_filesystem,test_is_remote_filesystem,function,4,14,9,147,10.5,0,0,['mockfs'],[None],[None],36,[],"['is_remote_filesystem', 'fsspec.filesystem']",2
repos/datasets/tests/test_filesystem.py:test_mockfs,test_mockfs,function,0,8,6,60,7.5,0,0,['mockfs'],[None],[None],15,[],[],0
repos/datasets/tests/test_filesystem.py:test_non_mockfs,test_non_mockfs,function,0,9,7,63,7.0,0,0,[],[],[],20,[],[],0
repos/datasets/tests/test_fingerprint.py:test_dependency_on_dill,test_dependency_on_dill,function,3,5,5,40,8.0,0,0,[],[],[],441,[],"['Hasher', 'hasher.update']",2
repos/datasets/tests/test_fingerprint.py:test_fingerprint_in_multiprocessing,test_fingerprint_in_multiprocessing,function,9,25,17,404,16.16,0,0,[],[],[],402,[],"['DatasetChild', 'dataset.func1', 'dataset.func2', 'Pool', 'p.apply_async']",5
repos/datasets/tests/test_fingerprint.py:test_fingerprint_when_transform_version_changes,test_fingerprint_when_transform_version_changes,function,7,45,26,858,19.07,0,0,[],[],[],414,[],"['DummyDatasetChild', 'func', 'len']",3
repos/datasets/tests/test_fingerprint.py:test_move_script_doesnt_change_hash,test_move_script_doesnt_change_hash,function,17,38,27,435,11.45,0,0,['tmp_path'],[' Path'],[None],377,[],"['dir1.mkdir', 'dir2.mkdir', 'dedent', 'script_path1.open', 'f.write', 'script_path2.open', 'subprocess.check_output', 'str']",8
repos/datasets/tests/test_fingerprint.py:DatasetChild,DatasetChild,class,4,18,11,285,15.83,0,0,[],[],[],40,[],[],0
repos/datasets/tests/test_fingerprint.py:Foo,Foo,class,5,9,7,69,7.67,0,0,[],[],[],32,[],[],0
repos/datasets/tests/test_fingerprint.py:HashingTest,HashingTest,class,64,279,119,3576,12.82,3,0,[],[],[],225,[],[],0
repos/datasets/tests/test_fingerprint.py:RecurseHashTest,RecurseHashTest,class,34,186,82,2688,14.45,0,0,[],[],[],134,[],[],0
repos/datasets/tests/test_fingerprint.py:TokenizersHashTest,TokenizersHashTest,class,26,102,61,1473,14.44,0,0,[],[],[],80,[],[],0
repos/datasets/tests/test_fingerprint.py:UnpicklableCallable,UnpicklableCallable,class,7,21,18,197,9.38,0,1,[],[],[],50,[],[],0
repos/datasets/tests/test_fingerprint.py:DatasetChild:func1,DatasetChild:func1,method,2,3,3,57,19.0,0,0,"['self', 'new_fingerprint', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",42,[],['DatasetChild'],1
repos/datasets/tests/test_fingerprint.py:DatasetChild:func2,DatasetChild:func2,method,2,3,3,57,19.0,0,0,"['self', 'new_fingerprint', '*args', '**kwargs']","[None, None, None, None]","[None, None, None, None]",46,[],['DatasetChild'],1
repos/datasets/tests/test_fingerprint.py:Foo:__call__,Foo:__call__,method,2,2,2,14,7.0,0,0,['self'],[None],[None],36,[],[],0
repos/datasets/tests/test_fingerprint.py:Foo:__init__,Foo:__init__,method,2,2,2,12,6.0,0,0,"['self', 'foo']","[None, None]","[None, None]",33,[],[],0
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_class_instance,HashingTest:test_hash_class_instance,method,6,10,9,158,15.8,0,0,['self'],[None],[None],233,[],"['Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",3
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_same_strings,HashingTest:test_hash_same_strings,method,19,46,34,471,10.24,0,0,['self'],[None],[None],260,[],"['json.loads', 'self.assertIs', 'self.assertIsNot', 'Hasher.hash', 'self.assertEqual']",5
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_simple,HashingTest:test_hash_simple,method,6,10,9,143,14.3,0,0,['self'],[None],[None],226,[],"['Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",3
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_spacy_model,HashingTest:test_hash_spacy_model,method,10,18,13,209,11.61,0,0,['self'],[None],[None],334,[],"['spacy.blank', 'Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",4
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_tiktoken_encoding,HashingTest:test_hash_tiktoken_encoding,method,10,18,13,253,14.06,0,0,['self'],[None],[None],294,[],"['tiktoken.get_encoding', 'Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",4
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_torch_compiled_function,HashingTest:test_hash_torch_compiled_function,method,10,15,14,143,9.53,0,0,['self'],[None],[None],348,[],"['f', 'torch.sin', 'torch.cos', 'Hasher.hash', 'torch.compile', 'self.assertEqual']",6
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_torch_compiled_module,HashingTest:test_hash_torch_compiled_module,method,10,22,15,314,14.27,0,0,['self'],[None],[None],361,[],"['TorchModule', 'next', 'Hasher.hash', 'torch.compile', 'self.assertEqual', 'self.assertNotEqual']",6
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_torch_generator,HashingTest:test_hash_torch_generator,method,10,20,13,285,14.25,0,0,['self'],[None],[None],320,[],"['torch.Generator', 'Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",4
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_torch_tensor,HashingTest:test_hash_torch_tensor,method,10,18,13,203,11.28,0,0,['self'],[None],[None],307,[],"['torch.tensor', 'Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",4
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_unpicklable,HashingTest:test_hash_unpicklable,method,3,3,3,91,30.33,0,0,['self'],[None],[None],256,[],"['self.assertRaises', 'Hasher.hash']",2
repos/datasets/tests/test_fingerprint.py:HashingTest:test_hash_update,HashingTest:test_hash_update,method,11,34,18,326,9.59,3,0,['self'],[None],[None],240,[],"['Hasher', 'Foo', 'hasher.update', 'hasher.hexdigest', 'self.assertEqual', 'self.assertNotEqual']",6
repos/datasets/tests/test_fingerprint.py:HashingTest:test_set_doesnt_depend_on_order,HashingTest:test_set_doesnt_depend_on_order,method,7,16,12,182,11.38,0,0,['self'],[None],[None],283,[],"['set', 'Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",4
repos/datasets/tests/test_fingerprint.py:HashingTest:test_set_stable,HashingTest:test_set_stable,method,6,14,13,179,12.79,0,0,['self'],[None],[None],277,[],"['range', 'Hasher.hash', 'Pool', 'set']",4
repos/datasets/tests/test_fingerprint.py:RecurseHashTest:test_hash_ignores_line_definition_of_function,RecurseHashTest:test_hash_ignores_line_definition_of_function,method,5,12,8,109,9.08,0,0,['self'],[None],[None],148,[],"['func', 'Hasher.hash', 'self.assertEqual']",3
repos/datasets/tests/test_fingerprint.py:RecurseHashTest:test_hash_ipython_function,RecurseHashTest:test_hash_ipython_function,method,19,73,40,1201,16.45,0,0,['self'],[None],[None],174,[],"['create_ipython_func', 'func', 'code.replace', 'FunctionType', 'Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",7
repos/datasets/tests/test_fingerprint.py:RecurseHashTest:test_recurse_hash_for_class,RecurseHashTest:test_recurse_hash_for_class,method,6,10,9,146,14.6,0,0,['self'],[None],[None],160,[],"['Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",3
repos/datasets/tests/test_fingerprint.py:RecurseHashTest:test_recurse_hash_for_function,RecurseHashTest:test_recurse_hash_for_function,method,9,20,15,179,8.95,0,0,['self'],[None],[None],135,[],"['func', 'Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",4
repos/datasets/tests/test_fingerprint.py:RecurseHashTest:test_recurse_hash_for_function_with_shuffled_globals,RecurseHashTest:test_recurse_hash_for_function_with_shuffled_globals,method,14,49,36,601,12.27,0,0,['self'],[None],[None],202,[],"['func', 'globalvars_mock1_side_effect', 'globalvars_mock2_side_effect', 'patch', 'Hasher.hash', 'self.assertGreater', 'self.assertEqual']",7
repos/datasets/tests/test_fingerprint.py:RecurseHashTest:test_recurse_hash_for_method,RecurseHashTest:test_recurse_hash_for_method,method,6,10,9,173,17.3,0,0,['self'],[None],[None],167,[],"['Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",3
repos/datasets/tests/test_fingerprint.py:TokenizersHashTest:test_hash_regex,TokenizersHashTest:test_hash_regex,method,10,18,13,212,11.78,0,0,['self'],[None],[None],121,[],"['regex.Regex', 'Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",4
repos/datasets/tests/test_fingerprint.py:TokenizersHashTest:test_hash_tokenizer,TokenizersHashTest:test_hash_tokenizer,method,20,50,37,821,16.42,0,0,['self'],[None],[None],83,[],"['encode', 'tokenizer', 'AutoTokenizer.from_pretrained', 'Hasher.hash', 'self.assertEqual', 'self.assertNotEqual']",6
repos/datasets/tests/test_fingerprint.py:TokenizersHashTest:test_hash_tokenizer_with_cache,TokenizersHashTest:test_hash_tokenizer_with_cache,method,10,23,22,232,10.09,0,0,['self'],[None],[None],111,[],"['AutoTokenizer.from_pretrained', 'Hasher.hash', 'tokenizer', 'self.assertEqual']",4
repos/datasets/tests/test_fingerprint.py:UnpicklableCallable:__call__,UnpicklableCallable:__call__,method,2,8,8,61,7.62,0,1,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",54,[],['self.callable'],1
repos/datasets/tests/test_fingerprint.py:UnpicklableCallable:__getstate__,UnpicklableCallable:__getstate__,method,1,2,2,27,13.5,0,0,['self'],[None],[None],58,[],['pickle.PicklingError'],1
repos/datasets/tests/test_fingerprint.py:UnpicklableCallable:__init__,UnpicklableCallable:__init__,method,2,2,2,22,11.0,0,0,"['self', 'callable']","[None, None]","[None, None]",51,[],[],0
repos/datasets/tests/test_formatting.py:_gen_any_arrays,_gen_any_arrays,function,3,7,7,57,8.14,1,0,[],[],[],32,[],"['range', 'AnyArray']",2
repos/datasets/tests/test_formatting.py:any_arrays_dataset,any_arrays_dataset,function,2,2,2,53,26.5,0,0,[],[],[],38,[],['IterableDataset.from_generator'],1
repos/datasets/tests/test_formatting.py:arrow_table,arrow_table,function,2,9,9,73,8.11,0,0,[],[],[],929,[],[],0
repos/datasets/tests/test_formatting.py:test_iterable_dataset_of_arrays_format_to_arrow,test_iterable_dataset_of_arrays_format_to_arrow,function,3,9,9,110,12.22,0,0,['any_arrays_dataset'],[' IterableDataset'],[None],1001,[],"['any_arrays_dataset.with_format', 'all']",2
repos/datasets/tests/test_formatting.py:test_iterable_dataset_of_arrays_format_to_jax,test_iterable_dataset_of_arrays_format_to_jax,function,6,13,13,141,10.85,0,0,['any_arrays_dataset'],[' IterableDataset'],[None],1028,[],"['any_arrays_dataset.with_format', 'all']",2
repos/datasets/tests/test_formatting.py:test_iterable_dataset_of_arrays_format_to_numpy,test_iterable_dataset_of_arrays_format_to_numpy,function,3,9,9,118,13.11,0,0,['any_arrays_dataset'],[' IterableDataset'],[None],1006,[],"['any_arrays_dataset.with_format', 'all']",2
repos/datasets/tests/test_formatting.py:test_iterable_dataset_of_arrays_format_to_tf,test_iterable_dataset_of_arrays_format_to_tf,function,6,13,13,138,10.62,0,0,['any_arrays_dataset'],[' IterableDataset'],[None],1020,[],"['any_arrays_dataset.with_format', 'all']",2
repos/datasets/tests/test_formatting.py:test_iterable_dataset_of_arrays_format_to_torch,test_iterable_dataset_of_arrays_format_to_torch,function,5,11,11,135,12.27,0,0,['any_arrays_dataset'],[' IterableDataset'],[None],1012,[],"['any_arrays_dataset.with_format', 'all']",2
repos/datasets/tests/test_formatting.py:test_tf_formatter_sets_default_dtypes,test_tf_formatter_sets_default_dtypes,function,22,41,36,834,20.34,0,1,"['cast_schema', 'arrow_table']","[None, None]","[None, None]",943,[],"['arrow_table.cast', 'arrow_table.to_pydict', 'TFFormatter', 'formatter.format_row', 'formatter.format_column', 'formatter.format_batch']",6
repos/datasets/tests/test_formatting.py:test_torch_formatter_sets_default_dtypes,test_torch_formatter_sets_default_dtypes,function,21,39,34,821,21.05,0,1,"['cast_schema', 'arrow_table']","[None, None]","[None, None]",977,[],"['arrow_table.cast', 'arrow_table.to_pydict', 'TorchFormatter', 'formatter.format_row', 'torch.tensor', 'formatter.format_column', 'formatter.format_batch']",7
repos/datasets/tests/test_formatting.py:AnyArray,AnyArray,class,6,13,11,104,8.0,0,0,[],[],[],24,[],[],0
repos/datasets/tests/test_formatting.py:ArrowExtractorTest,ArrowExtractorTest,class,37,314,119,5873,18.7,0,0,[],[],[],54,[],[],0
repos/datasets/tests/test_formatting.py:FormatterTest,FormatterTest,class,71,942,260,15150,16.08,0,0,[],[],[],219,[],[],0
repos/datasets/tests/test_formatting.py:LazyDictTest,LazyDictTest,class,17,33,30,550,16.67,0,0,[],[],[],201,[],[],0
repos/datasets/tests/test_formatting.py:QueryTest,QueryTest,class,40,812,253,9775,12.04,2,0,[],[],[],639,[],[],0
repos/datasets/tests/test_formatting.py:AnyArray:__array__,AnyArray:__array__,method,2,2,2,27,13.5,0,0,['self'],[None],[None],28,[],['np.asarray'],1
repos/datasets/tests/test_formatting.py:AnyArray:__init__,AnyArray:__init__,method,2,2,2,14,7.0,0,0,"['self', 'data']","[None, None]","[None, None]",25,[],[],0
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:_create_dummy_table,ArrowExtractorTest:_create_dummy_table,method,2,9,9,73,8.11,0,0,['self'],[None],[None],55,[],[],0
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_numpy_extractor,ArrowExtractorTest:test_numpy_extractor,method,11,23,21,378,16.43,0,0,['self'],[None],[None],68,[],"['self._create_dummy_table', 'NumpyArrowExtractor', 'extractor.extract_row', 'extractor.extract_column', 'np.array', 'extractor.extract_batch']",6
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_numpy_extractor_nested,ArrowExtractorTest:test_numpy_extractor_nested,method,11,28,23,546,19.5,0,0,['self'],[None],[None],78,[],"['self._create_dummy_table', 'NumpyArrowExtractor', 'extractor.extract_row', 'self.assertEqual', 'extractor.extract_column', 'extractor.extract_batch']",6
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_numpy_extractor_temporal,ArrowExtractorTest:test_numpy_extractor_temporal,method,11,22,19,507,23.05,0,0,['self'],[None],[None],93,[],"['self._create_dummy_table', 'NumpyArrowExtractor', 'extractor.extract_row', 'self.assertTrue', 'extractor.extract_column', 'extractor.extract_batch']",6
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_pandas_extractor,ArrowExtractorTest:test_pandas_extractor,method,12,29,24,612,21.1,0,0,['self'],[None],[None],105,[],"['self._create_dummy_table', 'PandasArrowExtractor', 'extractor.extract_row', 'self.assertIsInstance', 'pd.Series', 'extractor.extract_column', 'extractor.extract_batch']",7
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_pandas_extractor_nested,ArrowExtractorTest:test_pandas_extractor_nested,method,11,28,23,550,19.64,0,0,['self'],[None],[None],119,[],"['self._create_dummy_table', 'PandasArrowExtractor', 'extractor.extract_row', 'self.assertEqual', 'extractor.extract_column', 'extractor.extract_batch']",6
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_pandas_extractor_temporal,ArrowExtractorTest:test_pandas_extractor_temporal,method,11,19,18,525,27.63,0,0,['self'],[None],[None],134,[],"['self._create_dummy_table', 'PandasArrowExtractor', 'extractor.extract_row', 'self.assertTrue', 'extractor.extract_column', 'extractor.extract_batch']",6
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_polars_extractor,ArrowExtractorTest:test_polars_extractor,method,17,42,32,641,15.26,0,0,['self'],[None],[None],147,[],"['self._create_dummy_table', 'PolarsArrowExtractor', 'extractor.extract_row', 'self.assertIsInstance', 'pl.Series', 'extractor.extract_column', 'extractor.extract_batch']",7
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_polars_nested,ArrowExtractorTest:test_polars_nested,method,16,36,31,726,20.17,0,0,['self'],[None],[None],166,[],"['self._create_dummy_table', 'PolarsArrowExtractor', 'extractor.extract_row', 'self.assertEqual', 'pl.List', 'extractor.extract_column', 'extractor.extract_batch']",7
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_polars_temporal,ArrowExtractorTest:test_polars_temporal,method,14,23,22,520,22.61,0,0,['self'],[None],[None],186,[],"['self._create_dummy_table', 'PolarsArrowExtractor', 'extractor.extract_row', 'self.assertTrue', 'extractor.extract_column', 'extractor.extract_batch']",6
repos/datasets/tests/test_formatting.py:ArrowExtractorTest:test_python_extractor,ArrowExtractorTest:test_python_extractor,method,11,30,26,362,12.07,0,0,['self'],[None],[None],58,[],"['self._create_dummy_table', 'PythonArrowExtractor', 'extractor.extract_row', 'self.assertEqual', 'extractor.extract_column', 'extractor.extract_batch']",6
repos/datasets/tests/test_formatting.py:FormatterTest:_create_dummy_table,FormatterTest:_create_dummy_table,method,2,7,7,62,8.86,0,0,['self'],[None],[None],55,[],[],0
repos/datasets/tests/test_formatting.py:FormatterTest:test_jax_formatter,FormatterTest:test_jax_formatter,method,21,56,40,757,13.52,0,0,['self'],[None],[None],531,[],"['self._create_dummy_table', 'JaxFormatter', 'formatter.format_row', 'jnp.allclose', 'jnp.array', 'formatter.format_column', 'formatter.format_batch', 'np.array']",8
repos/datasets/tests/test_formatting.py:FormatterTest:test_jax_formatter_audio,FormatterTest:test_jax_formatter_audio,method,16,29,27,479,16.52,0,0,['self'],[None],[None],606,[],"['pa.table', 'str', 'JaxFormatter', 'Audio', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch']",8
repos/datasets/tests/test_formatting.py:FormatterTest:test_jax_formatter_device,FormatterTest:test_jax_formatter_device,method,21,33,24,460,13.94,0,0,['self'],[None],[None],621,[],"['self._create_dummy_table', 'jax.devices', 'JaxFormatter', 'formatter.format_row', 'formatter.format_column', 'col.devices', 'formatter.format_batch']",7
repos/datasets/tests/test_formatting.py:FormatterTest:test_jax_formatter_image,FormatterTest:test_jax_formatter_image,method,17,87,49,1233,14.17,0,0,['self'],[None],[None],569,[],"['pa.table', 'str', 'JaxFormatter', 'Image', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch', 'self.assertIsInstance']",9
repos/datasets/tests/test_formatting.py:FormatterTest:test_jax_formatter_jnp_array_kwargs,FormatterTest:test_jax_formatter_jnp_array_kwargs,method,16,26,23,443,17.04,0,0,['self'],[None],[None],552,[],"['self._create_dummy_table', 'JaxFormatter', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch']",6
repos/datasets/tests/test_formatting.py:FormatterTest:test_numpy_formatter,FormatterTest:test_numpy_formatter,method,13,29,26,446,15.38,0,0,['self'],[None],[None],247,[],"['self._create_dummy_table', 'NumpyFormatter', 'formatter.format_row', 'np.array', 'formatter.format_column', 'formatter.format_batch']",6
repos/datasets/tests/test_formatting.py:FormatterTest:test_numpy_formatter_audio,FormatterTest:test_numpy_formatter_audio,method,11,21,21,435,20.71,0,0,['self'],[None],[None],304,[],"['pa.table', 'str', 'NumpyFormatter', 'Audio', 'formatter.format_row', 'self.assertEqual', 'np.dtype', 'formatter.format_column', 'formatter.format_batch']",9
repos/datasets/tests/test_formatting.py:FormatterTest:test_numpy_formatter_image,FormatterTest:test_numpy_formatter_image,method,12,83,43,1261,15.19,0,0,['self'],[None],[None],270,[],"['pa.table', 'str', 'NumpyFormatter', 'Image', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch', 'self.assertIsInstance']",9
repos/datasets/tests/test_formatting.py:FormatterTest:test_numpy_formatter_np_array_kwargs,FormatterTest:test_numpy_formatter_np_array_kwargs,method,11,18,17,407,22.61,0,0,['self'],[None],[None],258,[],"['self._create_dummy_table', 'NumpyFormatter', 'formatter.format_row', 'self.assertEqual', 'np.dtype', 'formatter.format_column', 'formatter.format_batch']",7
repos/datasets/tests/test_formatting.py:FormatterTest:test_pandas_formatter,FormatterTest:test_pandas_formatter,method,12,29,24,604,20.83,0,0,['self'],[None],[None],314,[],"['self._create_dummy_table', 'PandasFormatter', 'formatter.format_row', 'self.assertIsInstance', 'pd.Series', 'formatter.format_column', 'formatter.format_batch']",7
repos/datasets/tests/test_formatting.py:FormatterTest:test_polars_formatter,FormatterTest:test_polars_formatter,method,17,42,32,611,14.55,0,0,['self'],[None],[None],329,[],"['self._create_dummy_table', 'PolarsFormatter', 'formatter.format_row', 'self.assertIsInstance', 'pl.Series', 'formatter.format_column', 'formatter.format_batch']",7
repos/datasets/tests/test_formatting.py:FormatterTest:test_python_formatter,FormatterTest:test_python_formatter,method,11,26,23,329,12.65,0,0,['self'],[None],[None],223,[],"['self._create_dummy_table', 'PythonFormatter', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch']",6
repos/datasets/tests/test_formatting.py:FormatterTest:test_python_formatter_lazy,FormatterTest:test_python_formatter_lazy,method,10,24,24,439,18.29,0,0,['self'],[None],[None],233,[],"['self._create_dummy_table', 'PythonFormatter', 'formatter.format_row', 'self.assertIsInstance', 'self.assertEqual', 'formatter.format_batch']",6
repos/datasets/tests/test_formatting.py:FormatterTest:test_tf_formatter,FormatterTest:test_tf_formatter,method,18,48,42,1037,21.6,0,0,['self'],[None],[None],438,[],"['self._create_dummy_table', 'TFFormatter', 'formatter.format_row', 'tf.convert_to_tensor', 'formatter.format_column', 'formatter.format_batch', 'self.assertIsInstance', 'self.assertEqual']",8
repos/datasets/tests/test_formatting.py:FormatterTest:test_tf_formatter_audio,FormatterTest:test_tf_formatter_audio,method,16,29,27,474,16.34,0,0,['self'],[None],[None],516,[],"['pa.table', 'str', 'TFFormatter', 'Audio', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch']",8
repos/datasets/tests/test_formatting.py:FormatterTest:test_tf_formatter_image,FormatterTest:test_tf_formatter_image,method,17,87,48,1227,14.1,0,0,['self'],[None],[None],479,[],"['pa.table', 'str', 'TFFormatter', 'Image', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch', 'self.assertIsInstance']",9
repos/datasets/tests/test_formatting.py:FormatterTest:test_tf_formatter_tf_tensor_kwargs,FormatterTest:test_tf_formatter_tf_tensor_kwargs,method,16,26,23,436,16.77,0,0,['self'],[None],[None],462,[],"['self._create_dummy_table', 'TFFormatter', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch']",6
repos/datasets/tests/test_formatting.py:FormatterTest:test_torch_formatter,FormatterTest:test_torch_formatter,method,19,40,33,717,17.93,0,0,['self'],[None],[None],348,[],"['self._create_dummy_table', 'TorchFormatter', 'formatter.format_row', 'torch.tensor', 'formatter.format_column', 'formatter.format_batch', 'np.array']",7
repos/datasets/tests/test_formatting.py:FormatterTest:test_torch_formatter_audio,FormatterTest:test_torch_formatter_audio,method,15,27,25,480,17.78,0,0,['self'],[None],[None],423,[],"['pa.table', 'str', 'TorchFormatter', 'Audio', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch']",8
repos/datasets/tests/test_formatting.py:FormatterTest:test_torch_formatter_image,FormatterTest:test_torch_formatter_image,method,16,85,47,1242,14.61,0,0,['self'],[None],[None],385,[],"['pa.table', 'str', 'TorchFormatter', 'Image', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch', 'self.assertIsInstance']",9
repos/datasets/tests/test_formatting.py:FormatterTest:test_torch_formatter_torch_tensor_kwargs,FormatterTest:test_torch_formatter_torch_tensor_kwargs,method,15,24,21,448,18.67,0,0,['self'],[None],[None],368,[],"['self._create_dummy_table', 'TorchFormatter', 'formatter.format_row', 'self.assertEqual', 'formatter.format_column', 'formatter.format_batch']",6
repos/datasets/tests/test_formatting.py:LazyDictTest:_create_dummy_formatter,LazyDictTest:_create_dummy_formatter,method,2,2,2,32,16.0,0,0,['self'],[None],[None],205,[],['PythonFormatter'],1
repos/datasets/tests/test_formatting.py:LazyDictTest:_create_dummy_table,LazyDictTest:_create_dummy_table,method,2,7,7,62,8.86,0,0,['self'],[None],[None],55,[],[],0
repos/datasets/tests/test_formatting.py:LazyDictTest:test_lazy_dict_copy,LazyDictTest:test_lazy_dict_copy,method,11,18,18,360,20.0,0,0,['self'],[None],[None],208,[],"['self._create_dummy_table', 'self._create_dummy_formatter', 'formatter.format_batch', 'lazy_batch.copy', 'self.assertEqual', 'type', 'lazy_batch_copy.items', 'self.assertNotEqual']",8
repos/datasets/tests/test_formatting.py:QueryTest:_create_dummy_arrow_indices,QueryTest:_create_dummy_arrow_indices,method,2,4,4,83,20.75,0,0,['self'],[None],[None],643,[],[],0
repos/datasets/tests/test_formatting.py:QueryTest:_create_dummy_table,QueryTest:_create_dummy_table,method,2,7,7,62,8.86,0,0,['self'],[None],[None],55,[],[],0
repos/datasets/tests/test_formatting.py:QueryTest:assertTableEqual,QueryTest:assertTableEqual,method,4,12,12,166,13.83,1,0,"['self', 'first', 'second']","[None, ' pa.Table', ' pa.Table']","[None, None, None]",646,[],"['self.assertEqual', 'zip']",2
repos/datasets/tests/test_formatting.py:QueryTest:test_query_table_indexable_type,QueryTest:test_query_table_indexable_type,method,12,69,45,1018,14.75,0,0,['self'],[None],[None],882,[],"['self._create_dummy_table', 'InMemoryTable', 'query_table', 'np.int64', 'self.assertTableEqual', 'self.assertRaises', 'len']",7
repos/datasets/tests/test_formatting.py:QueryTest:test_query_table_int,QueryTest:test_query_table_int,method,12,69,45,948,13.74,0,0,['self'],[None],[None],652,[],"['self._create_dummy_table', 'InMemoryTable', 'query_table', 'self.assertTableEqual', 'self.assertRaises', 'len']",6
repos/datasets/tests/test_formatting.py:QueryTest:test_query_table_invalid_key_type,QueryTest:test_query_table_invalid_key_type,method,9,30,21,363,12.1,1,0,['self'],[None],[None],909,[],"['self._create_dummy_table', 'InMemoryTable', 'self.assertRaises', 'query_table', 'iter_to_inf']",5
repos/datasets/tests/test_formatting.py:QueryTest:test_query_table_iterable,QueryTest:test_query_table_iterable,method,18,138,71,1630,11.81,0,0,['self'],[None],[None],829,[],"['self._create_dummy_table', 'InMemoryTable', 'np.array', 'query_table', 'self.assertTableEqual', 'len', 'self.assertRaises']",7
repos/datasets/tests/test_formatting.py:QueryTest:test_query_table_range,QueryTest:test_query_table_range,method,19,231,94,2472,10.7,0,0,['self'],[None],[None],734,[],"['self._create_dummy_table', 'InMemoryTable', 'np.array', 'query_table', 'range', 'self.assertTableEqual', 'np_A[range', 'np_B[range', 'np_C[range', 'len', 'self.assertRaises']",11
repos/datasets/tests/test_formatting.py:QueryTest:test_query_table_slice,QueryTest:test_query_table_slice,method,10,201,91,2236,11.12,0,0,['self'],[None],[None],679,[],"['self._create_dummy_table', 'InMemoryTable', 'query_table', 'slice', 'self.assertTableEqual', 'len', 'pa_table.slice']",7
repos/datasets/tests/test_formatting.py:QueryTest:test_query_table_str,QueryTest:test_query_table_str,method,10,27,22,412,15.26,0,0,['self'],[None],[None],818,[],"['self._create_dummy_table', 'InMemoryTable', 'query_table', 'self.assertTableEqual', 'self.assertRaises']",5
repos/datasets/tests/test_hf_gcp.py:list_datasets_on_hf_gcp_parameters,list_datasets_on_hf_gcp_parameters,function,14,47,31,497,10.57,2,4,"['with_config', 'with_revision']","[None, None]","['True', 'True']",42,[],"['columns.append', 'get_testcase_name']",2
repos/datasets/tests/test_hf_gcp.py:test_as_dataset_from_hf_gcs,test_as_dataset_from_hf_gcs,function,8,14,13,264,18.86,0,0,['tmp_path_factory'],[None],[None],89,[],"['tmp_path_factory.mktemp', 'load_dataset_builder', 'builder.download_and_prepare', 'builder.as_dataset']",4
repos/datasets/tests/test_hf_gcp.py:test_as_streaming_dataset_from_hf_gcs,test_as_streaming_dataset_from_hf_gcs,function,6,23,18,307,13.35,0,0,['tmp_path'],[None],[None],100,[],"['load_dataset_builder', 'builder.as_streaming_dataset', 'isinstance', 'next']",4
repos/datasets/tests/test_hf_gcp.py:TestDatasetOnHfGcp,TestDatasetOnHfGcp,class,13,35,32,497,14.2,0,0,[],[],[],63,[],[],0
repos/datasets/tests/test_hf_gcp.py:TestDatasetOnHfGcp:test_dataset_info_available,TestDatasetOnHfGcp:test_dataset_info_available,method,9,24,23,386,16.08,0,0,"['self', 'dataset', 'config_name', 'revision']","[None, None, None, None]","[None, None, None, None]",68,[],"['TemporaryDirectory', 'load_dataset_builder', 'builder._relative_data_dir', 'cached_path', 'self.assertTrue']",5
repos/datasets/tests/test_hub.py:NewDataset,NewDataset,class,5,29,26,424,14.62,1,0,[],[],[],19,[],[],0
repos/datasets/tests/test_hub.py:NewDataset:_generate_examples,NewDataset:_generate_examples,method,3,9,9,65,7.22,1,0,['self'],[None],[None],34,[],['range'],1
repos/datasets/tests/test_hub.py:NewDataset:_info,NewDataset:_info,method,2,4,4,90,22.5,0,0,['self'],[None],[None],26,[],"['datasets.DatasetInfo', 'datasets.Value']",2
repos/datasets/tests/test_hub.py:NewDataset:_split_generators,NewDataset:_split_generators,method,1,2,2,58,29.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",31,[],[],0
repos/datasets/tests/test_info.py:test_dataset_info_dump_and_reload,test_dataset_info_dump_and_reload,function,7,11,10,204,18.55,0,0,"['tmp_path', 'dataset_info']","[None, ' DatasetInfo']","[None, None]",51,[],"['str', 'dataset_info.write_to_directory', 'DatasetInfo.from_directory']",3
repos/datasets/tests/test_info.py:test_dataset_info_to_yaml_dict,test_dataset_info_to_yaml_dict,function,24,52,45,845,16.25,1,0,[],[],[],59,[],"['DatasetInfo', 'features=Features', 'Value', 'dataset_info._to_yaml_dict', 'sorted', 'isinstance', 'yaml.safe_dump', 'yaml.safe_load']",8
repos/datasets/tests/test_info.py:test_dataset_info_to_yaml_dict_empty,test_dataset_info_to_yaml_dict_empty,function,4,7,6,111,15.86,0,0,[],[],[],89,[],"['DatasetInfo', 'dataset_info._to_yaml_dict']",2
repos/datasets/tests/test_info.py:test_dataset_infos_dict_dump_and_reload,test_dataset_infos_dict_dump_and_reload,function,14,22,21,419,19.05,1,1,"['tmp_path', 'dataset_infos_dict']","[None, ' DatasetInfosDict']","[None, None]",122,[],"['str', 'dataset_infos_dict.write_to_directory', 'DatasetInfosDict.from_directory', 'dataset_infos_dict.items', 'DatasetInfo._from_yaml_dict']",5
repos/datasets/tests/test_info.py:test_from_dir,test_from_dir,function,8,46,28,538,11.7,0,3,"['files', 'tmp_path_factory']","[None, None]","[None, None]",19,[],"['tmp_path_factory.mktemp', 'open', 'f.write', 'DatasetInfosDict.from_directory']",4
repos/datasets/tests/test_info.py:test_from_merge_same_dataset_infos,test_from_merge_same_dataset_infos,function,6,31,21,322,10.39,0,2,['dataset_info'],[None],[None],156,[],"['range', 'DatasetInfo.from_merge', 'DatasetInfo']",3
repos/datasets/tests/test_info_utils.py:test_is_small_dataset,test_is_small_dataset,function,9,34,21,459,13.5,0,3,"['dataset_size', 'input_in_memory_max_size', 'monkeypatch']","[None, None, None]","[None, None, None]",9,[],"['monkeypatch.setattr', 'is_small_dataset']",2
repos/datasets/tests/test_inspect.py:test_get_dataset_config_info,test_get_dataset_config_info,function,4,9,8,142,15.78,0,0,"['path', 'config_name', 'expected_splits']","[None, None, None]","[None, None, None]",46,[],"['get_dataset_config_info', 'list']",2
repos/datasets/tests/test_inspect.py:test_get_dataset_config_info_error,test_get_dataset_config_info_error,function,3,4,4,92,23.0,0,0,"['path', 'config_name', 'expected_exception']","[None, None, None]","[None, None, None]",63,[],"['pytest.raises', 'get_dataset_config_info']",2
repos/datasets/tests/test_inspect.py:test_get_dataset_config_info_private,test_get_dataset_config_info_private,function,2,7,7,141,20.14,0,0,"['hf_token', 'hf_private_dataset_repo_txt_data']","[None, None]","[None, None]",52,[],"['get_dataset_config_info', 'list']",2
repos/datasets/tests/test_inspect.py:test_get_dataset_config_names,test_get_dataset_config_names,function,3,5,4,72,14.4,0,0,"['path', 'expected']","[None, None]","[None, None]",81,[],['get_dataset_config_names'],1
repos/datasets/tests/test_inspect.py:test_get_dataset_default_config_name,test_get_dataset_default_config_name,function,3,12,9,143,11.92,0,1,"['path', 'expected']","[None, None]","[None, None]",99,[],['get_dataset_default_config_name'],1
repos/datasets/tests/test_inspect.py:test_get_dataset_info,test_get_dataset_info,function,7,19,15,269,14.16,0,0,"['path', 'expected_configs', 'expected_splits_in_first_config']","[None, None, None]","[None, None, None]",115,[],"['get_dataset_infos', 'list']",2
repos/datasets/tests/test_inspect.py:test_get_dataset_split_names,test_get_dataset_split_names,function,6,14,12,174,12.43,0,0,"['path', 'expected_config', 'expected_splits']","[None, None, None]","[None, None, None]",133,[],"['get_dataset_infos', 'list']",2
repos/datasets/tests/test_inspect.py:test_get_dataset_split_names_error,test_get_dataset_split_names_error,function,3,4,4,92,23.0,0,0,"['path', 'config_name', 'expected_exception']","[None, None, None]","[None, None, None]",147,[],"['pytest.raises', 'get_dataset_split_names']",2
repos/datasets/tests/test_inspect.py:test_inspect_dataset,test_inspect_dataset,function,4,9,8,104,11.56,0,0,"['path', 'tmp_path']","[None, None]","[None, None]",22,[],"['inspect_dataset', 'Path', 'os.listdir']",3
repos/datasets/tests/test_inspect.py:test_inspect_metric,test_inspect_metric,function,4,14,11,137,9.79,0,0,"['path', 'tmp_path']","[None, None]","[None, None]",31,[],"['inspect_metric', 'os.listdir']",2
repos/datasets/tests/test_iterable_dataset.py:arrow_file,arrow_file,function,4,6,5,144,24.0,0,0,"['tmp_path_factory', 'dataset']","[None, ' IterableDataset']","[None, None]",112,[],"['str', 'Dataset.from_generator']",2
repos/datasets/tests/test_iterable_dataset.py:dataset,dataset,function,4,7,7,140,20.0,0,0,[],[],[],97,[],"['ExamplesIterable', 'IterableDataset', 'info=DatasetInfo']",3
repos/datasets/tests/test_iterable_dataset.py:dataset_with_several_columns,dataset_with_several_columns,function,4,15,15,235,15.67,0,0,[],[],[],103,[],"['ExamplesIterable', 'IterableDataset', 'info=DatasetInfo']",3
repos/datasets/tests/test_iterable_dataset.py:filter_func,filter_func,function,2,3,3,20,6.67,0,0,['batch'],[None],[None],1923,[],[],0
repos/datasets/tests/test_iterable_dataset.py:generate_examples_fn,generate_examples_fn,function,10,30,26,247,8.23,2,1,['**kwargs'],[None],[None],66,[],"['kwargs.copy', 'kwargs.pop', 'range']",3
repos/datasets/tests/test_iterable_dataset.py:generate_tables_fn,generate_tables_fn,function,17,49,40,470,9.59,2,2,['**kwargs'],[None],[None],77,[],"['kwargs.copy', 'kwargs.pop', 'range', 'buffer.append', 'len']",5
repos/datasets/tests/test_iterable_dataset.py:map_func,map_func,function,3,5,5,26,5.2,0,0,['batch'],[None],[None],1927,[],[],0
repos/datasets/tests/test_iterable_dataset.py:test_arrow_examples_iterable,test_arrow_examples_iterable,function,8,27,20,311,11.52,1,0,[],[],[],227,[],"['ArrowExamplesIterable', 'sum', 'generate_tables_fn', 'next', 'list']",5
repos/datasets/tests/test_iterable_dataset.py:test_arrow_examples_iterable_shuffle_data_sources,test_arrow_examples_iterable_shuffle_data_sources,function,11,36,29,442,12.28,1,0,[],[],[],248,[],"['ArrowExamplesIterable', 'ex_iterable.shuffle_data_sources', 'sum', 'generate_tables_fn', 'list']",5
repos/datasets/tests/test_iterable_dataset.py:test_arrow_examples_iterable_with_kwargs,test_arrow_examples_iterable_with_kwargs,function,9,52,32,507,9.75,1,0,[],[],[],236,[],"['ArrowExamplesIterable', 'sum', 'generate_tables_fn', 'all', 'sorted', 'list']",6
repos/datasets/tests/test_iterable_dataset.py:test_batch_arrow_tables,test_batch_arrow_tables,function,9,71,43,692,9.75,0,2,"['tables', 'batch_size', 'drop_last_batch']","[None, None, None]","[None, None, None]",160,[],"['pa.concat_tables', 'len', 'list', '_batch_arrow_tables', 'all', 'full_table.slice', 'reloaded.to_pydict']",7
repos/datasets/tests/test_iterable_dataset.py:test_buffer_shuffled_examples_iterable,test_buffer_shuffled_examples_iterable,function,28,64,53,985,15.39,1,0,['seed'],[None],[None],260,[],"['ExamplesIterable', 'BufferShuffledExamplesIterable', 'deepcopy', 'list', 'islice', 'all', 'enumerate', 'expected.append', 'rest.pop', 'rng.shuffle', 'next', 'sorted']",12
repos/datasets/tests/test_iterable_dataset.py:test_concatenate_datasets,test_concatenate_datasets,function,8,19,17,333,17.53,0,0,[],[],[],1660,[],"['ExamplesIterable', 'IterableDataset', 'concatenate_datasets', 'list']",4
repos/datasets/tests/test_iterable_dataset.py:test_concatenate_datasets_axis_1,test_concatenate_datasets_axis_1,function,10,40,37,492,12.3,0,0,[],[],[],1694,[],"['ExamplesIterable', 'IterableDataset', 'pytest.raises', 'concatenate_datasets', 'dataset2.remove_columns', 'list', 'zip']",7
repos/datasets/tests/test_iterable_dataset.py:test_concatenate_datasets_axis_1_resolves_features,test_concatenate_datasets_axis_1_resolves_features,function,10,26,24,414,15.92,0,0,[],[],[],1705,[],"['ExamplesIterable', 'IterableDataset', 'concatenate_datasets', 'sorted']",4
repos/datasets/tests/test_iterable_dataset.py:test_concatenate_datasets_axis_1_with_different_lengths,test_concatenate_datasets_axis_1_with_different_lengths,function,11,54,42,638,11.81,0,0,[],[],[],1715,[],"['ExamplesIterable', 'IterableDataset', 'list', 'concatenate_datasets', 'zip']",5
repos/datasets/tests/test_iterable_dataset.py:test_concatenate_datasets_resolves_features,test_concatenate_datasets_resolves_features,function,10,24,21,374,15.58,0,0,[],[],[],1669,[],"['ExamplesIterable', 'IterableDataset', 'concatenate_datasets', 'sorted']",4
repos/datasets/tests/test_iterable_dataset.py:test_concatenate_datasets_with_different_columns,test_concatenate_datasets_with_different_columns,function,9,33,27,525,15.91,0,0,[],[],[],1679,[],"['ExamplesIterable', 'IterableDataset', 'concatenate_datasets', 'list']",4
repos/datasets/tests/test_iterable_dataset.py:test_convert_to_arrow,test_convert_to_arrow,function,16,80,49,739,9.24,1,2,"['batch_size', 'drop_last_batch']","[None, None]","[None, None]",127,[],"['range', 'len', 'list', '_convert_to_arrow', 'all', 'pa.concat_tables', 'full_table.slice', 'reloaded.to_pydict']",8
repos/datasets/tests/test_iterable_dataset.py:test_cycling_multi_sources_examples_iterable,test_cycling_multi_sources_examples_iterable,function,9,41,36,497,12.12,0,0,[],[],[],293,[],"['ExamplesIterable', 'CyclingMultiSourcesExamplesIterable', 'list', 'generate_examples_fn', 'next', 'all', 'enumerate']",7
repos/datasets/tests/test_iterable_dataset.py:test_examples_iterable,test_examples_iterable,function,5,15,13,203,13.53,0,0,[],[],[],185,[],"['ExamplesIterable', 'list', 'next']",3
repos/datasets/tests/test_iterable_dataset.py:test_examples_iterable_shuffle_data_sources,test_examples_iterable_shuffle_data_sources,function,4,17,15,273,16.06,0,0,[],[],[],201,[],"['ExamplesIterable', 'ex_iterable.shuffle_data_sources', 'list']",3
repos/datasets/tests/test_iterable_dataset.py:test_examples_iterable_shuffle_shards_and_metadata,test_examples_iterable_shuffle_shards_and_metadata,function,10,66,50,567,8.59,1,0,[],[],[],208,[],"['gen', 'enumerate', 'ExamplesIterable', 'range', 'str', 'ex_iterable.shuffle_data_sources', 'list']",7
repos/datasets/tests/test_iterable_dataset.py:test_examples_iterable_with_kwargs,test_examples_iterable_with_kwargs,function,5,32,22,324,10.12,0,0,[],[],[],193,[],"['ExamplesIterable', 'list', 'all', 'sorted']",4
repos/datasets/tests/test_iterable_dataset.py:test_filtered_examples_iterable,test_filtered_examples_iterable,function,24,77,52,678,8.81,4,4,"['n', 'func', 'batched', 'batch_size']","[None, None, None, None]","[None, None, None, None]",863,[],"['ExamplesIterable', 'FilteredExamplesIterable', 'generate_examples_fn', 'func', 'len', 'range', '_examples_to_batch', 'expected.extend', 'zip', 'next']",10
repos/datasets/tests/test_iterable_dataset.py:test_filtered_examples_iterable_input_columns,test_filtered_examples_iterable_input_columns,function,27,93,63,842,9.05,4,4,"['n', 'func', 'batched', 'batch_size', 'input_columns']","[None, None, None, None, None]","[None, None, None, None, None]",927,[],"['ExamplesIterable', 'FilteredExamplesIterable', 'generate_examples_fn', 'isinstance', 'func', 'len', 'range', '_examples_to_batch', 'expected.extend', 'zip', 'next']",11
repos/datasets/tests/test_iterable_dataset.py:test_filtered_examples_iterable_with_indices,test_filtered_examples_iterable_with_indices,function,27,85,61,774,9.11,4,2,"['n', 'func', 'batched', 'batch_size']","[None, None, None, None]","[None, None, None, None]",894,[],"['ExamplesIterable', 'FilteredExamplesIterable', 'generate_examples_fn', 'enumerate', 'func', 'len', 'range', '_examples_to_batch', 'list', 'expected.extend', 'zip', 'next']",12
repos/datasets/tests/test_iterable_dataset.py:test_formatted_map,test_formatted_map,function,6,32,17,542,16.94,0,0,['dataset'],[' IterableDataset'],[None],1885,[],"['dataset.with_format', 'isinstance', 'add_one_numpy', 'dataset.map']",4
repos/datasets/tests/test_iterable_dataset.py:test_from_spark_streaming,test_from_spark_streaming,function,13,67,51,527,7.87,1,0,[],[],[],1146,[],"['spark.createDataFrame', 'IterableDataset.from_spark', 'isinstance', 'results.append']",4
repos/datasets/tests/test_iterable_dataset.py:test_from_spark_streaming_features,test_from_spark_streaming_features,function,16,44,42,521,11.84,1,0,[],[],[],1173,[],"['np.arange', 'spark.createDataFrame', 'Features', 'Value', 'Image', 'IterableDataset.from_spark', 'isinstance', 'results.append', 'len']",9
repos/datasets/tests/test_iterable_dataset.py:test_horizontally_concatenated_examples_iterable,test_horizontally_concatenated_examples_iterable,function,12,65,55,812,12.49,2,0,[],[],[],1004,[],"['ExamplesIterable', 'HorizontallyConcatenatedMultiSourcesExamplesIterable', 'pytest.raises', 'list', 'MappedExamplesIterable', 'zip', 'concatenated_ex_iterable.shuffle_data_sources']",7
repos/datasets/tests/test_iterable_dataset.py:test_interleave_dataset_with_sharding,test_interleave_dataset_with_sharding,function,18,66,48,828,12.55,0,0,"['n_shards1', 'n_shards2', 'num_workers']","[None, None, None]","[None, None, None]",1903,[],"['ExamplesIterable', 'range', 'IterableDataset', 'interleave_datasets', 'min', 'DataLoader', 'list', 'len']",8
repos/datasets/tests/test_iterable_dataset.py:test_interleave_datasets,test_interleave_datasets,function,32,121,84,1429,11.81,1,5,"['dataset', 'probas', 'seed', 'expected_length', 'stopping_strategy']","[' IterableDataset', None, None, None, None]","[None, None, None, None, None]",1747,[],"['dataset.map', 'dataset.with_format', 'interleave_datasets', 'fill_default', 'isinstance', 'list', 'Features', 'Value', 'next', 'len', 'any', 'np.array', 'RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices', 'bool_strategy_func']",14
repos/datasets/tests/test_iterable_dataset.py:test_interleave_datasets_with_features,test_interleave_datasets_with_features,function,10,23,22,354,15.39,0,0,"['dataset', '']","[' IterableDataset', None]","[None, None]",1794,[],"['Features', 'Value', 'ClassLabel', 'ExamplesIterable', 'IterableDataset', 'info=DatasetInfo', 'interleave_datasets']",7
repos/datasets/tests/test_iterable_dataset.py:test_interleave_datasets_with_oversampling,test_interleave_datasets_with_oversampling,function,8,112,54,667,5.96,2,0,[],[],[],1810,[],"['IterableDataset', 'interleave_datasets']",2
repos/datasets/tests/test_iterable_dataset.py:test_iter_arrow,test_iter_arrow,function,5,11,10,117,10.64,0,0,['ex_iterable'],[' _BaseExamplesIterable'],[None],1085,[],"['next', 'isinstance']",2
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset,test_iterable_dataset,function,7,16,14,178,11.12,1,0,[],[],[],1098,[],"['IterableDataset', 'generate_examples_fn', 'next', 'list']",4
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_add_column,test_iterable_dataset_add_column,function,4,23,20,321,13.96,0,0,['dataset_with_several_columns'],[None],[None],1526,[],"['list', 'dataset_with_several_columns.add_column', 'enumerate', 'new_dataset._resolve_features']",4
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_cast,test_iterable_dataset_cast,function,9,27,24,391,14.48,0,0,[],[],[],1616,[],"['ExamplesIterable', 'Features', 'Value', 'IterableDataset', 'info=DatasetInfo', 'dataset.cast', 'list']",7
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_cast_column,test_iterable_dataset_cast_column,function,12,27,27,414,15.33,0,0,[],[],[],1606,[],"['ExamplesIterable', 'Features', 'Value', 'IterableDataset', 'info=DatasetInfo', 'dataset.cast_column', 'features.copy', 'list']",8
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_features,test_iterable_dataset_features,function,9,27,20,272,10.07,1,1,['features'],[None],[None],1455,[],"['ExamplesIterable', 'IterableDataset', 'info=DatasetInfo', 'list']",4
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_features_cast_to_python,test_iterable_dataset_features_cast_to_python,function,7,37,31,392,10.59,0,0,[],[],[],1465,[],"['ExamplesIterable', 'pd.Timestamp', 'np.ones', 'Features', 'Value', 'IterableDataset', 'info=DatasetInfo', 'list']",8
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_filter,test_iterable_dataset_filter,function,5,18,17,187,10.39,0,0,['dataset'],[' IterableDataset'],[None],1401,[],"['dataset.filter', 'next']",2
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_from_file,test_iterable_dataset_from_file,function,10,18,15,364,20.22,0,0,"['dataset', 'arrow_file']","[' IterableDataset', ' str']","[None, None]",1133,[],"['assert_arrow_memory_doesnt_increase', 'IterableDataset.from_file', 'dataset._resolve_features', 'isinstance', 'list']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_from_generator,test_iterable_dataset_from_generator,function,6,40,29,292,7.3,0,0,[],[],[],1105,[],"['gen', 'IterableDataset.from_generator', 'isinstance', 'list']",4
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_from_generator_with_shards,test_iterable_dataset_from_generator_with_shards,function,10,31,26,327,10.55,3,0,[],[],[],1121,[],"['gen', 'range', 'IterableDataset.from_generator', 'isinstance', 'len']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_from_hub_torch_dataloader_parallel,test_iterable_dataset_from_hub_torch_dataloader_parallel,function,8,18,18,253,14.06,0,0,"['num_workers', 'tmp_path']","[None, None]","[None, None]",1261,[],"['load_dataset', 'cache_dir=str', 'DataLoader', 'list', 'len']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_info,test_iterable_dataset_info,function,13,22,19,331,15.05,0,0,[],[],[],1285,[],"['DatasetInfo', 'ExamplesIterable', 'IterableDataset']",3
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_is_torch_iterable_dataset,test_iterable_dataset_is_torch_iterable_dataset,function,10,15,14,190,12.67,0,0,['dataset'],[' IterableDataset'],[None],1487,[],"['DataLoader', 'list', 'len']",3
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_iter_batch,test_iterable_dataset_iter_batch,function,13,43,36,493,11.47,2,1,"['batch_size', 'drop_last_batch']","[None, None]","[None, None]",1272,[],"['IterableDataset', 'generate_examples_fn', 'range', 'len', 'expected.append', 'next', 'list']",7
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_map,test_iterable_dataset_map,function,9,26,22,345,13.27,0,0,"['dataset', '']","[' IterableDataset', None]","[None, None]",1319,[],"['dataset.map', 'isinstance', 'next']",3
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_map_batched,test_iterable_dataset_map_batched,function,10,35,32,323,9.23,0,0,"['dataset', '']","[' IterableDataset', None]","[None, None]",1330,[],"['dataset.map', 'isinstance', 'next']",3
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_map_complex_features,test_iterable_dataset_map_complex_features,function,16,56,46,578,10.32,1,0,"['dataset', '']","[' IterableDataset', None]","[None, None]",1342,[],"['ExamplesIterable', 'Features', 'Value', 'IterableDataset', 'info=DatasetInfo', 'dataset.cast_column', 'ClassLabel', 'dataset.map', 'isinstance', 'ex.items', 'features.encode_example']",11
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_map_with_features,test_iterable_dataset_map_with_features,function,10,49,29,594,12.12,0,0,['dataset'],[' IterableDataset'],[None],1363,[],"['ExamplesIterable', 'Features', 'Value', 'IterableDataset', 'info=DatasetInfo', 'dataset.map']",6
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_map_with_fn_kwargs,test_iterable_dataset_map_with_fn_kwargs,function,8,51,38,509,9.98,0,0,['dataset'],[' IterableDataset'],[None],1387,[],"['dataset.map', 'next', 'isinstance']",3
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_remove_columns,test_iterable_dataset_remove_columns,function,6,89,32,766,8.61,0,0,['dataset_with_several_columns'],[None],[None],1567,[],"['dataset_with_several_columns.remove_columns', 'list', 'example.items', 'dataset_with_several_columns._resolve_features', 'all']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_rename_column,test_iterable_dataset_rename_column,function,5,53,28,507,9.57,0,0,['dataset_with_several_columns'],[None],[None],1536,[],"['dataset_with_several_columns.rename_column', 'list', 'example.items', 'dataset_with_several_columns._resolve_features']",4
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_rename_columns,test_iterable_dataset_rename_columns,function,7,62,32,613,9.89,0,0,['dataset_with_several_columns'],[None],[None],1551,[],"['dataset_with_several_columns.rename_columns', 'list', 'example.items', 'dataset_with_several_columns._resolve_features', 'all']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_resolve_features,test_iterable_dataset_resolve_features,function,8,26,18,284,10.92,0,0,[],[],[],1625,[],"['ExamplesIterable', 'IterableDataset', 'dataset._resolve_features', 'Features', 'Value']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_resolve_features_keep_order,test_iterable_dataset_resolve_features_keep_order,function,8,26,22,238,9.15,0,0,[],[],[],1639,[],"['gen', 'zip', 'ExamplesIterable', 'IterableDataset', 'list']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_select_columns,test_iterable_dataset_select_columns,function,6,82,31,720,8.78,0,0,['dataset_with_several_columns'],[None],[None],1587,[],"['dataset_with_several_columns.select_columns', 'list', 'example.items', 'dataset_with_several_columns._resolve_features', 'all']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_set_epoch,test_iterable_dataset_set_epoch,function,2,7,5,70,10.0,0,0,['dataset'],[' IterableDataset'],[None],1295,[],['dataset.set_epoch'],1
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_set_epoch_of_shuffled_dataset,test_iterable_dataset_set_epoch_of_shuffled_dataset,function,13,44,31,623,14.16,0,2,"['dataset', 'seed', 'epoch']","[' IterableDataset', None, None]","[None, None, None]",1303,[],"['dataset.shuffle', 'shuffled_dataset.set_epoch', 'shuffled_dataset._effective_generator', 'is_rng_equal', 'deepcopy']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_shuffle,test_iterable_dataset_shuffle,function,17,59,49,1030,17.46,0,1,"['dataset', 'seed', 'epoch']","[' IterableDataset', None, None]","[None, None, None]",1410,[],"['deepcopy', 'dataset.shuffle', 'isinstance', 'is_rng_equal', 'dataset.set_epoch', 'next', 'iter', 'ExamplesIterable', 'list']",9
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_shuffle_after_skip_or_take,test_iterable_dataset_shuffle_after_skip_or_take,function,15,42,39,421,10.02,0,0,['method'],[None],[None],1513,[],"['ExamplesIterable', 'range', 'IterableDataset', 'dataset.skip', 'dataset.take', 'dataset.shuffle', 'sorted']",7
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_skip,test_iterable_dataset_skip,function,5,11,9,174,15.82,0,0,"['dataset', 'n']","[' IterableDataset', None]","[None, None]",1497,[],"['dataset.skip', 'isinstance', 'list']",3
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_take,test_iterable_dataset_take,function,5,11,9,174,15.82,0,0,"['dataset', 'n']","[' IterableDataset', None]","[None, None]",1505,[],"['dataset.take', 'isinstance', 'list']",3
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_torch_dataloader_parallel,test_iterable_dataset_torch_dataloader_parallel,function,14,36,29,340,9.44,1,0,[],[],[],1231,[],"['ExamplesIterable', 'IterableDataset', 'DataLoader', 'list', 'len']",5
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_torch_integration,test_iterable_dataset_torch_integration,function,8,17,13,254,14.94,0,0,[],[],[],1194,[],"['ExamplesIterable', 'IterableDataset', 'isinstance']",3
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_torch_picklable,test_iterable_dataset_torch_picklable,function,11,24,19,457,19.04,0,0,[],[],[],1205,[],"['ExamplesIterable', 'IterableDataset', 'formatting=FormattingConfig', 'pickle.loads', 'isinstance', 'len']",6
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_with_features_fill_with_none,test_iterable_dataset_with_features_fill_with_none,function,9,30,26,268,8.93,0,0,[],[],[],1650,[],"['gen', 'zip', 'ExamplesIterable', 'DatasetInfo', 'Value', 'IterableDataset', 'list']",7
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_with_format,test_iterable_dataset_with_format,function,4,5,5,139,27.8,0,0,"['dataset', 'format_type']","[' IterableDataset', None]","[None, None]",1481,[],"['dataset.with_format', 'get_format_type_from_alias']",2
repos/datasets/tests/test_iterable_dataset.py:test_iterable_dataset_with_format_torch,test_iterable_dataset_with_format_torch,function,9,14,14,210,15.0,0,0,[],[],[],1221,[],"['ExamplesIterable', 'IterableDataset', 'DataLoader', 'len']",4
repos/datasets/tests/test_iterable_dataset.py:test_map_array_are_not_converted_back_to_lists,test_map_array_are_not_converted_back_to_lists,function,8,14,14,155,11.07,0,0,['dataset'],[' IterableDataset'],[None],1875,[],"['func', 'np.array', 'dataset.map', 'next', 'isinstance']",5
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable,test_mapped_examples_iterable,function,26,71,52,855,12.04,3,2,"['n', 'func', 'batched', 'batch_size']","[None, None, None, None]","[None, None, None, None]",351,[],"['ExamplesIterable', 'MappedExamplesIterable', 'generate_examples_fn', 'len', 'range', '_examples_to_batch', 'func', 'all_transformed_examples.extend', 'expected.update', 'list', 'next']",11
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_arrow_format,test_mapped_examples_iterable_arrow_format,function,22,66,48,720,10.91,3,2,"['n', 'func', 'batched', 'batch_size']","[None, None, None, None]","[None, None, None, None]",596,[],"['ExamplesIterable', 'MappedExamplesIterable', 'formatting=FormattingConfig', 'generate_examples_fn', 'len', 'range', 'expected.extend', 'next']",8
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_drop_last_batch,test_mapped_examples_iterable_drop_last_batch,function,32,110,79,1144,10.4,3,6,"['n', 'func', 'batched', 'batch_size']","[None, None, None, None]","[None, None, None, None]",387,[],"['ExamplesIterable', 'MappedExamplesIterable', 'generate_examples_fn', 'len', 'range', '_examples_to_batch', 'func', 'all_transformed_examples.extend', 'expected.update', 'list', 'next', 'pytest.raises']",12
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_drop_last_batch_and_arrow_format,test_mapped_examples_iterable_drop_last_batch_and_arrow_format,function,33,122,89,1147,9.4,3,6,"['n', 'func', 'batched', 'batch_size']","[None, None, None, None]","[None, None, None, None]",633,[],"['ExamplesIterable', 'MappedExamplesIterable', 'formatting=FormattingConfig', 'generate_examples_fn', 'len', 'range', 'func', 'all_transformed_examples.extend', 'out.to_pylist', 'next', 'pytest.raises']",11
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_fn_kwargs,test_mapped_examples_iterable_fn_kwargs,function,27,82,60,933,11.38,3,3,"['n', 'func', 'batched', 'batch_size', 'fn_kwargs']","[None, None, None, None, None]","[None, None, None, None, None]",520,[],"['ExamplesIterable', 'MappedExamplesIterable', 'generate_examples_fn', 'len', 'range', '_examples_to_batch', 'func', 'all_transformed_examples.extend', 'expected.update', 'list', 'next']",11
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_fn_kwargs_and_arrow_format,test_mapped_examples_iterable_fn_kwargs_and_arrow_format,function,23,75,54,797,10.63,3,3,"['n', 'func', 'batched', 'batch_size', 'fn_kwargs']","[None, None, None, None, None]","[None, None, None, None, None]",785,[],"['ExamplesIterable', 'MappedExamplesIterable', 'formatting=FormattingConfig', 'generate_examples_fn', 'len', 'range', 'expected.extend', 'next']",8
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_input_columns,test_mapped_examples_iterable_input_columns,function,29,89,64,1031,11.58,3,3,"['n', 'func', 'batched', 'batch_size', 'input_columns']","[None, None, None, None, None]","[None, None, None, None, None]",557,[],"['ExamplesIterable', 'MappedExamplesIterable', 'generate_examples_fn', 'isinstance', 'len', 'range', '_examples_to_batch', 'func', 'all_transformed_examples.extend', 'expected.update', 'list', 'next']",12
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_input_columns_and_arrow_format,test_mapped_examples_iterable_input_columns_and_arrow_format,function,25,84,60,897,10.68,3,3,"['n', 'func', 'batched', 'batch_size', 'input_columns']","[None, None, None, None, None]","[None, None, None, None, None]",822,[],"['ExamplesIterable', 'MappedExamplesIterable', 'formatting=FormattingConfig', 'generate_examples_fn', 'isinstance', 'func', 'len', 'range', 'expected.extend', 'next']",10
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_remove_columns,test_mapped_examples_iterable_remove_columns,function,32,105,71,1086,10.34,5,3,"['n', 'func', 'batched', 'batch_size', 'remove_columns']","[None, None, None, None, None]","[None, None, None, None, None]",483,[],"['ExamplesIterable', 'MappedExamplesIterable', 'generate_examples_fn', 'isinstance', 'x.items', 'len', 'range', '_examples_to_batch', 'func', 'all_transformed_examples.extend', 'expected.update', 'list', 'next']",13
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_remove_columns_arrow_format,test_mapped_examples_iterable_remove_columns_arrow_format,function,25,106,69,968,9.13,4,3,"['n', 'func', 'batched', 'batch_size', 'remove_columns']","[None, None, None, None, None]","[None, None, None, None, None]",743,[],"['ExamplesIterable', 'MappedExamplesIterable', 'formatting=FormattingConfig', 'generate_examples_fn', 'isinstance', 'func', 'len', 'range', 'expected.extend', 'x.items', 'next']",11
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_with_indices,test_mapped_examples_iterable_with_indices,function,27,81,61,963,11.89,3,2,"['n', 'func', 'batched', 'batch_size']","[None, None, None, None]","[None, None, None, None]",440,[],"['ExamplesIterable', 'MappedExamplesIterable', 'generate_examples_fn', 'enumerate', 'len', 'range', '_examples_to_batch', 'list', 'func', 'all_transformed_examples.extend', 'expected.update', 'next']",12
repos/datasets/tests/test_iterable_dataset.py:test_mapped_examples_iterable_with_indices_and_arrow_format,test_mapped_examples_iterable_with_indices_and_arrow_format,function,22,72,53,804,11.17,3,2,"['n', 'func', 'batched', 'batch_size']","[None, None, None, None]","[None, None, None, None]",695,[],"['ExamplesIterable', 'MappedExamplesIterable', 'formatting=FormattingConfig', 'generate_examples_fn', 'enumerate', 'len', 'range', 'expected.extend', 'list', 'next']",10
repos/datasets/tests/test_iterable_dataset.py:test_no_iter_arrow,test_no_iter_arrow,function,1,4,4,34,8.5,0,0,['ex_iterable'],[' _BaseExamplesIterable'],[None],1044,[],[],0
repos/datasets/tests/test_iterable_dataset.py:test_pickle_after_many_transforms,test_pickle_after_many_transforms,function,14,33,23,559,16.94,0,0,['dataset_with_several_columns'],[None],[None],1932,[],"['dataset.remove_columns', 'dataset.take', 'dataset.map', 'dataset.shuffle', 'dataset.skip', 'dataset.filter', 'dataset.add_column', 'dataset.rename_column', 'dataset.rename_columns', 'dataset.select_columns', 'pickle.loads', 'list']",12
repos/datasets/tests/test_iterable_dataset.py:test_randomly_cycling_multi_sources_examples_iterable,test_randomly_cycling_multi_sources_examples_iterable,function,26,65,56,838,12.89,2,1,['probabilities'],[None],[None],308,[],"['ExamplesIterable', 'RandomlyCyclingMultiSourcesExamplesIterable', 'deepcopy', 'generate_examples_fn', 'RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices', 'len', 'expected.append', 'next', 'list']",9
repos/datasets/tests/test_iterable_dataset.py:test_sharded_iterable_dataset_torch_dataloader_parallel,test_sharded_iterable_dataset_torch_dataloader_parallel,function,14,41,32,395,9.63,1,0,"['n_shards', 'num_workers']","[None, None]","[None, None]",1246,[],"['ExamplesIterable', 'range', 'IterableDataset', 'DataLoader', 'list', 'len']",6
repos/datasets/tests/test_iterable_dataset.py:test_skip_examples_iterable,test_skip_examples_iterable,function,7,29,28,372,12.83,0,0,[],[],[],951,[],"['ExamplesIterable', 'SkipExamplesIterable', 'list', 'skip_ex_iterable.shuffle_data_sources']",4
repos/datasets/tests/test_iterable_dataset.py:test_take_examples_iterable,test_take_examples_iterable,function,7,29,28,372,12.83,0,0,[],[],[],962,[],"['ExamplesIterable', 'TakeExamplesIterable', 'list', 'take_ex_iterable.shuffle_data_sources']",4
repos/datasets/tests/test_iterable_dataset.py:test_vertically_concatenated_examples_iterable,test_vertically_concatenated_examples_iterable,function,11,32,19,342,10.69,3,0,[],[],[],973,[],"['ExamplesIterable', 'VerticallyConcatenatedMultiSourcesExamplesIterable']",2
repos/datasets/tests/test_iterable_dataset.py:test_vertically_concatenated_examples_iterable_shuffle_data_sources,test_vertically_concatenated_examples_iterable_shuffle_data_sources,function,15,38,25,494,13.0,3,0,[],[],[],991,[],"['ExamplesIterable', 'VerticallyConcatenatedMultiSourcesExamplesIterable', 'concatenated_ex_iterable.shuffle_data_sources', 'ex_iterable2.shuffle_data_sources', 'ex_iterable1.shuffle_data_sources']",5
repos/datasets/tests/test_iterable_dataset.py:test_vertically_concatenated_examples_iterable_with_different_columns,test_vertically_concatenated_examples_iterable_with_different_columns,function,11,31,19,333,10.74,3,0,[],[],[],981,[],"['ExamplesIterable', 'VerticallyConcatenatedMultiSourcesExamplesIterable']",2
repos/datasets/tests/test_iterable_dataset.py:test_with_format_tf,test_with_format_tf,function,11,40,27,588,14.7,0,0,['dataset_with_several_columns'],[' IterableDataset'],[None],1857,[],"['dataset_with_several_columns.with_format', 'next', 'isinstance', 'list']",4
repos/datasets/tests/test_iterable_dataset.py:test_with_format_torch,test_with_format_torch,function,10,47,31,679,14.45,0,0,['dataset_with_several_columns'],[' IterableDataset'],[None],1835,[],"['dataset_with_several_columns.with_format', 'next', 'len', 'isinstance', 'list']",5
repos/datasets/tests/test_load.py:data_dir,data_dir,function,4,15,11,105,7.0,0,0,['tmp_path'],[None],[None],124,[],['open'],1
repos/datasets/tests/test_load.py:data_dir_with_arrow,data_dir_with_arrow,function,13,44,27,520,11.82,0,0,['tmp_path'],[None],[None],135,[],"['data_dir.mkdir', 'ArrowWriter', 'writer.write_table', 'writer.finalize']",4
repos/datasets/tests/test_load.py:data_dir_with_metadata,data_dir_with_metadata,function,6,24,16,241,10.04,0,0,['tmp_path'],[None],[None],154,[],"['data_dir.mkdir', 'open', 'f.write']",3
repos/datasets/tests/test_load.py:data_dir_with_single_config_in_metadata,data_dir_with_single_config_in_metadata,function,3,10,9,125,12.5,0,0,['tmp_path'],[None],[None],172,[],"['f.write', 'str']",2
repos/datasets/tests/test_load.py:data_dir_with_two_config_in_metadata,data_dir_with_two_config_in_metadata,function,8,30,22,343,11.43,0,0,['tmp_path'],[None],[None],224,[],"['cats_data_dir.mkdir', 'dogs_data_dir.mkdir', 'open', 'f.write']",4
repos/datasets/tests/test_load.py:__DummyDataset1__,__DummyDataset1__,class,11,40,34,485,12.12,1,0,[],[],[],73,[],[],0
repos/datasets/tests/test_load.py:__DummyMetric1__,__DummyMetric1__,class,5,21,19,227,10.81,0,0,[],[],[],113,[],[],0
repos/datasets/tests/test_load.py:__DummyDataset1__:_generate_examples,__DummyDataset1__:_generate_examples,method,6,15,14,97,6.47,1,0,"['self', 'filepath', '**kwargs']","[None, None, None]","[None, None, None]",84,[],"['open', 'enumerate', 'line.strip']",3
repos/datasets/tests/test_load.py:__DummyDataset1__:_info,__DummyDataset1__:_info,method,2,3,3,62,20.67,0,0,['self'],[None],[None],75,[],"['DatasetInfo', 'Value']",2
repos/datasets/tests/test_load.py:__DummyDataset1__:_split_generators,__DummyDataset1__:_split_generators,method,1,11,9,209,19.0,0,0,"['self', 'dl_manager']","[None, None]","[None, None]",78,[],['SplitGenerator'],1
repos/datasets/tests/test_load.py:__DummyMetric1__:_compute,__DummyMetric1__:_compute,method,2,10,10,77,7.7,0,0,"['self', 'predictions', 'references']","[None, None, None]","[None, None, None]",118,[],"['sum', 'zip']",2
repos/datasets/tests/test_load.py:__DummyMetric1__:_info,__DummyMetric1__:_info,method,2,5,5,91,18.2,0,0,['self'],[None],[None],115,[],"['MetricInfo', 'Value']",2
repos/datasets/tests/test_metadata_util.py:_dedent,_dedent,function,3,23,19,180,7.83,0,0,['string'],[' str'],[None],16,[],"['min', 't.startswith', 'string.splitlines', 'len']",4
repos/datasets/tests/test_metadata_util.py:data_dir_with_two_subdirs,data_dir_with_two_subdirs,function,11,27,21,367,13.59,0,0,['tmp_path'],[None],[None],108,[],"['cats_data_dir.mkdir', 'dogs_data_dir.mkdir', 'open', 'f.write', 'str']",5
repos/datasets/tests/test_metadata_util.py:test_metadata_configs_dataset_card_data,test_metadata_configs_dataset_card_data,function,12,23,19,412,17.91,0,0,"['readme_content', 'expected_metadata_configs_dict', 'expected_default_config_name']","[None, None, None]","[None, None, None]",232,[],"['tempfile.TemporaryDirectory', 'Path', 'open', 'readme_file.write', 'DatasetCard.load', 'MetadataConfigs.from_dataset_card_data', 'metadata_configs_dict.get_default_config_name']",7
repos/datasets/tests/test_metadata_util.py:test_metadata_configs_incorrect_yaml,test_metadata_configs_incorrect_yaml,function,11,19,16,304,16.0,0,0,[],[],[],245,[],"['tempfile.TemporaryDirectory', 'Path', 'open', 'readme_file.write', 'DatasetCard.load', 'pytest.raises', 'MetadataConfigs.from_dataset_card_data']",7
repos/datasets/tests/test_metadata_util.py:test_split_order_in_metadata_configs_from_exported_parquet_files_and_dataset_infos,test_split_order_in_metadata_configs_from_exported_parquet_files_and_dataset_infos,function,6,139,74,1878,13.51,0,0,[],[],[],255,[],"['DatasetInfo', 'MetadataConfigs._from_exported_parquet_files_and_dataset_infos']",2
repos/datasets/tests/test_metadata_util.py:TestMetadataUtils,TestMetadataUtils,class,16,71,49,1040,14.65,0,1,[],[],[],123,[],[],0
repos/datasets/tests/test_metadata_util.py:TestMetadataUtils:test_from_yaml_string,TestMetadataUtils:test_from_yaml_string,method,4,10,7,219,21.9,0,0,['self'],[None],[None],147,[],"['_dedent', 'DatasetCardData']",2
repos/datasets/tests/test_metadata_util.py:TestMetadataUtils:test_metadata_dict_from_readme,TestMetadataUtils:test_metadata_dict_from_readme,method,10,57,41,747,13.11,0,1,['self'],[None],[None],124,[],"['tempfile.TemporaryDirectory', 'Path', 'open', 'readme_file.write', 'DatasetCard.load', 'self.assertDictEqual', 'dataset_card_data.to_dict', 'self.assertEqual']",8
repos/datasets/tests/test_metric.py:metric_add_and_compute,metric_add_and_compute,function,20,34,33,360,10.59,1,0,['arg'],[None],[None],114,"['    """"""Thread worker function for distributed evaluation testing.\n', '    On base level to be pickable.\n', '    """"""\n']","['DummyMetric', 'zip', 'metric.add', 'time.sleep', 'metric.compute', 'properly_del_metric']",6
repos/datasets/tests/test_metric.py:metric_add_batch_and_compute,metric_add_batch_and_compute,function,17,28,27,340,12.14,0,0,['arg'],[None],[None],96,"['    """"""Thread worker function for distributed evaluation testing.\n', '    On base level to be pickable.\n', '    """"""\n']","['DummyMetric', 'metric.add_batch', 'time.sleep', 'metric.compute', 'properly_del_metric']",5
repos/datasets/tests/test_metric.py:metric_compute,metric_compute,function,16,27,26,321,11.89,0,0,['arg'],[None],[None],79,"['    """"""Thread worker function for distributed evaluation testing.\n', '    On base level to be pickable.\n', '    """"""\n']","['DummyMetric', 'time.sleep', 'metric.compute', 'properly_del_metric']",4
repos/datasets/tests/test_metric.py:properly_del_metric,properly_del_metric,function,8,23,12,184,8.0,0,3,['metric'],[None],[None],67,"['    """"""properly delete a metric on windows if the process is killed during multiprocessing""""""\n']",[],0
repos/datasets/tests/test_metric.py:test_metric_with_multilabel,test_metric_with_multilabel,function,7,12,12,193,16.08,0,0,"['config_name', 'predictions', 'references', 'expected', 'tmp_path']","[None, None, None, None, None]","[None, None, None, None, None]",517,[],"['MetricWithMultiLabel', 'metric.compute']",2
repos/datasets/tests/test_metric.py:test_metric_with_non_standard_feature_names_add,test_metric_with_non_standard_feature_names_add,function,14,21,20,335,15.95,1,0,['tmp_path'],[None],[None],558,[],"['AccuracyWithNonStandardFeatureNames.inputs_and_targets', 'AccuracyWithNonStandardFeatureNames', 'zip', 'metric.add', 'metric.compute', 'AccuracyWithNonStandardFeatureNames.expected_results']",6
repos/datasets/tests/test_metric.py:test_metric_with_non_standard_feature_names_add_batch,test_metric_with_non_standard_feature_names_add_batch,function,11,15,14,305,20.33,0,0,['tmp_path'],[None],[None],568,[],"['AccuracyWithNonStandardFeatureNames.inputs_and_targets', 'AccuracyWithNonStandardFeatureNames', 'metric.add_batch', 'metric.compute', 'AccuracyWithNonStandardFeatureNames.expected_results']",5
repos/datasets/tests/test_metric.py:test_metric_with_non_standard_feature_names_compute,test_metric_with_non_standard_feature_names_compute,function,10,14,13,286,20.43,0,0,['tmp_path'],[None],[None],577,[],"['AccuracyWithNonStandardFeatureNames.inputs_and_targets', 'AccuracyWithNonStandardFeatureNames', 'metric.compute', 'AccuracyWithNonStandardFeatureNames.expected_results']",4
repos/datasets/tests/test_metric.py:test_safety_checks_process_vars,test_safety_checks_process_vars,function,4,9,6,132,14.67,0,0,[],[],[],524,[],"['pytest.raises', 'DummyMetric']",2
repos/datasets/tests/test_metric.py:AccuracyWithNonStandardFeatureNames,AccuracyWithNonStandardFeatureNames,class,7,57,46,430,7.54,0,1,[],[],[],532,[],[],0
repos/datasets/tests/test_metric.py:DummyMetric,DummyMetric,class,13,142,66,1159,8.16,0,1,[],[],[],16,[],[],0
repos/datasets/tests/test_metric.py:MetricWithMultiLabel,MetricWithMultiLabel,class,7,49,41,459,9.37,0,2,[],[],[],483,[],[],0
repos/datasets/tests/test_metric.py:TestMetric,TestMetric,class,61,729,165,10515,14.42,7,0,[],[],[],134,[],[],0
repos/datasets/tests/test_metric.py:AccuracyWithNonStandardFeatureNames:_compute,AccuracyWithNonStandardFeatureNames:_compute,method,2,19,18,92,4.84,0,1,"['self', 'inputs', 'targets']","[None, None, None]","[None, None, None]",540,[],"['sum', 'zip', 'len']",3
repos/datasets/tests/test_metric.py:AccuracyWithNonStandardFeatureNames:_info,AccuracyWithNonStandardFeatureNames:_info,method,2,14,14,156,11.14,0,0,['self'],[None],[None],17,[],"['MetricInfo', 'features=Features', 'Value']",3
repos/datasets/tests/test_metric.py:AccuracyWithNonStandardFeatureNames:expected_results,AccuracyWithNonStandardFeatureNames:expected_results,method,1,3,3,22,7.33,0,0,['cls'],[None],[None],39,"['    """"""properly delete a metric on windows if the process is killed during multiprocessing""""""\n']",[],0
repos/datasets/tests/test_metric.py:AccuracyWithNonStandardFeatureNames:inputs_and_targets,AccuracyWithNonStandardFeatureNames:inputs_and_targets,method,1,9,8,27,3.0,0,0,['cls'],[None],[None],550,[],[],0
repos/datasets/tests/test_metric.py:DummyMetric:_compute,DummyMetric:_compute,method,2,22,21,158,7.18,0,1,"['self', 'predictions', 'references']","[None, None, None]","[None, None, None]",24,[],"['sum', 'zip', 'len', 'set']",4
repos/datasets/tests/test_metric.py:DummyMetric:_info,DummyMetric:_info,method,2,14,14,164,11.71,0,0,['self'],[None],[None],17,[],"['MetricInfo', 'features=Features', 'Value']",3
repos/datasets/tests/test_metric.py:DummyMetric:distributed_expected_results,DummyMetric:distributed_expected_results,method,1,5,5,44,8.8,0,0,['cls'],[None],[None],55,[],[],0
repos/datasets/tests/test_metric.py:DummyMetric:distributed_predictions_and_references,DummyMetric:distributed_predictions_and_references,method,1,17,10,49,2.88,0,0,['cls'],[None],[None],51,[],[],0
repos/datasets/tests/test_metric.py:DummyMetric:expected_results,DummyMetric:expected_results,method,1,5,5,42,8.4,0,0,['cls'],[None],[None],39,[],[],0
repos/datasets/tests/test_metric.py:DummyMetric:other_expected_results,DummyMetric:other_expected_results,method,1,5,5,44,8.8,0,0,['cls'],[None],[None],47,[],[],0
repos/datasets/tests/test_metric.py:DummyMetric:other_predictions_and_references,DummyMetric:other_predictions_and_references,method,1,9,8,27,3.0,0,0,['cls'],[None],[None],43,[],[],0
repos/datasets/tests/test_metric.py:DummyMetric:predictions_and_references,DummyMetric:predictions_and_references,method,1,9,8,27,3.0,0,0,['cls'],[None],[None],35,[],[],0
repos/datasets/tests/test_metric.py:DummyMetric:separate_expected_results,DummyMetric:separate_expected_results,method,1,9,8,82,9.11,0,0,['cls'],[None],[None],63,[],[],0
repos/datasets/tests/test_metric.py:DummyMetric:separate_predictions_and_references,DummyMetric:separate_predictions_and_references,method,1,17,10,49,2.88,0,0,['cls'],[None],[None],59,[],[],0
repos/datasets/tests/test_metric.py:MetricWithMultiLabel:_compute,MetricWithMultiLabel:_compute,method,2,19,18,108,5.68,0,1,"['self', 'predictions', 'references']","[None, None, None]","[None, 'None', 'None']",495,[],"['sum', 'zip', 'len']",3
repos/datasets/tests/test_metric.py:MetricWithMultiLabel:_info,MetricWithMultiLabel:_info,method,4,24,22,282,11.75,0,1,['self'],[None],[None],17,[],"['MetricInfo', 'features=Features', 'Sequence', 'Value']",4
repos/datasets/tests/test_metric.py:TestMetric:test_concurrent_metrics,TestMetric:test_concurrent_metrics,method,25,113,47,2391,21.16,2,0,['self'],[None],[None],188,[],"['DummyMetric.predictions_and_references', 'DummyMetric.other_predictions_and_references', 'DummyMetric.expected_results', 'DummyMetric.other_expected_results', 'DummyMetric', 'self.assertDictEqual', 'metric.compute', 'other_metric.compute', 'metric.add_batch', 'other_metric.add_batch', 'zip', 'metric.add', 'other_metric.add']",13
repos/datasets/tests/test_metric.py:TestMetric:test_distributed_metrics,TestMetric:test_distributed_metrics,method,14,211,57,2296,10.88,0,0,['self'],[None],[None],298,[],"['tempfile.TemporaryDirectory', 'DummyMetric.distributed_predictions_and_references', 'DummyMetric.distributed_expected_results', 'Pool', 'pool.map', 'self.assertDictEqual', 'self.assertIsNone', 'self.assertRaises', 'DummyMetric']",9
repos/datasets/tests/test_metric.py:TestMetric:test_dummy_metric,TestMetric:test_dummy_metric,method,16,92,39,1552,16.87,2,0,['self'],[None],[None],135,[],"['DummyMetric.predictions_and_references', 'DummyMetric.expected_results', 'DummyMetric', 'self.assertDictEqual', 'metric.compute', 'metric.add_batch', 'zip', 'metric.add', 'self.assertRaisesRegex']",9
repos/datasets/tests/test_metric.py:TestMetric:test_dummy_metric_pickle,TestMetric:test_dummy_metric_pickle,method,16,35,26,456,13.03,0,0,['self'],[None],[None],397,[],"['tempfile.TemporaryDirectory', 'DummyMetric.predictions_and_references', 'DummyMetric.expected_results', 'DummyMetric', 'open', 'pickle.dump', 'pickle.load', 'self.assertDictEqual', 'metric.compute']",9
repos/datasets/tests/test_metric.py:TestMetric:test_input_numpy,TestMetric:test_input_numpy,method,18,42,29,671,15.98,1,0,['self'],[None],[None],414,[],"['DummyMetric.predictions_and_references', 'DummyMetric.expected_results', 'np.array', 'DummyMetric', 'self.assertDictEqual', 'metric.compute', 'metric.add_batch', 'zip', 'metric.add']",9
repos/datasets/tests/test_metric.py:TestMetric:test_input_tf,TestMetric:test_input_tf,method,18,42,29,673,16.02,1,0,['self'],[None],[None],460,[],"['DummyMetric.predictions_and_references', 'DummyMetric.expected_results', 'tf.constant', 'DummyMetric', 'self.assertDictEqual', 'metric.compute', 'metric.add_batch', 'zip', 'metric.add']",9
repos/datasets/tests/test_metric.py:TestMetric:test_input_torch,TestMetric:test_input_torch,method,17,40,27,675,16.88,1,0,['self'],[None],[None],437,[],"['DummyMetric.predictions_and_references', 'DummyMetric.expected_results', 'torch.tensor', 'DummyMetric', 'self.assertDictEqual', 'metric.compute', 'metric.add_batch', 'zip', 'metric.add']",9
repos/datasets/tests/test_metric.py:TestMetric:test_metric_with_cache_dir,TestMetric:test_metric_with_cache_dir,method,11,17,16,314,18.47,0,0,['self'],[None],[None],179,[],"['DummyMetric.predictions_and_references', 'DummyMetric.expected_results', 'tempfile.TemporaryDirectory', 'DummyMetric', 'self.assertDictEqual', 'metric.compute']",6
repos/datasets/tests/test_metric.py:TestMetric:test_separate_experiments_in_parallel,TestMetric:test_separate_experiments_in_parallel,method,10,117,37,1157,9.89,0,0,['self'],[None],[None],246,[],"['tempfile.TemporaryDirectory', 'DummyMetric.separate_predictions_and_references', 'DummyMetric.separate_expected_results', 'Pool', 'pool.map', 'self.assertDictEqual']",6
repos/datasets/tests/test_metric_common.py:get_local_metric_names,get_local_metric_names,function,2,22,18,163,7.41,0,0,[],[],[],82,[],['glob.glob'],1
repos/datasets/tests/test_metric_common.py:patch_bertscore,patch_bertscore,function,9,22,22,283,12.86,0,0,['module_name'],[None],[None],179,[],"['bert_cos_score_idf', 'torch.tensor', 'len', 'patch']",4
repos/datasets/tests/test_metric_common.py:patch_bleurt,patch_bleurt,function,14,34,32,373,10.97,0,0,['module_name'],[None],[None],161,[],"['MockedPredictor', 'predict', 'len', 'np.array', 'patch']",5
repos/datasets/tests/test_metric_common.py:patch_comet,patch_comet,function,11,34,30,402,11.82,0,0,['module_name'],[None],[None],195,[],"['load_from_checkpoint', 'predict', 'len', 'sum', 'Model', 'patch']",6
repos/datasets/tests/test_metric_common.py:skip_if_metric_requires_fairseq,skip_if_metric_requires_fairseq,function,8,19,19,184,9.68,0,1,['test_case'],[None],[None],49,[],"['wrapper', 'self.skipTest', 'test_case']",3
repos/datasets/tests/test_metric_common.py:skip_if_metric_requires_transformers,skip_if_metric_requires_transformers,function,8,19,19,199,10.47,0,1,['test_case'],[None],[None],60,[],"['wrapper', 'self.skipTest', 'test_case']",3
repos/datasets/tests/test_metric_common.py:skip_on_windows_if_not_windows_compatible,skip_on_windows_if_not_windows_compatible,function,8,20,20,193,9.65,0,1,['test_case'],[None],[None],71,[],"['wrapper', 'self.skipTest', 'test_case']",3
repos/datasets/tests/test_metric_common.py:test_seqeval_raises_when_incorrect_scheme,test_seqeval_raises_when_incorrect_scheme,function,7,25,25,289,11.56,0,0,[],[],[],214,[],"['load_metric', 'pytest.raises', 'metric.compute']",3
repos/datasets/tests/test_metric_common.py:LocalMetricTest,LocalMetricTest,class,40,129,87,2005,15.54,0,1,[],[],[],92,[],[],0
repos/datasets/tests/test_metric_common.py:LocalMetricTest:patch_intensive_calls,LocalMetricTest:patch_intensive_calls,method,4,9,9,122,13.56,0,1,"['self', 'metric_name', 'module_name']","[None, None, None]","[None, None, None]",130,[],[],0
repos/datasets/tests/test_metric_common.py:LocalMetricTest:register_intensive_calls_patcher,LocalMetricTest:register_intensive_calls_patcher,method,5,10,8,129,12.9,0,0,"['cls', 'metric_name']","[None, None]","[None, None]",147,[],"['wrapper', 'contextmanager']",2
repos/datasets/tests/test_metric_common.py:LocalMetricTest:test_load_metric,LocalMetricTest:test_load_metric,method,22,48,45,713,14.85,0,0,"['self', 'metric_name']","[None, None]","[None, None]",98,[],"['importlib.import_module', 'inspect.signature', 'self.assertTrue', 'parameters.values', 'self.patch_intensive_calls', 'self.use_local_metrics', 'doctest.testmod', 'self.assertEqual', 'self.assertGreater']",9
repos/datasets/tests/test_metric_common.py:LocalMetricTest:test_load_real_metric,LocalMetricTest:test_load_real_metric,method,9,17,17,334,19.65,0,0,"['self', 'metric_name']","[None, None]","[None, None]",118,[],"['importlib.import_module', 'self.use_local_metrics', 'doctest.testmod', 'self.assertEqual', 'self.assertGreater']",5
repos/datasets/tests/test_metric_common.py:LocalMetricTest:use_local_metrics,LocalMetricTest:use_local_metrics,method,6,16,15,225,14.06,0,0,['self'],[None],[None],138,[],"['load_local_metric', 'load_metric', 'patch']",3
repos/datasets/tests/test_offline_util.py:test_offline_with_connection_error,test_offline_with_connection_error,function,4,6,5,157,26.17,0,0,[],[],[],19,[],"['offline', 'pytest.raises', 'requests.request']",3
repos/datasets/tests/test_offline_util.py:test_offline_with_datasets_offline_mode_enabled,test_offline_with_datasets_offline_mode_enabled,function,4,5,4,136,27.2,0,0,[],[],[],25,[],"['offline', 'pytest.raises', 'http_head']",3
repos/datasets/tests/test_offline_util.py:test_offline_with_timeout,test_offline_with_timeout,function,4,11,8,275,25.0,0,0,[],[],[],10,[],"['offline', 'pytest.raises', 'requests.request']",3
repos/datasets/tests/test_parallel.py:add_one,add_one,function,6,25,16,313,12.52,0,0,['i'],[None],[None],9,[],"['parallel_backend', 'pytest.raises', 'map_nested']",3
repos/datasets/tests/test_parallel.py:test_parallel_backend_input,test_parallel_backend_input,function,6,25,16,313,12.52,0,0,[],[],[],16,[],"['parallel_backend', 'pytest.raises', 'map_nested']",3
repos/datasets/tests/test_parallel.py:test_parallel_backend_map_nested,test_parallel_backend_map_nested,function,13,87,49,691,7.94,0,0,['num_proc'],[None],[None],34,[],"['parallel_backend', 'map_nested']",2
repos/datasets/tests/test_patching.py:test_patch_submodule,test_patch_submodule,function,33,148,47,1725,11.66,0,0,[],[],[],6,[],"['patch_submodule', 'isinstance']",2
repos/datasets/tests/test_patching.py:test_patch_submodule_builtin,test_patch_submodule_builtin,function,5,22,12,221,10.05,0,0,[],[],[],72,[],['patch_submodule'],1
repos/datasets/tests/test_patching.py:test_patch_submodule_doesnt_exist,test_patch_submodule_doesnt_exist,function,3,12,9,240,20.0,0,0,[],[],[],146,[],['patch_submodule'],1
repos/datasets/tests/test_patching.py:test_patch_submodule_missing,test_patch_submodule_missing,function,3,7,7,109,15.57,0,0,[],[],[],86,[],['patch_submodule'],1
repos/datasets/tests/test_patching.py:test_patch_submodule_missing_builtin,test_patch_submodule_missing_builtin,function,5,20,14,208,10.4,0,0,[],[],[],93,[],"['getattr', 'patch_submodule']",2
repos/datasets/tests/test_patching.py:test_patch_submodule_start_and_stop,test_patch_submodule_start_and_stop,function,7,20,14,224,11.2,0,0,[],[],[],104,[],"['patch_submodule', 'patch.start', 'patch.stop']",3
repos/datasets/tests/test_patching.py:test_patch_submodule_successive,test_patch_submodule_successive,function,18,96,34,1250,13.02,0,0,[],[],[],114,[],['patch_submodule'],1
repos/datasets/tests/test_py_utils.py:_2seconds_generator_of_2items_with_timing,_2seconds_generator_of_2items_with_timing,function,1,7,5,67,9.57,0,0,['content'],[None],[None],237,[],['time.sleep'],1
repos/datasets/tests/test_py_utils.py:_split_text,_split_text,function,2,2,2,18,9.0,0,0,['text'],[' str'],[None],233,[],['text.split'],1
repos/datasets/tests/test_py_utils.py:add_one,add_one,function,3,4,4,11,2.75,0,0,['i'],[None],[None],28,[],[],0
repos/datasets/tests/test_py_utils.py:add_one_to_batch,add_one_to_batch,function,3,4,4,11,2.75,0,0,['batch'],[None],[None],32,[],[],0
repos/datasets/tests/test_py_utils.py:np_sum,np_sum,function,3,4,4,11,2.75,0,0,['x'],[None],[None],24,[],[],0
repos/datasets/tests/test_py_utils.py:test_asdict,test_asdict,function,6,39,27,320,8.21,0,0,[],[],[],220,[],"['A', 'asdict', 'pytest.raises']",3
repos/datasets/tests/test_py_utils.py:test_flatten,test_flatten,function,3,5,4,72,14.4,0,0,"['data', 'expected_output']","[None, None]","[None, None]",215,[],['NestedDataStructure'],1
repos/datasets/tests/test_py_utils.py:test_iflatmap_unordered,test_iflatmap_unordered,function,11,82,49,716,8.73,1,0,[],[],[],243,[],"['Pool', 'list', 'out.count', 'len', 'multiprocess.Pool', 'iflatmap_unordered', 'time.time', 'out.append']",8
repos/datasets/tests/test_py_utils.py:test_map_nested,test_map_nested,function,1,6,6,89,14.83,0,0,"['data_struct', 'expected_result', 'num_proc', 'batched', 'function']","[None, None, None, None, None]","[None, None, None, None, None]",58,[],['map_nested'],1
repos/datasets/tests/test_py_utils.py:test_map_nested_num_proc,test_map_nested_num_proc,function,12,41,31,534,13.02,1,1,"['iterable_length', 'num_proc', 'expected_num_proc']","[None, None, None]","[None, None, None]",118,[],"['patch', 'range', 'map_nested']",3
repos/datasets/tests/test_py_utils.py:test_nested_data_structure_data,test_nested_data_structure_data,function,3,5,4,78,15.6,0,0,['input_data'],[None],[None],187,[],['NestedDataStructure'],1
repos/datasets/tests/test_py_utils.py:A,A,class,3,4,4,11,2.75,0,0,[],[],[],37,[],[],0
repos/datasets/tests/test_py_utils.py:PyUtilsTest,PyUtilsTest,class,24,135,88,1289,9.55,0,0,[],[],[],62,[],[],0
repos/datasets/tests/test_py_utils.py:TempSeedTest,TempSeedTest,class,26,91,43,1063,11.68,0,0,[],[],[],133,[],[],0
repos/datasets/tests/test_py_utils.py:PyUtilsTest:test_map_nested,PyUtilsTest:test_map_nested,method,12,85,52,830,9.76,0,0,['self'],[None],[None],63,[],"['np.eye', 'np.zeros', 'np.ones', 'self.assertEqual', 'v.tolist', 'map_nested', 'expected_map_nested_sn1_int.items', 'self.assertRaises']",8
repos/datasets/tests/test_py_utils.py:PyUtilsTest:test_temporary_assignment,PyUtilsTest:test_temporary_assignment,method,6,16,14,188,11.75,0,0,['self'],[None],[None],92,[],"['Foo', 'self.assertEqual', 'temporary_assignment']",3
repos/datasets/tests/test_py_utils.py:PyUtilsTest:test_zip_dict,PyUtilsTest:test_zip_dict,method,6,28,23,183,6.54,0,0,['self'],[None],[None],85,[],"['sorted', 'self.assertEqual']",2
repos/datasets/tests/test_py_utils.py:TempSeedTest:test_numpy,TempSeedTest:test_numpy,method,10,20,16,243,12.15,0,0,['self'],[None],[None],172,[],"['gen_random_output', 'temp_seed', 'self.assertGreater']",3
repos/datasets/tests/test_py_utils.py:TempSeedTest:test_tensorflow,TempSeedTest:test_tensorflow,method,19,34,28,383,11.26,0,0,['self'],[None],[None],135,[],"['layers.Dense', 'gen_random_output', 'model', 'temp_seed', 'self.assertGreater']",5
repos/datasets/tests/test_py_utils.py:TempSeedTest:test_torch,TempSeedTest:test_torch,method,15,29,24,340,11.72,0,0,['self'],[None],[None],155,[],"['gen_random_output', 'torch.rand', 'model', 'temp_seed', 'self.assertGreater']",5
repos/datasets/tests/test_readme_util.py:test_readme_from_readme_correct,test_readme_from_readme_correct,function,9,27,22,327,12.11,0,0,"['readme_md', 'expected_dict']","[None, None]","[None, None]",443,[],"['tempfile.TemporaryDirectory', 'Path', 'open', 'readme_file.write', 'ReadMe.from_readme']",5
repos/datasets/tests/test_readme_util.py:test_readme_from_readme_error,test_readme_from_readme_error,function,12,22,19,321,14.59,0,0,"['readme_md', 'expected_error']","[None, None]","[None, None]",471,[],"['tempfile.TemporaryDirectory', 'Path', 'open', 'readme_file.write', 'expected_error.format', 'pytest.raises', 'ReadMe.from_readme', 'readme.validate']",8
repos/datasets/tests/test_readme_util.py:test_readme_from_readme_parsing_errors,test_readme_from_readme_parsing_errors,function,10,20,17,296,14.8,0,0,"['readme_md', 'expected_error']","[None, None]","[None, None]",488,[],"['tempfile.TemporaryDirectory', 'Path', 'open', 'readme_file.write', 'expected_error.format', 'pytest.raises', 'ReadMe.from_readme']",7
repos/datasets/tests/test_readme_util.py:test_readme_from_readme_suppress_parsing_errors,test_readme_from_readme_suppress_parsing_errors,function,7,16,14,214,13.38,0,0,['readme_md'],[None],[None],504,[],"['tempfile.TemporaryDirectory', 'Path', 'open', 'readme_file.write', 'ReadMe.from_readme']",5
repos/datasets/tests/test_readme_util.py:test_readme_from_string_correct,test_readme_from_string_correct,function,1,4,4,83,20.75,0,0,"['readme_md', 'expected_dict']","[None, None]","[None, None]",389,[],['ReadMe.from_string'],1
repos/datasets/tests/test_readme_util.py:test_readme_from_string_parsing_errors,test_readme_from_string_parsing_errors,function,3,5,5,135,27.0,0,0,"['readme_md', 'expected_error']","[None, None]","[None, None]",421,[],"['pytest.raises', 'ReadMe.from_string']",2
repos/datasets/tests/test_readme_util.py:test_readme_from_string_suppress_parsing_errors,test_readme_from_string_suppress_parsing_errors,function,1,3,3,81,27.0,0,0,['readme_md'],[None],[None],432,[],['ReadMe.from_string'],1
repos/datasets/tests/test_readme_util.py:test_readme_from_string_validation_errors,test_readme_from_string_validation_errors,function,5,7,7,160,22.86,0,0,"['readme_md', 'expected_error']","[None, None]","[None, None]",409,[],"['pytest.raises', 'ReadMe.from_string', 'readme.validate']",3
repos/datasets/tests/test_search.py:test_serialization_fs,test_serialization_fs,function,17,30,27,409,13.63,0,0,['mockfs'],[None],[None],178,[],"['FaissIndex', 'index.add_vectors', 'index.save', 'FaissIndex.load', 'np.zeros', 'index.search']",6
repos/datasets/tests/test_search.py:ElasticSearchIndexTest,ElasticSearchIndexTest,class,31,129,68,1539,11.93,4,0,[],[],[],197,[],[],0
repos/datasets/tests/test_search.py:FaissIndexTest,FaissIndexTest,class,39,122,77,1938,15.89,2,0,[],[],[],105,[],[],0
repos/datasets/tests/test_search.py:IndexableDatasetTest,IndexableDatasetTest,class,43,179,105,2839,15.86,0,0,[],[],[],20,[],[],0
repos/datasets/tests/test_search.py:ElasticSearchIndexTest:test_elasticsearch,ElasticSearchIndexTest:test_elasticsearch,method,30,127,66,1510,11.89,4,0,['self'],[None],[None],198,[],"['patch', 'Elasticsearch', 'ElasticSearchIndex', 'mocked_bulk.return_value', 'index.add_documents', 'index.search', 'self.assertEqual', 'index.search_batch', 'self.assertGreater', 'self.assertListEqual']",10
repos/datasets/tests/test_search.py:FaissIndexTest:test_custom,FaissIndexTest:test_custom,method,8,10,10,190,19.0,0,0,['self'],[None],[None],147,[],"['faiss.IndexFlat', 'FaissIndex', 'index.add_vectors', 'self.assertIsInstance']",4
repos/datasets/tests/test_search.py:FaissIndexTest:test_factory,FaissIndexTest:test_factory,method,9,19,15,398,20.95,0,0,['self'],[None],[None],135,[],"['FaissIndex', 'index.add_vectors', 'self.assertIsInstance', 'self.assertRaises']",4
repos/datasets/tests/test_search.py:FaissIndexTest:test_flat_ip,FaissIndexTest:test_flat_ip,method,26,59,52,847,14.36,2,0,['self'],[None],[None],106,[],"['FaissIndex', 'index.add_vectors', 'self.assertIsNotNone', 'self.assertEqual', 'np.zeros', 'index.search', 'self.assertRaises', 'query.reshape', 'self.assertGreater', 'np.eye', 'index.search_batch', 'self.assertListEqual']",12
repos/datasets/tests/test_search.py:FaissIndexTest:test_serialization,FaissIndexTest:test_serialization,method,18,26,25,403,15.5,0,0,['self'],[None],[None],155,[],"['FaissIndex', 'index.add_vectors', 'tempfile.NamedTemporaryFile', 'index.save', 'FaissIndex.load', 'os.unlink', 'np.zeros', 'index.search', 'self.assertGreater', 'self.assertEqual']",10
repos/datasets/tests/test_search.py:IndexableDatasetTest:_create_dummy_dataset,IndexableDatasetTest:_create_dummy_dataset,method,3,11,10,104,9.45,0,0,['self'],[None],[None],21,[],"['Dataset.from_dict', 'str', 'np.arange']",3
repos/datasets/tests/test_search.py:IndexableDatasetTest:test_add_elasticsearch_index,IndexableDatasetTest:test_add_elasticsearch_index,method,19,42,39,663,15.79,0,0,['self'],[None],[None],87,[],"['self._create_dummy_dataset', 'patch', 'mocked_bulk.return_value', 'Elasticsearch', 'dset.add_elasticsearch_index', 'dset.get_nearest_examples', 'self.assertEqual']",7
repos/datasets/tests/test_search.py:IndexableDatasetTest:test_add_faiss_index,IndexableDatasetTest:test_add_faiss_index,method,12,29,27,409,14.1,0,0,['self'],[None],[None],25,[],"['self._create_dummy_dataset', 'dset.map', 'np.ones', 'dset.add_faiss_index', 'dset.get_nearest_examples', 'self.assertEqual', 'dset.drop_index']",7
repos/datasets/tests/test_search.py:IndexableDatasetTest:test_add_faiss_index_errors,IndexableDatasetTest:test_add_faiss_index_errors,method,9,17,17,217,12.76,0,0,['self'],[None],[None],37,[],"['self._create_dummy_dataset', 'pytest.raises', 'dset.add_faiss_index']",3
repos/datasets/tests/test_search.py:IndexableDatasetTest:test_add_faiss_index_from_external_arrays,IndexableDatasetTest:test_add_faiss_index_from_external_arrays,method,13,21,21,373,17.76,0,0,['self'],[None],[None],44,[],"['self._create_dummy_dataset', 'dset.add_faiss_index_from_external_arrays', 'np.arange', 'dset.get_nearest_examples', 'np.ones', 'self.assertEqual']",6
repos/datasets/tests/test_search.py:IndexableDatasetTest:test_drop_index,IndexableDatasetTest:test_drop_index,method,6,16,16,291,18.19,0,0,['self'],[None],[None],79,[],"['self._create_dummy_dataset', 'dset.add_faiss_index_from_external_arrays', 'np.arange', 'dset.drop_index', 'self.assertRaises', 'partial', 'np.ones']",7
repos/datasets/tests/test_search.py:IndexableDatasetTest:test_serialization,IndexableDatasetTest:test_serialization,method,17,29,28,529,18.24,0,0,['self'],[None],[None],57,[],"['self._create_dummy_dataset', 'dset.add_faiss_index_from_external_arrays', 'np.arange', 'tempfile.NamedTemporaryFile', 'dset.save_faiss_index', 'dset.load_faiss_index', 'os.unlink', 'dset.get_nearest_examples', 'np.ones', 'self.assertEqual']",10
repos/datasets/tests/test_sharding_utils.py:test_distribute_shards,test_distribute_shards,function,3,5,4,52,10.4,0,0,"['kwargs', 'expected']","[None, None]","[None, None]",17,[],['_distribute_shards'],1
repos/datasets/tests/test_sharding_utils.py:test_number_of_shards_in_gen_kwargs,test_number_of_shards_in_gen_kwargs,function,6,13,10,172,13.23,0,1,"['gen_kwargs', 'expected']","[None, None]","[None, None]",48,[],"['pytest.raises', '_number_of_shards_in_gen_kwargs']",2
repos/datasets/tests/test_sharding_utils.py:test_split_gen_kwargs,test_split_gen_kwargs,function,3,6,5,66,11.0,0,0,"['gen_kwargs', 'max_num_jobs', 'expected']","[None, None, None]","[None, None, None]",32,[],['_split_gen_kwargs'],1
repos/datasets/tests/test_splits.py:test_split_dict_asdict_has_dataset_name,test_split_dict_asdict_has_dataset_name,function,3,10,9,180,18.0,0,0,['split_info'],[None],[None],31,[],['asdict'],1
repos/datasets/tests/test_splits.py:test_split_dict_to_yaml_list,test_split_dict_to_yaml_list,function,11,19,17,282,14.84,1,0,['split_dict'],[' SplitDict'],[None],16,[],"['split_dict._to_yaml_list', 'len', 'SplitDict._from_yaml_list', 'split_dict.items']",4
repos/datasets/tests/test_streaming_download_manager.py:_test_jsonl,_test_jsonl,function,8,18,16,175,9.72,1,0,"['path', 'file']","[None, None]","[None, None]",146,[],"['path.endswith', 'enumerate', 'json.loads', 'item.keys']",4
repos/datasets/tests/test_streaming_download_manager.py:test_iter_archive_file,test_iter_archive_file,function,11,58,27,631,10.88,4,0,"['archive_nested_jsonl', 'request']","[None, None]","[None, None]",171,[],"['request.getfixturevalue', 'StreamingDownloadManager', 'dl_manager.iter_archive', 'enumerate', '_test_jsonl']",5
repos/datasets/tests/test_streaming_download_manager.py:test_iter_archive_path,test_iter_archive_path,function,9,34,20,390,11.47,2,0,"['archive_jsonl', 'request']","[None, None]","[None, None]",155,[],"['request.getfixturevalue', 'StreamingDownloadManager', 'dl_manager.iter_archive', 'enumerate', '_test_jsonl']",5
repos/datasets/tests/test_streaming_download_manager.py:test_iter_files,test_iter_files,function,7,19,17,214,11.26,1,0,['data_dir_with_hidden_files'],[None],[None],190,[],"['StreamingDownloadManager', 'enumerate']",2
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_dl_manager_download,test_streaming_dl_manager_download,function,8,19,15,214,11.26,0,0,['text_path'],[None],[None],49,[],"['StreamingDownloadManager', 'dl_manager.download', 'xopen', 'open', 'f.read', 'expected_file.read']",6
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_dl_manager_download_and_extract_no_extraction,test_streaming_dl_manager_download_and_extract_no_extraction,function,3,5,5,93,18.6,0,0,['urlpath'],[None],[None],58,[],"['StreamingDownloadManager', 'dl_manager.download_and_extract']",2
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_dl_manager_download_and_extract_with_extraction,test_streaming_dl_manager_download_and_extract_with_extraction,function,14,26,21,375,14.42,0,0,"['text_gz_path', 'text_path']","[None, None]","[None, None]",74,[],"['StreamingDownloadManager', 'dl_manager.download_and_extract', 'path.rindex', 'xopen', 'open', 'f.read', 'expected_file.read']",7
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_dl_manager_download_and_extract_with_join,test_streaming_dl_manager_download_and_extract_with_join,function,7,10,9,172,17.2,0,0,"['input_path', 'filename', 'expected_path']","[None, None, None]","[None, None, None]",89,[],"['StreamingDownloadManager', 'dl_manager.download_and_extract', 'xjoin']",3
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_dl_manager_download_dummy_path,test_streaming_dl_manager_download_dummy_path,function,3,5,5,81,16.2,0,0,['urlpath'],[None],[None],30,[],"['StreamingDownloadManager', 'dl_manager.download']",2
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_dl_manager_extract,test_streaming_dl_manager_extract,function,14,26,21,362,13.92,0,0,"['text_gz_path', 'text_path']","[None, None]","[None, None]",63,[],"['StreamingDownloadManager', 'dl_manager.extract', 'path.rindex', 'xopen', 'open', 'f.read', 'expected_file.read']",7
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_dl_manager_extract_all_supported_single_file_compression_types,test_streaming_dl_manager_extract_all_supported_single_file_compression_types,function,22,62,51,796,12.84,0,2,"['compression_fs_class', 'gz_file', 'xz_file', 'zstd_file', 'bz2_file', 'lz4_file', 'text_file']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",97,[],"['pytest.skip', 'StreamingDownloadManager', 'dl_manager.extract', 'path.rindex', 'xopen', 'open', 'f.read', 'expected_file.read']",8
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_dl_manager_extract_throws,test_streaming_dl_manager_extract_throws,function,4,4,4,85,21.25,0,0,['urlpath'],[None],[None],44,[],"['pytest.raises', 'StreamingDownloadManager']",2
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_gg_drive_gzipped,test_streaming_gg_drive_gzipped,function,5,9,9,143,15.89,0,0,[],[],[],129,[],"['StreamingDownloadManager', 'xopen', 'f.read']",3
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_gg_drive_no_extract,test_streaming_gg_drive_no_extract,function,5,9,9,135,15.0,0,0,[],[],[],121,[],"['StreamingDownloadManager', 'xopen', 'f.read']",3
repos/datasets/tests/test_streaming_download_manager.py:test_streaming_gg_drive_zipped,test_streaming_gg_drive_zipped,function,7,18,16,267,14.83,0,0,[],[],[],137,[],"['StreamingDownloadManager', 'list', 'len', 'xbasename', 'xopen', 'f.read']",6
repos/datasets/tests/test_table.py:_interpolation_search_ground_truth,_interpolation_search_ground_truth,function,7,15,14,71,4.73,1,1,"['arr', 'x']","[' List[int]', ' int']","[None, None]",1048,[],['range'],1
repos/datasets/tests/test_table.py:_to_testing_blocks,_to_testing_blocks,function,2,20,18,159,7.95,0,0,['table'],[' TableBlock'],[None],39,[],"['len', 'table.slice']",2
repos/datasets/tests/test_table.py:add_suffix_to_column_names,add_suffix_to_column_names,function,2,6,6,74,12.33,0,0,"['table', 'suffix']","[None, None]","[None, None]",101,[],['table.rename_columns'],1
repos/datasets/tests/test_table.py:assert_deepcopy_does_bring_data_in_memory,assert_deepcopy_does_bring_data_in_memory,function,7,10,9,157,15.7,0,0,['table'],[' MemoryMappedTable'],[None],72,[],"['assert_arrow_memory_increases', 'copy.deepcopy', 'isinstance']",3
repos/datasets/tests/test_table.py:assert_deepcopy_without_bringing_data_in_memory,assert_deepcopy_without_bringing_data_in_memory,function,7,10,9,163,16.3,0,0,['table'],[' MemoryMappedTable'],[None],65,[],"['assert_arrow_memory_doesnt_increase', 'copy.deepcopy', 'isinstance']",3
repos/datasets/tests/test_table.py:assert_index_attributes_equal,assert_index_attributes_equal,function,5,8,7,132,16.5,0,0,"['table', 'other']","[' Table', ' Table']","[None, None]",95,[],[],0
repos/datasets/tests/test_table.py:assert_pickle_does_bring_data_in_memory,assert_pickle_does_bring_data_in_memory,function,9,12,11,207,17.25,0,0,['table'],[' MemoryMappedTable'],[None],87,[],"['assert_arrow_memory_increases', 'pickle.dumps', 'pickle.loads', 'isinstance']",4
repos/datasets/tests/test_table.py:assert_pickle_without_bringing_data_in_memory,assert_pickle_without_bringing_data_in_memory,function,9,12,11,213,17.75,0,0,['table'],[' MemoryMappedTable'],[None],79,[],"['assert_arrow_memory_doesnt_increase', 'pickle.dumps', 'pickle.loads', 'isinstance']",4
repos/datasets/tests/test_table.py:in_memory_blocks,in_memory_blocks,function,4,4,4,71,17.75,0,0,['in_memory_pa_table'],[None],[None],49,[],"['InMemoryTable', '_to_testing_blocks']",2
repos/datasets/tests/test_table.py:in_memory_pa_table,in_memory_pa_table,function,2,2,2,47,23.5,0,0,['arrow_file'],[None],[None],35,[],[],0
repos/datasets/tests/test_table.py:memory_mapped_blocks,memory_mapped_blocks,function,4,4,4,77,19.25,0,0,['arrow_file'],[None],[None],55,[],"['MemoryMappedTable.from_file', '_to_testing_blocks']",2
repos/datasets/tests/test_table.py:mixed_in_memory_and_memory_mapped_blocks,mixed_in_memory_and_memory_mapped_blocks,function,3,3,3,51,17.0,0,0,"['in_memory_blocks', 'memory_mapped_blocks']","[None, None]","[None, None]",61,[],[],0
repos/datasets/tests/test_table.py:test_cast_array_to_features_array_xd,test_cast_array_to_features_array_xd,function,6,32,22,378,11.81,0,0,[],[],[],1179,[],"['pa.array', 'pa.list_', 'cast_array_to_feature', 'Array2D', 'Array2DExtensionType']",5
repos/datasets/tests/test_table.py:test_cast_array_to_features_nested,test_cast_array_to_features_nested,function,4,11,11,157,14.27,0,0,[],[],[],1142,[],"['pa.array', 'cast_array_to_feature', 'Sequence', 'pa.list_', 'pa.struct']",5
repos/datasets/tests/test_table.py:test_cast_array_to_features_nested_with_nulls,test_cast_array_to_features_nested_with_nulls,function,7,36,19,554,15.39,0,0,[],[],[],1155,[],"['pa.array', 'pa.struct', 'pa.list_', 'cast_array_to_feature', 'casted_array.to_pylist', 'arr.to_pylist']",6
repos/datasets/tests/test_table.py:test_cast_array_to_features_sequence_classlabel,test_cast_array_to_features_sequence_classlabel,function,5,58,22,906,15.62,0,0,[],[],[],1189,[],"['pa.array', 'pa.list_', 'cast_array_to_feature', 'Sequence', 'pytest.raises']",5
repos/datasets/tests/test_table.py:test_cast_array_to_features_to_nested_with_no_fields,test_cast_array_to_features_to_nested_with_no_fields,function,3,10,8,139,13.9,0,0,[],[],[],1149,[],"['pa.array', 'cast_array_to_feature', 'pa.struct', 'arr.to_pylist']",4
repos/datasets/tests/test_table.py:test_cast_array_to_features_to_null_type,test_cast_array_to_features_to_null_type,function,5,14,11,216,15.43,0,0,[],[],[],1168,[],"['pa.array', 'cast_array_to_feature', 'Sequence', 'pa.list_', 'pytest.raises']",5
repos/datasets/tests/test_table.py:test_cast_array_xd_to_features_sequence,test_cast_array_xd_to_features_sequence,function,9,45,27,663,14.73,0,0,[],[],[],1262,[],"['Array2DExtensionType', 'pa.list_', 'cast_array_to_feature', 'Sequence', 'get_nested_type', 'casted_array.to_pylist', 'arr.to_pylist']",7
repos/datasets/tests/test_table.py:test_cast_boolean_array_to_features,test_cast_boolean_array_to_features,function,6,19,15,343,18.05,0,0,[],[],[],1122,[],"['pa.array', 'cast_array_to_feature', 'Sequence', 'pa.list_', 'pa.string', 'pytest.raises']",6
repos/datasets/tests/test_table.py:test_cast_decimal_array_to_features,test_cast_decimal_array_to_features,function,6,19,15,354,18.63,0,0,[],[],[],1132,[],"['pa.array', 'Decimal', 'cast_array_to_feature', 'Sequence', 'pa.list_', 'pa.string', 'pytest.raises']",7
repos/datasets/tests/test_table.py:test_cast_fixed_size_list_array_to_features_sequence,test_cast_fixed_size_list_array_to_features_sequence,function,10,43,25,762,17.72,0,1,"['arr', 'slice', 'target_value_feature']","[None, None, None]","[None, None, None]",1221,[],"['cast_array_to_feature', 'Sequence', 'get_nested_type', 'casted_array.to_pylist', 'arr.to_pylist', 'pytest.raises']",6
repos/datasets/tests/test_table.py:test_cast_float_array_to_features,test_cast_float_array_to_features,function,6,19,15,340,17.89,0,0,[],[],[],1112,[],"['pa.array', 'cast_array_to_feature', 'Sequence', 'pa.list_', 'pa.string', 'pytest.raises']",6
repos/datasets/tests/test_table.py:test_cast_integer_array_to_features,test_cast_integer_array_to_features,function,6,19,15,336,17.68,0,0,[],[],[],1102,[],"['pa.array', 'cast_array_to_feature', 'Sequence', 'pa.list_', 'pa.string', 'pytest.raises']",6
repos/datasets/tests/test_table.py:test_cast_list_array_to_features_sequence,test_cast_list_array_to_features_sequence,function,10,43,25,719,16.72,0,1,"['arr', 'slice', 'target_value_feature']","[None, None, None]","[None, None, None]",1246,[],"['cast_array_to_feature', 'Sequence', 'get_nested_type', 'casted_array.to_pylist', 'arr.to_pylist', 'arr.value_lengths']",6
repos/datasets/tests/test_table.py:test_concat_tables,test_concat_tables,function,18,86,55,1071,12.45,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",1023,[],"['InMemoryTable', 'MemoryMappedTable.from_file', 'ConcatenationTable.from_blocks', 'concat_tables', 'pa.concat_tables', 'isinstance', 'len', 'enumerate']",8
repos/datasets/tests/test_table.py:test_concat_tables_cast_with_features_metadata,test_concat_tables_cast_with_features_metadata,function,15,34,29,641,18.85,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",887,[],"['Features.from_arrow_schema', 'Value', 'ConcatenationTable.from_blocks']",3
repos/datasets/tests/test_table.py:test_concat_tables_with_features_metadata,test_concat_tables_with_features_metadata,function,20,33,28,597,18.09,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",761,[],"['Features.from_arrow_schema', 'Value', 'in_memory_pa_table.replace_schema_metadata', 'MemoryMappedTable.from_file', 'concat_tables']",5
repos/datasets/tests/test_table.py:test_concatenation_table_add_column,test_concatenation_table_add_column,function,8,20,20,360,18.0,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",923,[],"['len', 'pa.array', 'pytest.raises', 'ConcatenationTable.from_blocks']",4
repos/datasets/tests/test_table.py:test_concatenation_table_append_column,test_concatenation_table_append_column,function,7,17,17,322,18.94,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",942,[],"['pa.array', 'pytest.raises', 'ConcatenationTable.from_blocks']",3
repos/datasets/tests/test_table.py:test_concatenation_table_cast,test_concatenation_table_cast,function,13,69,35,843,12.22,2,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",856,[],"['pa.list_', 'pa.int64', 'pa.schema', 'zip', 'ConcatenationTable.from_blocks', 'in_memory_pa_table.cast', 'isinstance', 'pa.int32']",8
repos/datasets/tests/test_table.py:test_concatenation_table_combine_chunks,test_concatenation_table_combine_chunks,function,6,17,16,301,17.71,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",842,[],"['ConcatenationTable.from_blocks', 'in_memory_pa_table.combine_chunks', 'isinstance']",3
repos/datasets/tests/test_table.py:test_concatenation_table_deepcopy,test_concatenation_table_deepcopy,function,11,35,28,471,13.46,0,0,"['blocks_type', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None]","[None, None, None, None]",726,[],"['ConcatenationTable.from_blocks', 'copy.deepcopy', 'assert_index_attributes_equal', 'all', 'zip']",5
repos/datasets/tests/test_table.py:test_concatenation_table_drop,test_concatenation_table_drop,function,7,19,18,334,17.58,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",1009,[],"['ConcatenationTable.from_blocks', 'in_memory_pa_table.drop', 'isinstance']",3
repos/datasets/tests/test_table.py:test_concatenation_table_filter,test_concatenation_table_filter,function,8,26,25,353,13.58,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",813,[],"['pa.array', 'range', 'ConcatenationTable.from_blocks', 'in_memory_pa_table.filter', 'isinstance']",5
repos/datasets/tests/test_table.py:test_concatenation_table_flatten,test_concatenation_table_flatten,function,6,17,16,287,16.88,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",828,[],"['ConcatenationTable.from_blocks', 'in_memory_pa_table.flatten', 'isinstance']",3
repos/datasets/tests/test_table.py:test_concatenation_table_from_blocks,test_concatenation_table_from_blocks,function,11,57,25,828,14.53,0,0,"['in_memory_pa_table', 'in_memory_blocks']","[None, None]","[None, None]",632,[],"['len', 'InMemoryTable', 'in_memory_table.slice', 'ConcatenationTable.from_blocks', 'isinstance']",5
repos/datasets/tests/test_table.py:test_concatenation_table_from_blocks_doesnt_increase_memory,test_concatenation_table_from_blocks_doesnt_increase_memory,function,10,29,24,429,14.79,0,1,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",655,[],"['assert_arrow_memory_doesnt_increase', 'ConcatenationTable.from_blocks', 'isinstance']",3
repos/datasets/tests/test_table.py:test_concatenation_table_from_tables,test_concatenation_table_from_tables,function,26,86,58,1026,11.93,2,1,"['axis', 'in_memory_pa_table', 'arrow_file']","[None, None, None]","[None, None, None]",674,[],"['InMemoryTable', 'ConcatenationTable.from_blocks', 'MemoryMappedTable.from_file', 'pa.concat_tables', 'len', 'enumerate', 'zip', 'expected_table.append_column', 'assert_arrow_memory_doesnt_increase', 'ConcatenationTable.from_tables', 'isinstance']",11
repos/datasets/tests/test_table.py:test_concatenation_table_from_tables_axis1_misaligned_blocks,test_concatenation_table_from_tables_axis1_misaligned_blocks,function,9,61,33,646,10.59,0,0,['arrow_file'],[None],[None],701,[],"['MemoryMappedTable.from_file', 'table.slice', 'ConcatenationTable.from_tables', 'ConcatenationTable.from_blocks', 'len']",5
repos/datasets/tests/test_table.py:test_concatenation_table_init,test_concatenation_table_init,function,6,23,18,272,11.83,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",617,[],['ConcatenationTable'],1
repos/datasets/tests/test_table.py:test_concatenation_table_pickle,test_concatenation_table_pickle,function,12,23,22,401,17.43,0,0,"['blocks_type', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None]","[None, None, None, None]",745,[],"['ConcatenationTable.from_blocks', 'pickle.dumps', 'pickle.loads', 'assert_index_attributes_equal']",4
repos/datasets/tests/test_table.py:test_concatenation_table_remove_column,test_concatenation_table_remove_column,function,6,17,16,301,17.71,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",960,[],"['ConcatenationTable.from_blocks', 'in_memory_pa_table.remove_column', 'isinstance']",3
repos/datasets/tests/test_table.py:test_concatenation_table_rename_columns,test_concatenation_table_rename_columns,function,8,32,27,444,13.88,0,1,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",993,[],"['ConcatenationTable.from_blocks', 'isinstance', 'in_memory_pa_table.rename_columns']",3
repos/datasets/tests/test_table.py:test_concatenation_table_replace_schema_metadata,test_concatenation_table_replace_schema_metadata,function,7,20,19,397,19.85,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",908,[],"['ConcatenationTable.from_blocks', 'in_memory_pa_table.replace_schema_metadata', 'isinstance']",3
repos/datasets/tests/test_table.py:test_concatenation_table_set_column,test_concatenation_table_set_column,function,8,20,20,360,18.0,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",974,[],"['len', 'pa.array', 'pytest.raises', 'ConcatenationTable.from_blocks']",4
repos/datasets/tests/test_table.py:test_concatenation_table_slice,test_concatenation_table_slice,function,6,19,17,289,15.21,0,0,"['blocks_type', 'in_memory_pa_table', 'in_memory_blocks', 'memory_mapped_blocks', 'mixed_in_memory_and_memory_mapped_blocks']","[None, None, None, None, None]","[None, None, None, None, None]",778,[],"['ConcatenationTable.from_blocks', 'in_memory_pa_table.slice', 'isinstance']",3
repos/datasets/tests/test_table.py:test_concatenation_table_slice_mixed_schemas_vertically,test_concatenation_table_slice_mixed_schemas_vertically,function,14,51,39,688,13.49,0,0,['arrow_file'],[None],[None],791,[],"['MemoryMappedTable.from_file', 'InMemoryTable.from_pydict', 'pa.table', 't1.to_pydict', 'len', 'ConcatenationTable.from_blocks', 'table.to_pydict', 'expected.to_pydict', 'isinstance', 'pickle.loads', 'reloaded.to_pydict', 'expected.slice']",12
repos/datasets/tests/test_table.py:test_embed_array_storage,test_embed_array_storage,function,6,18,16,336,18.67,0,0,['image_file'],[None],[None],1276,[],"['pa.array', 'embed_array_storage', 'Image', 'isinstance', 'embedded_images_array.to_pylist']",5
repos/datasets/tests/test_table.py:test_embed_array_storage_nested,test_embed_array_storage_nested,function,5,33,24,596,18.06,0,0,['image_file'],[None],[None],1284,[],"['pa.array', 'embed_array_storage', 'isinstance', 'Image']",4
repos/datasets/tests/test_table.py:test_embed_table_storage,test_embed_table_storage,function,7,15,14,313,20.87,0,0,['image_file'],[None],[None],1295,[],"['Features', 'Image', 'table_cast', 'embed_table_storage', 'isinstance']",5
repos/datasets/tests/test_table.py:test_in_memory_arrow_table_from_buffer,test_in_memory_arrow_table_from_buffer,function,12,15,14,325,21.67,0,0,['in_memory_pa_table'],[None],[None],124,[],"['assert_arrow_memory_increases', 'pa.BufferOutputStream', 'pa.RecordBatchStreamWriter', 'writer.write_table', 'writer.close', 'buf_writer.close', '_in_memory_arrow_table_from_buffer']",7
repos/datasets/tests/test_table.py:test_in_memory_arrow_table_from_file,test_in_memory_arrow_table_from_file,function,5,7,6,125,17.86,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",118,[],"['assert_arrow_memory_increases', '_in_memory_arrow_table_from_file']",2
repos/datasets/tests/test_table.py:test_in_memory_table_add_column,test_in_memory_table_add_column,function,9,18,15,283,15.72,0,0,['in_memory_pa_table'],[None],[None],371,[],"['len', 'pa.array', 'InMemoryTable', 'in_memory_pa_table.add_column', 'isinstance']",5
repos/datasets/tests/test_table.py:test_in_memory_table_append_column,test_in_memory_table_append_column,function,8,14,12,246,17.57,0,0,['in_memory_pa_table'],[None],[None],380,[],"['pa.array', 'InMemoryTable', 'in_memory_pa_table.append_column', 'isinstance']",4
repos/datasets/tests/test_table.py:test_in_memory_table_cast,test_in_memory_table_cast,function,11,30,24,355,11.83,1,0,['in_memory_pa_table'],[None],[None],326,[],"['pa.list_', 'pa.schema', 'zip', 'InMemoryTable', 'in_memory_pa_table.cast', 'isinstance']",6
repos/datasets/tests/test_table.py:test_in_memory_table_cast_reorder_struct,test_in_memory_table_cast_reorder_struct,function,5,25,20,201,8.04,0,0,[],[],[],339,[],"['InMemoryTable', 'pa.schema', 'pa.struct', 'pa.string', 'table.cast']",5
repos/datasets/tests/test_table.py:test_in_memory_table_cast_with_hf_features,test_in_memory_table_cast_with_hf_features,function,8,16,14,260,16.25,0,0,[],[],[],356,[],"['InMemoryTable', 'Features', 'ClassLabel', 'table.cast', 'Features.from_arrow_schema']",5
repos/datasets/tests/test_table.py:test_in_memory_table_combine_chunks,test_in_memory_table_combine_chunks,function,5,8,7,149,18.62,0,0,['in_memory_pa_table'],[None],[None],320,[],"['InMemoryTable', 'in_memory_pa_table.combine_chunks', 'isinstance']",3
repos/datasets/tests/test_table.py:test_in_memory_table_deepcopy,test_in_memory_table_deepcopy,function,8,23,17,284,12.35,0,0,['in_memory_pa_table'],[None],[None],274,[],"['InMemoryTable', 'copy.deepcopy', 'assert_index_attributes_equal', 'all', 'zip']",5
repos/datasets/tests/test_table.py:test_in_memory_table_drop,test_in_memory_table_drop,function,6,10,9,182,18.2,0,0,['in_memory_pa_table'],[None],[None],411,[],"['InMemoryTable', 'in_memory_pa_table.drop', 'isinstance']",3
repos/datasets/tests/test_table.py:test_in_memory_table_filter,test_in_memory_table_filter,function,7,17,16,201,11.82,0,0,['in_memory_pa_table'],[None],[None],307,[],"['pa.array', 'range', 'InMemoryTable', 'in_memory_pa_table.filter', 'isinstance']",5
repos/datasets/tests/test_table.py:test_in_memory_table_flatten,test_in_memory_table_flatten,function,5,8,7,135,16.88,0,0,['in_memory_pa_table'],[None],[None],314,[],"['InMemoryTable', 'in_memory_pa_table.flatten', 'isinstance']",3
repos/datasets/tests/test_table.py:test_in_memory_table_from_arrays,test_in_memory_table_from_arrays,function,7,13,12,211,16.23,0,0,['in_memory_pa_table'],[None],[None],244,[],"['list', 'InMemoryTable.from_arrays', 'isinstance']",3
repos/datasets/tests/test_table.py:test_in_memory_table_from_batches,test_in_memory_table_from_batches,function,6,10,9,163,16.3,0,0,['in_memory_pa_table'],[None],[None],267,[],"['list', 'InMemoryTable.from_batches', 'isinstance']",3
repos/datasets/tests/test_table.py:test_in_memory_table_from_buffer,test_in_memory_table_from_buffer,function,14,18,17,354,19.67,0,0,['in_memory_pa_table'],[None],[None],220,[],"['assert_arrow_memory_increases', 'pa.BufferOutputStream', 'pa.RecordBatchStreamWriter', 'writer.write_table', 'writer.close', 'buf_writer.close', 'InMemoryTable.from_buffer', 'isinstance']",8
repos/datasets/tests/test_table.py:test_in_memory_table_from_file,test_in_memory_table_from_file,function,7,10,9,154,15.4,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",213,[],"['assert_arrow_memory_increases', 'InMemoryTable.from_file', 'isinstance']",3
repos/datasets/tests/test_table.py:test_in_memory_table_from_pandas,test_in_memory_table_from_pandas,function,9,18,14,289,16.06,0,0,['in_memory_pa_table'],[None],[None],232,[],"['in_memory_pa_table.to_pandas', 'assert_arrow_memory_increases', 'InMemoryTable.from_pandas', 'isinstance']",4
repos/datasets/tests/test_table.py:test_in_memory_table_from_pydict,test_in_memory_table_from_pydict,function,9,12,11,200,16.67,0,0,['in_memory_pa_table'],[None],[None],252,[],"['in_memory_pa_table.to_pydict', 'assert_arrow_memory_increases', 'InMemoryTable.from_pydict', 'isinstance']",4
repos/datasets/tests/test_table.py:test_in_memory_table_from_pylist,test_in_memory_table_from_pylist,function,6,10,8,162,16.2,0,0,['in_memory_pa_table'],[None],[None],260,[],"['InMemoryTable', 'InMemoryTable.from_pylist', 'isinstance', 'table.to_pylist']",4
repos/datasets/tests/test_table.py:test_in_memory_table_pickle,test_in_memory_table_pickle,function,9,11,11,211,19.18,0,0,['in_memory_pa_table'],[None],[None],284,[],"['InMemoryTable', 'pickle.dumps', 'pickle.loads', 'assert_index_attributes_equal']",4
repos/datasets/tests/test_table.py:test_in_memory_table_pickle_big_table,test_in_memory_table_pickle_big_table,function,5,18,15,211,11.72,0,0,[],[],[],293,[],"['InMemoryTable.from_pydict', 'len', 'pickle.dumps', 'pickle.loads']",4
repos/datasets/tests/test_table.py:test_in_memory_table_remove_column,test_in_memory_table_remove_column,function,5,8,7,149,18.62,0,0,['in_memory_pa_table'],[None],[None],388,[],"['InMemoryTable', 'in_memory_pa_table.remove_column', 'isinstance']",3
repos/datasets/tests/test_table.py:test_in_memory_table_rename_columns,test_in_memory_table_rename_columns,function,7,23,18,292,12.7,0,1,['in_memory_pa_table'],[None],[None],403,[],"['InMemoryTable', 'in_memory_pa_table.rename_columns', 'isinstance']",3
repos/datasets/tests/test_table.py:test_in_memory_table_replace_schema_metadata,test_in_memory_table_replace_schema_metadata,function,6,11,10,245,22.27,0,0,['in_memory_pa_table'],[None],[None],364,[],"['InMemoryTable', 'in_memory_pa_table.replace_schema_metadata', 'isinstance']",3
repos/datasets/tests/test_table.py:test_in_memory_table_set_column,test_in_memory_table_set_column,function,9,18,15,283,15.72,0,0,['in_memory_pa_table'],[None],[None],394,[],"['len', 'pa.array', 'InMemoryTable', 'in_memory_pa_table.set_column', 'isinstance']",5
repos/datasets/tests/test_table.py:test_in_memory_table_slice,test_in_memory_table_slice,function,5,10,8,137,13.7,0,0,['in_memory_pa_table'],[None],[None],301,[],"['InMemoryTable', 'in_memory_pa_table.slice', 'isinstance']",3
repos/datasets/tests/test_table.py:test_indexed_table_mixin,test_indexed_table_mixin,function,9,26,22,334,12.85,0,0,[],[],[],1091,[],"['pa.concat_tables', 'Table', 'all', 'np.cumsum', 'table.fast_slice', 'pa_table.slice']",6
repos/datasets/tests/test_table.py:test_inject_arrow_table_documentation,test_inject_arrow_table_documentation,function,7,25,22,322,12.88,0,0,['in_memory_pa_table'],[None],[None],105,[],"['function_to_wrap', 'method', 'inject_arrow_table_documentation', 'wrapped_method']",4
repos/datasets/tests/test_table.py:test_interpolation_search,test_interpolation_search,function,10,22,18,284,12.91,0,1,"['arr', 'x']","[None, None]","[None, None]",1077,[],"['_interpolation_search_ground_truth', 'isinstance', '_ListWithGetitemCounter', '_interpolation_search', 'pytest.raises']",5
repos/datasets/tests/test_table.py:test_memory_mapped_arrow_table_from_file,test_memory_mapped_arrow_table_from_file,function,5,7,6,135,19.29,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",135,[],"['assert_arrow_memory_doesnt_increase', '_memory_mapped_arrow_table_from_file']",2
repos/datasets/tests/test_table.py:test_memory_mapped_table_add_column,test_memory_mapped_table_add_column,function,12,27,22,460,17.04,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",551,[],"['len', 'pa.array', 'MemoryMappedTable.from_file', 'in_memory_pa_table.add_column', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",7
repos/datasets/tests/test_table.py:test_memory_mapped_table_append_column,test_memory_mapped_table_append_column,function,11,22,19,424,19.27,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",563,[],"['pa.array', 'MemoryMappedTable.from_file', 'in_memory_pa_table.append_column', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",6
repos/datasets/tests/test_table.py:test_memory_mapped_table_cast,test_memory_mapped_table_cast,function,14,37,30,512,13.84,1,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",523,[],"['pa.list_', 'pa.schema', 'zip', 'MemoryMappedTable.from_file', 'in_memory_pa_table.cast', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_does_bring_data_in_memory']",8
repos/datasets/tests/test_table.py:test_memory_mapped_table_combine_chunks,test_memory_mapped_table_combine_chunks,function,8,15,13,315,21.0,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",514,[],"['MemoryMappedTable.from_file', 'in_memory_pa_table.combine_chunks', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",5
repos/datasets/tests/test_table.py:test_memory_mapped_table_deepcopy,test_memory_mapped_table_deepcopy,function,10,26,19,326,12.54,0,0,['arrow_file'],[None],[None],447,[],"['MemoryMappedTable.from_file', 'copy.deepcopy', 'assert_index_attributes_equal', 'all', 'zip']",5
repos/datasets/tests/test_table.py:test_memory_mapped_table_drop,test_memory_mapped_table_drop,function,9,17,15,344,20.24,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",606,[],"['MemoryMappedTable.from_file', 'in_memory_pa_table.drop', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",5
repos/datasets/tests/test_table.py:test_memory_mapped_table_filter,test_memory_mapped_table_filter,function,10,24,22,358,14.92,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",493,[],"['pa.array', 'range', 'MemoryMappedTable.from_file', 'in_memory_pa_table.filter', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_does_bring_data_in_memory']",7
repos/datasets/tests/test_table.py:test_memory_mapped_table_flatten,test_memory_mapped_table_flatten,function,8,15,13,294,19.6,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",505,[],"['MemoryMappedTable.from_file', 'in_memory_pa_table.flatten', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",5
repos/datasets/tests/test_table.py:test_memory_mapped_table_from_file,test_memory_mapped_table_from_file,function,9,12,11,276,23.0,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",426,[],"['assert_arrow_memory_doesnt_increase', 'MemoryMappedTable.from_file', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",5
repos/datasets/tests/test_table.py:test_memory_mapped_table_from_file_with_replay,test_memory_mapped_table_from_file_with_replay,function,13,31,30,419,13.52,1,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",435,[],"['assert_arrow_memory_doesnt_increase', 'MemoryMappedTable.from_file', 'len', 'getattr', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",6
repos/datasets/tests/test_table.py:test_memory_mapped_table_init,test_memory_mapped_table_init,function,7,11,10,272,24.73,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",418,[],"['MemoryMappedTable', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",4
repos/datasets/tests/test_table.py:test_memory_mapped_table_pickle,test_memory_mapped_table_pickle,function,11,14,13,256,18.29,0,0,['arrow_file'],[None],[None],458,[],"['MemoryMappedTable.from_file', 'pickle.dumps', 'pickle.loads', 'assert_index_attributes_equal']",4
repos/datasets/tests/test_table.py:test_memory_mapped_table_pickle_applies_replay,test_memory_mapped_table_pickle_applies_replay,function,9,21,20,330,15.71,0,0,['arrow_file'],[None],[None],474,[],"['assert_arrow_memory_doesnt_increase', 'MemoryMappedTable.from_file', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",5
repos/datasets/tests/test_table.py:test_memory_mapped_table_pickle_doesnt_fill_memory,test_memory_mapped_table_pickle_doesnt_fill_memory,function,6,6,6,196,32.67,0,0,['arrow_file'],[None],[None],467,[],"['assert_arrow_memory_doesnt_increase', 'MemoryMappedTable.from_file', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",4
repos/datasets/tests/test_table.py:test_memory_mapped_table_remove_column,test_memory_mapped_table_remove_column,function,8,15,13,316,21.07,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",574,[],"['MemoryMappedTable.from_file', 'in_memory_pa_table.remove_column', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",5
repos/datasets/tests/test_table.py:test_memory_mapped_table_rename_columns,test_memory_mapped_table_rename_columns,function,10,30,24,464,15.47,0,1,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",595,[],"['MemoryMappedTable.from_file', 'in_memory_pa_table.rename_columns', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",5
repos/datasets/tests/test_table.py:test_memory_mapped_table_replace_schema_metadata,test_memory_mapped_table_replace_schema_metadata,function,9,18,16,429,23.83,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",541,[],"['MemoryMappedTable.from_file', 'in_memory_pa_table.replace_schema_metadata', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",5
repos/datasets/tests/test_table.py:test_memory_mapped_table_set_column,test_memory_mapped_table_set_column,function,12,27,22,460,17.04,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",583,[],"['len', 'pa.array', 'MemoryMappedTable.from_file', 'in_memory_pa_table.set_column', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",7
repos/datasets/tests/test_table.py:test_memory_mapped_table_slice,test_memory_mapped_table_slice,function,8,18,15,297,16.5,0,0,"['arrow_file', 'in_memory_pa_table']","[None, None]","[None, None]",484,[],"['MemoryMappedTable.from_file', 'in_memory_pa_table.slice', 'isinstance', 'assert_deepcopy_without_bringing_data_in_memory', 'assert_pickle_without_bringing_data_in_memory']",5
repos/datasets/tests/test_table.py:test_table_attributes,test_table_attributes,function,3,7,6,101,14.43,0,0,"['in_memory_pa_table', 'attribute']","[None, None]","[None, None]",208,[],"['Table', 'getattr']",2
repos/datasets/tests/test_table.py:test_table_column,test_table_column,function,3,9,8,145,16.11,0,0,['in_memory_pa_table'],[None],[None],177,[],"['Table', 'table.column', 'in_memory_pa_table.column']",3
repos/datasets/tests/test_table.py:test_table_equals,test_table_equals,function,3,4,4,70,17.5,0,0,['in_memory_pa_table'],[None],[None],151,[],"['Table', 'table.equals']",2
repos/datasets/tests/test_table.py:test_table_field,test_table_field,function,3,9,8,143,15.89,0,0,['in_memory_pa_table'],[None],[None],171,[],"['Table', 'table.field', 'in_memory_pa_table.field']",3
repos/datasets/tests/test_table.py:test_table_getitem,test_table_getitem,function,4,5,5,69,13.8,0,0,['in_memory_pa_table'],[None],[None],189,[],['Table'],1
repos/datasets/tests/test_table.py:test_table_init,test_table_init,function,4,5,5,69,13.8,0,0,['in_memory_pa_table'],[None],[None],141,[],['Table'],1
repos/datasets/tests/test_table.py:test_table_iter,test_table_iter,function,8,60,37,580,9.67,0,2,"['table', 'batch_size', 'drop_last_batch']","[None, None, None]","[None, None, None]",1313,[],"['len', 'list', 'all', 'pa.concat_tables', 'table.slice', 'reloaded.to_pydict']",6
repos/datasets/tests/test_table.py:test_table_itercolumns,test_table_itercolumns,function,3,8,7,180,22.5,0,0,['in_memory_pa_table'],[None],[None],183,[],"['Table', 'isinstance', 'type', 'list']",4
repos/datasets/tests/test_table.py:test_table_len,test_table_len,function,2,5,5,73,14.6,0,0,['in_memory_pa_table'],[None],[None],194,[],"['Table', 'len']",2
repos/datasets/tests/test_table.py:test_table_str,test_table_str,function,4,10,9,183,18.3,0,0,['in_memory_pa_table'],[None],[None],199,[],"['Table', 'str', 'repr']",3
repos/datasets/tests/test_table.py:test_table_to_batches,test_table_to_batches,function,3,5,5,89,17.8,0,0,['in_memory_pa_table'],[None],[None],156,[],"['Table', 'table.to_batches', 'in_memory_pa_table.to_batches']",3
repos/datasets/tests/test_table.py:test_table_to_pydict,test_table_to_pydict,function,3,5,5,87,17.4,0,0,['in_memory_pa_table'],[None],[None],161,[],"['Table', 'table.to_pydict', 'in_memory_pa_table.to_pydict']",3
repos/datasets/tests/test_table.py:test_table_to_string,test_table_to_string,function,3,5,5,87,17.4,0,0,['in_memory_pa_table'],[None],[None],166,[],"['Table', 'table.to_string', 'in_memory_pa_table.to_string']",3
repos/datasets/tests/test_table.py:test_table_validate,test_table_validate,function,3,5,5,85,17.0,0,0,['in_memory_pa_table'],[None],[None],146,[],"['Table', 'table.validate', 'in_memory_pa_table.validate']",3
repos/datasets/tests/test_table.py:_ListWithGetitemCounter,_ListWithGetitemCounter,class,8,21,18,270,12.86,0,0,[],[],[],1055,[],[],0
repos/datasets/tests/test_table.py:_ListWithGetitemCounter:__getitem__,_ListWithGetitemCounter:__getitem__,method,4,5,4,69,13.8,0,0,"['self', 'i']","[None, None]","[None, None]",1060,[],['super'],1
repos/datasets/tests/test_table.py:_ListWithGetitemCounter:__init__,_ListWithGetitemCounter:__init__,method,2,4,4,64,16.0,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",1056,[],"['super', 'set']",2
repos/datasets/tests/test_table.py:_ListWithGetitemCounter:getitem_unique_count,_ListWithGetitemCounter:getitem_unique_count,method,1,2,2,36,18.0,0,0,['self'],[None],[None],1066,[],['len'],1
repos/datasets/tests/test_tasks.py:test_reload_task_from_dict,test_reload_task_from_dict,function,6,9,7,103,11.44,0,0,['task_cls'],[None],[None],43,[],"['task_cls', 'asdict', 'task_template_from_dict']",3
repos/datasets/tests/test_tasks.py:AudioClassificationTest,AudioClassificationTest,class,16,50,41,971,19.42,0,0,[],[],[],153,[],[],0
repos/datasets/tests/test_tasks.py:AutomaticSpeechRecognitionTest,AutomaticSpeechRecognitionTest,class,11,33,31,667,20.21,0,0,[],[],[],135,[],[],0
repos/datasets/tests/test_tasks.py:DatasetWithTaskProcessingTest,DatasetWithTaskProcessingTest,class,27,83,55,1338,16.12,0,0,[],[],[],207,[],[],0
repos/datasets/tests/test_tasks.py:ImageClassificationTest,ImageClassificationTest,class,16,50,41,971,19.42,0,0,[],[],[],180,[],[],0
repos/datasets/tests/test_tasks.py:QuestionAnsweringTest,QuestionAnsweringTest,class,12,55,46,843,15.33,0,0,[],[],[],88,[],[],0
repos/datasets/tests/test_tasks.py:SummarizationTest,SummarizationTest,class,11,31,28,581,18.74,0,0,[],[],[],120,[],[],0
repos/datasets/tests/test_tasks.py:TestLanguageModeling,TestLanguageModeling,class,9,24,19,364,15.17,0,0,[],[],[],50,[],[],0
repos/datasets/tests/test_tasks.py:TextClassificationTest,TextClassificationTest,class,16,48,39,963,20.06,0,0,[],[],[],63,[],[],0
repos/datasets/tests/test_tasks.py:AudioClassificationTest:setUp,AudioClassificationTest:setUp,method,2,3,3,33,11.0,0,0,['self'],[None],[None],64,[],['sorted'],1
repos/datasets/tests/test_tasks.py:AudioClassificationTest:test_align_with_features,AudioClassificationTest:test_align_with_features,method,4,10,8,299,29.9,0,0,['self'],[None],[None],81,[],"['AudioClassification', 'self.assertEqual', 'task.align_with_features', 'ClassLabel']",4
repos/datasets/tests/test_tasks.py:AudioClassificationTest:test_column_mapping,AudioClassificationTest:test_column_mapping,method,3,8,8,168,21.0,0,0,['self'],[None],[None],67,[],"['AudioClassification', 'self.assertDictEqual']",2
repos/datasets/tests/test_tasks.py:AudioClassificationTest:test_from_dict,AudioClassificationTest:test_from_dict,method,7,21,21,362,17.24,0,0,['self'],[None],[None],71,[],"['Features', 'Audio', 'AudioClassification.from_dict', 'self.assertEqual']",4
repos/datasets/tests/test_tasks.py:AutomaticSpeechRecognitionTest:test_column_mapping,AutomaticSpeechRecognitionTest:test_column_mapping,method,3,8,8,206,25.75,0,0,['self'],[None],[None],51,[],"['AutomaticSpeechRecognition', 'self.assertDictEqual']",2
repos/datasets/tests/test_tasks.py:AutomaticSpeechRecognitionTest:test_from_dict,AutomaticSpeechRecognitionTest:test_from_dict,method,7,21,21,405,19.29,0,0,['self'],[None],[None],55,[],"['Features', 'Audio', 'Value', 'AutomaticSpeechRecognition.from_dict', 'self.assertEqual']",5
repos/datasets/tests/test_tasks.py:DatasetWithTaskProcessingTest:test_map_on_task_template,DatasetWithTaskProcessingTest:test_map_on_task_template,method,16,49,33,779,15.9,0,0,['self'],[None],[None],208,[],"['DatasetInfo', 'Dataset.from_dict', 'SAMPLE_QUESTION_ANSWERING_EXTRACTIVE.items', 'isinstance', 'len', 'keep_task', 'dont_keep_task', 'deepcopy', 'dataset.map']",9
repos/datasets/tests/test_tasks.py:DatasetWithTaskProcessingTest:test_remove_and_map_on_task_template,DatasetWithTaskProcessingTest:test_remove_and_map_on_task_template,method,16,30,29,475,15.83,0,0,['self'],[None],[None],234,[],"['Features', 'Value', 'ClassLabel', 'TextClassification', 'DatasetInfo', 'Dataset.from_dict', 'process', 'dataset.remove_columns', 'modified_dataset.map']",9
repos/datasets/tests/test_tasks.py:ImageClassificationTest:setUp,ImageClassificationTest:setUp,method,2,3,3,33,11.0,0,0,['self'],[None],[None],64,[],['sorted'],1
repos/datasets/tests/test_tasks.py:ImageClassificationTest:test_align_with_features,ImageClassificationTest:test_align_with_features,method,4,10,8,299,29.9,0,0,['self'],[None],[None],81,[],"['ImageClassification', 'self.assertEqual', 'task.align_with_features', 'ClassLabel']",4
repos/datasets/tests/test_tasks.py:ImageClassificationTest:test_column_mapping,ImageClassificationTest:test_column_mapping,method,3,8,8,168,21.0,0,0,['self'],[None],[None],67,[],"['ImageClassification', 'self.assertDictEqual']",2
repos/datasets/tests/test_tasks.py:ImageClassificationTest:test_from_dict,ImageClassificationTest:test_from_dict,method,7,21,21,362,17.24,0,0,['self'],[None],[None],71,[],"['Features', 'Image', 'ImageClassification.from_dict', 'self.assertEqual']",4
repos/datasets/tests/test_tasks.py:QuestionAnsweringTest:test_column_mapping,QuestionAnsweringTest:test_column_mapping,method,3,15,15,256,17.07,0,0,['self'],[None],[None],51,[],"['QuestionAnsweringExtractive', 'self.assertDictEqual']",2
repos/datasets/tests/test_tasks.py:QuestionAnsweringTest:test_from_dict,QuestionAnsweringTest:test_from_dict,method,8,36,31,531,14.75,0,0,['self'],[None],[None],55,[],"['Features', 'Value', 'Sequence', 'QuestionAnsweringExtractive.from_dict', 'self.assertEqual']",5
repos/datasets/tests/test_tasks.py:SummarizationTest:test_column_mapping,SummarizationTest:test_column_mapping,method,3,8,8,165,20.62,0,0,['self'],[None],[None],51,[],"['Summarization', 'self.assertDictEqual']",2
repos/datasets/tests/test_tasks.py:SummarizationTest:test_from_dict,SummarizationTest:test_from_dict,method,7,19,18,360,18.95,0,0,['self'],[None],[None],55,[],"['Features', 'Value', 'Summarization.from_dict', 'self.assertEqual']",4
repos/datasets/tests/test_tasks.py:TestLanguageModeling:test_column_mapping,TestLanguageModeling:test_column_mapping,method,2,6,6,96,16.0,0,0,['self'],[None],[None],51,[],['LanguageModeling'],1
repos/datasets/tests/test_tasks.py:TestLanguageModeling:test_from_dict,TestLanguageModeling:test_from_dict,method,6,14,12,212,15.14,0,0,['self'],[None],[None],55,[],"['Features', 'Value', 'LanguageModeling.from_dict']",3
repos/datasets/tests/test_tasks.py:TextClassificationTest:setUp,TextClassificationTest:setUp,method,2,3,3,33,11.0,0,0,['self'],[None],[None],64,[],['sorted'],1
repos/datasets/tests/test_tasks.py:TextClassificationTest:test_align_with_features,TextClassificationTest:test_align_with_features,method,4,10,8,296,29.6,0,0,['self'],[None],[None],81,[],"['TextClassification', 'self.assertEqual', 'task.align_with_features', 'ClassLabel']",4
repos/datasets/tests/test_tasks.py:TextClassificationTest:test_column_mapping,TextClassificationTest:test_column_mapping,method,3,8,8,163,20.38,0,0,['self'],[None],[None],67,[],"['TextClassification', 'self.assertDictEqual']",2
repos/datasets/tests/test_tasks.py:TextClassificationTest:test_from_dict,TextClassificationTest:test_from_dict,method,7,19,19,362,19.05,0,0,['self'],[None],[None],71,[],"['Features', 'Value', 'TextClassification.from_dict', 'self.assertEqual']",4
repos/datasets/tests/test_tqdm.py:TestTqdmUtils,TestTqdmUtils,class,27,145,59,2342,16.15,4,1,[],[],[],15,[],[],0
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:capsys,TestTqdmUtils:capsys,method,2,2,2,18,9.0,0,0,"['self', 'capsys']","[None, ' CaptureFixture']","[None, None]",17,"['        """"""Workaround to make capsys work in unittest framework.\n', '\n', '        Capsys is a convenient pytest fixture to capture stdout.\n', '        See https://waylonwalker.com/pytest-capsys/.\n', '\n', '        Taken from https://github.com/pytest-dev/pytest/issues/2504#issuecomment-309475790.\n', '        """"""\n']",[],0
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:setUp,TestTqdmUtils:setUp,method,4,4,4,92,23.0,0,0,['self'],[None],[None],27,"['        """"""Get verbosity to set it back after the tests.""""""\n']","['are_progress_bars_disabled', 'super']",2
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:tearDown,TestTqdmUtils:tearDown,method,3,5,5,97,19.4,0,1,['self'],[None],[None],32,"['        """"""Set back progress bars verbosity as before testing.""""""\n']","['disable_progress_bars', 'enable_progress_bars']",2
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:test_cannot_disable_tqdm_when_env_variable_is_set,TestTqdmUtils:test_cannot_disable_tqdm_when_env_variable_is_set,method,5,10,10,189,18.9,0,0,['self'],[None],[None],62,"['        """"""\n', '        Test helpers cannot enable/disable progress bars when\n', '        `HF_DATASETS_DISABLE_PROGRESS_BARS` is set.\n', '        """"""\n']","['enable_progress_bars', 'self.assertFalse', 'self.assertWarns', 'disable_progress_bars']",4
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:test_cannot_enable_tqdm_when_env_variable_is_set,TestTqdmUtils:test_cannot_enable_tqdm_when_env_variable_is_set,method,5,10,10,188,18.8,0,0,['self'],[None],[None],49,"['        """"""\n', '        Test helpers cannot enable/disable progress bars when\n', '        `HF_DATASETS_DISABLE_PROGRESS_BARS` is set.\n', '        """"""\n']","['disable_progress_bars', 'self.assertTrue', 'self.assertWarns', 'enable_progress_bars']",4
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:test_tqdm_can_be_disabled_when_globally_enabled,TestTqdmUtils:test_tqdm_can_be_disabled_when_globally_enabled,method,6,13,13,165,12.69,1,0,['self'],[None],[None],97,"['        """"""Test TQDM can still be locally disabled even when globally enabled.""""""\n']","['enable_progress_bars', 'tqdm', 'self.assertEqual']",3
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:test_tqdm_disabled,TestTqdmUtils:test_tqdm_disabled,method,6,12,12,153,12.75,1,0,['self'],[None],[None],75,"['        """"""Test TQDM not outputting anything when globally disabled.""""""\n']","['disable_progress_bars', 'tqdm', 'self.assertEqual']",3
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:test_tqdm_disabled_cannot_be_forced,TestTqdmUtils:test_tqdm_disabled_cannot_be_forced,method,6,13,13,167,12.85,1,0,['self'],[None],[None],86,"['        """"""Test TQDM cannot be forced when globally disabled.""""""\n']","['disable_progress_bars', 'tqdm', 'self.assertEqual']",3
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:test_tqdm_enabled,TestTqdmUtils:test_tqdm_enabled,method,7,15,15,162,10.8,1,0,['self'],[None],[None],108,"['        """"""Test TQDM work normally when globally enabled.""""""\n']","['enable_progress_bars', 'tqdm', 'self.assertEqual', 'self.assertIn']",4
repos/datasets/tests/test_tqdm.py:TestTqdmUtils:test_tqdm_helpers,TestTqdmUtils:test_tqdm_helpers,method,4,4,4,139,34.75,0,0,['self'],[None],[None],40,"['        """"""Test helpers to enable/disable progress bars.""""""\n']","['disable_progress_bars', 'self.assertTrue', 'enable_progress_bars', 'self.assertFalse']",4
repos/datasets/tests/test_upstream_hub.py:text_file_with_metadata,text_file_with_metadata,function,10,24,21,356,14.83,0,0,"['request', 'tmp_path', 'text_file']","[None, None, None]","[None, None, None]",882,[],"['data_dir.mkdir', 'shutil.copyfile', 'textwrap.dedent']",3
repos/datasets/tests/test_upstream_hub.py:DummyFolderBasedBuilder,DummyFolderBasedBuilder,class,5,8,8,107,13.38,0,0,[],[],[],873,[],[],0
repos/datasets/tests/test_upstream_hub.py:TestPushToHub,TestPushToHub,class,168,1999,486,29699,14.86,5,4,[],[],[],46,[],[],0
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub,TestPushToHub:test_push_dataset_dict_to_hub,method,14,39,35,583,14.95,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",115,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'list', 'sorted']",7
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_custom_features,TestPushToHub:test_push_dataset_dict_to_hub_custom_features,method,14,37,34,505,13.65,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",454,[],"['Features', 'Value', 'ClassLabel', 'Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'list']",9
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_custom_splits,TestPushToHub:test_push_dataset_dict_to_hub_custom_splits,method,12,30,28,421,14.03,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",490,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'list']",6
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_datasets_with_different_features,TestPushToHub:test_push_dataset_dict_to_hub_datasets_with_different_features,method,12,35,33,381,10.89,0,0,"['self', 'cleanup_repo']","[None, None]","[None, None]",84,[],"['Dataset.from_dict', 'DatasetDict', 'pytest.raises', 'local_ds.push_to_hub', 'cleanup_repo']",5
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_multiple_files,TestPushToHub:test_push_dataset_dict_to_hub_multiple_files,method,15,40,35,695,17.38,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",168,[],"['Dataset.from_dict', 'list', 'DatasetDict', 'temporary_repo', 'patch', 'local_ds.push_to_hub', 'load_dataset', 'sorted']",8
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_multiple_files_with_max_shard_size,TestPushToHub:test_push_dataset_dict_to_hub_multiple_files_with_max_shard_size,method,14,39,35,667,17.1,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",191,[],"['Dataset.from_dict', 'list', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'sorted']",7
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_multiple_files_with_num_shards,TestPushToHub:test_push_dataset_dict_to_hub_multiple_files_with_num_shards,method,14,40,36,668,16.7,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",213,[],"['Dataset.from_dict', 'list', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'sorted']",7
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_name_without_namespace,TestPushToHub:test_push_dataset_dict_to_hub_name_without_namespace,method,14,38,34,580,15.26,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",67,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'list', 'sorted']",7
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_no_token,TestPushToHub:test_push_dataset_dict_to_hub_no_token,method,14,37,33,547,14.78,0,0,"['self', 'temporary_repo', 'set_ci_hub_access_token']","[None, None, None]","[None, None, None]",50,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'list', 'sorted']",7
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_overwrite_files,TestPushToHub:test_push_dataset_dict_to_hub_overwrite_files,method,28,139,70,2188,15.74,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",265,[],"['Dataset.from_dict', 'list', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'tempfile.TemporaryDirectory', 'Path', 'open', 'f.write', 'path_or_fileobj=str', 'sorted', 'load_dataset', 'gc.collect']",13
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_private,TestPushToHub:test_push_dataset_dict_to_hub_private,method,14,41,37,614,14.98,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",98,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'list', 'sorted']",7
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_with_config_no_metadata_configs,TestPushToHub:test_push_dataset_dict_to_hub_with_config_no_metadata_configs,method,19,68,56,1211,17.81,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",838,[],"['Dataset.from_dict', 'BytesIO', 'ds.to_parquet', 'parquet_buf.getvalue', 'DatasetDict', 'temporary_repo', 'local_ds_another_config.push_to_hub', 'load_dataset_builder', 'len', 'fnmatch.fnmatch']",10
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_with_multiple_commits,TestPushToHub:test_push_dataset_dict_to_hub_with_multiple_commits,method,18,61,48,1087,17.82,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",235,[],"['Dataset.from_dict', 'list', 'DatasetDict', 'temporary_repo', 'len', 'patch', 'local_ds.push_to_hub', 'load_dataset', 'sorted']",9
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_with_pull_request,TestPushToHub:test_push_dataset_dict_to_hub_with_pull_request,method,12,43,36,614,14.28,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",132,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'list', 'sorted']",7
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_dict_to_hub_with_revision,TestPushToHub:test_push_dataset_dict_to_hub_with_revision,method,12,42,35,602,14.33,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",151,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'list', 'sorted']",7
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_to_hub,TestPushToHub:test_push_dataset_to_hub,method,17,42,38,574,13.67,1,0,"['self', 'temporary_repo']","[None, None]","[None, None]",356,[],"['Dataset.from_dict', 'temporary_repo', 'local_ds.push_to_hub', 'load_dataset', 'list', 'local_ds_dict.keys']",6
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_to_hub_custom_features,TestPushToHub:test_push_dataset_to_hub_custom_features,method,15,38,34,452,11.89,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",373,[],"['Features', 'Value', 'ClassLabel', 'Dataset.from_dict', 'temporary_repo', 'ds.push_to_hub', 'load_dataset', 'list']",8
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_to_hub_custom_features_audio,TestPushToHub:test_push_dataset_to_hub_custom_features_audio,method,31,79,70,981,12.42,1,0,"['self', 'temporary_repo']","[None, None]","[None, None]",387,[],"['Features', 'Audio', 'Value', 'Dataset.from_dict', 'temporary_repo', 'ds.push_to_hub', 'load_dataset', 'list', 'hub_ds.cast_column', 'isinstance', 'bool']",11
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_to_hub_custom_features_image,TestPushToHub:test_push_dataset_to_hub_custom_features_image,method,28,63,55,796,12.63,1,0,"['self', 'temporary_repo']","[None, None]","[None, None]",411,[],"['Features', 'Image', 'Value', 'Dataset.from_dict', 'temporary_repo', 'ds.push_to_hub', 'load_dataset', 'list', 'hub_ds.cast_column', 'isinstance', 'bool']",11
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_to_hub_custom_features_image_list,TestPushToHub:test_push_dataset_to_hub_custom_features_image_list,method,28,64,56,824,12.88,1,0,"['self', 'temporary_repo']","[None, None]","[None, None]",433,[],"['Features', 'Value', 'Dataset.from_dict', 'temporary_repo', 'ds.push_to_hub', 'load_dataset', 'list', 'hub_ds.cast_column', 'isinstance', 'bool']",10
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_to_hub_custom_splits,TestPushToHub:test_push_dataset_to_hub_custom_splits,method,10,28,26,366,13.07,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",468,[],"['Dataset.from_dict', 'temporary_repo', 'ds.push_to_hub', 'load_dataset', 'list']",5
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_to_hub_multiple_splits_one_by_one,TestPushToHub:test_push_dataset_to_hub_multiple_splits_one_by_one,method,11,35,30,456,13.03,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",479,[],"['Dataset.from_dict', 'temporary_repo', 'ds.push_to_hub', 'load_dataset', 'sorted', 'list']",6
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_dataset_to_hub_with_config_no_metadata_configs,TestPushToHub:test_push_dataset_to_hub_with_config_no_metadata_configs,method,17,65,53,1132,17.42,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",806,[],"['Dataset.from_dict', 'BytesIO', 'ds.to_parquet', 'parquet_buf.getvalue', 'temporary_repo', 'ds_another_config.push_to_hub', 'load_dataset_builder', 'len', 'fnmatch.fnmatch']",9
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_multiple_dataset_configs_to_hub_load_dataset,TestPushToHub:test_push_multiple_dataset_configs_to_hub_load_dataset,method,30,106,82,1619,15.27,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",553,[],"['Dataset.from_dict', 'temporary_repo', 'ds_default.push_to_hub', 'ds_config1.push_to_hub', 'ds_config2.push_to_hub', 'sorted', 'load_dataset', 'len', 'pytest.raises']",9
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_multiple_dataset_configs_to_hub_load_dataset_builder,TestPushToHub:test_push_multiple_dataset_configs_to_hub_load_dataset_builder,method,13,87,60,1370,15.75,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",519,[],"['Dataset.from_dict', 'temporary_repo', 'ds_default.push_to_hub', 'ds_config1.push_to_hub', 'ds_config2.push_to_hub', 'load_dataset_builder', 'len', 'fnmatch.fnmatch', 'pytest.raises']",9
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_multiple_dataset_configs_to_hub_readme_metadata_content,TestPushToHub:test_push_multiple_dataset_configs_to_hub_readme_metadata_content,method,17,121,75,1283,10.6,0,2,"['self', 'specific_default_config_name', 'temporary_repo']","[None, None, None]","[None, None, None]",595,[],"['Dataset.from_dict', 'temporary_repo', 'ds_default.push_to_hub', 'ds_config1.push_to_hub', 'ds_config2.push_to_hub', 'cached_path', 'DatasetCard.load', 'isinstance', 'sorted']",9
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_multiple_dataset_dict_configs_to_hub_load_dataset,TestPushToHub:test_push_multiple_dataset_dict_configs_to_hub_load_dataset,method,37,136,104,2137,15.71,1,0,"['self', 'temporary_repo']","[None, None]","[None, None]",690,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'ds_default.push_to_hub', 'ds_config1.push_to_hub', 'ds_config2.push_to_hub', 'sorted', 'load_dataset', 'len', 'pytest.raises']",10
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_multiple_dataset_dict_configs_to_hub_load_dataset_builder,TestPushToHub:test_push_multiple_dataset_dict_configs_to_hub_load_dataset_builder,method,16,99,65,1526,15.41,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",653,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'ds_default.push_to_hub', 'ds_config1.push_to_hub', 'ds_config2.push_to_hub', 'load_dataset_builder', 'len', 'fnmatch.fnmatch', 'pytest.raises']",10
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_multiple_dataset_dict_configs_to_hub_readme_metadata_content,TestPushToHub:test_push_multiple_dataset_dict_configs_to_hub_readme_metadata_content,method,18,152,88,1659,10.91,0,2,"['self', 'specific_default_config_name', 'temporary_repo']","[None, None, None]","[None, None, None]",741,[],"['Dataset.from_dict', 'DatasetDict', 'temporary_repo', 'ds_default.push_to_hub', 'ds_config1.push_to_hub', 'ds_config2.push_to_hub', 'cached_path', 'DatasetCard.load', 'isinstance', 'sorted']",10
repos/datasets/tests/test_upstream_hub.py:TestPushToHub:test_push_streaming_dataset_dict_to_hub,TestPushToHub:test_push_streaming_dataset_dict_to_hub,method,14,38,33,525,13.82,0,0,"['self', 'temporary_repo']","[None, None]","[None, None]",504,[],"['Dataset.from_dict', 'DatasetDict', 'tempfile.TemporaryDirectory', 'local_ds.save_to_disk', 'load_dataset', 'temporary_repo', 'local_ds.push_to_hub', 'list']",8
repos/datasets/tests/test_version.py:test_version_equality_and_hash,test_version_equality_and_hash,function,2,18,11,165,9.17,0,0,"['other', 'expected_equality']","[None, None]","[None, None]",19,[],"['Version', 'hash']",2
repos/datasets/tests/test_warnings.py:mock_emitted_deprecation_warnings,mock_emitted_deprecation_warnings,function,1,2,2,91,45.5,0,0,['monkeypatch'],[None],[None],7,[],"['monkeypatch.setattr', 'set']",2
repos/datasets/tests/test_warnings.py:mock_hfh,mock_hfh,function,10,24,22,288,12.0,0,0,['monkeypatch'],[None],[None],13,[],"['__init__', 'list_metrics', 'monkeypatch.setattr', 'HfhMock']",4
repos/datasets/tests/test_warnings.py:test_metric_deprecation_warning,test_metric_deprecation_warning,function,5,19,15,166,8.74,0,1,"['func', 'args', 'mock_emitted_deprecation_warnings', 'mock_hfh', 'tmp_path']","[None, None, None, None, None]","[None, None, None, None, None]",30,[],"['tuple', 'pytest.warns', 'func']",3
repos/datasets/tests/utils.py:assert_arrow_memory_doesnt_increase,assert_arrow_memory_doesnt_increase,function,5,16,15,181,11.31,0,0,[],[],[],412,[],"['gc.collect', 'pa.total_allocated_bytes']",2
repos/datasets/tests/utils.py:assert_arrow_memory_increases,assert_arrow_memory_increases,function,5,15,14,170,11.33,0,0,[],[],[],402,[],"['gc.collect', 'pa.total_allocated_bytes']",2
repos/datasets/tests/utils.py:execute_subprocess_async,execute_subprocess_async,function,12,47,41,453,9.64,0,2,"['cmd', 'env', 'stdin', 'timeout', 'quiet', 'echo']","[None, None, None, None, None, None]","[None, 'None', 'None', '180', 'False', 'True']",503,[],"['asyncio.get_event_loop', 'loop.run_until_complete', '_stream_subprocess', 'RuntimeError']",4
repos/datasets/tests/utils.py:for_all_test_methods,for_all_test_methods,function,10,24,19,182,7.58,2,1,['*decorators'],[None],[None],316,[],"['decorate', 'callable', 'name.startswith', 'decorator', 'setattr']",5
repos/datasets/tests/utils.py:get_torch_dist_unique_port,get_torch_dist_unique_port,function,4,7,5,68,9.71,0,0,[],[],[],535,"['    """"""\n', ""    Returns a port number that can be fed to `torchrun`'s `--master_port` argument.\n"", '\n', ""    Under `pytest-xdist` it adds a delta number based on a worker id so that concurrent tests don't try to use the same\n"", '    port at once.\n', '    """"""\n']",['pytest_xdist_worker_id'],1
repos/datasets/tests/utils.py:is_rng_equal,is_rng_equal,function,2,7,5,92,13.14,0,0,"['rng1', 'rng2']","[None, None]","[None, None]",421,[],['deepcopy'],1
repos/datasets/tests/utils.py:local,local,function,5,12,10,109,9.08,0,1,['test_case'],[None],[None],280,"['    """"""\n', '    Decorator marking a test as local\n', '\n', '    Local tests are run by default. Set the RUN_LOCAL environment variable\n', '    to a falsy value to not run them.\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:offline,offline,function,23,97,76,1105,11.39,0,2,"['mode', 'timeout']","[None, None]","['OfflineSimulationMode.CONNECTION_FAILS', '1e-16']",339,"['    """"""\n', '    Simulate offline mode.\n', '\n', '    There are three offline simulatiom modes:\n', '\n', '    CONNECTION_FAILS (default mode): a ConnectionError is raised for each network call.\n', '        Connection errors are created by mocking socket.socket\n', '    CONNECTION_TIMES_OUT: the connection hangs until it times out.\n', '        The default timeout value is low (1e-16) to speed up the tests.\n', '        Timeout errors are created by mocking requests.request\n', '    HF_DATASETS_OFFLINE_SET_TO_1: the HF_DATASETS_OFFLINE environment variable is set to 1.\n', '        This makes the http/ftp calls of the library instantly fail and raise an OfflineModeEmabled error.\n', '    """"""\n']","['requests.Session', 'timeout_request', 'kwargs.get', 'RequestWouldHangIndefinitelyError', 'online_request', 'raise_connection_error', 'requests.ConnectionError', 'patch', 'ValueError']",9
repos/datasets/tests/utils.py:packaged,packaged,function,5,12,10,118,9.83,0,1,['test_case'],[None],[None],292,"['    """"""\n', '    Decorator marking a test as packaged\n', '\n', '    Packaged tests are run by default. Set the RUN_PACKAGED environment variable\n', '    to a falsy value to not run them.\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:parse_flag_from_env,parse_flag_from_env,function,8,24,20,169,7.04,0,0,"['key', 'default']","[None, None]","[None, 'False']",24,[],"['strtobool', 'ValueError']",2
repos/datasets/tests/utils.py:pytest_xdist_worker_id,pytest_xdist_worker_id,function,4,11,10,107,9.73,0,0,[],[],[],525,"['    """"""\n', ""    Returns an int value of worker's numerical id under `pytest-xdist`'s concurrent workers `pytest -n N` regime, or 0\n"", ""    if `-n 1` or `pytest-xdist` isn't being used.\n"", '    """"""\n']","['re.sub', 'int']",2
repos/datasets/tests/utils.py:remote,remote,function,5,12,10,118,9.83,0,1,['test_case'],[None],[None],304,"['    """"""\n', '    Decorator marking a test as one that relies on GitHub or the Hugging Face Hub.\n', '\n', '    Remote tests are skipped by default. Set the RUN_REMOTE environment variable\n', '    to a falsy value to not run them.\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_elasticsearch,require_elasticsearch,function,7,13,12,128,9.85,0,0,['test_case'],[None],[None],104,"['    """"""\n', '    Decorator marking a test that requires ElasticSearch.\n', '\n', ""    These tests are skipped when ElasticSearch isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_faiss,require_faiss,function,7,13,12,112,8.62,0,0,['test_case'],[None],[None],76,"['    """"""\n', '    Decorator marking a test that requires Faiss.\n', '\n', ""    These tests are skipped when Faiss isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_jax,require_jax,function,4,9,8,96,10.67,0,1,['test_case'],[None],[None],168,"['    """"""\n', '    Decorator marking a test that requires JAX.\n', '\n', ""    These tests are skipped when JAX isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_joblibspark,require_joblibspark,function,8,15,14,130,8.67,0,0,['test_case'],[None],[None],252,"['    """"""\n', '    Decorator marking a test that requires joblibspark.\n', '\n', ""    These tests are skipped when pyspark isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_pil,require_pil,function,4,9,8,99,11.0,0,1,['test_case'],[None],[None],180,"['    """"""\n', '    Decorator marking a test that requires Pillow.\n', '\n', ""    These tests are skipped when Pillow isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_polars,require_polars,function,4,9,8,102,11.33,0,1,['test_case'],[None],[None],144,"['    """"""\n', '    Decorator marking a test that requires Polars.\n', '\n', ""    These tests are skipped when Polars isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_pyspark,require_pyspark,function,8,15,14,122,8.13,0,0,['test_case'],[None],[None],237,"['    """"""\n', '    Decorator marking a test that requires pyspark.\n', '\n', ""    These tests are skipped when pyspark isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_regex,require_regex,function,7,13,12,112,8.62,0,0,['test_case'],[None],[None],90,"['    """"""\n', '    Decorator marking a test that requires regex.\n', '\n', ""    These tests are skipped when Regex isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_spacy,require_spacy,function,8,15,14,118,7.87,0,0,['test_case'],[None],[None],222,"['    """"""\n', '    Decorator marking a test that requires spacy.\n', '\n', ""    These tests are skipped when they aren't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_sqlalchemy,require_sqlalchemy,function,7,13,12,122,9.38,0,0,['test_case'],[None],[None],118,"['    """"""\n', '    Decorator marking a test that requires SQLAlchemy.\n', '\n', ""    These tests are skipped when SQLAlchemy isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_tf,require_tf,function,4,9,8,102,11.33,0,1,['test_case'],[None],[None],156,"['    """"""\n', '    Decorator marking a test that requires TensorFlow.\n', '\n', ""    These tests are skipped when TensorFlow isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_tiktoken,require_tiktoken,function,8,15,14,124,8.27,0,0,['test_case'],[None],[None],207,"['    """"""\n', '    Decorator marking a test that requires tiktoken.\n', '\n', ""    These tests are skipped when transformers isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_torch,require_torch,function,4,9,8,102,11.33,0,1,['test_case'],[None],[None],132,"['    """"""\n', '    Decorator marking a test that requires PyTorch.\n', '\n', ""    These tests are skipped when PyTorch isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:require_transformers,require_transformers,function,8,15,14,132,8.8,0,0,['test_case'],[None],[None],192,"['    """"""\n', '    Decorator marking a test that requires transformers.\n', '\n', ""    These tests are skipped when transformers isn't installed.\n"", '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:set_current_working_directory_to_temp_dir,set_current_working_directory_to_temp_dir,function,6,12,12,169,14.08,0,0,"['*args', '**kwargs']","[None, None]","[None, None]",391,[],"['str', 'tempfile.TemporaryDirectory', 'os.chdir']",3
repos/datasets/tests/utils.py:slow,slow,function,5,12,10,106,8.83,0,1,['test_case'],[None],[None],267,"['    """"""\n', '    Decorator marking a test as slow.\n', '\n', '    Slow tests are skipped by default. Set the RUN_SLOW environment variable\n', '    to a truthy value to run them.\n', '\n', '    """"""\n']",['unittest.skip'],1
repos/datasets/tests/utils.py:xfail_if_500_502_http_error,xfail_if_500_502_http_error,function,12,28,26,273,9.75,0,1,['func'],[None],[None],425,[],"['_wrapper', 'func', 'str', 'pytest.xfail', 'decorator.decorator']",5
repos/datasets/tests/utils.py:OfflineSimulationMode,OfflineSimulationMode,class,3,6,6,72,12.0,0,0,[],[],[],332,[],[],0
repos/datasets/tests/utils.py:RequestWouldHangIndefinitelyError,RequestWouldHangIndefinitelyError,class,0,1,1,4,4.0,0,0,[],[],[],328,[],[],0
repos/datasets/tests/utils.py:_RunOutput,_RunOutput,class,7,11,11,108,9.82,0,0,[],[],[],446,[],[],0
repos/datasets/tests/utils.py:_RunOutput:__init__,_RunOutput:__init__,method,6,6,6,64,10.67,0,0,"['self', 'returncode', 'stdout', 'stderr']","[None, None, None, None]","[None, None, None, None]",447,[],[],0
repos/datasets/utils/release.py:get_version,get_version,function,8,11,11,169,15.36,0,0,[],[],[],48,"['    """"""Reads the current version in the __init__.""""""\n']","['open', 'f.read']",2
repos/datasets/utils/release.py:global_version_update,global_version_update,function,4,8,8,86,10.75,1,0,['version'],[None],[None],42,"['    """"""Update the version in all needed files.""""""\n']","['REPLACE_FILES.items', 'update_version_in_file']",2
repos/datasets/utils/release.py:post_release_work,post_release_work,function,8,25,21,319,12.76,0,1,[],[],[],78,"['    """"""Do all the necesarry post-release steps.""""""\n']","['get_version', 'input', 'len', 'print', 'global_version_update']",5
repos/datasets/utils/release.py:pre_release_work,pre_release_work,function,11,50,38,593,11.86,0,3,['patch'],[None],['False'],56,"['    """"""Do all the necessary pre-release steps.""""""\n']","['get_version', 'ValueError', 'input', 'len', 'print', 'global_version_update']",6
repos/datasets/utils/release.py:update_version_in_file,update_version_in_file,function,10,26,18,257,9.88,0,0,"['fname', 'version', 'pattern']","[None, None, None]","[None, None, None]",31,"['    """"""Update the version in one file using a specific pattern.""""""\n']","['open', 'f.read', 'replace.replace', 're_pattern.sub', 'f.write']",5
repos/llama_index/benchmarks/agent/agent_utils.py:get_model,get_model,function,14,41,32,435,10.61,0,1,['model'],[' str'],[None],38,[],"['OpenAI', 'Anthropic', 'Replicate', 'ValueError']",4
repos/llama_index/benchmarks/agent/agent_utils.py:is_valid_combination,is_valid_combination,function,3,19,17,129,6.79,0,1,"['agent', 'model']","[' str', ' str']","[None, None]",62,[],['print'],1
repos/llama_index/benchmarks/agent/button_tasks.py:get_dial_then_enter,get_dial_then_enter,function,7,20,20,273,13.65,0,0,[],[],[],39,[],"['Phone', 'FunctionTool.from_defaults', 'Task']",3
repos/llama_index/benchmarks/agent/button_tasks.py:get_search_then_dial,get_search_then_dial,function,7,23,23,301,13.09,0,0,[],[],[],53,[],"['Phone', 'FunctionTool.from_defaults', 'Task']",3
repos/llama_index/benchmarks/agent/button_tasks.py:search_number,search_number,function,2,20,18,122,6.1,0,1,"['first_name', 'last_name']","[' str', ' str']","[None, None]",28,"['    """"""Search for a person by first and last name.""""""\n']",[],0
repos/llama_index/benchmarks/agent/button_tasks.py:Phone,Phone,class,10,46,32,357,7.76,0,1,[],[],[],7,[],[],0
repos/llama_index/benchmarks/agent/button_tasks.py:Phone:__init__,Phone:__init__,method,2,4,4,33,8.25,0,0,['self'],[None],[None],8,[],[],0
repos/llama_index/benchmarks/agent/button_tasks.py:Phone:dial_digit,Phone:dial_digit,method,2,8,8,59,7.38,0,0,"['self', 'number']","[None, ' str']","[None, None]",12,"['        """"""Dial a digit  on the phone.""""""\n']","['len', 'number.isdigit']",2
repos/llama_index/benchmarks/agent/button_tasks.py:Phone:enter,Phone:enter,method,1,7,7,66,9.43,0,1,['self'],[None],[None],17,"['        """"""Press the enter key on the phone.""""""\n']",['Exception'],1
repos/llama_index/benchmarks/agent/button_tasks.py:Phone:evaluate,Phone:evaluate,method,5,5,5,51,10.2,0,0,"['self', 'response', 'expected_response']","[None, ' str', ' str']","[None, None, None]",24,[],[],0
repos/llama_index/benchmarks/agent/eval.py:contains_expected_response,contains_expected_response,function,3,4,4,33,8.25,0,0,"['response', 'expected_response']","[' str', ' str']","[None, None]",1,"['    """"""Check if the response contains the expected response.""""""\n']",[],0
repos/llama_index/benchmarks/agent/main.py:benchmark,benchmark,function,17,43,37,289,6.72,3,2,"['AGENTS.keys())', 'models', 'tasks', 'verbose', 'output', 'save', '']","[None, ' List[str] ', ' List[str] ', ' bool ', ' str ', ' bool ', None]","[None, ' ALL_MODELS', ' ALL_TASKS', ' False', ' ""results.csv""', ' True', None]",57,[],"['is_valid_combination', 'evaluate', 'data.append', 'pd.DataFrame', 'df.to_csv']",5
repos/llama_index/benchmarks/agent/main.py:evaluate,evaluate,function,23,96,70,985,10.26,0,4,"['agent', 'model', 'task_name', 'verbose']","[' str', ' str', ' str', ' bool ']","[None, None, None, ' False']",13,[],"['ValueError', 'print', 'get_model', 'agent_cls.from_tools', 'cast', 'agent_.chat', 'task.eval_fn']",7
repos/llama_index/benchmarks/agent/math_tasks.py:add,add,function,3,3,3,9,3.0,0,0,"['a', 'b']","[' int', ' int']","[None, None]",8,"['    """"""Add two integers and returns the result integer.""""""\n']",[],0
repos/llama_index/benchmarks/agent/math_tasks.py:multiply,multiply,function,3,3,3,9,3.0,0,0,"['a', 'b']","[' int', ' int']","[None, None]",13,"['    """"""Multiple two integers and returns the result integer.""""""\n']",[],0
repos/llama_index/benchmarks/agent/task.py:Task,Task,class,11,14,13,129,9.21,0,0,[],[],[],7,[],[],0
repos/llama_index/benchmarks/embeddings/bench_embeddings.py:generate_strings,generate_strings,function,33,99,79,1064,10.75,5,1,"['num_strings', 'string_length']","[' int ', ' int ']","[' 100', ' 10']",14,"['    """"""\n', '    Generate random strings sliced from the paul graham essay of the following form:\n', '\n', '    offset 0: [0:string_length], [string_length:2*string_length], ...\n', '    offset 1: [1:1+string_length], [1+string_length:1+2*string_length],...\n', '    ...\n', '    """"""  # noqa: D415\n', '    content = (\n', '        SimpleDirectoryReader(""../../examples/paul_graham_essay/data"")\n', '        .load_data()[0]\n', '        .get_content()\n', '    )\n', '    content_length = len(content)\n', '\n', '    strings_per_loop = content_length / string_length\n', '    num_loops_upper_bound = int(num_strings / strings_per_loop) + 1\n', '    strings = []\n', '\n', '    for offset in range(num_loops_upper_bound + 1):\n', '        ptr = offset % string_length\n', '        while ptr + string_length < content_length:\n', '            strings.append(content[ptr : ptr + string_length])\n', '            ptr += string_length\n', '            if len(strings) == num_strings:\n', '                break\n', '\n', '    return strings\n', '\n', '\n', 'def create_open_ai_embedding(batch_size: int) -> Tuple[BaseEmbedding, str, int]:\n', '    return (\n', '        OpenAIEmbedding(embed_batch_size=batch_size),\n', '        ""OpenAIEmbedding"",\n', '        4096,\n', '    )\n', '\n', '\n', 'def create_local_embedding(\n', '    model_name: str, batch_size: int\n', ') -> Tuple[BaseEmbedding, str, int]:\n', '    model = resolve_embed_model(f""local:{model_name}"")\n', '    return (\n', '        model,\n', '        ""hf/"" + model_name,\n', '        model._langchain_embedding.client.max_seq_length,  # type: ignore\n', '    )\n', '\n', '\n', 'def bench_simple_vector_store(\n', '    embed_models: List[Callable[[int], Tuple[BaseEmbedding, str, int]]],\n', '    num_strings: List[int] = [100],\n', '    string_lengths: List[int] = [64, 256],\n', '    embed_batch_sizes: List[int] = [1, DEFAULT_EMBED_BATCH_SIZE],\n', '    torch_num_threads: Optional[int] = None,\n', ') -> None:\n', '    """"""Benchmark embeddings.""""""\n']","['print', 'torch.set_num_threads', 'max', 'generate_strings', 'models.append', 'time.time', 'results.append', 'pd.DataFrame']",8
repos/llama_index/benchmarks/struct_indices/spider/evaluate.py:_answer,_answer,function,7,13,13,198,15.23,0,0,"['llm', 'question', 'sql_query', 'sql_result']","[' OpenAI', ' str', ' str', ' Optional[str]']","[None, None, None, None]",46,[],"['answer_template.format', 'llm.chat']",2
repos/llama_index/benchmarks/struct_indices/spider/evaluate.py:_get_answers,_get_answers,function,30,93,77,940,10.11,1,3,"['llm', 'indexes', 'SQLStructStoreIndex]', 'db_names', 'sql_queries', 'examples', 'output_filename', 'use_cache', '']","[' OpenAI', ' Dict[str', None, ' List[str]', ' List[str]', ' List[dict]', ' str', ' bool', None]","[None, None, None, None, None, None, None, None, None]",69,[],"['open', 'json.load', 'tqdm', 'list', 'results.append', 'sql_query.strip', 'query_engine.query', 'isinstance', '_answer', 'print', 'json.dump']",11
repos/llama_index/benchmarks/struct_indices/spider/evaluate.py:_match,_match,function,9,17,17,260,15.29,0,0,"['llm', 'question', 'reference_answer', 'hypothesis_answer']","[' OpenAI', ' str', ' str', ' str']","[None, None, None, None]",56,[],"['match_template.format', 'llm.chat', 'content.lower']",3
repos/llama_index/benchmarks/struct_indices/spider/evaluate.py:_match_answers,_match_answers,function,25,145,104,1268,8.74,1,2,"['llm', 'gold_results', 'pred_results', 'examples', 'output_filename', '']","[' OpenAI', ' List[dict]', ' List[dict]', ' List[dict]', ' str', None]","[None, None, None, None, None, None]",114,[],"['tqdm', 'list', 'set', 'print', '_match', 'results.append', 'sum', 'float', 'open', 'json.dump', 'len']",11
repos/llama_index/benchmarks/struct_indices/spider/generate_sql.py:_generate_sql,_generate_sql,function,11,36,28,348,9.67,0,1,"['llama_index', 'nl_query_text', '']","[' SQLStructStoreIndex', ' str', None]","[None, None, None]",22,"['    """"""Generate SQL query for the given NL query text.""""""\n']","['llama_index.as_query_engine', 'query_engine.query', 'RuntimeError', '_newlines.sub', '_spaces.sub', 'query.strip']",6
repos/llama_index/benchmarks/struct_indices/spider/generate_sql.py:generate_sql,generate_sql,function,11,42,39,390,9.29,1,0,"['llama_indexes', 'examples', 'output_file']","[' dict', ' list', ' str']","[None, None, None]",42,"['    """"""Generate SQL queries for the given examples and write them to the output file.""""""\n']","['open', 'tqdm', '_generate_sql', 'print', 'f.write']",5
repos/llama_index/benchmarks/struct_indices/spider/spider_utils.py:create_indexes,create_indexes,function,23,51,45,584,11.45,2,1,"['spider_dir', 'llm']","[' str', ' OpenAI']","[None, None]",24,"['    """"""Create indexes for all databases.""""""\n']","['os.listdir', 'create_engine', 'SQLDatabase', 'engine.connect', 'connection.execute', 'text', 'LLMPredictor', 'databases.items', 'SQLStructStoreIndex']",9
repos/llama_index/benchmarks/struct_indices/spider/spider_utils.py:load_examples,load_examples,function,8,25,13,272,10.88,0,0,['spider_dir'],[' str'],[None],13,"['    """"""Load examples.""""""\n']","['open', 'json.load']",2
repos/llama_index/benchmarks/vector_stores/bench_simple_vector_store.py:bench_simple_vector_store,bench_simple_vector_store,function,15,47,39,600,12.77,2,0,"['num_vectors', '50', '100', '500', '1000]']","[' List[int] ', None, None, None, None]","[' [10', None, None, None, None]",25,"['    """"""Benchmark simple vector store.""""""\n']","['print', 'generate_nodes', 'SimpleVectorStore', 'time.time', 'vector_store.add', 'VectorStoreQuery', 'vector_store.query']",7
repos/llama_index/benchmarks/vector_stores/bench_simple_vector_store.py:generate_nodes,generate_nodes,function,2,20,17,145,7.25,0,0,"['num_vectors', 'embedding_length']","[' int ', ' int ']","[' 100', ' 1536']",13,[],"['random.seed', 'TextNode', 'range']",3
repos/llama_index/experimental/classifier/utils.py:extract_float_given_response,extract_float_given_response,function,5,21,16,212,10.1,0,2,"['response', 'n']","[' str', ' int ']","[None, ' 1']",57,"['    """"""Extract number given the GPT-generated response.\n', '\n', '    Used by tree-structured indices.\n', '\n', '    """"""\n']","['re.findall', 'len', 'extract_numbers_given_response', 'float']",4
repos/llama_index/experimental/classifier/utils.py:get_eval_preds,get_eval_preds,function,14,29,27,337,11.62,1,1,"['train_prompt', 'train_str', 'eval_df', 'n']","[' BasePromptTemplate', ' str', ' pd.DataFrame', ' int ']","[None, None, None, ' 20']",75,"['    """"""Get eval preds.""""""\n']","['OpenAI', 'range', 'get_sorted_dict_str', 'llm.predict', 'extract_float_given_response', 'print', 'eval_preds.append']",7
repos/llama_index/experimental/classifier/utils.py:get_label_str,get_label_str,function,1,3,3,39,13.0,0,0,"['labels', 'i']","[' pd.Series', ' int']","[None, None]",35,"['    """"""Get label string.""""""\n']",[],0
repos/llama_index/experimental/classifier/utils.py:get_sorted_dict_str,get_sorted_dict_str,function,3,8,8,64,8.0,0,0,['d'],[' dict'],[None],29,"['    """"""Get sorted dict string.""""""\n']",['sorted'],1
repos/llama_index/experimental/classifier/utils.py:get_train_and_eval_data,get_train_and_eval_data,function,13,29,24,305,10.52,0,0,"['csv_path', '']","[' str', None]","[None, None]",14,"['    """"""Get train and eval data.""""""\n']","['pd.read_csv', 'df.drop', 'df.pop', 'train_test_split']",4
repos/llama_index/experimental/classifier/utils.py:get_train_str,get_train_str,function,13,28,26,299,10.68,1,0,"['train_df', 'train_labels', 'train_n']","[' pd.DataFrame', ' pd.Series', ' int ']","[None, None, ' 10']",40,"['    """"""Get train str.""""""\n']","['train_df.to_dict', 'enumerate', 'get_sorted_dict_str', 'get_label_str', 'item_list.append']",5
repos/llama_index/experimental/cli/__main__.py:main,main,function,16,42,39,601,14.31,0,1,[],[],[],11,[],"['ArgumentParser', 'parser.add_argument', 'print_help', 'parser.print_help', 'parser.add_subparsers', 'register_init_cli', 'register_add_cli', 'register_query_cli', 'parser.set_defaults', 'parser.parse_args', 'logger.setLevel', 'args.func']",12
repos/llama_index/experimental/cli/cli_add.py:add_cli,add_cli,function,12,29,20,332,11.45,3,2,['args'],[' Namespace'],[None],9,"['    """"""Handle subcommand ""add"".""""""\n']","['load_index', 'FileNotFoundError', 'SimpleDirectoryReader', 'index.insert', 'save_index']",5
repos/llama_index/experimental/cli/cli_add.py:register_add_cli,register_add_cli,function,4,11,11,144,13.09,0,0,['subparsers'],[' _SubParsersAction'],[None],28,"['    """"""Register subcommand ""add"" to ArgumentParser.""""""\n']","['subparsers.add_parser', 'parser.add_argument', 'parser.set_defaults']",3
repos/llama_index/experimental/cli/cli_init.py:init_cli,init_cli,function,3,4,4,69,17.25,0,0,['args'],[' Namespace'],[None],6,"['    """"""Handle subcommand ""init"".""""""\n']","['load_config', 'save_config']",2
repos/llama_index/experimental/cli/cli_init.py:register_init_cli,register_init_cli,function,4,11,11,155,14.09,0,0,['subparsers'],[' _SubParsersAction'],[None],12,"['    """"""Register subcommand ""init"" to ArgumentParser.""""""\n']","['subparsers.add_parser', 'parser.add_argument', 'parser.set_defaults']",3
repos/llama_index/experimental/cli/cli_query.py:query_cli,query_cli,function,5,5,5,93,18.6,0,0,['args'],[' Namespace'],[None],6,"['    """"""Handle subcommand ""query"".""""""\n']","['load_index', 'index.as_query_engine', 'print']",3
repos/llama_index/experimental/cli/cli_query.py:register_query_cli,register_query_cli,function,4,7,7,119,17.0,0,0,['subparsers'],[' _SubParsersAction'],[None],13,"['    """"""Register subcommand ""query"" to ArgumentParser.""""""\n']","['subparsers.add_parser', 'parser.add_argument', 'parser.set_defaults']",3
repos/llama_index/experimental/cli/configuration.py:_load_embed_model,_load_embed_model,function,5,10,9,129,12.9,0,1,['config'],[' ConfigParser'],[None],112,"['    """"""Internal function to load embedding model based on configuration.""""""\n']","['OpenAIEmbedding', 'KeyError']",2
repos/llama_index/experimental/cli/configuration.py:_load_llm,_load_llm,function,2,9,8,80,8.89,0,1,['section'],[' SectionProxy'],[None],105,[],['OpenAI'],1
repos/llama_index/experimental/cli/configuration.py:_load_llm_predictor,_load_llm_predictor,function,8,19,14,291,15.32,0,1,['config'],[' ConfigParser'],[None],92,"['    """"""Internal function to load LLM predictor based on configuration.""""""\n']","['_load_llm', 'LLMPredictor', 'StructuredLLMPredictor', 'KeyError']",4
repos/llama_index/experimental/cli/configuration.py:_load_service_context,_load_service_context,function,6,9,9,169,18.78,0,0,['config'],[' ConfigParser'],[None],78,"['    """"""Internal function to load service context based on configuration.""""""\n']","['_load_embed_model', '_load_llm_predictor', 'ServiceContext.from_defaults']",3
repos/llama_index/experimental/cli/configuration.py:_load_storage_context,_load_storage_context,function,4,4,4,102,25.5,0,0,['config'],[' ConfigParser'],[None],87,[],['StorageContext.from_defaults'],1
repos/llama_index/experimental/cli/configuration.py:load_config,load_config,function,5,7,6,116,16.57,0,0,['root'],[' str '],"[' "".""']",30,"['    """"""Load configuration from file.""""""\n']","['ConfigParser', 'config.read_dict', 'config.read']",3
repos/llama_index/experimental/cli/configuration.py:load_index,load_index,function,19,41,35,597,14.56,0,1,['root'],[' str '],"[' "".""']",44,"['    """"""Load existing index file.""""""\n']","['load_config', '_load_service_context', 'KeyError', '_load_storage_context', 'load_index_from_storage', 'StorageContext.from_defaults', 'index_type']",7
repos/llama_index/experimental/cli/configuration.py:save_config,save_config,function,3,7,7,71,10.14,0,0,"['config', 'root']","[' ConfigParser', ' str ']","[None, ' "".""']",38,"['    """"""Load configuration to file.""""""\n']","['open', 'config.write']",2
repos/llama_index/experimental/cli/configuration.py:save_index,save_index,function,5,5,5,122,24.4,0,0,"['index', 'root']","[' BaseIndex[Any]', ' str ']","[None, ' "".""']",71,"['    """"""Save index to file.""""""\n']",['load_config'],1
repos/llama_index/experimental/openai_fine_tuning/launch_training.py:launch_training,launch_training,function,10,39,38,387,9.92,1,0,['data_path'],[' str'],[None],12,[],"['validate_json', 'open', 'print', 'time.sleep']",4
repos/llama_index/experimental/openai_fine_tuning/validate_json.py:validate_json,validate_json,function,59,409,245,3710,9.07,8,13,['data_path'],[' str'],[None],19,[],"['open', 'print', 'len', 'defaultdict', 'isinstance', 'ex.get', 'any', 'message.get', 'format_errors.items', 'tiktoken.get_encoding', 'num_tokens_from_messages', 'message.items', 'num_assistant_tokens_from_messages', 'print_distribution', 'n_messages.append', 'convo_lens.append', 'assistant_message_lens.append', 'sum', 'min', 'max']",20
repos/llama_index/experimental/splitter_playground/app.py:load_document,load_document,function,13,19,19,252,13.26,1,0,['uploaded_files'],[' List[UploadedFile]'],[None],31,[],"['tempfile.TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'reader.load_data']",5
repos/llama_index/llama-index-cli/llama_index/cli/command_line.py:default_rag_cli,default_rag_cli,function,30,164,96,2020,12.32,0,1,[],[],[],70,[],"['ImportError', 'default_ragcli_persist_dir', 'chromadb.PersistentClient', 'chroma_client.create_collection', 'ChromaVectorStore', 'SimpleDocumentStore', 'IngestionPipeline', 'OpenAIEmbedding', 'cache=IngestionCache', 'ingestion_pipeline.load', 'RagCLI', 'print']",12
repos/llama_index/llama-index-cli/llama_index/cli/command_line.py:handle_download_llama_dataset,handle_download_llama_dataset,function,4,23,19,390,16.96,0,0,"['llama_dataset_class', 'download_dir', 'llama_hub_url', 'llama_datasets_lfs_url', 'llama_datasets_source_files_tree_url', '**kwargs', '']","[' Optional[str] ', ' Optional[str] ', ' str ', ' str ', ' str ', ' Any', None]","[' None', ' None', ' LLAMA_HUB_URL', ' LLAMA_DATASETS_LFS_URL', ' LLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL', None, None]",47,[],"['download_llama_dataset', 'print']",2
repos/llama_index/llama-index-cli/llama_index/cli/command_line.py:handle_download_llama_pack,handle_download_llama_pack,function,4,22,18,265,12.05,0,0,"['llama_pack_class', 'download_dir', 'llama_pack_url', '**kwargs', '']","[' Optional[str] ', ' Optional[str] ', ' str ', ' Any', None]","[' None', ' None', ' LLAMA_PACKS_CONTENTS_URL', None, None]",30,[],"['download_llama_pack', 'print']",2
repos/llama_index/llama-index-cli/llama_index/cli/command_line.py:handle_init_package,handle_init_package,function,2,6,6,116,19.33,0,0,"['name', 'kind', 'prefix', '**kwargs']","[' str', ' str', ' Optional[str] ', ' Any']","[None, None, ' None', None]",23,[],"['init_new_package', 'print']",2
repos/llama_index/llama-index-cli/llama_index/cli/command_line.py:main,main,function,25,249,132,2742,11.01,0,0,[],[],[],149,[],"['argparse.ArgumentParser', 'parser.add_subparsers', 'subparsers.add_parser', 'RagCLI.add_parser_args', 'llamapack_parser.add_argument', 'llamapack_parser.set_defaults', 'handle_download_llama_pack', 'llamadataset_parser.add_argument', 'llamadataset_parser.set_defaults', 'handle_download_llama_dataset', 'upgrade_parser.add_argument', 'upgrade_parser.set_defaults', 'upgrade_dir', 'upgrade_file_parser.add_argument', 'upgrade_file_parser.set_defaults', 'upgrade_file', 'new_package_parser.add_argument', 'new_package_parser.set_defaults', 'handle_init_package', 'parser.parse_args', 'args.func']",21
repos/llama_index/llama-index-core/llama_index/core/async_utils.py:asyncio_module,asyncio_module,function,8,13,10,105,8.08,0,1,['show_progress'],[' bool '],[' False'],12,[],[],0
repos/llama_index/llama-index-core/llama_index/core/async_utils.py:asyncio_run,asyncio_run,function,6,9,8,114,12.67,0,0,['coro'],[' Coroutine'],[None],23,"['    """"""Gets an existing event loop to run the coroutine.\n', '\n', '    If there is no existing event loop, creates a new one.\n', '    """"""\n']","['asyncio.get_event_loop', 'loop.run_until_complete', 'asyncio.run']",3
repos/llama_index/llama-index-core/llama_index/core/async_utils.py:chunks,chunks,function,3,6,6,66,11.0,0,0,"['iterable', 'size']","[' Iterable', ' int']","[None, None]",70,[],['zip_longest'],1
repos/llama_index/llama-index-core/llama_index/core/async_utils.py:get_asyncio_module,get_asyncio_module,function,8,13,10,105,8.08,0,1,['show_progress'],[' bool '],[' False'],87,[],[],0
repos/llama_index/llama-index-core/llama_index/core/async_utils.py:run_async_tasks,run_async_tasks,function,23,45,34,485,10.78,0,1,"['tasks', 'show_progress', 'progress_bar_desc', '']","[' List[Coroutine]', ' bool ', ' str ', None]","[None, ' False', ' ""Running async tasks""', None]",35,"['    """"""Run a list of async tasks.""""""\n']","['nest_asyncio.apply', 'asyncio.get_event_loop', '_tqdm_gather', 'tqdm.gather', 'loop.run_until_complete', '_gather', 'asyncio.gather', 'asyncio_run']",8
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:_contains_protected_access,_contains_protected_access,function,15,41,30,524,12.78,2,2,['code'],[' str'],[None],117,[],"['ast.parse', 'ast.iter_child_nodes', 'isinstance', 'DunderVisitor', 'dunder_visitor.visit']",5
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:_get_restricted_globals,_get_restricted_globals,function,5,7,6,125,17.86,0,1,"['__globals', 'None]']","[' Union[dict', None]","[None, None]",82,[],"['copy.deepcopy', 'restricted_globals.update']",2
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:_restricted_import,_restricted_import,function,5,18,18,133,7.39,0,1,"['name', 'globals', 'object]', 'None] ', 'locals', 'object]', 'None] ', ')', 'level', '']","[' str', ' Union[Mapping[str', None, None, ' Union[Mapping[str', None, None, None, ' int ', None]","[None, None, None, ' None', None, None, ' None', None, ' 0', None]",19,[],"['__import__', 'ImportError']",2
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:_verify_source_safety,_verify_source_safety,function,5,38,31,319,8.39,0,3,"['__source', 'bytes', 'CodeType]']","[' Union[str', None, None]","[None, None, None]",143,"['    """"""\n', '    Verify that the source is safe to execute. For now, this means that it\n', '    does not contain any references to private or dunder methods.\n', '    """"""\n']","['isinstance', 'RuntimeError', '__source.decode', '_contains_protected_access']",4
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:safe_eval,safe_eval,function,3,5,5,96,19.2,0,0,"['__source', 'bytes', 'CodeType]', '__globals', 'Any]', 'None] ', '__locals', 'object]', 'None] ', '']","[' Union[str', None, None, ' Union[Dict[str', None, None, ' Union[Mapping[str', None, None, None]","[None, None, None, None, None, ' None', None, None, ' None', None]",159,"['    """"""\n', '    eval within safe global context.\n', '    """"""\n']","['_verify_source_safety', 'eval', '_get_restricted_globals']",3
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:safe_exec,safe_exec,function,3,5,5,96,19.2,0,0,"['__source', 'bytes', 'CodeType]', '__globals', 'Any]', 'None] ', '__locals', 'object]', 'None] ', '']","[' Union[str', None, None, ' Union[Dict[str', None, None, ' Union[Mapping[str', None, None, None]","[None, None, None, None, None, ' None', None, None, ' None', None]",171,"['    """"""\n', '    eval within safe global context.\n', '    """"""\n']","['_verify_source_safety', 'exec', '_get_restricted_globals']",3
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:DunderVisitor,DunderVisitor,class,15,56,29,656,11.71,0,4,[],[],[],94,[],[],0
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:DunderVisitor:__init__,DunderVisitor:__init__,method,5,8,6,149,18.62,0,0,['self'],[None],[None],95,[],['globals'],1
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:DunderVisitor:visit_Attribute,DunderVisitor:visit_Attribute,method,8,16,12,197,12.31,0,2,"['self', 'node']","[None, ' ast.Attribute']","[None, None]",109,[],['self.generic_visit'],1
repos/llama_index/llama-index-core/llama_index/core/exec_utils.py:DunderVisitor:visit_Name,DunderVisitor:visit_Name,method,8,16,12,191,11.94,0,2,"['self', 'node']","[None, ' ast.Name']","[None, None]",102,[],['self.generic_visit'],1
repos/llama_index/llama-index-core/llama_index/core/image_retriever.py:BaseImageRetriever,BaseImageRetriever,class,18,108,37,1453,13.45,0,4,[],[],[],9,[],[],0
repos/llama_index/llama-index-core/llama_index/core/image_retriever.py:BaseImageRetriever:_image_to_image_retrieve,BaseImageRetriever:_image_to_image_retrieve,method,12,57,29,746,13.09,0,2,"['self', 'query_bundle', '']","[None, ' QueryBundle', None]","[None, None, None]",53,[],"['atext_to_image_retrieve', 'isinstance', 'QueryBundle', 'self._atext_to_image_retrieve', '_atext_to_image_retrieve', 'aimage_to_image_retrieve', 'self._aimage_to_image_retrieve', '_aimage_to_image_retrieve']",8
repos/llama_index/llama-index-core/llama_index/core/image_retriever.py:BaseImageRetriever:_text_to_image_retrieve,BaseImageRetriever:_text_to_image_retrieve,method,15,84,34,1109,13.2,0,3,"['self', 'query_bundle', '']","[None, ' QueryBundle', None]","[None, None, None]",26,[],"['image_to_image_retrieve', 'isinstance', 'QueryBundle', 'self._image_to_image_retrieve', '_image_to_image_retrieve', 'atext_to_image_retrieve', 'self._atext_to_image_retrieve', '_atext_to_image_retrieve', 'aimage_to_image_retrieve', 'self._aimage_to_image_retrieve', '_aimage_to_image_retrieve']",11
repos/llama_index/llama-index-core/llama_index/core/image_retriever.py:BaseImageRetriever:image_to_image_retrieve,BaseImageRetriever:image_to_image_retrieve,method,5,10,10,174,17.4,0,1,"['self', 'str_or_query_bundle']","[None, ' QueryType']","[None, None]",36,"['        """"""Retrieve image nodes given single image input.\n', '\n', '        Args:\n', '            str_or_query_bundle (QueryType): a image path\n', '            string or a QueryBundle object.\n', '        """"""\n']","['isinstance', 'QueryBundle', 'self._image_to_image_retrieve']",3
repos/llama_index/llama-index-core/llama_index/core/image_retriever.py:BaseImageRetriever:text_to_image_retrieve,BaseImageRetriever:text_to_image_retrieve,method,5,7,7,157,22.43,0,1,"['self', 'str_or_query_bundle']","[None, ' QueryType']","[None, None]",12,"['        """"""Retrieve image nodes given query or single image input.\n', '\n', '        Args:\n', '            str_or_query_bundle (QueryType): a query text\n', '            string or a QueryBundle object.\n', '        """"""\n']","['isinstance', 'QueryBundle', 'self._text_to_image_retrieve']",3
repos/llama_index/llama-index-core/llama_index/core/img_utils.py:b64_2_img,b64_2_img,function,4,4,4,59,14.75,0,0,['data'],[' str'],[None],16,"['    """"""Convert base64 encoded image str to a PIL.Image.""""""\n']","['BytesIO', 'Image.open']",2
repos/llama_index/llama-index-core/llama_index/core/img_utils.py:img_2_b64,img_2_b64,function,5,7,7,95,13.57,0,0,"['image', 'format']","[' Image', ' str ']","[None, ' ""JPEG""']",9,"['    """"""Convert a PIL.Image to a base64 encoded image str.""""""\n']","['BytesIO', 'image.save', 'cast', 'base64.b64encode']",4
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent,BaseComponent,class,39,159,90,1480,9.31,2,3,[],[],[],33,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode,BaseNode,class,61,347,159,3409,9.82,0,12,[],[],[],182,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document,Document,class,56,239,151,2570,10.75,0,1,[],[],[],631,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:ImageDocument,ImageDocument,class,2,7,7,59,8.43,0,0,[],[],[],762,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:ImageNode,ImageNode,class,23,80,53,681,8.51,0,1,[],[],[],452,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:IndexNode,IndexNode,class,31,124,83,1128,9.1,0,2,[],[],[],492,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:MetadataMode,MetadataMode,class,4,8,8,45,5.62,0,0,[],[],[],160,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeRelationship,NodeRelationship,class,6,10,7,68,6.8,0,0,[],[],[],134,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore,NodeWithScore,class,29,134,72,1112,8.3,0,5,[],[],[],569,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:ObjectType,ObjectType,class,5,8,6,53,6.62,0,0,[],[],[],153,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:QueryBundle,QueryBundle,class,15,49,32,461,9.41,0,3,[],[],[],771,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:RelatedNodeInfo,RelatedNodeInfo,class,12,19,18,184,9.68,0,0,[],[],[],167,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode,TextNode,class,40,211,122,2063,9.78,3,6,[],[],[],357,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:TransformComponent,TransformComponent,class,7,25,18,241,9.64,0,0,[],[],[],119,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent:__getstate__,BaseComponent:__getstate__,method,13,29,22,335,11.55,2,2,['self'],[None],[None],64,[],"['super', 'key.endswith', 'keys_to_remove.append', 'str']",4
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent:__setstate__,BaseComponent:__setstate__,method,2,8,8,96,12.0,0,0,"['self', 'state', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",86,[],"['self.__init__', 'super']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent:class_name,BaseComponent:class_name,method,1,2,2,22,11.0,0,0,['cls'],[None],[None],47,"['        """"""\n', '        Get the class name, used as a unique ID in serialization.\n', '\n', '        This provides a key that makes serialization robust against actual class\n', '        name changes.\n', '        """"""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent:dict,BaseComponent:dict,method,5,6,5,75,12.5,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",59,[],"['super', 'self.class_name']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent:from_dict,BaseComponent:from_dict,method,1,1,1,19,19.0,0,0,"['cls', 'data', 'Any]', '**kwargs', 'dict)']","[None, ' Dict[str', None, ' Any) -> Self:  # type: ignorekwargs', '']","[None, None, None, None, None]",106,[],['data.update'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent:from_json,BaseComponent:from_json,method,5,16,16,123,7.69,0,0,"['cls', 'data_str', '**kwargs', '**kwargs)']","[None, ' str', ' Any) -> Self:  # type: ignoredata_str)data', None]","[None, None, None, None]",114,[],"['from_json', 'json.loads', 'cls.from_dict']",3
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent:json,BaseComponent:json,method,2,2,2,28,14.0,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",56,[],['self.to_json'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent:to_dict,BaseComponent:to_dict,method,5,6,5,72,12.0,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",95,[],"['self.dict', 'self.class_name']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseComponent:to_json,BaseComponent:to_json,method,4,4,4,50,12.5,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",100,[],"['self.to_dict', 'json.dumps']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:__str__,BaseNode:__str__,method,5,15,14,223,14.87,0,0,['self'],[None],[None],328,[],"['truncate_text', 'self.get_content', 'textwrap.fill']",3
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:as_related_node_info,BaseNode:as_related_node_info,method,4,7,7,113,16.14,0,0,['self'],[None],[None],347,"['        """"""Get node as RelatedNodeInfo.""""""\n']",['RelatedNodeInfo'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:child_nodes,BaseNode:child_nodes,method,7,25,21,226,9.04,0,2,['self'],[None],[None],305,"['        """"""Child nodes.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:extra_info,BaseNode:extra_info,method,2,2,2,19,9.5,0,0,['self'],[None],[None],324,"['        """"""TODO: DEPRECATED: Extra info.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:get_content,BaseNode:get_content,method,42,249,104,2483,9.97,0,12,"['self', 'metadata_mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.ALL']",232,"['        """"""Get object content.""""""\n']","['get_metadata_str', 'set_content', 'hash', 'node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",20
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:get_embedding,BaseNode:get_embedding,method,3,10,9,80,8.0,0,1,['self'],[None],[None],337,"['        """"""Get embedding.\n', '\n', '        Errors if embedding is None.\n', '\n', '        """"""\n']",['ValueError'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:get_metadata_str,BaseNode:get_metadata_str,method,41,241,100,2400,9.96,0,12,"['self', 'mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.ALL']",236,"['        """"""Metadata string.""""""\n']","['set_content', 'hash', 'node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",19
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:get_type,BaseNode:get_type,method,43,257,106,2570,10.0,0,12,['cls'],[None],[None],228,"['        """"""Get Object type.""""""\n']","['get_content', 'get_metadata_str', 'set_content', 'hash', 'node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",21
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:hash,BaseNode:hash,method,39,228,96,2300,10.09,0,12,['self'],[None],[None],245,"['        """"""Get hash of node.""""""\n']","['node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",17
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:next_node,BaseNode:next_node,method,7,24,20,231,9.62,0,2,['self'],[None],[None],283,"['        """"""Next node.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:node_id,BaseNode:node_id,method,2,2,2,14,7.0,0,0,['self'],[None],[None],249,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:node_id,BaseNode:node_id,method,2,2,2,14,7.0,0,0,['self'],[None],[None],253,"['        """"""Source object node.\n', '\n', '        Extracted from the relationships field.\n', '\n', '        """"""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:parent_node,BaseNode:parent_node,method,7,24,20,237,9.88,0,2,['self'],[None],[None],294,"['        """"""Parent node.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:prev_node,BaseNode:prev_node,method,7,24,20,243,10.12,0,2,['self'],[None],[None],272,"['        """"""Prev node.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:ref_doc_id,BaseNode:ref_doc_id,method,4,10,8,86,8.6,0,1,['self'],[None],[None],316,"['        """"""Deprecated: Get ref doc id.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:set_content,BaseNode:set_content,method,40,234,98,2346,10.03,0,12,"['self', 'value']","[None, ' Any']","[None, None]",240,"['        """"""Set the content of the node.""""""\n']","['hash', 'node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",18
repos/llama_index/llama-index-core/llama_index/core/schema.py:BaseNode:source_node,BaseNode:source_node,method,7,23,20,223,9.7,0,2,['self'],[None],[None],257,"['        """"""Source object node.\n', '\n', '        Extracted from the relationships field.\n', '\n', '        """"""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:__setattr__,Document:__setattr__,method,4,8,7,91,11.38,0,1,"['self', 'name', 'value']","[None, ' str', ' object']","[None, None, None]",670,[],['super'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:__str__,Document:__str__,method,5,15,14,221,14.73,0,0,['self'],[None],[None],328,[],"['truncate_text', 'self.get_content', 'textwrap.fill']",3
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:class_name,Document:class_name,method,1,2,2,16,8.0,0,0,['cls'],[None],[None],385,"['        """"""Get Object type.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:doc_id,Document:doc_id,method,2,2,2,14,7.0,0,0,['self'],[None],[None],653,"['        """"""Get document ID.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:example,Document:example,method,2,8,8,92,11.5,0,0,['cls'],[None],[None],751,[],['Document'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:from_embedchain_format,Document:from_embedchain_format,method,2,6,6,95,15.83,0,0,"['cls', 'doc', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",710,"['        """"""Convert struct from EmbedChain document format.""""""\n']",['cls'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:from_haystack_format,Document:from_haystack_format,method,2,7,7,82,11.71,0,0,"['cls', 'doc']","[None, ' ""HaystackDocument""']","[None, None]",696,"['        """"""Convert struct from Haystack document format.""""""\n']",['cls'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:from_langchain_format,Document:from_langchain_format,method,2,3,3,54,18.0,0,0,"['cls', 'doc']","[None, ' ""LCDocument""']","[None, None]",683,"['        """"""Convert struct from LangChain document format.""""""\n']",['cls'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:from_semantic_kernel_format,Document:from_semantic_kernel_format,method,3,15,15,168,11.2,0,0,"['cls', 'doc']","[None, ' ""MemoryRecord""']","[None, None]",731,"['        """"""Convert struct from Semantic Kernel document format.""""""\n']",['cls'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:get_doc_id,Document:get_doc_id,method,2,2,2,14,7.0,0,0,['self'],[None],[None],666,"['        """"""TODO: Deprecated: Get document ID.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:get_type,Document:get_type,method,2,2,2,25,12.5,0,0,['cls'],[None],[None],228,"['        """"""Get Object type.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:to_embedchain_format,Document:to_embedchain_format,method,1,10,10,84,8.4,0,0,['self'],[None],[None],702,"['        """"""Convert struct to EmbedChain document format.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:to_haystack_format,Document:to_haystack_format,method,6,13,13,151,11.62,0,0,['self'],[None],[None],687,"['        """"""Convert struct to Haystack document format.""""""\n']",['HaystackDocument'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:to_langchain_format,Document:to_langchain_format,method,9,13,13,149,11.46,0,0,['self'],[None],[None],675,"['        """"""Convert struct to LangChain document format.""""""\n']",['LCDocument'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:to_semantic_kernel_format,Document:to_semantic_kernel_format,method,8,19,18,230,12.11,0,0,['self'],[None],[None],718,"['        """"""Convert struct to Semantic Kernel document format.""""""\n']",['MemoryRecord'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:Document:to_vectorflow,Document:to_vectorflow,method,7,9,9,118,13.11,0,0,"['self', 'client']","[None, ' Any']","[None, None]",740,"['        """"""Send a document to vectorflow, since they don\'t have a document object.""""""\n']","['tempfile.NamedTemporaryFile', 'f.write', 'f.flush', 'client.embed']",4
repos/llama_index/llama-index-core/llama_index/core/schema.py:ImageDocument:class_name,ImageDocument:class_name,method,1,2,2,21,10.5,0,0,['cls'],[None],[None],47,"['        """"""\n', '        Get the class name, used as a unique ID in serialization.\n', '\n', '        This provides a key that makes serialization robust against actual class\n', '        name changes.\n', '        """"""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:ImageNode:class_name,ImageNode:class_name,method,1,2,2,17,8.5,0,0,['cls'],[None],[None],471,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:ImageNode:get_type,ImageNode:get_type,method,2,2,2,22,11.0,0,0,['cls'],[None],[None],467,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:ImageNode:resolve_image,ImageNode:resolve_image,method,11,34,24,290,8.53,0,1,['self'],[None],[None],474,"['        """"""Resolve an image such that PIL can read it.""""""\n']","['BytesIO', 'requests.get', 'ValueError']",3
repos/llama_index/llama-index-core/llama_index/core/schema.py:IndexNode:class_name,IndexNode:class_name,method,1,2,2,17,8.5,0,0,['cls'],[None],[None],565,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:IndexNode:dict,IndexNode:dict,method,14,38,31,392,10.32,0,1,"['self', '**kwargs']","[None, ' Any']","[None, None]",505,[],"['super', 'isinstance', 'doc_to_json', 'json.dumps', 'ValueError', 'str']",6
repos/llama_index/llama-index-core/llama_index/core/schema.py:IndexNode:from_dict,IndexNode:from_dict,method,2,2,2,29,14.5,0,0,"['cls', 'data', 'Any]', '**kwargs', '**kwargs)""obj""', 'None)parsed_obj ', 'str)']","[None, ' Dict[str', None, ' Any) -> Self:  # type: ignore).from_dict(data', None, None, '']","[None, None, None, None, None, ' Noneobj', None]",539,[],['TextNode'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:IndexNode:from_text_node,IndexNode:from_text_node,method,3,5,5,46,9.2,0,0,"['cls', 'node', 'index_id', '']","[None, ' TextNode', ' str', None]","[None, None, None, None]",525,"['        """"""Create index node from text node.""""""\n']",['cls'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:IndexNode:get_type,IndexNode:get_type,method,2,2,2,22,11.0,0,0,['cls'],[None],[None],561,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:__str__,NodeWithScore:__str__,method,3,12,12,100,8.33,0,1,['self'],[None],[None],573,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:class_name,NodeWithScore:class_name,method,1,2,2,21,10.5,0,0,['cls'],[None],[None],588,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:embedding,NodeWithScore:embedding,method,2,2,2,25,12.5,0,0,['self'],[None],[None],612,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:get_content,NodeWithScore:get_content,method,2,2,2,56,28.0,0,0,"['self', 'metadata_mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.NONE']",621,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:get_embedding,NodeWithScore:get_embedding,method,2,2,2,31,15.5,0,0,['self'],[None],[None],624,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:get_score,NodeWithScore:get_score,method,4,16,12,105,6.56,0,2,"['self', 'raise_error']","[None, ' bool ']","[None, ' False']",577,"['        """"""Get score.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:get_text,NodeWithScore:get_text,method,4,15,15,115,7.67,0,1,['self'],[None],[None],615,[],"['isinstance', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:id_,NodeWithScore:id_,method,2,2,2,19,9.5,0,0,['self'],[None],[None],597,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:metadata,NodeWithScore:metadata,method,2,2,2,24,12.0,0,0,['self'],[None],[None],608,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:node_id,NodeWithScore:node_id,method,2,2,2,23,11.5,0,0,['self'],[None],[None],593,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:NodeWithScore:text,NodeWithScore:text,method,4,15,15,109,7.27,0,1,['self'],[None],[None],601,[],"['isinstance', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/schema.py:QueryBundle:__str__,QueryBundle:__str__,method,2,2,2,20,10.0,0,0,['self'],[None],[None],808,"['        """"""Convert to string representation.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:QueryBundle:embedding_image,QueryBundle:embedding_image,method,2,8,7,57,7.12,0,1,['self'],[None],[None],802,"['        """"""Use image path for image retrieval.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:QueryBundle:embedding_strs,QueryBundle:embedding_strs,method,2,14,10,132,9.43,0,2,['self'],[None],[None],792,"['        """"""Use custom embedding strs if specified, otherwise use query str.""""""\n']",['len'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:RelatedNodeInfo:class_name,RelatedNodeInfo:class_name,method,1,2,2,23,11.5,0,0,['cls'],[None],[None],174,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode:class_name,TextNode:class_name,method,1,2,2,16,8.0,0,0,['cls'],[None],[None],385,[],[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode:get_content,TextNode:get_content,method,5,12,11,184,15.33,0,1,"['self', 'metadata_mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.NONE']",398,"['        """"""Get object content.""""""\n']",['self.get_metadata_str'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode:get_metadata_str,TextNode:get_metadata_str,method,14,47,27,508,10.81,3,5,"['self', 'mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.ALL']",408,"['        """"""Metadata info string.""""""\n']","['set', 'usable_metadata_keys.remove', 'value=str']",3
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode:get_node_info,TextNode:get_node_info,method,1,5,5,59,11.8,0,0,['self'],[None],[None],435,"['        """"""Get node info.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode:get_text,TextNode:get_text,method,2,2,2,55,27.5,0,0,['self'],[None],[None],439,[],['self.get_content'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode:get_type,TextNode:get_type,method,2,2,2,21,10.5,0,0,['cls'],[None],[None],394,"['        """"""Get Object type.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode:hash,TextNode:hash,method,3,6,6,122,20.33,0,0,['self'],[None],[None],389,[],['str'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode:node_info,TextNode:node_info,method,2,2,2,26,13.0,0,0,['self'],[None],[None],443,"['        """"""Deprecated: Get node info.""""""\n']",['self.get_node_info'],1
repos/llama_index/llama-index-core/llama_index/core/schema.py:TextNode:set_content,TextNode:set_content,method,2,2,2,15,7.5,0,0,"['self', 'value']","[None, ' str']","[None, None]",431,"['        """"""Set the content of the node.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/schema.py:TransformComponent:__call__,TransformComponent:__call__,method,4,12,12,110,9.17,0,0,"['self', 'nodes', '**kwargs']","[None, ' List[""BaseNode""]', ' Any']","[None, None, None]",126,"['        """"""Transform nodes.""""""\n']","['acall', 'self.__call__']",2
repos/llama_index/llama-index-core/llama_index/core/service_context.py:_get_default_node_parser,_get_default_node_parser,function,2,8,8,131,16.38,0,0,"['chunk_size', 'chunk_overlap', 'callback_manager', '']","[' int ', ' int ', ' Optional[CallbackManager] ', None]","[' DEFAULT_CHUNK_SIZE', ' SENTENCE_CHUNK_OVERLAP', ' None', None]",33,"['    """"""Get default node parser.""""""\n']","['SentenceSplitter', 'CallbackManager']",2
repos/llama_index/llama-index-core/llama_index/core/service_context.py:_get_default_prompt_helper,_get_default_prompt_helper,function,6,16,12,191,11.94,0,2,"['llm_metadata', 'context_window', 'num_output', '']","[' LLMMetadata', ' Optional[int] ', ' Optional[int] ', None]","[None, ' None', ' None', None]",46,"['    """"""Get default prompt helper.""""""\n']",['PromptHelper.from_llm_metadata'],1
repos/llama_index/llama-index-core/llama_index/core/service_context.py:set_global_service_context,set_global_service_context,function,18,23,23,427,18.57,0,1,['service_context'],[' Optional[ServiceContext]'],[None],399,"['    """"""Helper function to set the global service context.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/service_context.py:ServiceContext,ServiceContext,class,86,607,237,8044,13.25,3,21,[],[],[],68,[],[],0
repos/llama_index/llama-index-core/llama_index/core/service_context.py:ServiceContextData,ServiceContextData,class,6,10,7,90,9.0,0,0,[],[],[],59,[],[],0
repos/llama_index/llama-index-core/llama_index/core/service_context.py:ServiceContext:from_defaults,ServiceContext:from_defaults,method,41,190,105,2517,13.25,0,9,"['cls', 'llm_predictor', 'llm', 'prompt_helper', 'embed_model', 'node_parser', 'text_splitter', 'transformations', 'llama_logger', 'callback_manager', 'system_prompt', 'query_wrapper_prompt', 'pydantic_program_mode', 'chunk_size', 'chunk_overlap', 'context_window', 'num_output', 'chunk_size_limit', '']","[None, ' Optional[BaseLLMPredictor] ', ' Optional[LLMType] ', ' Optional[PromptHelper] ', ' Optional[Any] ', ' Optional[NodeParser] ', ' Optional[TextSplitter] ', ' Optional[List[TransformComponent]] ', ' Optional[LlamaLogger] ', ' Optional[CallbackManager] ', ' Optional[str] ', ' Optional[BasePromptTemplate] ', ' PydanticProgramMode ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', None]","[None, ' None', ' ""default""', ' None', ' ""default""', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' PydanticProgramMode.DEFAULT', ' None', ' None', ' None', ' None', ' None', None]",94,"['        """"""Create a ServiceContext from defaults.\n', '        If an argument is specified, then use the argument value provided for that\n', '        parameter. If an argument is not specified, then use the default value.\n', '\n', '        You can change the base defaults by setting llama_index.global_service_context\n', '        to a ServiceContext object with your desired settings.\n', '\n', '        Args:\n', '            llm_predictor (Optional[BaseLLMPredictor]): LLMPredictor\n', '            prompt_helper (Optional[PromptHelper]): PromptHelper\n', '            embed_model (Optional[BaseEmbedding]): BaseEmbedding\n', '                or ""local"" (use local model)\n', '            node_parser (Optional[NodeParser]): NodeParser\n', '            llama_logger (Optional[LlamaLogger]): LlamaLogger (deprecated)\n', '            chunk_size (Optional[int]): chunk_size\n', '            callback_manager (Optional[CallbackManager]): CallbackManager\n', '            system_prompt (Optional[str]): System-wide prompt to be prepended\n', '                to all input prompts, used to guide system ""decision making""\n', '            query_wrapper_prompt (Optional[BasePromptTemplate]): A format to wrap\n', '                passed-in input queries.\n', '\n', '        Deprecated Args:\n', '            chunk_size_limit (Optional[int]): renamed to chunk_size\n', '\n', '        """"""\n']","['cast', 'logger.warning', 'cls.from_service_context', 'CallbackManager', 'ValueError', 'resolve_llm', 'print', 'LLMPredictor', 'isinstance', 'resolve_embed_model', '_get_default_prompt_helper', '_get_default_node_parser', 'LlamaLogger', 'cls']",14
repos/llama_index/llama-index-core/llama_index/core/service_context.py:ServiceContext:from_dict,ServiceContext:from_dict,method,24,45,39,880,19.56,1,0,"['cls', 'data']","[None, ' dict']","[None, None]",368,[],"['ServiceContextData.parse_obj', 'load_predictor', 'load_embed_model', 'PromptHelper.from_dict', 'transformations.append', 'cls.from_defaults']",6
repos/llama_index/llama-index-core/llama_index/core/service_context.py:ServiceContext:from_service_context,ServiceContext:from_service_context,method,44,176,102,2087,11.86,1,11,"['cls', 'service_context', 'llm_predictor', 'llm', 'prompt_helper', 'embed_model', 'node_parser', 'text_splitter', 'transformations', 'llama_logger', 'callback_manager', 'system_prompt', 'query_wrapper_prompt', 'chunk_size', 'chunk_overlap', 'context_window', 'num_output', 'chunk_size_limit', '']","[None, ' ""ServiceContext""', ' Optional[BaseLLMPredictor] ', ' Optional[LLMType] ', ' Optional[PromptHelper] ', ' Optional[Any] ', ' Optional[NodeParser] ', ' Optional[TextSplitter] ', ' Optional[List[TransformComponent]] ', ' Optional[LlamaLogger] ', ' Optional[CallbackManager] ', ' Optional[str] ', ' Optional[BasePromptTemplate] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', None]","[None, None, ' None', ' ""default""', ' None', ' ""default""', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', None]",236,"['        """"""Instantiate a new service context using a previous as the defaults.""""""\n']","['cast', 'logger.warning', 'ValueError', 'resolve_llm', 'LLMPredictor', 'isinstance', 'resolve_embed_model', '_get_default_prompt_helper', '_get_default_node_parser', 'cls']",10
repos/llama_index/llama-index-core/llama_index/core/service_context.py:ServiceContext:llm,ServiceContext:llm,method,2,2,2,28,14.0,0,0,['self'],[None],[None],337,[],[],0
repos/llama_index/llama-index-core/llama_index/core/service_context.py:ServiceContext:node_parser,ServiceContext:node_parser,method,5,14,14,125,8.93,1,1,['self'],[None],[None],341,"['        """"""Get the node parser.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/service_context.py:ServiceContext:to_dict,ServiceContext:to_dict,method,11,22,22,423,19.23,0,0,['self'],[None],[None],348,"['        """"""Convert service context to dict.""""""\n']",['ServiceContextData'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:callback_manager_from_settings_or_context,callback_manager_from_settings_or_context,function,4,9,8,82,9.11,0,1,"['settings', 'context']","[' _Settings', ' Optional[""ServiceContext""]']","[None, None]",277,"['    """"""Get settings from either settings or context.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:embed_model_from_settings_or_context,embed_model_from_settings_or_context,function,4,9,8,72,8.0,0,1,"['settings', 'context']","[' _Settings', ' Optional[""ServiceContext""]']","[None, None]",267,"['    """"""Get settings from either settings or context.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:llm_from_settings_or_context,llm_from_settings_or_context,function,4,9,8,56,6.22,0,1,"['settings', 'context']","[' _Settings', ' Optional[""ServiceContext""]']","[None, None]",257,"['    """"""Get settings from either settings or context.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:node_parser_from_settings_or_context,node_parser_from_settings_or_context,function,4,9,8,72,8.0,0,1,"['settings', 'context']","[' _Settings', ' Optional[""ServiceContext""]']","[None, None]",287,"['    """"""Get settings from either settings or context.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:transformations_from_settings_or_context,transformations_from_settings_or_context,function,4,9,8,80,8.89,0,1,"['settings', 'context']","[' _Settings', ' Optional[""ServiceContext""]']","[None, None]",297,"['    """"""Get settings from either settings or context.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings,_Settings,class,70,404,193,4658,11.53,0,15,[],[],[],21,[],[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:callback_manager,_Settings:callback_manager,method,3,8,6,101,12.62,0,1,['self'],[None],[None],98,"['        """"""Get the callback manager.""""""\n']",['CallbackManager'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:callback_manager,_Settings:callback_manager,method,3,8,6,101,12.62,0,1,['self'],[None],[None],105,"['        """"""Set the callback manager.""""""\n']",['CallbackManager'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:chunk_overlap,_Settings:chunk_overlap,method,4,15,15,151,10.07,0,1,['self'],[None],[None],173,"['        """"""Get the chunk overlap.""""""\n']","['hasattr', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:chunk_overlap,_Settings:chunk_overlap,method,4,15,15,151,10.07,0,1,['self'],[None],[None],181,"['        """"""Set the chunk overlap.""""""\n']","['hasattr', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:chunk_size,_Settings:chunk_size,method,4,15,15,142,9.47,0,1,['self'],[None],[None],157,"['        """"""Get the chunk size.""""""\n']","['hasattr', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:chunk_size,_Settings:chunk_size,method,4,15,15,142,9.47,0,1,['self'],[None],[None],165,"['        """"""Set the chunk size.""""""\n']","['hasattr', 'ValueError']",2
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:context_window,_Settings:context_window,method,2,2,2,39,19.5,0,0,['self'],[None],[None],226,"['        """"""Get the context window.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:context_window,_Settings:context_window,method,2,2,2,39,19.5,0,0,['self'],[None],[None],231,"['        """"""Set the context window.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:embed_model,_Settings:embed_model,method,5,15,10,192,12.8,0,2,['self'],[None],[None],64,"['        """"""Get the embedding model.""""""\n']",['resolve_embed_model'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:embed_model,_Settings:embed_model,method,5,15,10,192,12.8,0,2,['self'],[None],[None],75,"['        """"""Set the embedding model.""""""\n']",['resolve_embed_model'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:global_handler,_Settings:global_handler,method,4,4,4,60,15.0,0,0,['self'],[None],[None],82,"['        """"""Get the global handler.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:global_handler,_Settings:global_handler,method,4,4,4,60,15.0,0,0,['self'],[None],[None],90,"['        """"""Set the global handler.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:llm,_Settings:llm,method,5,15,10,152,10.13,0,2,['self'],[None],[None],47,"['        """"""Set the LLM.""""""\n']",['resolve_llm'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:llm,_Settings:llm,method,5,15,10,152,10.13,0,2,['self'],[None],[None],36,"['        """"""Get the LLM.""""""\n']",['resolve_llm'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:node_parser,_Settings:node_parser,method,5,15,10,180,12.0,0,2,['self'],[None],[None],141,"['        """"""Get the node parser.""""""\n']",['SentenceSplitter'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:node_parser,_Settings:node_parser,method,5,15,10,180,12.0,0,2,['self'],[None],[None],152,"['        """"""Set the node parser.""""""\n']",['SentenceSplitter'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:num_output,_Settings:num_output,method,2,2,2,35,17.5,0,0,['self'],[None],[None],216,"['        """"""Get the number of outputs.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:num_output,_Settings:num_output,method,2,2,2,35,17.5,0,0,['self'],[None],[None],221,"['        """"""Set the number of outputs.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:prompt_helper,_Settings:prompt_helper,method,6,19,12,212,11.16,0,1,['self'],[None],[None],201,"['        """"""Get the prompt helper.""""""\n']","['PromptHelper.from_llm_metadata', 'PromptHelper']",2
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:prompt_helper,_Settings:prompt_helper,method,6,19,12,212,11.16,0,1,['self'],[None],[None],211,"['        """"""Set the prompt helper.""""""\n']","['PromptHelper.from_llm_metadata', 'PromptHelper']",2
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:pydantic_program_mode,_Settings:pydantic_program_mode,method,2,2,2,36,18.0,0,0,['self'],[None],[None],52,"['        """"""Get the pydantic program mode.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:pydantic_program_mode,_Settings:pydantic_program_mode,method,2,2,2,36,18.0,0,0,['self'],[None],[None],57,"['        """"""Set the pydantic program mode.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:text_splitter,_Settings:text_splitter,method,2,2,2,22,11.0,0,0,['self'],[None],[None],191,"['        """"""Get the text splitter.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:text_splitter,_Settings:text_splitter,method,2,2,2,22,11.0,0,0,['self'],[None],[None],196,"['        """"""Set the text splitter.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:tokenizer,_Settings:tokenizer,method,5,10,8,127,12.7,0,1,['self'],[None],[None],112,"['        """"""Get the tokenizer.""""""\n']",['get_tokenizer'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:tokenizer,_Settings:tokenizer,method,5,10,8,127,12.7,0,1,['self'],[None],[None],123,"['        """"""Set the tokenizer.""""""\n']",['get_tokenizer'],1
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:transformations,_Settings:transformations,method,2,8,6,99,12.38,0,1,['self'],[None],[None],238,"['        """"""Get the transformations.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/settings.py:_Settings:transformations,_Settings:transformations,method,2,8,6,99,12.38,0,1,['self'],[None],[None],245,"['        """"""Set the transformations.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/types.py:BaseOutputParser,BaseOutputParser,class,13,47,36,459,9.77,0,2,[],[],[],30,[],[],0
repos/llama_index/llama-index-core/llama_index/core/types.py:BasePydanticProgram,BasePydanticProgram,class,5,29,18,206,7.1,0,0,[],[],[],59,[],[],0
repos/llama_index/llama-index-core/llama_index/core/types.py:PydanticProgramMode,PydanticProgramMode,class,6,12,12,123,10.25,0,0,[],[],[],78,[],[],0
repos/llama_index/llama-index-core/llama_index/core/types.py:BaseOutputParser:__modify_schema__,BaseOutputParser:__modify_schema__,method,1,2,2,39,19.5,0,0,"['cls', 'schema', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",34,"['        """"""Avoids serialization issues.""""""\n']",['schema.update'],1
repos/llama_index/llama-index-core/llama_index/core/types.py:BaseOutputParser:format,BaseOutputParser:format,method,2,2,2,11,5.5,0,0,"['self', 'query']","[None, ' str']","[None, None]",42,"['        """"""Format a query with structured output formatting instructions.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/types.py:BaseOutputParser:format_messages,BaseOutputParser:format_messages,method,7,16,13,188,11.75,0,2,"['self', 'messages']","[None, ' List[ChatMessage]']","[None, None]",46,"['        """"""Format a list of messages with structured output formatting instructions.""""""\n']",['self.format'],1
repos/llama_index/llama-index-core/llama_index/core/types.py:BaseOutputParser:parse,BaseOutputParser:parse,method,10,30,24,304,10.13,0,2,"['self', 'output']","[None, ' str']","[None, None]",39,"['        """"""Parse, validate, and correct errors programmatically.""""""\n']","['format', 'format_messages', 'self.format']",3
repos/llama_index/llama-index-core/llama_index/core/types.py:BasePydanticProgram:__call__,BasePydanticProgram:__call__,method,0,1,1,4,4.0,0,0,"['self', '*args', '**kwds']","[None, ' Any', ' Any']","[None, None, None]",71,[],[],0
repos/llama_index/llama-index-core/llama_index/core/types.py:BasePydanticProgram:output_cls,BasePydanticProgram:output_cls,method,0,1,1,4,4.0,0,0,['self'],[None],[None],67,[],[],0
repos/llama_index/llama-index-core/llama_index/core/utils.py:_get_colored_text,_get_colored_text,function,3,19,17,193,10.16,0,1,"['text', 'color']","[' str', ' str']","[None, None]",424,"['    """"""\n', '    Get the colored version of the input text.\n', '\n', '    Args:\n', '        text (str): Input text.\n', '        color (str): Color to be applied to the text.\n', '\n', '    Returns:\n', '        str: Colored version of the input text.\n', '    """"""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/utils.py:add_sync_version,add_sync_version,function,6,17,16,192,11.29,0,0,['func'],[' Any'],[None],338,"['    """"""Decorator for adding sync version of an async function. The sync version\n', '    is added as a function attribute to the original function, func.\n', '\n', '    Args:\n', '        func(Any): the async function for which a sync variant will be built.\n', '    """"""\n']","['asyncio.iscoroutinefunction', '_wrapper', 'asyncio.get_event_loop']",3
repos/llama_index/llama-index-core/llama_index/core/utils.py:concat_dirs,concat_dirs,function,3,11,10,73,6.64,0,0,"['dirname', 'basename']","[' str', ' str']","[None, None]",260,"['    """"""\n', '    Append basename to dirname, avoiding backslashes when running on windows.\n', '\n', '    os.path.join(dirname, basename) will add a backslash before dirname if\n', '    basename does not end with a slash, so we make sure it does.\n', '    """"""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/utils.py:count_tokens,count_tokens,function,4,6,6,66,11.0,0,0,['text'],[' str'],[None],286,[],"['get_tokenizer', 'tokenizer', 'len']",3
repos/llama_index/llama-index-core/llama_index/core/utils.py:get_cache_dir,get_cache_dir,function,10,43,36,514,11.95,0,2,[],[],[],308,"['    """"""Locate a platform-appropriate cache directory for llama_index,\n', ""    and create it if it doesn't yet exist.\n"", '    """"""\n']","['Path', 'os.makedirs', 'str']",3
repos/llama_index/llama-index-core/llama_index/core/utils.py:get_color_mapping,get_color_mapping,function,7,19,18,188,9.89,0,1,"['items', 'use_llama_index_colors']","[' List[str]', ' bool ']","[None, ' True']",400,"['    """"""\n', '    Get a mapping of items to colors.\n', '\n', '    Args:\n', '        items (List[str]): List of items to be mapped to colors.\n', '        use_llama_index_colors (bool, optional): Flag to indicate\n', '        whether to use LlamaIndex colors or ANSI colors.\n', '            Defaults to True.\n', '\n', '    Returns:\n', '        Dict[str, str]: Mapping of items to colors.\n', '    """"""\n']","['list', 'len', 'enumerate']",3
repos/llama_index/llama-index-core/llama_index/core/utils.py:get_new_id,get_new_id,function,5,12,10,70,5.83,1,1,['d'],[' Set'],[None],140,"['    """"""Get a new ID.""""""\n']",['str'],1
repos/llama_index/llama-index-core/llama_index/core/utils.py:get_new_int_id,get_new_int_id,function,5,13,11,82,6.31,1,1,['d'],[' Set'],[None],149,"['    """"""Get a new integer ID.""""""\n']",['random.randint'],1
repos/llama_index/llama-index-core/llama_index/core/utils.py:get_tokenizer,get_tokenizer,function,17,56,46,696,12.43,0,3,[],[],[],108,[],"['ImportError', 'tiktoken.encoding_for_model', 'partial', 'set_global_tokenizer']",4
repos/llama_index/llama-index-core/llama_index/core/utils.py:get_tqdm_iterable,get_tqdm_iterable,function,9,17,15,129,7.59,0,1,"['items', 'show_progress', 'desc']","[' Iterable', ' bool', ' str']","[None, None, None]",271,"['    """"""\n', '    Optionally get a tqdm iterable. Ensures tqdm.auto is used.\n', '    """"""\n']",['tqdm'],1
repos/llama_index/llama-index-core/llama_index/core/utils.py:get_transformer_tokenizer_fn,get_transformer_tokenizer_fn,function,12,26,26,240,9.23,0,0,['model_name'],[' str'],[None],292,"['    """"""\n', '    Args:\n', '        model_name(str): the model name of the tokenizer.\n', '                        For instance, fxmarty/tiny-llama-fast-tokenizer.\n', '    """"""\n']","['ValueError', 'AutoTokenizer.from_pretrained']",2
repos/llama_index/llama-index-core/llama_index/core/utils.py:infer_torch_device,infer_torch_device,function,9,22,17,208,9.45,0,2,[],[],[],463,"['    """"""Infer the input to torch.device.""""""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/utils.py:iter_batch,iter_batch,function,4,13,12,103,7.92,1,1,"['iterable', 'Generator]', 'size']","[' Union[Iterable', None, ' int']","[None, None, None]",246,"['    """"""Iterate over an iterable in batches.\n', '\n', '    >>> list(iter_batch([1,2,3,4,5], 3))\n', '    [[1, 2, 3], [4, 5]]\n', '    """"""\n']","['iter', 'list', 'len']",3
repos/llama_index/llama-index-core/llama_index/core/utils.py:print_text,print_text,function,3,12,12,96,8.0,0,0,"['text', 'color', 'end']","[' str', ' Optional[str] ', ' str ']","[None, ' None', ' """"']",445,"['    """"""\n', '    Print the text with the specified color.\n', '\n', '    Args:\n', '        text (str): Text to be printed.\n', '        color (str, optional): Color to be applied to the text. Supported colors are:\n', '            llama_pink, llama_blue, llama_turquoise, llama_lavender,\n', '            red, green, yellow, blue, magenta, cyan, pink.\n', '        end (str, optional): String appended after the last character of the text.\n', '\n', '    Returns:\n', '        None\n', '    """"""\n']","['_get_colored_text', 'print']",2
repos/llama_index/llama-index-core/llama_index/core/utils.py:retry_on_exceptions_with_backoff,retry_on_exceptions_with_backoff,function,20,59,49,546,9.25,2,3,"['lambda_fn', 'errors_to_retry', 'max_tries', 'min_backoff_secs', 'max_backoff_secs', '']","[' Callable', ' List[ErrorToRetry]', ' int ', ' float ', ' float ', None]","[None, None, ' 10', ' 0.5', ' 60.0', None]",192,"['    """"""Execute lambda function with retries and exponential backoff.\n', '\n', '    Args:\n', '        lambda_fn (Callable): Function to be called and output we want.\n', '        errors_to_retry (List[ErrorToRetry]): List of errors to retry.\n', '            At least one needs to be provided.\n', '        max_tries (int): Maximum number of tries, including the first. Defaults to 10.\n', '        min_backoff_secs (float): Minimum amount of backoff time between attempts.\n', '            Defaults to 0.5.\n', '        max_backoff_secs (float): Maximum amount of backoff time between attempts.\n', '            Defaults to 60.\n', '\n', '    """"""\n']","['ValueError', 'tuple', 'lambda_fn', 'traceback.print_exc', 'error_checks.get', 'check_fn', 'time.sleep', 'min']",8
repos/llama_index/llama-index-core/llama_index/core/utils.py:set_global_tokenizer,set_global_tokenizer,function,6,10,9,158,15.8,0,1,"['tokenizer', 'Callable[[str]', 'list]]']","[' Union[Tokenizer', None, None]","[None, None, None]",99,[],['isinstance'],1
repos/llama_index/llama-index-core/llama_index/core/utils.py:temp_set_attrs,temp_set_attrs,function,8,27,18,149,5.52,2,0,"['obj', '**kwargs']","[' Any', ' Any']","[None, None]",159,"['    """"""Temporary setter.\n', '\n', '    Utility class for setting a temporary value for an attribute on a class.\n', '    Taken from: https://tinyurl.com/2p89xymh\n', '\n', '    """"""\n']","['getattr', 'kwargs.items', 'setattr', 'prev_values.items']",4
repos/llama_index/llama-index-core/llama_index/core/utils.py:truncate_text,truncate_text,function,4,10,9,67,6.7,0,1,"['text', 'max_length']","[' str', ' int']","[None, None]",239,"['    """"""Truncate text to a maximum length.""""""\n']",['len'],1
repos/llama_index/llama-index-core/llama_index/core/utils.py:unit_generator,unit_generator,function,1,2,2,6,3.0,0,0,['x'],[' Any'],[None],478,"['    """"""A function that returns a generator of a single element.\n', '\n', '    Args:\n', '        x (Any): the element to build yield\n', '\n', '    Yields:\n', '        Any: the single element\n', '    """"""\n']",[],0
repos/llama_index/llama-index-core/llama_index/core/utils.py:ErrorToRetry,ErrorToRetry,class,5,6,6,74,12.33,0,0,[],[],[],177,[],[],0
repos/llama_index/llama-index-core/llama_index/core/utils.py:GlobalsHelper,GlobalsHelper,class,21,81,54,1030,12.72,0,2,[],[],[],32,[],[],0
repos/llama_index/llama-index-core/llama_index/core/utils.py:Tokenizer,Tokenizer,class,1,11,11,63,5.73,0,0,[],[],[],94,[],[],0
repos/llama_index/llama-index-core/llama_index/core/utils.py:GlobalsHelper:__init__,GlobalsHelper:__init__,method,9,30,25,520,17.33,0,1,['self'],[None],[None],43,"['        """"""Initialize NLTK stopwords and punkt.""""""\n']",['nltk.download'],1
repos/llama_index/llama-index-core/llama_index/core/utils.py:GlobalsHelper:stopwords,GlobalsHelper:stopwords,method,12,36,31,373,10.36,0,1,['self'],[None],[None],70,"['        """"""Get stopwords.""""""\n']","['ImportError', 'nltk.download', 'stopwords.words']",3
repos/llama_index/llama-index-core/llama_index/core/utils.py:Tokenizer:encode,Tokenizer:encode,method,0,1,1,3,3.0,0,0,"['self', 'text', '*args', '**kwargs']","[None, ' str', ' Any', ' Any']","[None, None, None, None]",95,[],[],0
repos/llama_index/llama-index-core/tests/callbacks/test_llama_debug.py:test_flush_events,test_flush_events,function,6,22,15,472,21.45,0,0,[],[],[],58,"['    """"""Test flush events.""""""\n']","['LlamaDebugHandler', 'handler.on_event_start', 'handler.on_event_end', 'len', 'handler.flush_event_logs']",5
repos/llama_index/llama-index-core/tests/callbacks/test_llama_debug.py:test_get_event_stats,test_get_event_stats,function,9,19,17,355,18.68,0,0,[],[],[],43,"['    """"""Test get event stats.""""""\n']","['LlamaDebugHandler', 'handler.on_event_start', 'handler.on_event_end', 'len', 'handler.get_event_time_info']",5
repos/llama_index/llama-index-core/tests/callbacks/test_llama_debug.py:test_ignore_events,test_ignore_events,function,7,25,19,590,23.6,0,0,[],[],[],76,"['    """"""Test ignore event starts and ends.""""""\n']","['LlamaDebugHandler', 'CallbackManager', 'manager.on_event_start', 'manager.on_event_end', 'len']",5
repos/llama_index/llama-index-core/tests/callbacks/test_llama_debug.py:test_on_event_end,test_on_event_end,function,10,22,17,350,15.91,0,0,[],[],[],28,"['    """"""Test event end.""""""\n']","['LlamaDebugHandler', 'handler.on_event_end', 'len', 'isinstance']",4
repos/llama_index/llama-index-core/tests/callbacks/test_llama_debug.py:test_on_event_start,test_on_event_start,function,10,25,19,346,13.84,0,0,[],[],[],11,"['    """"""Test event start.""""""\n']","['LlamaDebugHandler', 'handler.on_event_start', 'len', 'isinstance']",4
repos/llama_index/llama-index-core/tests/callbacks/test_token_counter.py:test_on_event_end,test_on_event_end,function,7,32,17,556,17.38,0,0,[],[],[],29,"['    """"""Test event end.""""""\n']","['TokenCountingHandler', 'handler.on_event_end', 'len']",3
repos/llama_index/llama-index-core/tests/callbacks/test_token_counter.py:test_on_event_start,test_on_event_start,function,5,26,15,346,13.31,0,0,[],[],[],10,"['    """"""Test event start.""""""\n']","['TokenCountingHandler', 'handler.on_event_start', 'len']",3
repos/llama_index/llama-index-core/tests/chat_engine/test_condense_plus_context.py:override_predict,override_predict,function,2,2,2,34,17.0,0,0,"['self', 'prompt', '**prompt_args']","[' Any', ' BasePromptTemplate', ' Any']","[None, None, None]",15,[],['prompt.format'],1
repos/llama_index/llama-index-core/tests/chat_engine/test_condense_plus_context.py:test_condense_plus_context_chat_engine,test_condense_plus_context_chat_engine,function,32,174,108,2056,11.82,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",24,[],"['Mock', 'source_url', 'query.replace', 'override_retrieve', 'NodeWithScore', 'node=TextNode', 'CondensePlusContextChatEngine', 'llm=MockLLM', 'engine.reset', 'engine.chat', 'str']",11
repos/llama_index/llama-index-core/tests/chat_engine/test_condense_question.py:test_condense_question_chat_engine,test_condense_question_chat_engine,function,12,54,35,552,10.22,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",12,[],"['Mock', 'Response', 'CondenseQuestionChatEngine.from_defaults', 'engine.reset', 'engine.chat', 'str']",6
repos/llama_index/llama-index-core/tests/chat_engine/test_condense_question.py:test_condense_question_chat_engine_with_init_history,test_condense_question_chat_engine_with_init_history,function,13,43,36,554,12.88,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",39,[],"['Mock', 'Response', 'CondenseQuestionChatEngine.from_defaults', 'ChatMessage', 'print', 'engine.chat', 'str']",7
repos/llama_index/llama-index-core/tests/chat_engine/test_simple.py:test_simple_chat_engine,test_simple_chat_engine,function,6,48,23,443,9.23,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",6,[],"['SimpleChatEngine.from_defaults', 'engine.reset', 'engine.chat', 'str']",4
repos/llama_index/llama-index-core/tests/chat_engine/test_simple.py:test_simple_chat_engine_with_init_history,test_simple_chat_engine_with_init_history,function,5,34,25,375,11.03,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",27,[],"['SimpleChatEngine.from_defaults', 'ChatMessage', 'engine.chat', 'str']",4
repos/llama_index/llama-index-core/tests/conftest.py:allow_networking,allow_networking,function,1,1,1,18,18.0,0,0,['monkeypatch'],[' pytest.MonkeyPatch'],[None],38,[],['monkeypatch.undo'],1
repos/llama_index/llama-index-core/tests/conftest.py:mock_llm,mock_llm,function,2,2,2,15,7.5,0,0,[],[],[],105,[],['MockLLM'],1
repos/llama_index/llama-index-core/tests/conftest.py:mock_openai_credentials,mock_openai_credentials,function,2,7,7,82,11.71,0,1,[],[],[],110,[],[],0
repos/llama_index/llama-index-core/tests/conftest.py:mock_service_context,mock_service_context,function,2,2,2,63,31.5,0,0,"['patch_token_text_splitter', 'patch_llm_predictor', '']","[' Any', ' Any', None]","[None, None, None]",97,[],['ServiceContext.from_defaults'],1
repos/llama_index/llama-index-core/tests/conftest.py:patch_llm_predictor,patch_llm_predictor,function,1,35,13,475,13.57,0,0,['monkeypatch'],[' pytest.MonkeyPatch'],[None],57,[],"['monkeypatch.setattr', 'MockLLM', 'LLMMetadata']",3
repos/llama_index/llama-index-core/tests/conftest.py:patch_token_text_splitter,patch_token_text_splitter,function,1,16,13,358,22.38,0,0,['monkeypatch'],[' pytest.MonkeyPatch'],[None],43,[],['monkeypatch.setattr'],1
repos/llama_index/llama-index-core/tests/conftest.py:pytest_addoption,pytest_addoption,function,1,8,8,100,12.5,0,0,['parser'],[' pytest.Parser'],[None],157,[],['parser.addoption'],1
repos/llama_index/llama-index-core/tests/conftest.py:pytest_collection_modifyitems,pytest_collection_modifyitems,function,7,18,16,198,11.0,1,2,"['config', 'items']","[' pytest.Config', ' List[pytest.Item]']","[None, None]",170,[],"['config.getoption', 'item.add_marker']",2
repos/llama_index/llama-index-core/tests/conftest.py:pytest_configure,pytest_configure,function,1,6,6,70,11.67,0,0,['config'],[' pytest.Config'],[None],166,[],['config.addinivalue_line'],1
repos/llama_index/llama-index-core/tests/conftest.py:set_env_vars,set_env_vars,function,2,5,4,62,12.4,0,0,[],[],[],29,[],[],0
repos/llama_index/llama-index-core/tests/conftest.py:CachedOpenAIApiKeys,CachedOpenAIApiKeys,class,24,73,61,1040,14.25,0,1,[],[],[],115,[],[],0
repos/llama_index/llama-index-core/tests/conftest.py:CachedOpenAIApiKeys:__enter__,CachedOpenAIApiKeys:__enter__,method,12,20,18,373,18.65,0,1,['self'],[None],[None],137,[],['str'],1
repos/llama_index/llama-index-core/tests/conftest.py:CachedOpenAIApiKeys:__exit__,CachedOpenAIApiKeys:__exit__,method,7,8,8,196,24.5,0,0,"['self', '*exc']","[None, ' object']","[None, None]",150,[],['str'],1
repos/llama_index/llama-index-core/tests/conftest.py:CachedOpenAIApiKeys:__init__,CachedOpenAIApiKeys:__init__,method,10,10,10,190,19.0,0,0,"['self', 'set_env_key_to', 'set_library_key_to', 'set_fake_key', 'set_env_type_to', 'set_library_type_to', '# default value in openai package']","[None, ' Optional[str] ', ' Optional[str] ', ' bool ', ' Optional[str] ', ' str ', None]","[None, ' """"', ' None', ' False', ' """"', ' ""open_ai""', None]",123,[],[],0
repos/llama_index/llama-index-core/tests/embeddings/test_base.py:mock_get_text_embedding,mock_get_text_embedding,function,3,89,28,387,4.35,0,1,['text'],[' str'],[None],9,"['    """"""Mock get text embedding.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-core/tests/embeddings/test_base.py:mock_get_text_embeddings,mock_get_text_embeddings,function,1,6,6,51,8.5,0,0,['texts'],[' List[str]'],[None],31,"['    """"""Mock get text embeddings.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/embeddings/test_base.py:test_embedding_similarity,test_embedding_similarity,function,6,16,14,177,11.06,0,0,[],[],[],66,"['    """"""Test embedding similarity.""""""\n']","['MockEmbedding', 'embed_model.similarity']",2
repos/llama_index/llama-index-core/tests/embeddings/test_base.py:test_embedding_similarity_euclidean,test_embedding_similarity_euclidean,function,14,37,26,463,12.51,0,0,[],[],[],75,[],"['MockEmbedding', 'embed_model.similarity']",2
repos/llama_index/llama-index-core/tests/embeddings/test_base.py:test_get_text_embeddings,test_get_text_embeddings,function,9,84,34,585,6.96,8,0,"['_mock_get_text_embeddings', '_mock_get_text_embedding']","[' Any', ' Any']","[None, None]",40,"['    """"""Test get queued text embeddings.""""""\n']","['MockEmbedding', 'range', 'texts_to_embed.append', 'embed_model.get_text_embedding_batch']",4
repos/llama_index/llama-index-core/tests/embeddings/test_base.py:test_mean_agg,test_mean_agg,function,4,16,14,122,7.62,0,0,[],[],[],89,"['    """"""Test mean aggregation for embeddings.""""""\n']",['mean_agg'],1
repos/llama_index/llama-index-core/tests/embeddings/test_utils.py:test_resolve_embed_model,test_resolve_embed_model,function,3,5,5,81,16.2,0,0,['monkeypatch'],[' MonkeyPatch'],[None],6,[],"['resolve_embed_model', 'isinstance']",2
repos/llama_index/llama-index-core/tests/embeddings/todo_hf_test_utils.py:mock_hf_embeddings,mock_hf_embeddings,function,6,8,8,125,15.62,0,0,"['self', '*args', '**kwargs', 'Any]']","[' Any', ' Any', ' Dict[str', None]","[None, None, None, None]",13,"['    """"""Mock HuggingFaceEmbeddings.""""""\n']",['super'],1
repos/llama_index/llama-index-core/tests/embeddings/todo_hf_test_utils.py:mock_openai_embeddings,mock_openai_embeddings,function,3,7,7,96,13.71,0,0,"['self', '*args', '**kwargs', 'Any]']","[' Any', ' Any', ' Dict[str', None]","[None, None, None, None]",24,"['    """"""Mock OpenAIEmbedding.""""""\n']",['super'],1
repos/llama_index/llama-index-core/tests/embeddings/todo_hf_test_utils.py:test_resolve_embed_model,test_resolve_embed_model,function,4,28,16,593,21.18,0,0,['monkeypatch'],[' MonkeyPatch'],[None],32,[],"['monkeypatch.setattr', 'resolve_embed_model', 'isinstance']",3
repos/llama_index/llama-index-core/tests/evaluation/test_base.py:test_evaluator_basic,test_evaluator_basic,function,7,37,25,448,12.11,0,0,[],[],[],45,[],"['MockEvaluator', 'test_evaluator.evaluate', 'test_evaluator.evaluate_response', 'response=Response', 'NodeWithScore']",5
repos/llama_index/llama-index-core/tests/evaluation/test_base.py:MockEvaluator,MockEvaluator,class,13,61,47,617,10.11,0,0,[],[],[],10,[],[],0
repos/llama_index/llama-index-core/tests/evaluation/test_base.py:MockEvaluator:__init__,MockEvaluator:__init__,method,6,6,6,93,15.5,0,0,"['self', 'mock_score', 'mock_passing', 'mock_feedback', '']","[None, ' float ', ' bool ', ' str ', None]","[None, ' 1.0', ' True', ' ""test feedback""', None]",11,[],[],0
repos/llama_index/llama-index-core/tests/evaluation/test_base.py:MockEvaluator:_get_prompts,MockEvaluator:_get_prompts,method,1,2,2,8,4.0,0,0,['self'],[None],[None],21,"['        """"""Get prompts.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/evaluation/test_base.py:MockEvaluator:_update_prompts,MockEvaluator:_update_prompts,method,4,27,23,312,11.56,0,0,"['self', 'prompts']","[None, ' PromptDictType']","[None, None]",25,"['        """"""Update prompts.""""""\n']","['aevaluate', 'EvaluationResult']",2
repos/llama_index/llama-index-core/tests/evaluation/test_batch_runner.py:get_eval_results,get_eval_results,function,6,16,14,111,6.94,1,1,"['key', 'eval_results']","[None, None]","[None, None]",50,[],['len'],1
repos/llama_index/llama-index-core/tests/evaluation/test_batch_runner.py:test_batch_runner,test_batch_runner,function,13,115,42,1582,13.76,0,0,[],[],[],60,[],"['BatchEvalRunner', 'MockEvaluator', 'Response', 'runner.evaluate_response_strs', 'get_eval_results', 'runner.evaluate_responses']",6
repos/llama_index/llama-index-core/tests/evaluation/test_batch_runner.py:MockEvaluator,MockEvaluator,class,15,71,55,695,9.79,0,0,[],[],[],12,[],[],0
repos/llama_index/llama-index-core/tests/evaluation/test_batch_runner.py:MockEvaluator:__init__,MockEvaluator:__init__,method,6,6,6,93,15.5,0,0,"['self', 'mock_score', 'mock_passing', 'mock_feedback', '']","[None, ' float ', ' bool ', ' str ', None]","[None, ' 1.0', ' True', ' ""test feedback""', None]",13,[],[],0
repos/llama_index/llama-index-core/tests/evaluation/test_batch_runner.py:MockEvaluator:_get_prompts,MockEvaluator:_get_prompts,method,1,2,2,8,4.0,0,0,['self'],[None],[None],23,"['        """"""Get prompts.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/evaluation/test_batch_runner.py:MockEvaluator:_update_prompts,MockEvaluator:_update_prompts,method,6,37,31,390,10.54,0,0,"['self', 'prompts']","[None, ' PromptDictType']","[None, None]",27,"['        """"""Update prompts.""""""\n']","['aevaluate', 'EvaluationResult', 'str']",3
repos/llama_index/llama-index-core/tests/evaluation/test_dataset_generation.py:test_dataset_generation,test_dataset_generation,function,3,5,5,102,20.4,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",10,"['    """"""Test dataset generation.""""""\n']","['TextNode', 'PromptTemplate']",2
repos/llama_index/llama-index-core/tests/evaluation/test_platform_eval.py:test_upload_eval_dataset,test_upload_eval_dataset,function,9,29,27,443,15.28,0,0,[],[],[],17,[],"['upload_eval_dataset', 'PlatformApi', 'len']",3
repos/llama_index/llama-index-core/tests/evaluation/test_rr_mrr_hitrate.py:test_exceptions,test_exceptions,function,11,14,13,258,18.43,0,0,"['expected_ids', 'retrieved_ids', 'use_granular']","[None, None, None]","[None, None, None]",69,[],"['pytest.raises', 'HitRate', 'hr.compute', 'MRR', 'mrr.compute']",5
repos/llama_index/llama-index-core/tests/evaluation/test_rr_mrr_hitrate.py:test_hit_rate,test_hit_rate,function,8,10,10,174,17.4,0,0,"['expected_ids', 'retrieved_ids', 'use_granular', 'expected_result']","[None, None, None, None]","[None, None, None, None]",15,[],"['HitRate', 'hr.compute', 'pytest.approx']",3
repos/llama_index/llama-index-core/tests/evaluation/test_rr_mrr_hitrate.py:test_mrr,test_mrr,function,8,10,10,168,16.8,0,0,"['expected_ids', 'retrieved_ids', 'use_granular', 'expected_result']","[None, None, None, None]","[None, None, None, None]",45,[],"['MRR', 'mrr.compute', 'pytest.approx']",3
repos/llama_index/llama-index-core/tests/indices/conftest.py:documents,documents,function,2,20,14,117,5.85,0,0,[],[],[],13,"['    """"""Get documents.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/indices/conftest.py:nodes,nodes,function,1,46,19,464,10.09,0,0,[],[],[],26,"['    """"""Get documents.""""""\n']","['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-core/tests/indices/test_loading.py:test_load_index_from_storage_multiple,test_load_index_from_storage_multiple,function,21,68,38,988,14.53,2,0,"['nodes', 'tmp_path', 'mock_service_context', '']","[' List[BaseNode]', ' Path', ' ServiceContext', None]","[None, None, None, None]",46,[],"['StorageContext.from_defaults', 'VectorStoreIndex', 'SummaryIndex', 'storage_context.persist', 'pytest.raises', 'load_index_from_storage', 'load_indices_from_storage', 'len']",8
repos/llama_index/llama-index-core/tests/indices/test_loading.py:test_load_index_from_storage_retrieval_result_identical,test_load_index_from_storage_retrieval_result_identical,function,12,27,22,523,19.37,0,0,"['documents', 'tmp_path', 'mock_service_context', '']","[' List[Document]', ' Path', ' ServiceContext', None]","[None, None, None, None]",100,[],"['StorageContext.from_defaults', 'VectorStoreIndex.from_documents', 'index.as_retriever', 'storage_context.persist', 'load_index_from_storage', 'new_index.as_retriever']",6
repos/llama_index/llama-index-core/tests/indices/test_loading.py:test_load_index_from_storage_simple,test_load_index_from_storage_simple,function,10,19,18,445,23.42,0,0,"['documents', 'tmp_path', 'mock_service_context']","[' List[Document]', ' Path', ' ServiceContext']","[None, None, None]",19,[],"['StorageContext.from_defaults', 'VectorStoreIndex.from_documents', 'storage_context.persist', 'load_index_from_storage']",4
repos/llama_index/llama-index-core/tests/indices/test_loading.py:test_load_index_query_engine_service_context,test_load_index_query_engine_service_context,function,15,29,25,670,23.1,0,0,"['documents', 'tmp_path', 'mock_service_context', '']","[' List[Document]', ' Path', ' ServiceContext', None]","[None, None, None, None]",133,[],"['StorageContext.from_defaults', 'VectorStoreIndex.from_documents', 'storage_context.persist', 'load_index_from_storage', 'index.as_query_engine', 'new_index.as_query_engine', 'isinstance']",7
repos/llama_index/llama-index-core/tests/indices/test_loading_graph.py:test_load_graph_from_storage_simple,test_load_graph_from_storage_simple,function,22,59,44,1160,19.66,0,0,"['documents', 'tmp_path', 'mock_service_context', '']","[' List[Document]', ' Path', ' ServiceContext', None]","[None, None, None, None]",13,[],"['StorageContext.from_defaults', 'VectorStoreIndex.from_documents', 'SummaryIndex.from_documents', 'ComposableGraph.from_indices', 'graph.as_query_engine', 'query_engine.query', 'storage_context.persist', 'load_graph_from_storage', 'new_graph.as_query_engine', 'new_query_engine.query', 'str']",11
repos/llama_index/llama-index-core/tests/indices/test_prompt_helper.py:test_get_biggest_prompt,test_get_biggest_prompt,function,6,23,14,234,10.17,0,0,[],[],[],189,"['    """"""Test get_biggest_prompt from PromptHelper.""""""\n']","['PromptTemplate', 'get_biggest_prompt']",2
repos/llama_index/llama-index-core/tests/indices/test_prompt_helper.py:test_get_chunk_size,test_get_chunk_size,function,8,28,22,433,15.46,0,1,"['prompt', 'chunk_size_limit', 'num_chunks', 'padding', 'expected', 'Type[Exception]]', '']","[' str', ' Optional[int]', ' int', ' int', ' Union[int', None, None]","[None, None, None, None, None, None, None]",38,"['    """"""Test get chunk size given prompt.""""""\n']","['PromptHelper', 'isinstance', 'prompt_helper._get_available_chunk_size', 'PromptTemplate', 'pytest.raises']",5
repos/llama_index/llama-index-core/tests/indices/test_prompt_helper.py:test_get_numbered_text_from_nodes,test_get_numbered_text_from_nodes,function,13,41,38,494,12.05,0,0,[],[],[],152,"['    """"""Test get_text_from_nodes.""""""\n']","['PromptTemplate', 'PromptHelper', 'TextNode', 'prompt_helper.get_text_splitter_given_prompt', 'get_numbered_text_from_nodes', 'str']",6
repos/llama_index/llama-index-core/tests/indices/test_prompt_helper.py:test_get_text_splitter,test_get_text_splitter,function,19,71,50,876,12.34,0,0,[],[],[],65,"['    """"""Test get text splitter.""""""\n']","['PromptTemplate', 'PromptHelper', 'prompt_helper.get_text_splitter_given_prompt', 'text_splitter.split_text', 'truncate_text']",5
repos/llama_index/llama-index-core/tests/indices/test_prompt_helper.py:test_get_text_splitter_partial,test_get_text_splitter_partial,function,20,93,45,1099,11.82,0,0,[],[],[],97,"['    """"""Test get text splitter with a partially formatted prompt.""""""\n']","['PromptTemplate', 'PromptHelper', 'prompt_helper.get_text_splitter_given_prompt', 'text_splitter.split_text', 'truncate_text', 'test_prompt.partial_format', 'get_empty_prompt_txt']",7
repos/llama_index/llama-index-core/tests/indices/test_prompt_helper.py:test_repack,test_repack,function,8,29,27,410,14.14,0,0,[],[],[],173,"['    """"""Test repack.""""""\n']","['PromptTemplate', 'PromptHelper', 'prompt_helper.repack']",3
repos/llama_index/llama-index-core/tests/indices/test_prompt_helper.py:test_truncate,test_truncate,function,12,39,32,382,9.79,0,0,[],[],[],131,"['    """"""Test truncate.""""""\n']","['PromptTemplate', 'PromptHelper', 'prompt_helper.truncate']",3
repos/llama_index/llama-index-core/tests/indices/test_service_context.py:test_service_context_serialize,test_service_context_serialize,function,27,67,50,1320,19.7,0,0,[],[],[],16,[],"['SummaryExtractor', 'QuestionsAnsweredExtractor', 'TitleExtractor', 'SentenceSplitter', 'MockLLM', 'MockEmbedding', 'PromptHelper', 'ServiceContext.from_defaults', 'service_context.to_dict', 'ServiceContext.from_dict', 'isinstance', 'len']",12
repos/llama_index/llama-index-core/tests/indices/test_utils.py:test_expand_tokens_with_subtokens,test_expand_tokens_with_subtokens,function,4,26,22,188,7.23,0,0,[],[],[],6,"['    """"""Test expand tokens.""""""\n']",['expand_tokens_with_subtokens'],1
repos/llama_index/llama-index-core/tests/ingestion/test_cache.py:test_cache,test_cache,function,14,30,25,396,13.2,0,0,[],[],[],15,[],"['IngestionCache', 'DummyTransform', 'TextNode', 'get_transformation_hash', 'transformation', 'cache.put', 'cache.get']",7
repos/llama_index/llama-index-core/tests/ingestion/test_cache.py:test_cache_clear,test_cache_clear,function,13,25,22,286,11.44,0,0,[],[],[],33,[],"['IngestionCache', 'DummyTransform', 'TextNode', 'get_transformation_hash', 'transformation', 'cache.put', 'cache.get', 'cache.clear']",8
repos/llama_index/llama-index-core/tests/ingestion/test_cache.py:DummyTransform,DummyTransform,class,5,16,16,146,9.12,1,0,[],[],[],8,[],[],0
repos/llama_index/llama-index-core/tests/ingestion/test_cache.py:DummyTransform:__call__,DummyTransform:__call__,method,4,8,8,77,9.62,1,0,"['self', 'nodes', '**kwargs']","[None, ' List[BaseNode]', ' Any']","[None, None, None]",9,[],['node.set_content'],1
repos/llama_index/llama-index-core/tests/ingestion/test_data_sinks.py:test_build_configured_data_sink,test_build_configured_data_sink,function,13,25,22,523,20.92,0,0,[],[],[],52,[],"['MagicMock', 'WeaviateVectorStore', 'isinstance', 'pytest.raises']",4
repos/llama_index/llama-index-core/tests/ingestion/test_data_sinks.py:test_can_build_configured_data_sink_from_component,test_can_build_configured_data_sink_from_component,function,10,25,22,496,19.84,0,0,[],[],[],33,[],"['MagicMock', 'WeaviateVectorStore', 'ConfiguredDataSink.from_component', 'isinstance']",4
repos/llama_index/llama-index-core/tests/ingestion/test_data_sinks.py:test_can_generate_schema_for_data_sink_component_type,test_can_generate_schema_for_data_sink_component_type,function,6,30,18,305,10.17,0,0,"['configurable_data_sink_type', '']","[' ConfigurableDataSinks', None]","[None, None]",17,[],['len'],1
repos/llama_index/llama-index-core/tests/ingestion/test_data_sinks.py:test_unique_configurable_data_sink_names,test_unique_configurable_data_sink_names,function,5,15,13,216,14.4,1,0,[],[],[],71,[],"['set', 'names.add', 'len']",3
repos/llama_index/llama-index-core/tests/ingestion/test_data_sources.py:test_build_configured_data_source,test_build_configured_data_source,function,4,14,14,214,15.29,0,0,[],[],[],38,[],"['Document.example', 'isinstance']",2
repos/llama_index/llama-index-core/tests/ingestion/test_data_sources.py:test_can_build_configured_data_source_from_component,test_can_build_configured_data_source_from_component,function,5,17,16,279,16.41,0,0,[],[],[],25,[],"['Document.example', 'ConfiguredDataSource.from_component', 'isinstance']",3
repos/llama_index/llama-index-core/tests/ingestion/test_data_sources.py:test_can_generate_schema_for_data_source_component_type,test_can_generate_schema_for_data_source_component_type,function,6,30,18,311,10.37,0,0,"['configurable_data_source_type', '']","[' ConfigurableDataSources', None]","[None, None]",10,[],['len'],1
repos/llama_index/llama-index-core/tests/ingestion/test_data_sources.py:test_unique_configurable_data_source_names,test_unique_configurable_data_source_names,function,5,19,15,245,12.89,1,0,[],[],[],49,[],"['set', 'names.add', 'len']",3
repos/llama_index/llama-index-core/tests/ingestion/test_pipeline.py:teardown_function,teardown_function,function,3,4,4,64,16.0,0,0,[],[],[],14,[],['shutil.rmtree'],1
repos/llama_index/llama-index-core/tests/ingestion/test_pipeline.py:test_build_pipeline,test_build_pipeline,function,5,22,20,304,13.82,0,0,[],[],[],20,[],"['IngestionPipeline', 'ReaderConfig', 'reader=StringIterableReader', 'SentenceSplitter', 'KeywordExtractor', 'MockEmbedding', 'len']",7
repos/llama_index/llama-index-core/tests/ingestion/test_pipeline.py:test_pipeline_dedup_duplicates_only,test_pipeline_dedup_duplicates_only,function,6,27,23,366,13.56,0,0,[],[],[],170,[],"['Document', 'IngestionPipeline', 'SentenceSplitter', 'docstore=SimpleDocumentStore', 'pipeline.run', 'len']",6
repos/llama_index/llama-index-core/tests/ingestion/test_pipeline.py:test_pipeline_parallel,test_pipeline_parallel,function,13,33,31,439,13.3,0,0,[],[],[],191,[],"['Document.example', 'Document', 'IngestionPipeline', 'SentenceSplitter', 'docstore=SimpleDocumentStore', 'min', 'cpu_count', 'pipeline.run', 'len']",9
repos/llama_index/llama-index-core/tests/ingestion/test_pipeline.py:test_pipeline_update,test_pipeline_update,function,11,47,30,561,11.94,0,0,[],[],[],142,[],"['Document.example', 'IngestionPipeline', 'SentenceSplitter', 'docstore=SimpleDocumentStore', 'pipeline.run', 'len', 'Document', 'next']",8
repos/llama_index/llama-index-core/tests/ingestion/test_pipeline.py:test_run_pipeline,test_run_pipeline,function,7,27,24,309,11.44,0,0,[],[],[],39,[],"['IngestionPipeline', 'ReaderConfig', 'reader=StringIterableReader', 'SentenceSplitter', 'KeywordExtractor', 'pipeline.run', 'len']",7
repos/llama_index/llama-index-core/tests/ingestion/test_pipeline.py:test_save_load_pipeline,test_save_load_pipeline,function,10,65,31,817,12.57,0,0,[],[],[],60,[],"['Document', 'IngestionPipeline', 'SentenceSplitter', 'docstore=SimpleDocumentStore', 'pipeline.run', 'len', 'pipeline.persist', 'pipeline2.load']",8
repos/llama_index/llama-index-core/tests/ingestion/test_pipeline.py:test_save_load_pipeline_without_docstore,test_save_load_pipeline_without_docstore,function,9,52,28,665,12.79,0,0,[],[],[],103,[],"['Document', 'IngestionPipeline', 'SentenceSplitter', 'pipeline.run', 'len', 'pipeline.persist', 'pipeline2.load']",7
repos/llama_index/llama-index-core/tests/ingestion/test_transformations.py:test_build_configured_transformation,test_build_configured_transformation,function,8,19,17,376,19.79,0,0,[],[],[],46,[],"['SentenceSplitter', 'isinstance', 'pytest.raises']",3
repos/llama_index/llama-index-core/tests/ingestion/test_transformations.py:test_can_build_configured_transform_from_component,test_can_build_configured_transform_from_component,function,5,26,18,418,16.08,0,0,[],[],[],29,[],"['SentenceSplitter', 'ConfiguredTransformation.from_component', 'isinstance']",3
repos/llama_index/llama-index-core/tests/ingestion/test_transformations.py:test_can_generate_schema_for_transformation_component_type,test_can_generate_schema_for_transformation_component_type,function,6,32,19,323,10.09,0,0,"['configurable_transformation_type', '']","[' ConfigurableTransformations', None]","[None, None]",12,[],['len'],1
repos/llama_index/llama-index-core/tests/ingestion/test_transformations.py:test_unique_configurable_transformations_names,test_unique_configurable_transformations_names,function,5,19,15,262,13.79,1,0,[],[],[],62,[],"['set', 'names.add', 'len']",3
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:func,func,function,3,3,3,9,3.0,0,0,"['a', 'b', '**kwargs']","[None, None, None]","[None, '3', None]",43,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:func_exc,func_exc,function,1,2,2,16,8.0,0,0,"['a', 'b', 'c', '**kwargs']","[None, None, None, None]","[None, '3', '4', None]",53,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:func_with_event,func_with_event,function,2,3,3,80,26.67,0,0,"['a', 'b', '**kwargs']","[None, None, None]","[None, '3', None]",63,[],"['dispatcher.get_dispatch_event', 'dispatch_event']",2
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:test_dispatcher_fire_event,test_dispatcher_fire_event,function,11,20,19,349,17.45,0,0,"['mock_uuid', 'mock_span_enter', 'mock_span_drop', 'mock_span_exit', '']","[' MagicMock', ' MagicMock', ' MagicMock', ' MagicMock', None]","[None, None, None, None, None]",418,[],"['_TestEventHandler', 'dispatcher.add_event_handler', 'func_with_event', 'all', 'mock_span_enter.assert_called_once', 'mock_span_drop.assert_not_called', 'mock_span_exit.assert_called_once']",7
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:test_dispatcher_fire_event_with_instance,test_dispatcher_fire_event_with_instance,function,13,22,21,392,17.82,0,0,"['mock_uuid', 'mock_span_enter', 'mock_span_drop', 'mock_span_exit']","[None, None, None, None]","[None, None, None, None]",486,[],"['_TestEventHandler', 'dispatcher.add_event_handler', '_TestObject', 'instance.func_with_event', 'all', 'mock_span_enter.assert_called_once', 'mock_span_drop.assert_not_called', 'mock_span_exit.assert_called_once']",8
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:test_dispatcher_span_args,test_dispatcher_span_args,function,12,47,29,464,9.87,0,0,"['mock_uuid', 'mock_span_enter', 'mock_span_exit']","[None, None, None]","[None, None, None]",115,[],"['func', 'inspect.signature', 'mock_span_enter.assert_called_once']",3
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:test_dispatcher_span_args_with_instance,test_dispatcher_span_args_with_instance,function,14,49,32,522,10.65,0,0,"['mock_uuid', 'mock_span_enter', 'mock_span_exit']","[None, None, None]","[None, None, None]",150,[],"['_TestObject', 'instance.func', 'inspect.signature', 'mock_span_enter.assert_called_once']",4
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:test_dispatcher_span_drop_args,test_dispatcher_span_drop_args,function,15,37,32,445,12.03,0,0,"['mock_uuid', 'mock_span_enter', 'mock_span_drop', 'mock_span_exit', '']","[' MagicMock', ' MagicMock', ' MagicMock', ' MagicMock', None]","[None, None, None, None, None]",187,[],"['pytest.raises', 'func_exc', 'mock_span_enter.assert_called_once', 'mock_span_drop.assert_called_once', 'inspect.signature', 'mock_span_exit.assert_not_called']",6
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:test_dispatcher_span_drop_args,test_dispatcher_span_drop_args,function,15,37,32,445,12.03,0,0,"['mock_uuid', 'mock_span_enter', 'mock_span_drop', 'mock_span_exit', '']","[' MagicMock', ' MagicMock', ' MagicMock', ' MagicMock', None]","[None, None, None, None, None]",187,[],"['pytest.raises', 'func_exc', 'mock_span_enter.assert_called_once', 'mock_span_drop.assert_called_once', 'inspect.signature', 'mock_span_exit.assert_not_called']",6
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestEndEvent,_TestEndEvent,class,2,5,5,54,10.8,0,0,[],[],[],25,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestEventHandler,_TestEventHandler,class,5,12,11,119,9.92,0,0,[],[],[],31,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestObject,_TestObject,class,17,69,32,680,9.86,0,0,[],[],[],78,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestStartEvent,_TestStartEvent,class,2,5,5,56,11.2,0,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestEndEvent:class_name,_TestEndEvent:class_name,method,1,2,2,21,10.5,0,0,['cls'],[None],[None],21,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestEventHandler:class_name,_TestEventHandler:class_name,method,1,2,2,25,12.5,0,0,['cls'],[None],[None],35,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestEventHandler:handle,_TestEventHandler:handle,method,1,1,1,21,21.0,0,0,"['self', 'e']","[None, ' BaseEvent']","[None, None]",38,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestObject:func,_TestObject:func,method,3,3,3,9,3.0,0,0,"['self', 'a', 'b', '**kwargs']","[None, None, None, None]","[None, None, '3', None]",80,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestObject:func_exc,_TestObject:func_exc,method,1,2,2,16,8.0,0,0,"['self', 'a', 'b', 'c', '**kwargs']","[None, None, None, None, None]","[None, None, '3', '4', None]",88,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestObject:func_with_event,_TestObject:func_with_event,method,2,3,3,80,26.67,0,0,"['self', 'a', 'b', '**kwargs']","[None, None, None, None]","[None, None, '3', None]",96,[],"['dispatcher.get_dispatch_event', 'dispatch_event']",2
repos/llama_index/llama-index-core/tests/instrumentation/test_dispatcher.py:_TestStartEvent:class_name,_TestStartEvent:class_name,method,1,2,2,23,11.5,0,0,['cls'],[None],[None],21,[],[],0
repos/llama_index/llama-index-core/tests/instrumentation/test_manager.py:test_root_manager_add_dispatcher,test_root_manager_add_dispatcher,function,4,12,10,159,13.25,0,0,[],[],[],4,[],['instrument.get_dispatcher'],1
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:llm,llm,function,2,2,2,15,7.5,0,0,[],[],[],14,[],['MockLLM'],1
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:nonyielding_llm,nonyielding_llm,function,2,2,2,40,20.0,0,0,[],[],[],9,[],['MockLLMWithNonyieldingChatStream'],1
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:prompt,prompt,function,1,3,3,18,6.0,0,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:test_llm_complete_prompt_arg,test_llm_complete_prompt_arg,function,5,7,6,83,11.86,0,0,"['llm', 'prompt']","[' LLM', ' str']","[None, None]",42,[],['llm.complete'],1
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:test_llm_complete_prompt_kwarg,test_llm_complete_prompt_kwarg,function,5,7,6,90,12.86,0,0,"['llm', 'prompt']","[' LLM', ' str']","[None, None]",48,[],['llm.complete'],1
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:test_llm_complete_throws_if_duplicate_prompt,test_llm_complete_throws_if_duplicate_prompt,function,3,4,4,64,16.0,0,0,"['llm', 'prompt']","[' LLM', ' str']","[None, None]",54,[],"['pytest.raises', 'llm.complete']",2
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:test_llm_complete_throws_if_no_prompt,test_llm_complete_throws_if_no_prompt,function,3,3,3,45,15.0,0,0,['llm'],[' LLM'],[None],59,[],"['pytest.raises', 'llm.complete']",2
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:test_llm_stream_chat_handles_nonyielding_stream,test_llm_stream_chat_handles_nonyielding_stream,function,3,8,8,100,12.5,1,0,"['nonyielding_llm', 'prompt']","[' LLM', ' str']","[None, None]",23,[],['nonyielding_llm.stream_chat'],1
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:test_llm_stream_complete_prompt_arg,test_llm_stream_complete_prompt_arg,function,3,11,9,117,10.64,0,0,"['llm', 'prompt']","[' LLM', ' str']","[None, None]",64,[],['llm.stream_complete'],1
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:test_llm_stream_complete_prompt_kwarg,test_llm_stream_complete_prompt_kwarg,function,3,11,9,124,11.27,0,0,"['llm', 'prompt']","[' LLM', ' str']","[None, None]",70,[],['llm.stream_complete'],1
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:test_llm_stream_complete_throws_if_duplicate_prompt,test_llm_stream_complete_throws_if_duplicate_prompt,function,3,4,4,71,17.75,0,0,"['llm', 'prompt']","[' LLM', ' str']","[None, None]",76,[],"['pytest.raises', 'llm.stream_complete']",2
repos/llama_index/llama-index-core/tests/llms/test_callbacks.py:test_llm_stream_complete_throws_if_no_prompt,test_llm_stream_complete_throws_if_no_prompt,function,3,3,3,52,17.33,0,0,['llm'],[' LLM'],[None],81,[],"['pytest.raises', 'llm.stream_complete']",2
repos/llama_index/llama-index-core/tests/llms/test_custom.py:test_basic,test_basic,function,7,11,11,129,11.73,0,0,[],[],[],51,[],"['TestLLM', 'ChatMessage', 'llm.complete', 'llm.chat']",4
repos/llama_index/llama-index-core/tests/llms/test_custom.py:test_streaming,test_streaming,function,7,11,11,143,13.0,0,0,[],[],[],61,[],"['TestLLM', 'ChatMessage', 'llm.stream_complete', 'llm.stream_chat']",4
repos/llama_index/llama-index-core/tests/llms/test_custom.py:TestLLM,TestLLM,class,13,76,48,612,8.05,1,0,[],[],[],12,[],[],0
repos/llama_index/llama-index-core/tests/llms/test_custom.py:TestLLM:__init__,TestLLM:__init__,method,1,1,1,39,39.0,0,0,['self'],[None],[None],15,[],['super'],1
repos/llama_index/llama-index-core/tests/llms/test_custom.py:TestLLM:complete,TestLLM:complete,method,2,9,9,86,9.56,0,0,"['self', 'prompt', 'formatted', '**kwargs']","[None, ' str', ' bool ', ' Any']","[None, None, ' False', None]",22,[],['CompletionResponse'],1
repos/llama_index/llama-index-core/tests/llms/test_custom.py:TestLLM:metadata,TestLLM:metadata,method,2,2,2,19,9.5,0,0,['self'],[None],[None],19,[],['LLMMetadata'],1
repos/llama_index/llama-index-core/tests/llms/test_custom.py:TestLLM:stream_complete,TestLLM:stream_complete,method,6,27,25,201,7.44,1,0,"['self', 'prompt', 'formatted', '**kwargs']","[None, ' str', ' bool ', ' Any']","[None, None, ' False', None]",32,[],"['gen', 'CompletionResponse']",2
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_dict_save_load,test_dict_save_load,function,7,15,14,226,15.07,0,0,[],[],[],209,[],"['ChatMemoryBuffer.from_defaults', 'memory.to_dict', 'ChatMemoryBuffer.from_dict', 'len']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_get_when_initial_tokens_exceed_limit_raises_value_error,test_get_when_initial_tokens_exceed_limit_raises_value_error,function,7,18,17,211,11.72,0,0,[],[],[],53,[],"['ChatMemoryBuffer.from_defaults', 'pytest.raises', 'memory.get', 'str']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_get_when_initial_tokens_less_than_limit_returns_history,test_get_when_initial_tokens_less_than_limit_returns_history,function,7,15,14,200,13.33,0,0,[],[],[],36,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_get_when_initial_tokens_same_as_limit_removes_message,test_get_when_initial_tokens_same_as_limit_removes_message,function,5,12,12,174,14.5,0,0,[],[],[],66,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_get_when_space_for_all_but_first_message_removes_first_message_and_answer,test_get_when_space_for_all_but_first_message_removes_first_message_and_answer,function,11,32,28,501,15.66,0,0,[') -> (None'],[None],[None],131,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_get_when_space_for_assistant_message_removes_assistant_message_at_start_of_history,test_get_when_space_for_assistant_message_removes_assistant_message_at_start_of_history,function,7,16,15,245,15.31,0,0,[') -> (None'],[None],[None],82,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_get_when_space_for_second_message_and_answer_removes_only_first_message_and_answer,test_get_when_space_for_second_message_and_answer_removes_only_first_message_and_answer,function,10,29,25,431,14.86,0,0,[') -> (None'],[None],[None],102,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_max_tokens,test_max_tokens,function,4,27,18,406,15.04,0,0,[],[],[],174,[],"['ChatMemoryBuffer.from_defaults', 'memory.put', 'len', 'memory.get']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_pickle,test_pickle,function,5,7,7,123,17.57,0,0,[],[],[],222,"['    """"""Unpickleable tiktoken tokenizer should be circumvented when pickling.""""""\n']","['ChatMemoryBuffer.from_defaults', 'pickle.dumps', 'isinstance']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_put_get,test_put_get,function,6,10,9,166,16.6,0,0,[],[],[],24,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_set,test_set,function,4,10,8,187,18.7,0,0,[],[],[],163,[],"['ChatMemoryBuffer.from_defaults', 'memory.put', 'len', 'memory.set']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_memory_buffer.py:test_string_save_load,test_string_save_load,function,7,15,14,228,15.2,0,0,[],[],[],196,[],"['ChatMemoryBuffer.from_defaults', 'memory.to_string', 'ChatMemoryBuffer.from_string', 'len']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:_get_role_alternating_order,_get_role_alternating_order,function,4,9,8,60,6.67,0,1,['i'],[' int'],[None],17,[],[],0
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:summarizer_llm,summarizer_llm,function,3,13,10,193,14.85,0,0,[],[],[],85,[],"['MockSummarizerLLM', 'ChatMessage']",2
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_assistant_never_first_message,test_assistant_never_first_message,function,12,37,31,573,15.49,0,0,['summarizer_llm'],[None],[None],269,[],"['sum', 'range', 'ChatSummaryMemoryBuffer.from_defaults', 'memory.get', 'len', 'summarizer_llm.get_role_count']",6
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_assistant_tool_pairs,test_assistant_tool_pairs,function,12,38,32,587,15.45,0,0,['summarizer_llm'],[None],[None],296,[],"['sum', 'range', 'ChatSummaryMemoryBuffer.from_defaults', 'memory.get', 'len', 'summarizer_llm.get_role_count']",6
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_dict_save_load,test_dict_save_load,function,11,31,26,465,15.0,0,0,['summarizer_llm'],[None],[None],346,[],"['ChatSummaryMemoryBuffer.from_defaults', 'memory.to_dict', 'ChatSummaryMemoryBuffer.from_dict', 'len']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_get_when_initial_tokens_exceed_limit_raises_value_error,test_get_when_initial_tokens_exceed_limit_raises_value_error,function,7,22,21,282,12.82,0,0,[],[],[],185,[],"['ChatSummaryMemoryBuffer.from_defaults', 'pytest.raises', 'memory.get', 'str']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_get_when_initial_tokens_less_than_limit_returns_history,test_get_when_initial_tokens_less_than_limit_returns_history,function,7,15,14,207,13.8,0,0,[],[],[],168,[],"['ChatSummaryMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_max_tokens_with_summarizer,test_max_tokens_with_summarizer,function,11,48,31,727,15.15,0,0,['summarizer_llm'],[None],[None],235,[],"['summarizer_llm.set_max_tokens', 'ChatSummaryMemoryBuffer.from_defaults', 'memory.put', 'memory.get', 'len', 'FIRST_SUMMARY_RESPONSE.split', 'SECOND_SUMMARY_RESPONSE.split']",7
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_max_tokens_without_summarizer,test_max_tokens_without_summarizer,function,4,27,17,413,15.3,0,0,[],[],[],213,[],"['ChatSummaryMemoryBuffer.from_defaults', 'memory.put', 'len', 'memory.get']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_pickle,test_pickle,function,5,7,7,137,19.57,0,0,[],[],[],368,"['    """"""Unpickleable tiktoken tokenizer should be circumvented when pickling.""""""\n']","['ChatSummaryMemoryBuffer.from_defaults', 'pickle.dumps', 'isinstance']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_put_get,test_put_get,function,6,13,12,194,14.92,0,0,['summarizer_llm'],[None],[None],100,[],"['ChatSummaryMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_put_get_summarize_long_message,test_put_get_summarize_long_message,function,6,14,13,213,15.21,0,0,['summarizer_llm'],[None],[None],114,[],"['ChatSummaryMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_put_get_summarize_part_of_conversation,test_put_get_summarize_part_of_conversation,function,18,61,38,824,13.51,1,0,['summarizer_llm'],[None],[None],130,[],"['sum', 'len', 'range', 'ChatSummaryMemoryBuffer.from_defaults', 'memory.get', 'memory.put']",6
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_set,test_set,function,4,10,8,194,19.4,0,0,[],[],[],202,[],"['ChatSummaryMemoryBuffer.from_defaults', 'memory.put', 'len', 'memory.set']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:test_string_save_load,test_string_save_load,function,11,31,26,467,15.06,0,0,['summarizer_llm'],[None],[None],324,[],"['ChatSummaryMemoryBuffer.from_defaults', 'memory.to_string', 'ChatSummaryMemoryBuffer.from_string', 'len']",4
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:MockSummarizerLLM,MockSummarizerLLM,class,33,98,75,1006,10.27,2,1,[],[],[],40,[],[],0
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:MockSummarizerLLM:__init__,MockSummarizerLLM:__init__,method,15,27,22,195,7.22,1,0,"['self', 'responses', 'max_tokens']","[None, ' List[ChatMessage]', ' int ']","[None, None, ' 512']",45,[],['super'],1
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:MockSummarizerLLM:chat,MockSummarizerLLM:chat,method,13,34,31,434,12.76,1,1,"['self', 'messages', '**kwargs']","[None, ' Sequence[ChatMessage]', ' Any']","[None, None, None]",51,[],"['len', 'ChatResponse', 'message=ChatMessage']",3
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:MockSummarizerLLM:get_role_count,MockSummarizerLLM:get_role_count,method,2,2,2,29,14.5,0,0,"['self', 'role']","[None, ' MessageRole']","[None, None]",74,[],[],0
repos/llama_index/llama-index-core/tests/memory/test_chat_summary_memory_buffer.py:MockSummarizerLLM:set_max_tokens,MockSummarizerLLM:set_max_tokens,method,2,2,2,26,13.0,0,0,"['self', 'max_tokens']","[None, None]","[None, None]",71,[],[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_answer,_mock_answer,function,2,4,4,61,15.25,0,0,['prompt_args'],[' Dict'],[None],86,"['    """"""Mock answer.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_choice_select,_mock_choice_select,function,1,5,5,25,5.0,0,0,['prompt_args'],[' Dict'],[None],141,"['    """"""Mock choice select prompt.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_conversation,_mock_conversation,function,2,4,4,55,13.75,0,0,['prompt_args'],[' Dict'],[None],160,[],[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_decompose_query,_mock_decompose_query,function,2,4,4,61,15.25,0,0,['prompt_args'],[' Dict'],[None],130,"['    """"""Mock decompose query.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_input,_mock_input,function,2,2,2,30,15.0,0,0,['prompt_args'],[' Dict'],[None],125,"['    """"""Mock input prompt.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_insert_predict,_mock_insert_predict,function,1,3,3,16,5.33,0,0,[],[],[],18,"['    """"""Mock insert predict.\n', '\n', '    Used in GPT tree index during insertion\n', '    to select the next node.\n', '\n', '    """"""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_keyword_extract,_mock_keyword_extract,function,2,2,2,57,28.5,0,0,['prompt_args'],[' Dict'],[None],96,"['    """"""Mock keyword extract.""""""\n']",['mock_extract_keywords_response'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_kg_triplet_extract,_mock_kg_triplet_extract,function,2,2,2,25,12.5,0,0,['prompt_args'],[' Dict'],[None],120,"['    """"""Mock kg triplet extract.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_multi_select,_mock_multi_select,function,6,27,16,208,7.7,0,0,['prompt_args'],[' Dict'],[None],50,"['    """"""Mock single select.""""""\n']",['json.dumps'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_pandas,_mock_pandas,function,4,4,4,61,15.25,0,0,['prompt_args'],[' Dict'],[None],135,"['    """"""Mock pandas prompt.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_query_keyword_extract,_mock_query_keyword_extract,function,2,2,2,61,30.5,0,0,['prompt_args'],[' Dict'],[None],101,"['    """"""Mock query keyword extract.""""""\n']",['mock_extract_keywords_response'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_query_select,_mock_query_select,function,1,3,3,16,5.33,0,0,[],[],[],28,"['    """"""Mock query predict.\n', '\n', '    Used in GPT tree index during query traversal\n', '    to select the next node.\n', '\n', '    """"""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_refine,_mock_refine,function,2,4,4,67,16.75,0,0,['prompt_args'],[' Dict'],[None],91,"['    """"""Mock refine.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_schema_extract,_mock_schema_extract,function,2,2,2,25,12.5,0,0,['prompt_args'],[' Dict'],[None],106,"['    """"""Mock schema extract.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_single_select,_mock_single_select,function,2,11,11,56,5.09,0,0,[],[],[],38,"['    """"""Mock single select.""""""\n']",['json.dumps'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_sql_response_synthesis,_mock_sql_response_synthesis,function,2,2,2,37,18.5,0,0,['prompt_args'],[' Dict'],[None],146,"['    """"""Mock sql response synthesis prompt.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_sql_response_synthesis_v2,_mock_sql_response_synthesis_v2,function,2,2,2,32,16.0,0,0,['prompt_args'],[' Dict'],[None],151,"['    """"""Mock sql response synthesis prompt.\n', '\n', '    TODO: deprecate the above\n', '\n', '    """"""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_sub_questions,_mock_sub_questions,function,3,17,17,141,8.29,0,0,[],[],[],72,"['    """"""Mock sub questions.""""""\n']",['json.dumps'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_summary_predict,_mock_summary_predict,function,2,2,2,32,16.0,0,0,['prompt_args'],[' Dict'],[None],13,"['    """"""Mock summary predict.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:_mock_text_to_sql,_mock_text_to_sql,function,7,14,14,149,10.64,0,0,['prompt_args'],[' Dict'],[None],111,"['    """"""Mock text to sql.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:mock_llmpredictor_predict,mock_llmpredictor_predict,function,46,112,53,1901,16.97,0,1,"['prompt', '**prompt_args']","[' BasePromptTemplate', ' Any']","[None, None]",164,"['    """"""Mock predict method of LLMPredictor.\n', '\n', '    Depending on the prompt, return response.\n', '\n', '    """"""\n']","['_mock_summary_predict', '_mock_insert_predict', '_mock_query_select', '_mock_refine', '_mock_answer', '_mock_keyword_extract', '_mock_query_keyword_extract', '_mock_schema_extract', '_mock_text_to_sql', '_mock_kg_triplet_extract', '_mock_input', '_mock_single_select', '_mock_multi_select', '_mock_sub_questions', '_mock_pandas', '_mock_sql_response_synthesis', '_mock_sql_response_synthesis_v2', '_mock_decompose_query', '_mock_choice_select', '_mock_conversation', 'str']",21
repos/llama_index/llama-index-core/tests/mock_utils/mock_predict.py:patch_llmpredictor_predict,patch_llmpredictor_predict,function,2,3,3,53,17.67,0,0,"['self', 'prompt', '**prompt_args']","[' Any', ' BasePromptTemplate', ' Any']","[None, None, None]",221,"['    """"""Mock predict method of LLMPredictor.\n', '\n', '    Depending on the prompt, return response.\n', '\n', '    """"""\n']",['mock_llmpredictor_predict'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_text_splitter.py:mock_token_splitter_newline,mock_token_splitter_newline,function,3,7,6,43,6.14,0,1,"['text', 'metadata_str']","[' str', ' Optional[str] ']","[None, ' None']",15,"['    """"""Mock token splitter by newline.""""""\n']",['text.split'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_text_splitter.py:patch_token_splitter_newline,patch_token_splitter_newline,function,3,7,6,43,6.14,0,1,"['self', 'text', 'metadata_str']","[' Any', ' str', ' Optional[str] ']","[None, None, ' None']",6,"['    """"""Mock token splitter by newline.""""""\n']",['text.split'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_utils.py:mock_extract_keywords,mock_extract_keywords,function,2,6,6,92,15.33,0,0,"['text_chunk', 'max_keywords', 'filter_stopwords']","[' str', ' Optional[int] ', ' bool ']","[None, ' None', ' True']",20,"['    """"""Extract keywords (mock).\n', '\n', '    Same as simple_extract_keywords but without filtering stopwords.\n', '\n', '    """"""\n']",['simple_extract_keywords'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_utils.py:mock_extract_keywords_response,mock_extract_keywords_response,function,1,8,8,104,13.0,0,0,"['text_chunk', 'max_keywords', 'filter_stopwords']","[' str', ' Optional[int] ', ' bool ']","[None, ' None', ' True']",33,"['    """"""Extract keywords mock response.\n', '\n', '    Same as simple_extract_keywords but without filtering stopwords.\n', '\n', '    """"""\n']",['simple_extract_keywords'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_utils.py:mock_extract_kg_triplets_response,mock_extract_kg_triplets_response,function,5,28,19,158,5.64,1,1,"['text_chunk', 'max_triplets']","[' str', ' Optional[int] ']","[None, ' None']",48,"['    """"""Generate 1 or more fake triplets.""""""\n']",['range'],1
repos/llama_index/llama-index-core/tests/mock_utils/mock_utils.py:mock_tokenizer,mock_tokenizer,function,8,23,22,151,6.57,1,1,['text'],[' str'],[None],9,"['    """"""Mock tokenizer.""""""\n']","['re.split', 'token.strip', 'result.append']",3
repos/llama_index/llama-index-core/tests/node_parser/metadata_extractor.py:test_metadata_extractor,test_metadata_extractor,function,13,41,33,582,14.2,0,0,['mock_service_context'],[' ServiceContext'],[None],15,[],"['TitleExtractor', 'QuestionsAnsweredExtractor', 'SummaryExtractor', 'KeywordExtractor', 'SentenceSplitter', 'Document', 'run_transformations']",7
repos/llama_index/llama-index-core/tests/node_parser/sentence_window.py:test_split_and_window,test_split_and_window,function,10,71,30,481,6.77,0,0,[],[],[],7,[],"['Document', 'SentenceWindowNodeParser.from_defaults', 'node_parser.get_nodes_from_documents', 'len']",4
repos/llama_index/llama-index-core/tests/node_parser/test_file.py:test_unsupported_extension,test_unsupported_extension,function,4,8,8,125,15.62,0,0,[],[],[],7,[],"['SimpleFileNodeParser', 'simple_file_node_parser._parse_nodes', 'Document', 'evenOdd']",4
repos/llama_index/llama-index-core/tests/node_parser/test_hierarchical.py:nodes,nodes,function,4,9,9,160,17.78,0,0,[],[],[],18,[],"['HierarchicalNodeParser.from_defaults', 'node_parser.get_nodes_from_documents']",2
repos/llama_index/llama-index-core/tests/node_parser/test_hierarchical.py:test_get_child_nodes,test_get_child_nodes,function,2,6,6,109,18.17,0,0,['nodes'],[' list'],[None],41,[],"['get_child_nodes', 'len']",2
repos/llama_index/llama-index-core/tests/node_parser/test_hierarchical.py:test_get_deeper_nodes,test_get_deeper_nodes,function,5,29,14,402,13.86,0,0,['nodes'],[' list'],[None],46,[],"['get_deeper_nodes', 'get_root_nodes', 'get_child_nodes', 'get_leaf_nodes']",4
repos/llama_index/llama-index-core/tests/node_parser/test_hierarchical.py:test_get_deeper_nodes_with_negative_depth,test_get_deeper_nodes_with_negative_depth,function,3,7,7,80,11.43,0,0,['nodes'],[' list'],[None],67,[],"['pytest.raises', 'get_deeper_nodes']",2
repos/llama_index/llama-index-core/tests/node_parser/test_hierarchical.py:test_get_deeper_nodes_with_no_root_nodes,test_get_deeper_nodes_with_no_root_nodes,function,3,6,6,89,14.83,0,0,['nodes'],[' list'],[None],62,[],"['pytest.raises', 'get_deeper_nodes']",2
repos/llama_index/llama-index-core/tests/node_parser/test_hierarchical.py:test_get_leaf_nodes,test_get_leaf_nodes,function,2,5,5,80,16.0,0,0,['nodes'],[' list'],[None],36,[],"['get_leaf_nodes', 'len']",2
repos/llama_index/llama-index-core/tests/node_parser/test_hierarchical.py:test_get_root_nodes,test_get_root_nodes,function,2,5,5,70,14.0,0,0,['nodes'],[' list'],[None],26,[],"['get_root_nodes', 'len']",2
repos/llama_index/llama-index-core/tests/node_parser/test_hierarchical.py:test_get_root_nodes_empty,test_get_root_nodes_empty,function,2,5,4,69,13.8,0,0,['nodes'],[' list'],[None],31,[],['get_root_nodes'],1
repos/llama_index/llama-index-core/tests/node_parser/test_html.py:test_multiple_tags_splits,test_multiple_tags_splits,function,4,8,8,109,13.62,0,0,[],[],[],74,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_html.py:test_neighbor_tags_splits,test_neighbor_tags_splits,function,4,7,7,104,14.86,0,0,[],[],[],150,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_html.py:test_nesting_tags_splits,test_nesting_tags_splits,function,4,8,8,109,13.62,0,0,[],[],[],113,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_html.py:test_no_splits,test_no_splits,function,4,7,7,105,15.0,0,0,[],[],[],13,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_html.py:test_single_splits,test_single_splits,function,4,7,7,105,15.0,0,0,[],[],[],43,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_json.py:test_split_empty_text,test_split_empty_text,function,6,9,8,136,15.11,0,0,[],[],[],5,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents']",3
repos/llama_index/llama-index-core/tests/node_parser/test_json.py:test_split_invalid_json,test_split_invalid_json,function,6,12,11,161,13.42,0,0,[],[],[],39,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents']",3
repos/llama_index/llama-index-core/tests/node_parser/test_json.py:test_split_valid_dict_json,test_split_valid_dict_json,function,7,17,16,204,12.0,0,0,[],[],[],31,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents', 'len']",4
repos/llama_index/llama-index-core/tests/node_parser/test_json.py:test_split_valid_json,test_split_valid_json,function,8,28,24,275,9.82,0,0,[],[],[],12,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents', 'len']",4
repos/llama_index/llama-index-core/tests/node_parser/test_json.py:test_split_valid_json_defaults,test_split_valid_json_defaults,function,7,17,16,206,12.12,0,0,[],[],[],23,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents', 'len']",4
repos/llama_index/llama-index-core/tests/node_parser/test_markdown.py:test_header_metadata,test_header_metadata,function,4,9,9,117,13.0,0,0,[],[],[],136,[],"['MarkdownNodeParser', 'markdown_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_markdown.py:test_header_splits,test_header_splits,function,4,9,9,117,13.0,0,0,[],[],[],5,[],"['MarkdownNodeParser', 'markdown_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_markdown.py:test_header_splits_with_indented_code_blocks,test_header_splits_with_indented_code_blocks,function,4,8,8,114,14.25,0,0,[],[],[],28,[],"['MarkdownNodeParser', 'markdown_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_markdown.py:test_non_header_splits,test_non_header_splits,function,4,9,9,114,12.67,0,0,[],[],[],99,[],"['MarkdownNodeParser', 'markdown_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_markdown.py:test_pre_header_content,test_pre_header_content,function,4,7,7,106,15.14,0,0,[],[],[],117,[],"['MarkdownNodeParser', 'markdown_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-core/tests/node_parser/test_markdown_element.py:test_complex_md,test_complex_md,function,2,3,3,28,9.33,0,0,[],[],[],88,[],['Document'],1
repos/llama_index/llama-index-core/tests/node_parser/test_markdown_element.py:test_extract_ref_doc_id,test_extract_ref_doc_id,function,2,3,3,32,10.67,0,0,[],[],[],2654,[],['Document'],1
repos/llama_index/llama-index-core/tests/node_parser/test_markdown_element.py:test_llama2_bad_md,test_llama2_bad_md,function,2,3,3,28,9.33,0,0,[],[],[],227,[],['Document'],1
repos/llama_index/llama-index-core/tests/node_parser/test_markdown_element.py:test_md_table_extraction,test_md_table_extraction,function,2,3,3,28,9.33,0,0,[],[],[],8,[],['Document'],1
repos/llama_index/llama-index-core/tests/node_parser/test_markdown_element.py:test_md_table_extraction_broken_table,test_md_table_extraction_broken_table,function,2,3,3,28,9.33,0,0,[],[],[],48,[],['Document'],1
repos/llama_index/llama-index-core/tests/node_parser/test_markdown_element.py:test_start_end_char_idx,test_start_end_char_idx,function,2,3,3,32,10.67,0,0,[],[],[],2670,[],['Document'],1
repos/llama_index/llama-index-core/tests/node_parser/test_semantic_splitter.py:test_grouped_semantically,test_grouped_semantically,function,8,50,31,373,7.46,0,0,[],[],[],74,[],"['Document', 'MockEmbedding', 'SemanticSplitterNodeParser.from_defaults', 'node_parser.get_nodes_from_documents', 'len']",5
repos/llama_index/llama-index-core/tests/node_parser/test_semantic_splitter.py:test_split_and_permutated,test_split_and_permutated,function,13,109,41,811,7.44,0,0,[],[],[],92,[],"['Document', 'MockEmbedding', 'SemanticSplitterNodeParser.from_defaults', 'node_parser.sentence_splitter', 'node_parser._build_sentence_groups', 'len']",6
repos/llama_index/llama-index-core/tests/node_parser/test_semantic_splitter.py:MockEmbedding,MockEmbedding,class,13,285,58,1388,4.87,0,2,[],[],[],10,[],[],0
repos/llama_index/llama-index-core/tests/node_parser/test_semantic_splitter.py:MockEmbedding:_get_query_embedding,MockEmbedding:_get_query_embedding,method,3,8,7,26,3.25,0,0,"['self', 'query']","[None, ' str']","[None, None]",68,"['        """"""Mock get query embedding.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/node_parser/test_semantic_splitter.py:MockEmbedding:_get_text_embedding,MockEmbedding:_get_text_embedding,method,5,118,39,524,4.44,0,1,"['self', 'text']","[None, ' str']","[None, None]",43,"['        """"""Mock get text embedding.""""""\n']","['text.strip', 'print', 'ValueError']",3
repos/llama_index/llama-index-core/tests/node_parser/test_semantic_splitter.py:MockEmbedding:class_name,MockEmbedding:class_name,method,1,2,2,21,10.5,0,0,['cls'],[None],[None],12,[],[],0
repos/llama_index/llama-index-core/tests/node_parser/test_unstructured.py:test_html_table_extraction,test_html_table_extraction,function,7,107,60,1094,10.22,0,0,[],[],[],21,[],"['Document', 'UnstructuredElementNodeParser', 'node_parser.get_nodes_from_documents', 'len', 'isinstance']",5
repos/llama_index/llama-index-core/tests/objects/test_base.py:test_object_index,test_object_index,function,6,24,17,315,13.12,0,0,['mock_service_context'],[' ServiceContext'],[None],12,"['    """"""Test object index.""""""\n']","['SimpleObjectNodeMapping.from_objects', 'ObjectIndex.from_objects', 'obj_index.as_retriever', 'obj_index.insert_object']",4
repos/llama_index/llama-index-core/tests/objects/test_base.py:test_object_index_default_mapping,test_object_index_default_mapping,function,4,17,12,231,13.59,0,0,['mock_service_context'],[' ServiceContext'],[None],26,"['    """"""Test object index.""""""\n']","['ObjectIndex.from_objects', 'obj_index.as_retriever', 'obj_index.insert_object']",3
repos/llama_index/llama-index-core/tests/objects/test_base.py:test_object_index_fn_mapping,test_object_index_fn_mapping,function,12,47,34,471,10.02,1,0,['mock_service_context'],[' ServiceContext'],[None],37,"['    """"""Test object index.""""""\n']","['print', 'to_node_fn', 'TextNode', 'from_node_fn', 'ObjectIndex.from_objects', 'obj_index.as_retriever', 'obj_index.insert_object']",7
repos/llama_index/llama-index-core/tests/objects/test_base.py:test_object_index_persist,test_object_index_persist,function,11,41,25,834,20.34,0,0,['mock_service_context'],[' ServiceContext'],[None],63,"['    """"""Test object index persist/load.""""""\n']","['SimpleObjectNodeMapping.from_objects', 'ObjectIndex.from_objects', 'obj_index.persist', 'ObjectIndex.from_persist_dir']",4
repos/llama_index/llama-index-core/tests/objects/test_base.py:test_object_index_with_tools,test_object_index_with_tools,function,8,26,23,358,13.77,0,0,['mock_service_context'],[' ServiceContext'],[None],91,"['    """"""Test object index with tools.""""""\n']","['FunctionTool.from_defaults', 'SimpleToolNodeMapping.from_objects', 'ObjectIndex.from_objects', 'obj_retriever.as_retriever']",4
repos/llama_index/llama-index-core/tests/objects/test_node_mapping.py:test_simple_object_node_mapping,test_simple_object_node_mapping,function,6,24,19,452,18.83,0,0,[],[],[],36,"['    """"""Test simple object node mapping.""""""\n']","['SimpleObjectNodeMapping.from_objects', 'node_mapping.to_node', 'node_mapping.from_node', 'TestObject']",4
repos/llama_index/llama-index-core/tests/objects/test_node_mapping.py:test_simple_object_node_mapping_persist,test_simple_object_node_mapping_persist,function,8,12,12,234,19.5,0,0,[],[],[],49,"['    """"""Test persist/load.""""""\n']","['SimpleObjectNodeMapping.from_objects', 'node_mapping.persist', 'SimpleObjectNodeMapping.from_persist_dir']",3
repos/llama_index/llama-index-core/tests/objects/test_node_mapping.py:test_sql_table_node_mapping_to_node,test_sql_table_node_mapping_to_node,function,16,35,31,442,12.63,2,0,['mocker'],[' MockerFixture'],[None],92,"['    """"""Test to add node for sql table node mapping object to ensure no \'None\' values in metadata output to avoid issues with nulls when upserting to indexes.""""""\n']","['mocker.patch', 'SQLTableSchema', 'TestSQLDatabase', 'SQLTableNodeMapping', 'mapping.to_node', 'nodes.append']",6
repos/llama_index/llama-index-core/tests/objects/test_node_mapping.py:test_tool_object_node_mapping,test_tool_object_node_mapping,function,9,77,45,887,11.52,0,0,[],[],[],59,"['    """"""Test tool object node mapping.""""""\n']","['FunctionTool.from_defaults', 'SimpleToolNodeMapping.from_objects', 'node_mapping.to_node', 'node_mapping.from_node', 'recon_tool2', 'node_mapping.add_object']",6
repos/llama_index/llama-index-core/tests/objects/test_node_mapping.py:TestObject,TestObject,class,8,16,13,132,8.25,0,0,[],[],[],15,[],[],0
repos/llama_index/llama-index-core/tests/objects/test_node_mapping.py:TestSQLDatabase,TestSQLDatabase,class,1,5,5,29,5.8,0,0,[],[],[],29,[],[],0
repos/llama_index/llama-index-core/tests/objects/test_node_mapping.py:TestObject:__hash__,TestObject:__hash__,method,2,2,2,21,10.5,0,0,['self'],[None],[None],22,[],['hash'],1
repos/llama_index/llama-index-core/tests/objects/test_node_mapping.py:TestObject:__str__,TestObject:__str__,method,2,2,2,39,19.5,0,0,['self'],[None],[None],25,[],"['f""TestObject']",1
repos/llama_index/llama-index-core/tests/objects/test_node_mapping.py:TestSQLDatabase:__init__,TestSQLDatabase:__init__,method,0,1,1,4,4.0,0,0,['self'],[None],[None],32,[],[],0
repos/llama_index/llama-index-core/tests/output_parsers/test_base.py:test_lc_output_parser,test_lc_output_parser,function,13,65,45,751,11.55,0,0,[],[],[],20,"['    """"""Test langchain output parser.""""""\n']","['MockOutputParser', 'get_format_instructions', 'parse', 'ResponseSchema', 'LangchainOutputParser', 'output_parser.format']",6
repos/llama_index/llama-index-core/tests/output_parsers/test_pydantic.py:test_pydantic,test_pydantic,function,15,45,36,502,11.16,0,0,[],[],[],19,"['    """"""Test pydantic output parser.""""""\n']","['PydanticOutputParser', 'parser.parse', 'isinstance', 'pytest.raises']",4
repos/llama_index/llama-index-core/tests/output_parsers/test_pydantic.py:test_pydantic_format,test_pydantic_format,function,5,12,10,140,11.67,0,0,[],[],[],47,"['    """"""Test pydantic format.""""""\n']","['PydanticOutputParser', 'parser.format']",2
repos/llama_index/llama-index-core/tests/output_parsers/test_pydantic.py:AttrDict,AttrDict,class,3,4,4,21,5.25,0,0,[],[],[],8,[],[],0
repos/llama_index/llama-index-core/tests/output_parsers/test_pydantic.py:TestModel,TestModel,class,5,6,6,43,7.17,0,0,[],[],[],13,[],[],0
repos/llama_index/llama-index-core/tests/output_parsers/test_selection.py:output_parser,output_parser,function,2,2,2,29,14.5,0,0,[],[],[],7,[],['SelectionOutputParser'],1
repos/llama_index/llama-index-core/tests/output_parsers/test_selection.py:test_format,test_format,function,4,13,13,152,11.69,0,0,['output_parser'],[' SelectionOutputParser'],[None],11,[],"['output_parser.format', 'new_test_template.format']",2
repos/llama_index/llama-index-core/tests/output_parsers/test_utils.py:test_extract_json_str,test_extract_json_str,function,1,2,2,10,5.0,0,0,[],[],[],4,[],[],0
repos/llama_index/llama-index-core/tests/playground/test_base.py:test_from_docs,test_from_docs,function,7,35,31,489,13.97,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",117,"['    """"""Test initialization via a list of documents.""""""\n']","['MockEmbedding', 'Document', 'Playground.from_docs', 'len', 'pytest.raises']",5
repos/llama_index/llama-index-core/tests/playground/test_base.py:test_get_set_compare,test_get_set_compare,function,9,52,39,727,13.98,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",83,"['    """"""Test basic comparison of indices.""""""\n']","['MockEmbedding', 'VectorStoreIndex.from_documents', 'SummaryIndex.from_documents', 'TreeIndex.from_documents', 'Playground', 'len', 'playground.compare']",7
repos/llama_index/llama-index-core/tests/playground/test_base.py:test_validation,test_validation,function,4,32,16,337,10.53,0,0,[],[],[],142,"['    """"""Test validation of indices and modes.""""""\n']","['pytest.raises', 'Playground']",2
repos/llama_index/llama-index-core/tests/playground/test_base.py:MockEmbedding,MockEmbedding,class,13,285,58,1388,4.87,0,2,[],[],[],19,[],[],0
repos/llama_index/llama-index-core/tests/playground/test_base.py:MockEmbedding:_get_query_embedding,MockEmbedding:_get_query_embedding,method,3,8,7,26,3.25,0,0,"['self', 'query']","[None, ' str']","[None, None]",77,"['        """"""Mock get query embedding.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/playground/test_base.py:MockEmbedding:_get_text_embedding,MockEmbedding:_get_text_embedding,method,5,118,39,524,4.44,0,1,"['self', 'text']","[None, ' str']","[None, None]",52,"['        """"""Mock get text embedding.""""""\n']","['text.strip', 'print', 'ValueError']",3
repos/llama_index/llama-index-core/tests/playground/test_base.py:MockEmbedding:class_name,MockEmbedding:class_name,method,1,2,2,21,10.5,0,0,['cls'],[None],[None],21,[],[],0
repos/llama_index/llama-index-core/tests/postprocessor/test_base.py:test_embedding_recency_postprocessor,test_embedding_recency_postprocessor,function,10,85,53,1090,12.82,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",194,"['    """"""Test fixed recency processor.""""""\n']","['TextNode', 'EmbeddingRecencyPostprocessor', 'QueryBundle', 'postprocessor.postprocess_nodes', 'cast']",5
repos/llama_index/llama-index-core/tests/postprocessor/test_base.py:test_fixed_recency_postprocessor,test_fixed_recency_postprocessor,function,8,75,52,907,12.09,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",147,"['    """"""Test fixed recency processor.""""""\n']","['TextNode', 'FixedRecencyPostprocessor', 'QueryBundle', 'postprocessor.postprocess_nodes', 'len']",5
repos/llama_index/llama-index-core/tests/postprocessor/test_base.py:test_forward_back_processor,test_forward_back_processor,function,21,228,85,3159,13.86,1,2,['tmp_path'],[' Path'],[None],31,"['    """"""Test forward-back processor.""""""\n']","['TextNode', 'enumerate', 'RelatedNodeInfo', 'len', 'SimpleDocumentStore', 'docstore.add_documents', 'PrevNextNodePostprocessor', 'node_postprocessor.postprocess_nodes', 'pytest.raises']",9
repos/llama_index/llama-index-core/tests/postprocessor/test_base.py:test_keyword_postprocessor,test_keyword_postprocessor,function,10,107,49,1305,12.2,0,0,[],[],[],300,"['    """"""Test keyword processor.""""""\n']","['TextNode', 'KeywordNodePostprocessor', 'postprocessor.postprocess_nodes', 'len']",4
repos/llama_index/llama-index-core/tests/postprocessor/test_base.py:test_keyword_postprocessor_for_non_english,test_keyword_postprocessor_for_non_english,function,11,87,51,1391,15.99,0,0,[],[],[],337,"['    """"""Test keyword processor for non English.""""""\n']","['TextNode', 'KeywordNodePostprocessor', 'postprocessor.postprocess_nodes', 'len']",4
repos/llama_index/llama-index-core/tests/postprocessor/test_base.py:test_time_weighted_postprocessor,test_time_weighted_postprocessor,function,9,129,57,1402,10.87,0,0,[],[],[],254,"['    """"""Test time weighted processor.""""""\n']","['TextNode', 'TimeWeightedPostprocessor', 'postprocessor.postprocess_nodes', 'len', 'cast', 'NodeWithScore', 'enumerate']",7
repos/llama_index/llama-index-core/tests/postprocessor/test_llm_rerank.py:mock_format_node_batch_fn,mock_format_node_batch_fn,function,1,6,6,51,8.5,0,0,['nodes'],[' List[BaseNode]'],[None],39,"['    """"""Mock format node batch fn.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/postprocessor/test_llm_rerank.py:mock_llmpredictor_predict,mock_llmpredictor_predict,function,16,58,47,520,8.97,1,1,"['self', 'prompt', '**prompt_args']","[' Any', ' BasePromptTemplate', ' Any']","[None, None, None]",13,"['    """"""Patch llm predictor predict.""""""\n']","['context_str.split', 'enumerate', 'choices_and_scores.append']",3
repos/llama_index/llama-index-core/tests/postprocessor/test_llm_rerank.py:test_llm_rerank,test_llm_rerank,function,10,44,40,674,15.32,0,0,['mock_service_context'],[' ServiceContext'],[None],49,"['    """"""Test LLM rerank.""""""\n']","['TextNode', 'LLMRerank', 'llm_rerank.postprocess_nodes', 'QueryBundle', 'len']",5
repos/llama_index/llama-index-core/tests/postprocessor/test_metadata_replacement.py:test_metadata_replacement,test_metadata_replacement,function,7,31,23,315,10.16,0,0,[],[],[],5,[],"['TextNode', 'MetadataReplacementPostProcessor', 'postprocessor.postprocess_nodes', 'len']",4
repos/llama_index/llama-index-core/tests/postprocessor/test_optimizer.py:mock_get_text_embedding,mock_get_text_embedding,function,3,51,20,242,4.75,0,1,['text'],[' str'],[None],23,"['    """"""Mock get text embedding.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-core/tests/postprocessor/test_optimizer.py:mock_get_text_embedding_chinese,mock_get_text_embedding_chinese,function,3,52,21,243,4.67,0,1,['text'],[' str'],[None],45,"['    """"""Mock get text embedding.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-core/tests/postprocessor/test_optimizer.py:mock_get_text_embeddings,mock_get_text_embeddings,function,1,6,6,51,8.5,0,0,['texts'],[' List[str]'],[None],40,"['    """"""Mock get text embeddings.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/postprocessor/test_optimizer.py:mock_get_text_embeddings_chinese,mock_get_text_embeddings_chinese,function,1,6,6,59,9.83,0,0,['texts'],[' List[str]'],[None],62,"['    """"""Mock get text embeddings.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/postprocessor/test_optimizer.py:mock_tokenizer_fn,mock_tokenizer_fn,function,2,3,3,20,6.67,0,0,['text'],[' str'],[None],11,"['    """"""Mock tokenizer function.""""""\n']",['text.split'],1
repos/llama_index/llama-index-core/tests/postprocessor/test_optimizer.py:mock_tokenizer_fn2,mock_tokenizer_fn2,function,2,2,2,21,10.5,0,0,['text'],[' str'],[None],17,"['    """"""Mock tokenizer function.""""""\n']",['text.split'],1
repos/llama_index/llama-index-core/tests/postprocessor/test_optimizer.py:test_optimizer,test_optimizer,function,17,130,40,2039,15.68,0,0,"['_mock_embeds', '_mock_embed']","[' Any', ' Any']","[None, None]",71,"['    """"""Test optimizer.""""""\n']","['SentenceEmbeddingOptimizer', 'embed_model=MockEmbedding', 'QueryBundle', 'TextNode', 'optimizer.postprocess_nodes']",5
repos/llama_index/llama-index-core/tests/postprocessor/test_rankgpt_rerank.py:mock_rankgpt_chat,mock_rankgpt_chat,function,2,9,8,88,9.78,0,0,"['self', 'messages', '**kwargs']","[' Any', None, ' Any']","[None, None, None]",12,[],"['ChatResponse', 'message=ChatMessage']",2
repos/llama_index/llama-index-core/tests/postprocessor/test_rankgpt_rerank.py:test_rankgpt_rerank,test_rankgpt_rerank,function,6,18,16,243,13.5,0,0,[],[],[],39,[],"['RankGPTRerank', 'llm=MockLLM', 'rankgpt_rerank.postprocess_nodes', 'len']",4
repos/llama_index/llama-index-core/tests/program/test_function_program.py:_get_mock_album_response,_get_mock_album_response,function,6,26,25,268,10.31,0,1,"['allow_parallel_tool_calls', '']","[' bool ', None]","[' False', None]",44,"['    """"""Get mock album.""""""\n']","['ToolOutput', 'content=str', 'AgentChatResponse']",3
repos/llama_index/llama-index-core/tests/program/test_function_program.py:test_function_program,test_function_program,function,9,31,27,407,13.13,0,0,[],[],[],104,"['    """"""Test Function program.""""""\n']","['FunctionCallingProgram.from_defaults', 'llm=MockLLM', 'llm_program', 'isinstance']",4
repos/llama_index/llama-index-core/tests/program/test_function_program.py:test_function_program_multiple,test_function_program_multiple,function,10,45,37,601,13.36,0,0,[],[],[],120,"['    """"""Test Function program multiple.""""""\n']","['FunctionCallingProgram.from_defaults', 'llm=MockLLM', 'llm_program', 'isinstance', 'len']",5
repos/llama_index/llama-index-core/tests/program/test_function_program.py:MockAlbum,MockAlbum,class,5,6,5,41,6.83,0,0,[],[],[],25,[],[],0
repos/llama_index/llama-index-core/tests/program/test_function_program.py:MockLLM,MockLLM,class,7,62,30,753,12.15,0,0,[],[],[],70,[],[],0
repos/llama_index/llama-index-core/tests/program/test_function_program.py:MockSong,MockSong,class,2,2,2,9,4.5,0,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-core/tests/program/test_function_program.py:MockLLM:metadata,MockLLM:metadata,method,2,2,2,49,24.5,0,0,['self'],[None],[None],100,[],['LLMMetadata'],1
repos/llama_index/llama-index-core/tests/program/test_function_program.py:MockLLM:predict_and_call,MockLLM:predict_and_call,method,2,4,4,85,21.25,0,0,"['self', 'tools', 'user_msg', 'ChatMessage]] ', 'chat_history', 'verbose', 'allow_parallel_tool_calls', '**kwargs', '']","[None, ' List[""BaseTool""]', ' Optional[Union[str', None, ' Optional[List[ChatMessage]] ', ' bool ', ' bool ', ' Any', None]","[None, None, None, ' None', ' None', ' False', ' False', None, None]",71,"['        """"""Predict and call the tool.""""""\n']",['_get_mock_album_response'],1
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:test_llm_program,test_llm_program,function,7,23,21,327,14.22,0,0,[],[],[],50,"['    """"""Test LLM program.""""""\n']","['PydanticOutputParser', 'LLMTextCompletionProgram.from_defaults', 'llm=MockLLM', 'llm_program', 'isinstance']",5
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:test_llm_program_with_messages,test_llm_program_with_messages,function,10,21,20,380,18.1,0,0,[],[],[],64,"['    """"""Test LLM program.""""""\n']","['ChatPromptTemplate', 'PydanticOutputParser', 'LLMTextCompletionProgram.from_defaults', 'llm=MockLLM', 'llm_program', 'isinstance']",6
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:test_llm_program_with_messages_and_chat,test_llm_program_with_messages_and_chat,function,10,21,20,383,18.24,0,0,[],[],[],80,"['    """"""Test LLM program.""""""\n']","['ChatPromptTemplate', 'PydanticOutputParser', 'LLMTextCompletionProgram.from_defaults', 'llm=MockChatLLM', 'llm_program', 'isinstance']",6
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:MockChatLLM,MockChatLLM,class,9,27,23,288,10.67,0,0,[],[],[],30,[],[],0
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:MockLLM,MockLLM,class,8,20,17,206,10.3,0,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:TestModel,TestModel,class,3,4,4,24,6.0,0,0,[],[],[],45,[],[],0
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:MockChatLLM:chat,MockChatLLM:chat,method,5,10,10,140,14.0,0,0,"['self', 'prompt']","[None, ' str']","[None, None]",31,[],"['json.dumps', 'ChatResponse', 'message=ChatMessage']",3
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:MockChatLLM:metadata,MockChatLLM:metadata,method,4,6,5,65,10.83,0,0,['self'],[None],[None],39,[],['LLMMetadata'],1
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:MockLLM:complete,MockLLM:complete,method,5,7,7,94,13.43,0,0,"['self', 'prompt']","[None, ' str']","[None, None]",20,[],"['json.dumps', 'CompletionResponse']",2
repos/llama_index/llama-index-core/tests/program/test_llm_program.py:MockLLM:metadata,MockLLM:metadata,method,2,2,2,19,9.5,0,0,['self'],[None],[None],26,[],['LLMMetadata'],1
repos/llama_index/llama-index-core/tests/program/test_multi_modal_llm_program.py:test_multi_modal_llm_program,test_multi_modal_llm_program,function,8,24,22,414,17.25,0,0,[],[],[],35,"['    """"""Test Multi Modal LLM Pydantic program.""""""\n']","['PydanticOutputParser', 'MultiModalLLMCompletionProgram.from_defaults', 'multi_modal_llm=MockMultiModalLLM', 'multi_modal_llm_program', 'isinstance']",5
repos/llama_index/llama-index-core/tests/program/test_multi_modal_llm_program.py:MockMultiModalLLM,MockMultiModalLLM,class,8,24,21,268,11.17,0,0,[],[],[],17,[],[],0
repos/llama_index/llama-index-core/tests/program/test_multi_modal_llm_program.py:TestModel,TestModel,class,3,4,4,24,6.0,0,0,[],[],[],30,[],[],0
repos/llama_index/llama-index-core/tests/program/test_multi_modal_llm_program.py:MockMultiModalLLM:complete,MockMultiModalLLM:complete,method,5,7,7,94,13.43,0,0,"['self', 'prompt', 'image_documents']","[None, ' str', ' Sequence[ImageDocument]']","[None, None, None]",18,[],"['json.dumps', 'CompletionResponse']",2
repos/llama_index/llama-index-core/tests/program/test_multi_modal_llm_program.py:MockMultiModalLLM:metadata,MockMultiModalLLM:metadata,method,2,2,2,29,14.5,0,0,['self'],[None],[None],26,[],['MultiModalLLMMetadata'],1
repos/llama_index/llama-index-core/tests/prompts/test_base.py:output_parser,output_parser,function,2,2,2,58,29.0,0,0,[],[],[],32,[],['MockOutputParser'],1
repos/llama_index/llama-index-core/tests/prompts/test_base.py:test_chat_template,test_chat_template,function,10,58,42,618,10.66,0,0,[],[],[],59,[],"['ChatPromptTemplate', 'ChatMessage', 'chat_template.partial_format', 'partial_template.format_messages', 'partial_template.format']",5
repos/llama_index/llama-index-core/tests/prompts/test_base.py:test_chat_template_output_parser,test_chat_template_output_parser,function,8,38,30,440,11.58,0,0,['output_parser'],[' BaseOutputParser'],[None],85,[],"['ChatPromptTemplate', 'ChatMessage', 'chat_template.format_messages']",3
repos/llama_index/llama-index-core/tests/prompts/test_base.py:test_function_mappings,test_function_mappings,function,24,136,93,1572,11.56,0,0,[],[],[],204,"['    """"""Test function mappings.""""""\n']","['_format_abc', 'PromptTemplate', 'test_prompt.format', 'test_prompt.partial_format', 'test_prompt_partial.format', '_format_abc_2', 'test_prompt_2.format', 'pytest.raises', '_format_prompt_key1', 'test_prompt_3.format', 'ChatPromptTemplate', 'ChatMessage', 'chat_template.format']",13
repos/llama_index/llama-index-core/tests/prompts/test_base.py:test_selector_template,test_selector_template,function,17,62,48,762,12.29,0,0,[],[],[],107,[],"['PromptTemplate', 'ChatPromptTemplate', 'ChatMessage', 'SelectorPromptTemplate', 'isinstance', 'selector_template.partial_format', 'partial_template.format', 'partial_template.format_messages']",8
repos/llama_index/llama-index-core/tests/prompts/test_base.py:test_template,test_template,function,8,24,20,317,13.21,0,0,[],[],[],36,"['    """"""Test partial format.""""""\n']","['PromptTemplate', 'prompt.partial_format', 'isinstance', 'prompt_fmt.format', 'prompt_fmt.format_messages', 'ChatMessage']",6
repos/llama_index/llama-index-core/tests/prompts/test_base.py:test_template_output_parser,test_template_output_parser,function,5,15,13,194,12.93,0,0,['output_parser'],[' BaseOutputParser'],[None],51,[],"['PromptTemplate', 'prompt.format']",2
repos/llama_index/llama-index-core/tests/prompts/test_base.py:test_template_var_mappings,test_template_var_mappings,function,1,2,2,19,9.5,0,0,[],[],[],138,"['    """"""Test template variable mappings.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/prompts/test_base.py:MockOutputParser,MockOutputParser,class,7,27,19,201,7.44,0,0,[],[],[],18,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_base.py:MockOutputParser:__init__,MockOutputParser:__init__,method,2,2,2,33,16.5,0,0,"['self', 'format_string']","[None, ' str']","[None, None]",21,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_base.py:MockOutputParser:format,MockOutputParser:format,method,2,4,4,36,9.0,0,0,"['self', 'query']","[None, ' str']","[None, None]",27,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_base.py:MockOutputParser:parse,MockOutputParser:parse,method,1,3,3,23,7.67,0,0,"['self', 'output']","[None, ' str']","[None, None]",24,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_guidance_utils.py:test_convert_pydantic_to_guidance_output_template_nested,test_convert_pydantic_to_guidance_output_template_nested,function,3,5,4,102,20.4,0,0,[],[],[],51,[],['pydantic_to_guidance_output_template'],1
repos/llama_index/llama-index-core/tests/prompts/test_guidance_utils.py:test_convert_pydantic_to_guidance_output_template_simple,test_convert_pydantic_to_guidance_output_template_simple,function,3,5,4,102,20.4,0,0,[],[],[],46,[],['pydantic_to_guidance_output_template'],1
repos/llama_index/llama-index-core/tests/prompts/test_guidance_utils.py:test_convert_to_handlebars,test_convert_to_handlebars,function,3,23,16,165,7.17,0,0,[],[],[],10,[],['convert_to_handlebars'],1
repos/llama_index/llama-index-core/tests/prompts/test_guidance_utils.py:TestNestedModel,TestNestedModel,class,3,4,4,42,10.5,0,0,[],[],[],31,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_guidance_utils.py:TestSimpleModel,TestSimpleModel,class,4,6,6,34,5.67,0,0,[],[],[],17,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:test_prompt_mixin,test_prompt_mixin,function,7,48,25,585,12.19,0,0,[],[],[],50,[],"['MockObject1', 'mock_obj1.get_prompts', 'PromptTemplate', 'mock_obj1.update_prompts']",4
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject1,MockObject1,class,10,45,34,498,11.07,0,2,[],[],[],29,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject2,MockObject2,class,8,34,26,320,9.41,0,1,[],[],[],12,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject1:__init__,MockObject1:__init__,method,4,10,10,131,13.1,0,0,['self'],[None],[None],13,[],"['MockObject2', 'PromptTemplate']",2
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject1:_get_prompt_modules,MockObject1:_get_prompt_modules,method,1,3,3,42,14.0,0,0,['self'],[None],[None],21,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject1:_get_prompts,MockObject1:_get_prompts,method,2,2,2,25,12.5,0,0,['self'],[None],[None],18,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject1:_update_prompts,MockObject1:_update_prompts,method,1,12,9,131,10.92,0,2,"['self', 'prompts']","[None, ' PromptDictType']","[None, None]",24,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject2:__init__,MockObject2:__init__,method,2,6,6,59,9.83,0,0,['self'],[None],[None],13,[],['PromptTemplate'],1
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject2:_get_prompt_modules,MockObject2:_get_prompt_modules,method,1,2,2,8,4.0,0,0,['self'],[None],[None],21,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject2:_get_prompts,MockObject2:_get_prompts,method,2,2,2,25,12.5,0,0,['self'],[None],[None],18,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_mixin.py:MockObject2:_update_prompts,MockObject2:_update_prompts,method,1,6,6,59,9.83,0,1,"['self', 'prompts']","[None, ' PromptDictType']","[None, None]",24,[],[],0
repos/llama_index/llama-index-core/tests/prompts/test_utils.py:test_get_template_vars,test_get_template_vars,function,3,10,9,105,10.5,0,0,[],[],[],4,[],['get_template_vars'],1
repos/llama_index/llama-index-core/tests/query_engine/test_cogniswitch_query_engine.py:query_engine,query_engine,function,2,6,6,90,15.0,0,0,[],[],[],12,[],['CogniswitchQueryEngine'],1
repos/llama_index/llama-index-core/tests/query_engine/test_cogniswitch_query_engine.py:test_query_knowledge_successful,test_query_knowledge_successful,function,6,19,18,235,12.37,0,0,"['mock_post', 'query_engine']","[' Any', ' CogniswitchQueryEngine']","[None, None]",19,[],"['query_engine.query_knowledge', 'isinstance']",2
repos/llama_index/llama-index-core/tests/query_engine/test_cogniswitch_query_engine.py:test_query_knowledge_unsuccessful,test_query_knowledge_unsuccessful,function,6,17,15,231,13.59,0,0,"['mock_post', 'query_engine']","[' Any', ' CogniswitchQueryEngine']","[None, None]",30,[],"['query_engine.query_knowledge', 'isinstance']",2
repos/llama_index/llama-index-core/tests/query_engine/test_retriever_query_engine.py:test_query_engine_falls_back_to_inheriting_retrievers_service_context,test_query_engine_falls_back_to_inheriting_retrievers_service_context,function,11,37,30,740,20.0,0,0,[],[],[],21,[],"['OpenAI', 'ServiceContext.from_defaults', 'TreeIndex.from_documents', 'TreeSelectLeafRetriever', 'RetrieverQueryEngine']",5
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:bar_fn,bar_fn,function,2,4,4,23,5.75,0,0,"['a', 'b']","[' Any', ' Any']","[None, None]",32,"['    """"""Bar function.""""""\n']",['str'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:foo_fn,foo_fn,function,4,4,4,11,2.75,0,0,"['a', 'b', 'c']","[' int', ' int ', ' int ']","[None, ' 1', ' 2']",27,"['    """"""Foo function.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:sum_fn,sum_fn,function,2,2,2,12,6.0,0,0,['a'],[' List[int]'],[None],37,"['    """"""Mock list function.""""""\n']",['sum'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:test_arg_component,test_arg_component,function,8,17,15,167,9.82,0,0,[],[],[],85,"['    """"""Test arg component.""""""\n']","['ArgPackComponent', 'arg_c.run_component', 'FnComponent', 'QueryPipeline', 'p.run']",5
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:test_fn_components,test_fn_components,function,7,39,27,539,13.82,0,0,[],[],[],42,"['    """"""Test components.""""""\n']","['FnComponent', 'foo_c.run_component', 'pytest.raises', 'bar_c.run_component']",4
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:test_fn_pipeline,test_fn_pipeline,function,8,37,29,412,11.14,0,0,[],[],[],65,"['    """"""Test pipeline with function components.""""""\n']","['QueryPipeline', 'FnComponent', 'p.run', 'p2.add_modules', 'InputComponent', 'p2.add_link', 'p2.run']",7
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:test_kwarg_component,test_kwarg_component,function,11,29,28,281,9.69,0,0,[],[],[],96,"['    """"""Test kwarg component.""""""\n']","['KwargPackComponent', 'arg_c.run_component', 'convert_fn', 'list', 'FnComponent', 'QueryPipeline', 'p.run']",7
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:test_selector_component,test_selector_component,function,15,40,33,490,12.25,0,0,[],[],[],136,"['    """"""Test selector component.""""""\n']","['bar1_fn', 'str', 'bar2_fn', 'MockSelector', 'RouterComponent', 'FnComponent', 'router.run_component', 'SelectorComponent', 'selector_c.run_component', 'SingleSelection']",10
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:MockSelector,MockSelector,class,8,42,27,405,9.64,0,0,[],[],[],112,[],[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:MockSelector:_get_prompts,MockSelector:_get_prompts,method,1,2,2,8,4.0,0,0,['self'],[None],[None],128,"['        """"""Get prompts.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:MockSelector:_select,MockSelector:_select,method,2,6,6,87,14.5,0,0,"['self', 'choices', 'query']","[None, ' Sequence[ToolMetadata]', ' QueryBundle']","[None, None, None]",115,"['        """"""Select.""""""\n']",['MultiSelection'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_components.py:MockSelector:_update_prompts,MockSelector:_update_prompts,method,0,0,0,0,0.0,0,0,[],[],[],132,"['        """"""Update prompts.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_batch_chain_str,test_query_pipeline_batch_chain_str,function,9,43,37,399,9.28,0,0,[],[],[],423,"['    """"""Test add_chain with only module strings.""""""\n']","['QueryPipeline', 'InputComponent', 'QueryComponent3', 'QueryComponent1', 'p.add_links', 'Link', 'p.add_chain', 'p.run']",8
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_chain,test_query_pipeline_chain,function,4,8,7,90,11.25,0,0,[],[],[],136,"['    """"""Test query pipeline.""""""\n']","['QueryPipeline', 'p.run']",2
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_chain_str,test_query_pipeline_chain_str,function,9,39,33,375,9.62,0,0,[],[],[],400,"['    """"""Test add_chain with only module strings.""""""\n']","['QueryPipeline', 'InputComponent', 'QueryComponent3', 'QueryComponent1', 'p.add_links', 'Link', 'p.add_chain', 'p.run']",8
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_chain_str_intermediate_output,test_query_pipeline_chain_str_intermediate_output,function,13,86,59,907,10.55,0,0,[],[],[],446,"['    """"""Test add_chain with only module strings, showing intermediate outputs.""""""\n']","['QueryPipeline', 'InputComponent', 'QueryComponent3', 'QueryComponent1', 'p.add_links', 'Link', 'p.add_chain', 'p.run_with_intermediates']",8
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_conditional_edges,test_query_pipeline_conditional_edges,function,13,83,60,739,8.9,0,1,[],[],[],484,"['    """"""Test conditional edges.""""""\n']","['choose_fn', 'QueryPipeline', 'InputComponent', 'FnComponent', 'QueryComponent1', 'QueryComponent2', 'p.add_links', 'Link', 'p.run']",9
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_init,test_query_pipeline_init,function,13,90,49,893,9.92,0,0,[],[],[],349,"['    """"""Test query pipeline init params.""""""\n']","['QueryComponent1', 'QueryComponent2', 'InputComponent', 'QueryPipeline', 'Link', 'p.run', 'p.add_modules', 'p.add_links']",8
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_input_component,test_query_pipeline_input_component,function,12,35,27,390,11.14,0,0,[],[],[],154,"['    """"""Test query pipeline input component.""""""\n']","['QueryComponent1', 'QueryComponent2', 'InputComponent', 'QueryPipeline', 'p.add_modules', 'p.add_link', 'p.run']",7
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_multi,test_query_pipeline_multi,function,11,38,32,357,9.39,0,0,[],[],[],219,"['    """"""Test query pipeline.""""""\n']","['QueryComponent1', 'QueryComponent2', 'QueryPipeline', 'p.add_modules', 'p.add_link', 'p.run_multi']",6
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_multi_batch,test_query_pipeline_multi_batch,function,11,46,40,398,8.65,0,0,[],[],[],236,"['    """"""Test query pipeline.""""""\n']","['QueryComponent1', 'QueryComponent2', 'QueryPipeline', 'p.add_modules', 'p.add_link', 'p.run_multi']",6
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_multi_intermediate_output,test_query_pipeline_multi_intermediate_output,function,14,69,45,728,10.55,0,0,[],[],[],257,"['    """"""Test query pipeline showing intermediate outputs.""""""\n']","['QueryComponent1', 'QueryComponent2', 'QueryPipeline', 'p.add_modules', 'p.add_link', 'p.run_multi_with_intermediates']",6
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_partial,test_query_pipeline_partial,function,14,48,33,599,12.48,0,0,[],[],[],175,"['    """"""Test query pipeline.""""""\n']","['QueryComponent1', 'QueryComponent2', 'qc2.partial', 'QueryPipeline', 'p.run', 'p.add_modules', 'p.add_link', 'Chainable2']",8
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_single_arg_inp,test_query_pipeline_single_arg_inp,function,4,8,7,93,11.62,0,0,[],[],[],145,"['    """"""Test query pipeline with single arg input (no kwargs).""""""\n']","['QueryPipeline', 'QueryComponent3', 'p.run']",3
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_sub,test_query_pipeline_sub,function,11,24,21,254,10.58,0,0,[],[],[],204,"['    """"""Test query pipeline.""""""\n']","['QueryComponent2', 'QueryComponent3', 'QueryPipeline', 'p.add_modules', 'p.add_link', 'p.run']",6
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:test_query_pipeline_super_conditional,test_query_pipeline_super_conditional,function,14,152,81,1529,10.06,0,0,[],[],[],534,"['    """"""This tests that paths are properly pruned and maintained for many conditional edges.""""""\n']","['simple_fn', 'print', 'over_twenty_fn', 'final_fn', 'FnComponent', 'QueryPipeline', 'qp.add_link', 'qp.run']",8
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:Chainable2,Chainable2,class,3,8,8,84,10.5,0,0,[],[],[],128,[],[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent1,QueryComponent1,class,13,70,40,608,8.69,0,2,[],[],[],19,[],[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent2,QueryComponent2,class,13,69,39,615,8.91,0,2,[],[],[],56,[],[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent3,QueryComponent3,class,13,59,38,537,9.1,0,1,[],[],[],93,[],[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:Chainable2:_as_query_component,Chainable2:_as_query_component,method,2,2,2,23,11.5,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",131,"['        """"""Get query component.""""""\n']",['QueryComponent2'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent1:_run_component,QueryComponent1:_run_component,method,1,4,4,50,12.5,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",37,"['        """"""Run component.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent1:_validate_component_inputs,QueryComponent1:_validate_component_inputs,method,3,22,12,127,5.77,0,2,"['self', 'input', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",29,"['        """"""Validate component inputs during run_component.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent1:input_keys,QueryComponent1:input_keys,method,2,3,3,46,15.33,0,0,['self'],[None],[None],46,"['        """"""Input keys.""""""\n']",['InputKeys.from_keys'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent1:output_keys,QueryComponent1:output_keys,method,2,2,2,38,19.0,0,0,['self'],[None],[None],51,"['        """"""Output keys.""""""\n']",['OutputKeys.from_keys'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent1:set_callback_manager,QueryComponent1:set_callback_manager,method,12,64,37,550,8.59,0,2,"['self', 'callback_manager']","[None, ' Any']","[None, None]",26,"['        """"""Set callback manager.""""""\n']","['_validate_component_inputs', 'ValueError', '_run_component', '_arun_component', 'self._run_component', 'input_keys', 'InputKeys.from_keys', 'output_keys', 'OutputKeys.from_keys']",9
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent2:_run_component,QueryComponent2:_run_component,method,1,3,3,57,19.0,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",37,"['        """"""Run component.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent2:_validate_component_inputs,QueryComponent2:_validate_component_inputs,method,3,22,12,127,5.77,0,2,"['self', 'input', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",29,"['        """"""Validate component inputs during run_component.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent2:input_keys,QueryComponent2:input_keys,method,2,3,3,46,15.33,0,0,['self'],[None],[None],46,"['        """"""Input keys.""""""\n']",['InputKeys.from_keys'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent2:output_keys,QueryComponent2:output_keys,method,2,2,2,38,19.0,0,0,['self'],[None],[None],51,"['        """"""Output keys.""""""\n']",['OutputKeys.from_keys'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent2:set_callback_manager,QueryComponent2:set_callback_manager,method,12,63,36,557,8.84,0,2,"['self', 'callback_manager']","[None, ' Any']","[None, None]",26,"['        """"""Set callback manager.""""""\n']","['_validate_component_inputs', 'ValueError', '_run_component', '_arun_component', 'self._run_component', 'input_keys', 'InputKeys.from_keys', 'output_keys', 'OutputKeys.from_keys']",9
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent3:_run_component,QueryComponent3:_run_component,method,1,4,4,48,12.0,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",109,"['        """"""Run component.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent3:_validate_component_inputs,QueryComponent3:_validate_component_inputs,method,3,12,10,67,5.58,0,1,"['self', 'input', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",29,"['        """"""Validate component inputs during run_component.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent3:input_keys,QueryComponent3:input_keys,method,2,2,2,36,18.0,0,0,['self'],[None],[None],46,"['        """"""Input keys.""""""\n']",['InputKeys.from_keys'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent3:output_keys,QueryComponent3:output_keys,method,2,2,2,38,19.0,0,0,['self'],[None],[None],51,"['        """"""Output keys.""""""\n']",['OutputKeys.from_keys'],1
repos/llama_index/llama-index-core/tests/query_pipeline/test_query.py:QueryComponent3:set_callback_manager,QueryComponent3:set_callback_manager,method,12,53,35,479,9.04,0,1,"['self', 'callback_manager']","[None, ' Any']","[None, None]",26,"['        """"""Set callback manager.""""""\n']","['_validate_component_inputs', 'ValueError', '_run_component', '_arun_component', 'self._run_component', 'input_keys', 'InputKeys.from_keys', 'output_keys', 'OutputKeys.from_keys']",9
repos/llama_index/llama-index-core/tests/question_gen/test_llm_generators.py:test_llm_question_gen,test_llm_question_gen,function,8,27,25,363,13.44,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",8,[],"['LLMQuestionGenerator.from_defaults', 'ToolMetadata', 'QueryBundle', 'question_gen.generate', 'isinstance']",5
repos/llama_index/llama-index-core/tests/readers/test_json.py:test_basic,test_basic,function,11,28,24,293,10.46,0,0,[],[],[],8,"['    """"""Test JSON reader in basic mode.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'JSONReader', 'reader.load_data', 'len', 'isinstance']",7
repos/llama_index/llama-index-core/tests/readers/test_json.py:test_clean_json,test_clean_json,function,10,46,34,483,10.5,0,0,[],[],[],75,"['    """"""Test JSON reader using the clean_json function.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'JSONReader', 'reader1.load_data']",5
repos/llama_index/llama-index-core/tests/readers/test_json.py:test_collapse_length,test_collapse_length,function,15,45,36,491,10.91,0,0,[],[],[],39,"['    """"""Test JSON reader using the collapse_length function.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'JSONReader', 'reader1.load_data', 'isinstance', 'reader2.load_data']",7
repos/llama_index/llama-index-core/tests/readers/test_json.py:test_jsonl,test_jsonl,function,13,46,31,538,11.7,0,0,[],[],[],57,"['    """"""Test JSON reader using the is_jsonl function.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'JSONReader', 'reader.load_data', 'len', 'isinstance']",7
repos/llama_index/llama-index-core/tests/readers/test_json.py:test_levels_back0,test_levels_back0,function,14,35,32,332,9.49,0,0,[],[],[],23,"['    """"""Test JSON reader using the levels_back function.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'JSONReader', 'reader1.load_data', 'reader2.load_data']",6
repos/llama_index/llama-index-core/tests/readers/test_load_reader.py:test_loading_readers,test_loading_readers,function,8,12,12,237,19.75,0,0,[],[],[],7,[],"['StringIterableReader', 'string_iterable.to_dict', 'cast', 'load_reader']",4
repos/llama_index/llama-index-core/tests/readers/test_string_iterable.py:test_load,test_load,function,4,15,15,124,8.27,0,0,[],[],[],6,"['    """"""Test loading data into StringIterableReader.""""""\n']","['StringIterableReader', 'reader.load_data', 'len']",3
repos/llama_index/llama-index-core/tests/response_synthesizers/test_refine.py:mock_refine_service_context,mock_refine_service_context,function,4,7,7,132,18.86,0,0,['patch_llm_predictor'],[' Any'],[None],56,[],"['CallbackManager', 'ServiceContext.from_defaults']",2
repos/llama_index/llama-index-core/tests/response_synthesizers/test_refine.py:refine_instance,refine_instance,function,2,7,7,125,17.86,0,0,['mock_refine_service_context'],[' ServiceContext'],[None],65,[],['Refine'],1
repos/llama_index/llama-index-core/tests/response_synthesizers/test_refine.py:test_constructor_args,test_constructor_args,function,4,16,12,303,18.94,0,0,['mock_refine_service_context'],[' ServiceContext'],[None],74,[],"['pytest.raises', 'Refine', 'MockRefineProgram']",3
repos/llama_index/llama-index-core/tests/response_synthesizers/test_refine.py:MockRefineProgram,MockRefineProgram,class,16,75,39,879,11.72,0,0,[],[],[],13,[],[],0
repos/llama_index/llama-index-core/tests/response_synthesizers/test_refine.py:MockRefineProgram:__call__,MockRefineProgram:__call__,method,9,14,13,205,14.64,0,0,"['self', '*args', 'context_str', 'context_msg', '**kwargs']","[None, ' Any', ' Optional[str] ', ' Optional[str] ', ' Any']","[None, None, ' None', ' None', None]",26,[],"['cast', 'StructuredRefineResponse']",2
repos/llama_index/llama-index-core/tests/response_synthesizers/test_refine.py:MockRefineProgram:__init__,MockRefineProgram:__init__,method,2,2,2,55,27.5,0,0,"['self', 'input_to_query_satisfied', 'bool]']","[None, ' Dict[str', None]","[None, None, None]",19,[],[],0
repos/llama_index/llama-index-core/tests/response_synthesizers/test_refine.py:MockRefineProgram:output_cls,MockRefineProgram:output_cls,method,2,2,2,30,15.0,0,0,['self'],[None],[None],23,[],[],0
repos/llama_index/llama-index-core/tests/retrievers/test_composable_retriever.py:test_composable_retrieval,test_composable_retrieval,function,12,39,33,465,11.92,0,0,[],[],[],5,"['    """"""Test composable retrieval.""""""\n']","['TextNode', 'IndexNode', 'obj=TextNode', 'SummaryIndex', 'index.as_retriever', 'retriever.retrieve', 'len']",7
repos/llama_index/llama-index-core/tests/selectors/test_llm_selectors.py:test_llm_multi_selector,test_llm_multi_selector,function,7,22,22,208,9.45,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",29,[],"['LLMMultiSelector.from_defaults', 'selector.select']",2
repos/llama_index/llama-index-core/tests/selectors/test_llm_selectors.py:test_llm_multi_selector_max_choices,test_llm_multi_selector_max_choices,function,7,24,24,222,9.25,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",45,[],"['LLMMultiSelector.from_defaults', 'selector.select']",2
repos/llama_index/llama-index-core/tests/selectors/test_llm_selectors.py:test_llm_single_selector,test_llm_single_selector,function,13,34,32,489,14.38,0,0,[],[],[],12,[],"['ServiceContext.from_defaults', 'LLMSingleSelector.from_defaults', 'patch.object', 'type', 'return_value=CompletionResponse', 'selector.select', 'mock_complete.assert_called_once']",7
repos/llama_index/llama-index-core/tests/test_schema.py:node_with_score,node_with_score,function,2,5,5,49,9.8,0,0,['text_node'],[' TextNode'],[None],15,[],['NodeWithScore'],1
repos/llama_index/llama-index-core/tests/test_schema.py:test_node_with_score_passthrough,test_node_with_score_passthrough,function,9,16,9,220,13.75,0,0,['node_with_score'],[' NodeWithScore'],[None],22,[],"['node_with_score.get_text', 'node_with_score.get_content', 'node_with_score.get_embedding']",3
repos/llama_index/llama-index-core/tests/test_schema.py:test_text_node_hash,test_text_node_hash,function,9,36,21,512,14.22,0,0,[],[],[],33,[],"['TextNode', 'node.set_content']",2
repos/llama_index/llama-index-core/tests/test_schema.py:text_node,text_node,function,2,10,10,85,8.5,0,0,[],[],[],6,[],['TextNode'],1
repos/llama_index/llama-index-core/tests/test_utils.py:fn_with_exception,fn_with_exception,function,3,11,11,77,7.0,0,1,"['exception_cls', 'Exception]]']","[' Optional[Union[Type[Exception]', None]","[None, None]",34,"['    """"""Return true unless exception is specified.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/test_utils.py:test_get_color_mapping,test_get_color_mapping,function,5,38,25,461,12.13,0,0,[],[],[],119,"['    """"""Test get_color_mapping function.""""""\n']","['get_color_mapping', 'len', 'set', 'all', 'color_mapping.values', 'color_mapping_ansi.values']",6
repos/llama_index/llama-index-core/tests/test_utils.py:test_get_colored_text,test_get_colored_text,function,6,40,22,469,11.72,1,0,[],[],[],133,"['    """"""Test _get_colored_text function.""""""\n']","['_get_colored_text', 'colored_text.startswith', 'colored_text.endswith']",3
repos/llama_index/llama-index-core/tests/test_utils.py:test_iter_batch,test_iter_batch,function,2,29,21,169,5.83,0,0,[],[],[],108,"['    """"""Check iter_batch works as expected on regular, lazy and empty sequences.""""""\n']","['list', 'range']",2
repos/llama_index/llama-index-core/tests/test_utils.py:test_print_text,test_print_text,function,8,47,24,592,12.6,2,0,['capsys'],[' CaptureFixture'],[None],151,"['    """"""Test print_text function.""""""\n']","['print_text', 'capsys.readouterr']",2
repos/llama_index/llama-index-core/tests/test_utils.py:test_retry_on_conditional_exceptions,test_retry_on_conditional_exceptions,function,6,36,20,537,14.92,0,0,[],[],[],84,"['    """"""Make sure retry function works on conditional exceptions.""""""\n']","['pytest.raises', 'retry_on_exceptions_with_backoff', 'fn_with_exception']",3
repos/llama_index/llama-index-core/tests/test_utils.py:test_retry_on_exceptions_with_backoff,test_retry_on_exceptions_with_backoff,function,7,39,21,518,13.28,0,0,[],[],[],53,"['    """"""Make sure retry function has accurate number of attempts.""""""\n']","['fn_with_exception', 'pytest.raises', 'retry_on_exceptions_with_backoff']",3
repos/llama_index/llama-index-core/tests/test_utils.py:test_tokenizer,test_tokenizer,function,6,10,10,79,7.9,0,0,[],[],[],20,"['    """"""Make sure tokenizer works.\n', '\n', '    NOTE: we use a different tokenizer for python >= 3.9.\n', '\n', '    """"""\n']","['get_tokenizer', 'len']",2
repos/llama_index/llama-index-core/tests/test_utils.py:ConditionalException,ConditionalException,class,3,8,8,73,9.12,0,0,[],[],[],45,[],[],0
repos/llama_index/llama-index-core/tests/test_utils.py:ConditionalException:__init__,ConditionalException:__init__,method,2,2,2,30,15.0,0,0,"['self', 'should_retry']","[None, ' bool']","[None, None]",48,"['        """"""Initialize with parameters.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:baz,baz,function,5,9,8,133,14.78,0,0,[],[],[],23,[],"['print', 'code_splitter.split_text', 'foo', 'baz']",4
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:baz,baz,function,5,9,8,133,14.78,0,0,[],[],[],23,[],"['print', 'code_splitter.split_text', 'foo', 'baz']",4
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:baz,baz,function,5,9,8,133,14.78,0,0,[],[],[],23,[],"['print', 'code_splitter.split_text', 'foo', 'baz']",4
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:foo,foo,function,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:foo,foo,function,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:foo,foo,function,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:test__py_custom_parser_code_splitter,test__py_custom_parser_code_splitter,function,9,21,21,222,10.57,0,1,[],[],[],168,"['    """"""Test case for code splitting using custom parser generated from tree_sitter_languages.""""""\n']","['get_parser', 'CodeSplitter']",2
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:test_cpp_code_splitter,test_cpp_code_splitter,function,4,14,14,130,9.29,0,1,[],[],[],145,"['    """"""Test case for code splitting using typescript.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:test_html_code_splitter,test_html_code_splitter,function,4,14,14,131,9.36,0,1,[],[],[],74,"['    """"""Test case for code splitting using typescript.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:test_python_code_splitter,test_python_code_splitter,function,4,14,14,133,9.5,0,1,[],[],[],10,"['    """"""Test case for code splitting using python.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:test_start_end_char_idx,test_start_end_char_idx,function,1,2,2,9,4.5,0,0,[],[],[],31,[],[],0
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:test_tsx_code_splitter,test_tsx_code_splitter,function,4,14,14,137,9.79,0,1,[],[],[],107,"['    """"""Test case for code splitting using typescript.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-core/tests/text_splitter/test_code_splitter.py:test_typescript_code_splitter,test_typescript_code_splitter,function,4,14,14,137,9.79,0,1,[],[],[],51,"['    """"""Test case for code splitting using typescript.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_chinese_text,test_chinese_text,function,4,8,8,119,14.88,0,0,['chinese_text'],[' str'],[None],41,[],"['SentenceSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_contiguous_text,test_contiguous_text,function,4,8,8,123,15.38,0,0,['contiguous_text'],[' str'],[None],47,[],"['SentenceSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_edge_case,test_edge_case,function,6,193,85,1053,5.46,0,0,[],[],[],75,"['    """"""Test case from: https://github.com/jerryjliu/llama_index/issues/7287.""""""\n']","['SentenceSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_overlap,test_overlap,function,6,50,31,323,6.46,0,0,[],[],[],88,[],"['SentenceSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_paragraphs,test_paragraphs,function,7,23,15,269,11.7,0,0,[],[],[],8,"['    """"""Test case of a string with multiple paragraphs.""""""\n']","['SentenceSplitter', 'sentence_text_splitter.split_text']",2
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_sentences,test_sentences,function,7,25,16,268,10.72,0,0,[],[],[],30,"['    """"""Test case of a string with multiple sentences.""""""\n']","['SentenceSplitter', 'sentence_text_splitter.split_text']",2
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_split_texts_multiple,test_split_texts_multiple,function,13,45,22,459,10.2,0,0,[],[],[],111,"['    """"""Test case for a list of texts.""""""\n']","['SentenceSplitter', 'sentence_text_splitter.split_texts', 'print']",3
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_split_texts_singleton,test_split_texts_singleton,function,8,25,17,284,11.36,0,0,[],[],[],100,"['    """"""Test case for a singleton list of texts.""""""\n']","['SentenceSplitter', 'sentence_text_splitter.split_texts']",2
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_split_texts_with_metadata,test_split_texts_with_metadata,function,9,30,26,379,12.63,0,0,['english_text'],[' str'],[None],126,"['    """"""Test case for a list of texts with metadata.""""""\n']","['tiktoken.get_encoding', 'SentenceSplitter', 'splitter.split_texts', 'len', 'splitter.split_texts_metadata_aware']",5
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_split_with_metadata,test_split_with_metadata,function,11,35,30,440,12.57,1,0,['english_text'],[' str'],[None],57,[],"['tiktoken.get_encoding', 'SentenceSplitter', 'splitter.split_text', 'len', 'splitter.split_text_metadata_aware']",5
repos/llama_index/llama-index-core/tests/text_splitter/test_sentence_splitter.py:test_start_end_char_idx,test_start_end_char_idx,function,10,34,27,391,11.5,1,0,[],[],[],18,[],"['Document', 'SentenceSplitter', 'text_splitter.get_nodes_from_documents', 'len', 'node.get_content']",5
repos/llama_index/llama-index-core/tests/text_splitter/test_token_splitter.py:test_contiguous_text,test_contiguous_text,function,4,8,8,124,15.5,0,0,['contiguous_text'],[' str'],[None],69,[],"['TokenTextSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-core/tests/text_splitter/test_token_splitter.py:test_split_chinese,test_split_chinese,function,4,8,8,130,16.25,0,0,['chinese_text'],[' str'],[None],63,[],"['TokenTextSplitter', 'text_splitter.split_text', 'len']",3
repos/llama_index/llama-index-core/tests/text_splitter/test_token_splitter.py:test_split_long_token,test_split_long_token,function,7,38,26,438,11.53,0,0,[],[],[],43,"['    """"""Test split a really long token.""""""\n']","['tiktoken.get_encoding', 'TokenTextSplitter', 'text_splitter.split_text', 'len']",4
repos/llama_index/llama-index-core/tests/text_splitter/test_token_splitter.py:test_split_token,test_split_token,function,6,30,22,318,10.6,0,0,[],[],[],10,"['    """"""Test split normal token.""""""\n']","['TokenTextSplitter', 'text_splitter.split_text']",2
repos/llama_index/llama-index-core/tests/text_splitter/test_token_splitter.py:test_split_with_metadata,test_split_with_metadata,function,11,35,30,434,12.4,1,0,['english_text'],[' str'],[None],75,[],"['tiktoken.get_encoding', 'TokenTextSplitter', 'splitter.split_text', 'len', 'splitter.split_text_metadata_aware']",5
repos/llama_index/llama-index-core/tests/text_splitter/test_token_splitter.py:test_start_end_char_idx,test_start_end_char_idx,function,10,33,26,368,11.15,1,0,[],[],[],23,[],"['Document', 'TokenTextSplitter', 'text_splitter.get_nodes_from_documents', 'len', 'node.get_content']",5
repos/llama_index/llama-index-core/tests/text_splitter/test_token_splitter.py:test_truncate_token,test_truncate_token,function,6,12,11,134,11.17,0,0,[],[],[],35,"['    """"""Test truncate normal token.""""""\n']","['TokenTextSplitter', 'truncate_text']",2
repos/llama_index/llama-index-core/tests/token_predictor/test_base.py:test_token_predictor,test_token_predictor,function,18,49,34,667,13.61,0,0,['mock_split'],[' Any'],[None],17,"['    """"""Test token predictor.""""""\n']","['Document', 'MockLLM', 'ServiceContext.from_defaults', 'TreeIndex.from_documents', 'index.as_query_engine', 'query_engine.query', 'KeywordTableIndex.from_documents', 'index_keyword.as_query_engine', 'SummaryIndex.from_documents', 'index_list.as_query_engine']",10
repos/llama_index/llama-index-core/tests/tools/conftest.py:documents,documents,function,2,20,14,117,5.85,0,0,[],[],[],10,"['    """"""Get documents.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/tools/test_base.py:test_function_tool,test_function_tool,function,10,46,29,614,13.35,0,0,[],[],[],23,"['    """"""Test function tool.""""""\n']","['FunctionTool.from_defaults', 'str', 'function_tool']",3
repos/llama_index/llama-index-core/tests/tools/test_base.py:test_function_tool_to_langchain,test_function_tool_to_langchain,function,17,46,36,541,11.76,0,0,[],[],[],49,[],"['FunctionTool.from_defaults', 'function_tool.to_langchain_tool', 'langchain_tool.run', 'TestSchema', 'str', 'function_tool.to_langchain_structured_tool', 'langchain_tool2.run']",7
repos/llama_index/llama-index-core/tests/tools/test_base.py:test_retreiver_tool,test_retreiver_tool,function,16,69,51,766,11.1,0,0,[],[],[],162,[],"['Document', 'ServiceContext.from_defaults', 'embed_model=MockEmbedding', 'VectorStoreIndex.from_documents', 'vs_index.as_retriever', 'RetrieverTool', 'metadata=ToolMetadata', 'vs_ret_tool.call']",8
repos/llama_index/llama-index-core/tests/tools/test_base.py:test_tool_fn_schema,test_tool_fn_schema,function,9,21,21,268,12.76,0,0,[],[],[],195,[],"['TestSchema', 'ToolMetadata', 'json.loads', 'set']",4
repos/llama_index/llama-index-core/tests/tools/test_base.py:tmp_function,tmp_function,function,2,2,2,12,6.0,0,0,['x'],[' int'],[None],15,[],['str'],1
repos/llama_index/llama-index-core/tests/tools/test_eval_query_engine_tool.py:MockEvaluator,MockEvaluator,class,4,31,23,264,8.52,0,0,[],[],[],15,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_eval_query_engine_tool.py:MockQueryEngine,MockQueryEngine,class,2,9,9,67,7.44,0,0,[],[],[],34,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_eval_query_engine_tool.py:TestEvalQueryEngineTool,TestEvalQueryEngineTool,class,21,78,64,1248,16.0,0,0,[],[],[],42,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_eval_query_engine_tool.py:MockEvaluator:_get_prompts,MockEvaluator:_get_prompts,method,0,1,1,3,3.0,0,0,['self'],[None],[None],18,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_eval_query_engine_tool.py:MockEvaluator:_update_prompts,MockEvaluator:_update_prompts,method,0,1,1,3,3.0,0,0,"['self', 'prompts_dict']","[None, ' PromptDictType']","[None, None]",21,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_eval_query_engine_tool.py:MockQueryEngine:custom_query,MockQueryEngine:custom_query,method,1,3,3,25,8.33,0,0,"['self', 'query_str']","[None, ' str']","[None, None]",37,"['        """"""Query.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/tools/test_eval_query_engine_tool.py:TestEvalQueryEngineTool:setUp,TestEvalQueryEngineTool:setUp,method,14,30,30,582,19.4,0,0,['self'],[None],[None],43,[],"['MockEvaluator', 'AsyncMock', 'EvaluationResult', 'ToolOutput', 'raw_output=Response', 'EvalQueryEngineTool.from_defaults', 'MockQueryEngine']",7
repos/llama_index/llama-index-core/tests/tools/test_eval_query_engine_tool.py:TestEvalQueryEngineTool:test_eval_query_engine_tool_with_eval_failing,TestEvalQueryEngineTool:test_eval_query_engine_tool_with_eval_failing,method,7,32,30,406,12.69,0,0,['self'],[None],[None],69,"['        """"""Test eval query engine tool with evaluation failing.""""""\n']","['EvaluationResult', 'self.eval_query_engine_tool', 'self.assertEqual']",3
repos/llama_index/llama-index-core/tests/tools/test_eval_query_engine_tool.py:TestEvalQueryEngineTool:test_eval_query_engine_tool_with_eval_passing,TestEvalQueryEngineTool:test_eval_query_engine_tool_with_eval_passing,method,3,4,4,112,28.0,0,0,['self'],[None],[None],64,"['        """"""Test eval query engine tool with evaluation passing.""""""\n']","['self.eval_query_engine_tool', 'self.assertEqual']",2
repos/llama_index/llama-index-core/tests/tools/test_ondemand_loader.py:test_ondemand_loader_tool,test_ondemand_loader_tool,function,3,10,10,93,9.3,0,0,"['tool', '']","[' OnDemandLoaderTool', None]","[None, None]",40,"['    """"""Test ondemand loader.""""""\n']","['tool', 'str']",2
repos/llama_index/llama-index-core/tests/tools/test_ondemand_loader.py:test_ondemand_loader_tool_langchain,test_ondemand_loader_tool_langchain,function,7,17,15,198,11.65,0,0,"['tool', '']","[' OnDemandLoaderTool', None]","[None, None]",49,[],"['tool.to_langchain_structured_tool', 'lc_tool.run', 'str']",3
repos/llama_index/llama-index-core/tests/tools/test_ondemand_loader.py:tool,tool,function,4,12,12,265,22.08,0,0,['mock_service_context'],[' ServiceContext'],[None],27,[],"['StringIterableReader', 'OnDemandLoaderTool.from_defaults']",2
repos/llama_index/llama-index-core/tests/tools/test_ondemand_loader.py:TestSchemaSpec,TestSchemaSpec,class,4,4,4,29,7.25,0,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_query_engine_tool.py:test_query_engine_tool,test_query_engine_tool,function,11,50,35,747,14.94,0,0,[],[],[],18,"['    """"""Test query engine tool.""""""\n']","['MockQueryEngine', 'QueryEngineTool.from_defaults', 'query_tool', 'str', 'cast', 'fn_schema_cls', 'pytest.raises']",7
repos/llama_index/llama-index-core/tests/tools/test_query_engine_tool.py:MockQueryEngine,MockQueryEngine,class,2,9,9,67,7.44,0,0,[],[],[],10,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_query_engine_tool.py:MockQueryEngine:custom_query,MockQueryEngine:custom_query,method,1,3,3,25,8.33,0,0,"['self', 'query_str']","[None, ' str']","[None, None]",13,"['        """"""Query.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/tools/test_retriever_tool.py:test_retriever_tool,test_retriever_tool,function,10,29,25,521,17.97,0,0,[],[],[],38,"['    """"""Test retriever tool.""""""\n']","['MockRetriever', 'RetrieverTool.from_defaults', 'retriever_tool', 'str', 'pr_retriever_tool']",5
repos/llama_index/llama-index-core/tests/tools/test_retriever_tool.py:MockPostProcessor,MockPostProcessor,class,7,29,25,265,9.14,1,0,[],[],[],22,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_retriever_tool.py:MockRetriever,MockRetriever,class,4,19,12,261,13.74,0,0,[],[],[],10,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_retriever_tool.py:MockPostProcessor:_postprocess_nodes,MockPostProcessor:_postprocess_nodes,method,5,11,10,76,6.91,1,0,"['self', 'nodes', 'query_bundle', '']","[None, ' List[NodeWithScore]', ' Optional[QueryBundle] ', None]","[None, None, ' None', None]",27,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_retriever_tool.py:MockPostProcessor:class_name,MockPostProcessor:class_name,method,1,2,2,29,14.5,0,0,['cls'],[None],[None],24,[],[],0
repos/llama_index/llama-index-core/tests/tools/test_retriever_tool.py:MockRetriever:_retrieve,MockRetriever:_retrieve,method,1,3,3,72,24.0,0,0,"['self', 'query_str']","[None, ' str']","[None, None]",13,"['        """"""Mock retrieval.""""""\n']",[],0
repos/llama_index/llama-index-core/tests/tools/test_utils.py:test_create_schema_from_function,test_create_schema_from_function,function,9,55,36,617,11.22,0,0,[],[],[],8,"['    """"""Test create schema from function.""""""\n']","['test_fn', 'create_schema_from_function', 'SchemaCls.schema', 'test_fn2']",4
repos/llama_index/llama-index-core/tests/tools/test_utils.py:test_create_schema_from_function_with_field,test_create_schema_from_function_with_field,function,15,45,36,486,10.8,0,0,[],[],[],33,"['    """"""Test create_schema_from_function with pydantic.Field.""""""\n']","['tmp_function', 'Field', 'str', 'create_schema_from_function', 'schema.schema', 'schema']",6
repos/llama_index/llama-index-core/tests/utilities/test_sql_wrapper.py:sql_database,sql_database,function,12,35,34,435,12.43,0,0,['request'],[' pytest.FixtureRequest'],[None],10,[],"['create_engine', 'MetaData', 'Table', 'Column', 'metadata.create_all', 'getattr', 'SQLDatabase', 'metadata.drop_all']",8
repos/llama_index/llama-index-core/tests/utilities/test_sql_wrapper.py:test_get_single_table_info,test_get_single_table_info,function,1,19,17,141,7.42,0,0,['sql_database'],[' SQLDatabase'],[None],56,[],['sql_database.get_single_table_info'],1
repos/llama_index/llama-index-core/tests/utilities/test_sql_wrapper.py:test_get_table_columns,test_get_table_columns,function,2,10,10,108,10.8,0,0,['sql_database'],[' SQLDatabase'],[None],50,[],['sql_database.get_table_columns'],1
repos/llama_index/llama-index-core/tests/utilities/test_sql_wrapper.py:test_init,test_init,function,2,5,4,78,15.6,0,0,['sql_database'],[' SQLDatabase'],[None],36,[],['isinstance'],1
repos/llama_index/llama-index-core/tests/utilities/test_sql_wrapper.py:test_insert_and_run_sql,test_insert_and_run_sql,function,5,24,17,261,10.88,0,0,['sql_database'],[' SQLDatabase'],[None],66,[],"['sql_database.run_sql', 'sql_database.insert_into_table']",2
repos/llama_index/llama-index-core/tests/utilities/test_sql_wrapper.py:test_long_string_no_truncation,test_long_string_no_truncation,function,7,25,18,280,11.2,0,0,['sql_database'],[' SQLDatabase'],[None],92,[],"['sql_database.run_sql', 'sql_database.insert_into_table']",2
repos/llama_index/llama-index-core/tests/utilities/test_sql_wrapper.py:test_run_sql_truncation,test_run_sql_truncation,function,5,23,16,255,11.09,0,0,['sql_database'],[' SQLDatabase'],[None],79,[],"['sql_database.run_sql', 'sql_database.insert_into_table']",2
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:_node_embeddings_for_test,_node_embeddings_for_test,function,1,42,25,591,14.07,0,0,[],[],[],18,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest,SimpleVectorStoreTest,class,29,276,82,4945,17.92,0,0,[],[],[],44,[],[],0
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_clear,SimpleVectorStoreTest:test_clear,method,9,12,12,265,22.08,0,0,['self'],[None],[None],185,[],"['SimpleVectorStore', 'simple_vector_store.add', 'simple_vector_store.clear', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_delete_nodes,SimpleVectorStoreTest:test_delete_nodes,method,9,15,15,345,23.0,0,0,['self'],[None],[None],193,[],"['SimpleVectorStore', 'simple_vector_store.add', 'simple_vector_store.delete_nodes', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_delete_removes_document_from_query_results,SimpleVectorStoreTest:test_delete_removes_document_from_query_results,method,9,15,15,323,21.53,0,0,['self'],[None],[None],141,[],"['SimpleVectorStore', 'simple_vector_store.add', 'simple_vector_store.delete', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_contradictive_filter_returns_no_matches,SimpleVectorStoreTest:test_query_with_contradictive_filter_returns_no_matches,method,12,25,24,379,15.16,0,0,['self'],[None],[None],114,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'ExactMatchFilter', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",7
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_exact_filters_returns_single_match,SimpleVectorStoreTest:test_query_with_exact_filters_returns_single_match,method,11,20,20,375,18.75,0,0,['self'],[None],[None],100,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'ExactMatchFilter', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",7
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_filter_applies_node_id_filter,SimpleVectorStoreTest:test_query_with_filter_applies_node_id_filter,method,10,18,18,389,21.61,0,0,['self'],[None],[None],86,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_filter_applies_top_k,SimpleVectorStoreTest:test_query_with_filter_applies_top_k,method,10,17,17,349,20.53,0,0,['self'],[None],[None],75,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_filter_on_unknown_field_returns_no_matches,SimpleVectorStoreTest:test_query_with_filter_on_unknown_field_returns_no_matches,method,11,21,21,345,16.43,0,0,['self'],[None],[None],129,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_filters_returns_multiple_matches,SimpleVectorStoreTest:test_query_with_filters_returns_multiple_matches,method,10,20,20,376,18.8,0,0,['self'],[None],[None],62,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_filters_with_filter_condition,SimpleVectorStoreTest:test_query_with_filters_with_filter_condition,method,13,45,25,723,16.07,0,0,['self'],[None],[None],153,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'ExactMatchFilter', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",7
repos/llama_index/llama-index-core/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_without_filters_returns_all_rows_sorted_by_similarity,SimpleVectorStoreTest:test_query_without_filters_returns_all_rows_sorted_by_similarity,method,10,24,24,404,16.83,0,0,['self'],[None],[None],45,[],"['SimpleVectorStore', 'simple_vector_store.add', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertCountEqual', 'self.assertEqual']",6
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:_contains_protected_access,_contains_protected_access,function,11,31,26,384,12.39,1,1,['code'],[' str'],[None],112,[],"['ast.parse', 'ast.iter_child_nodes', 'isinstance', 'DunderVisitor', 'dunder_visitor.visit']",5
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:_get_restricted_globals,_get_restricted_globals,function,5,7,6,125,17.86,0,1,"['__globals', 'None]']","[' Union[dict', None]","[None, None]",82,[],"['copy.deepcopy', 'restricted_globals.update']",2
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:_restricted_import,_restricted_import,function,5,18,18,133,7.39,0,1,"['name', 'globals', 'object]', 'None] ', 'locals', 'object]', 'None] ', ')', 'level', '']","[' str', ' Union[Mapping[str', None, None, ' Union[Mapping[str', None, None, None, ' int ', None]","[None, None, None, ' None', None, None, ' None', None, ' 0', None]",19,[],"['__import__', 'ImportError']",2
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:_verify_source_safety,_verify_source_safety,function,5,38,31,319,8.39,0,3,"['__source', 'bytes', 'CodeType]']","[' Union[str', None, None]","[None, None, None]",134,"['    """"""\n', '    Verify that the source is safe to execute. For now, this means that it\n', '    does not contain any references to private or dunder methods.\n', '    """"""\n']","['isinstance', 'RuntimeError', '__source.decode', '_contains_protected_access']",4
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:safe_eval,safe_eval,function,3,5,5,96,19.2,0,0,"['__source', 'bytes', 'CodeType]', '__globals', 'Any]', 'None] ', '__locals', 'object]', 'None] ', '']","[' Union[str', None, None, ' Union[Dict[str', None, None, ' Union[Mapping[str', None, None, None]","[None, None, None, None, None, ' None', None, None, ' None', None]",150,"['    """"""\n', '    eval within safe global context.\n', '    """"""\n']","['_verify_source_safety', 'eval', '_get_restricted_globals']",3
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:safe_exec,safe_exec,function,3,5,5,96,19.2,0,0,"['__source', 'bytes', 'CodeType]', '__globals', 'Any]', 'None] ', '__locals', 'object]', 'None] ', '']","[' Union[str', None, None, ' Union[Dict[str', None, None, ' Union[Mapping[str', None, None, None]","[None, None, None, None, None, ' None', None, None, ' None', None]",162,"['    """"""\n', '    eval within safe global context.\n', '    """"""\n']","['_verify_source_safety', 'exec', '_get_restricted_globals']",3
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:DunderVisitor,DunderVisitor,class,15,56,29,656,11.71,0,4,[],[],[],89,[],[],0
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:DunderVisitor:__init__,DunderVisitor:__init__,method,5,8,6,149,18.62,0,0,['self'],[None],[None],90,[],['globals'],1
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:DunderVisitor:visit_Attribute,DunderVisitor:visit_Attribute,method,8,16,12,197,12.31,0,2,"['self', 'node']","[None, ' ast.Attribute']","[None, None]",104,[],['self.generic_visit'],1
repos/llama_index/llama-index-experimental/llama_index/experimental/exec_utils.py:DunderVisitor:visit_Name,DunderVisitor:visit_Name,method,8,16,12,191,11.94,0,2,"['self', 'node']","[None, ' ast.Name']","[None, None]",97,[],['self.generic_visit'],1
repos/llama_index/llama-index-experimental/tests/param_tuner/test_base.py:_mock_obj_function,_mock_obj_function,function,3,7,7,107,15.29,0,0,['param_dict'],[' Dict'],[None],8,"['    """"""Mock obj function.""""""\n']","['RunResult', 'score=int', 'int']",3
repos/llama_index/llama-index-experimental/tests/param_tuner/test_base.py:test_param_tuner,test_param_tuner,function,9,29,27,315,10.86,0,0,[],[],[],28,"['    """"""Test param tuner.""""""\n']","['ParamTuner', 'tuner.tune']",2
repos/llama_index/llama-index-experimental/tests/param_tuner/test_param_tuner_classes.py:test_class,test_class,function,6,30,11,347,11.57,3,0,[],[],[],9,[],[],0
repos/llama_index/llama-index-experimental/tests/test_exec_utils.py:test_contains_protected_access,test_contains_protected_access,function,1,92,32,802,8.72,0,0,[],[],[],4,[],"['_contains_protected_access', '_a', '_b', 'b']",4
repos/llama_index/llama-index-experimental/tests/test_pandas.py:_mock_predict,_mock_predict,function,4,4,4,56,14.0,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",22,"['    """"""Mock predict.""""""\n']",[],0
repos/llama_index/llama-index-experimental/tests/test_pandas.py:test_pandas_query_engine,test_pandas_query_engine,function,3,39,30,450,11.54,0,0,['monkeypatch'],[' pytest.MonkeyPatch'],[None],28,"['    """"""Test pandas query engine.""""""\n']","['monkeypatch.setattr', 'MockLLM', 'pd.DataFrame']",3
repos/llama_index/llama-index-finetuning/llama_index/finetuning/types.py:BaseCohereRerankerFinetuningEngine,BaseCohereRerankerFinetuningEngine,class,2,13,10,112,8.62,0,0,[],[],[],50,[],[],0
repos/llama_index/llama-index-finetuning/llama_index/finetuning/types.py:BaseCrossEncoderFinetuningEngine,BaseCrossEncoderFinetuningEngine,class,2,17,14,142,8.35,0,0,[],[],[],36,[],[],0
repos/llama_index/llama-index-finetuning/llama_index/finetuning/types.py:BaseEmbeddingFinetuneEngine,BaseEmbeddingFinetuneEngine,class,2,12,9,120,10.0,0,0,[],[],[],24,[],[],0
repos/llama_index/llama-index-finetuning/llama_index/finetuning/types.py:BaseLLMFinetuneEngine,BaseLLMFinetuneEngine,class,2,12,9,110,9.17,0,0,[],[],[],12,[],[],0
repos/llama_index/llama-index-finetuning/tests/callbacks/test_callback_classes.py:test_class,test_class,function,4,10,8,129,12.9,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-finetuning/tests/cross_encoders/test_cross_encoder_classes.py:test_class,test_class,function,4,10,8,145,14.5,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-finetuning/tests/embeddings/test_embeddings_classes.py:test_class,test_class,function,4,22,13,295,13.41,1,0,[],[],[],8,[],[],0
repos/llama_index/llama-index-finetuning/tests/gradient/test_gradient_classes.py:test_class,test_class,function,4,10,8,130,13.0,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-finetuning/tests/openai/test_openai_classes.py:test_class,test_class,function,4,10,8,128,12.8,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-finetuning/tests/rerankers/test_rerankers_classes.py:test_classes,test_classes,function,4,10,8,149,14.9,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-finetuning/tests/test_base.py:test_torch_imports,test_torch_imports,function,12,35,19,402,11.49,0,1,[],[],[],7,"['    """"""Test that torch is an optional dependency.""""""\n']","['pkgutil.find_loader', 'pytest.raises']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/async_utils.py:asyncio_module,asyncio_module,function,8,13,10,105,8.08,0,1,['show_progress'],[' bool '],[' False'],7,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/async_utils.py:chunks,chunks,function,3,6,6,66,11.0,0,0,"['iterable', 'size']","[' Iterable', ' int']","[None, None]",53,[],['zip_longest'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/async_utils.py:get_asyncio_module,get_asyncio_module,function,8,13,10,105,8.08,0,1,['show_progress'],[' bool '],[' False'],70,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/async_utils.py:run_async_tasks,run_async_tasks,function,23,45,34,485,10.78,0,1,"['tasks', 'show_progress', 'progress_bar_desc', '']","[' List[Coroutine]', ' bool ', ' str ', None]","[None, ' False', ' ""Running async tasks""', None]",18,"['    """"""Run a list of async tasks.""""""\n']","['nest_asyncio.apply', 'asyncio.get_event_loop', '_tqdm_gather', 'tqdm.gather', 'loop.run_until_complete', '_gather', 'asyncio.gather', 'asyncio.run']",8
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:_contains_protected_access,_contains_protected_access,function,7,7,7,128,18.29,0,0,['code'],[' str'],[None],109,[],"['ast.parse', 'DunderVisitor', 'dunder_visitor.visit']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:_get_restricted_globals,_get_restricted_globals,function,5,7,6,125,17.86,0,1,"['__globals', 'None]']","[' Union[dict', None]","[None, None]",87,[],"['copy.deepcopy', 'restricted_globals.update']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:_restricted_import,_restricted_import,function,5,18,18,133,7.39,0,1,"['name', 'globals', 'object]', 'None] ', 'locals', 'object]', 'None] ', ')', 'level', '']","[' str', ' Union[Mapping[str', None, None, ' Union[Mapping[str', None, None, None, ' int ', None]","[None, None, None, ' None', None, None, ' None', None, ' 0', None]",19,[],"['__import__', 'ImportError']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:_verify_source_safety,_verify_source_safety,function,5,32,26,283,8.84,0,3,"['__source', 'bytes', 'CodeType]']","[' Union[str', None, None]","[None, None, None]",116,"['    """"""\n', '    Verify that the source is safe to execute. For now, this means that it\n', '    does not contain any references to private or dunder methods.\n', '    """"""\n']","['isinstance', 'RuntimeError', '__source.decode', '_contains_protected_access']",4
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:safe_eval,safe_eval,function,3,5,5,96,19.2,0,0,"['__source', 'bytes', 'CodeType]', '__globals', 'Any]', 'None] ', '__locals', 'object]', 'None] ', '']","[' Union[str', None, None, ' Union[Dict[str', None, None, ' Union[Mapping[str', None, None, None]","[None, None, None, None, None, ' None', None, None, ' None', None]",131,"['    """"""\n', '    eval within safe global context.\n', '    """"""\n']","['_verify_source_safety', 'eval', '_get_restricted_globals']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:safe_exec,safe_exec,function,3,5,5,96,19.2,0,0,"['__source', 'bytes', 'CodeType]', '__globals', 'Any]', 'None] ', '__locals', 'object]', 'None] ', '']","[' Union[str', None, None, ' Union[Dict[str', None, None, ' Union[Mapping[str', None, None, None]","[None, None, None, None, None, ' None', None, None, ' None', None]",143,"['    """"""\n', '    eval within safe global context.\n', '    """"""\n']","['_verify_source_safety', 'exec', '_get_restricted_globals']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:DunderVisitor,DunderVisitor,class,7,28,17,340,12.14,0,2,[],[],[],94,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:DunderVisitor:__init__,DunderVisitor:__init__,method,1,2,2,39,19.5,0,0,['self'],[None],[None],95,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:DunderVisitor:visit_Attribute,DunderVisitor:visit_Attribute,method,3,5,5,92,18.4,0,1,"['self', 'node']","[None, ' ast.Attribute']","[None, None]",103,[],['self.generic_visit'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/exec_utils.py:DunderVisitor:visit_Name,DunderVisitor:visit_Name,method,3,5,5,90,18.0,0,1,"['self', 'node']","[None, ' ast.Name']","[None, None]",98,[],['self.generic_visit'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/img_utils.py:b64_2_img,b64_2_img,function,4,4,4,59,14.75,0,0,['data'],[' str'],[None],16,"['    """"""Convert base64 encoded image str to a PIL.Image.""""""\n']","['BytesIO', 'Image.open']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/img_utils.py:img_2_b64,img_2_b64,function,5,7,7,95,13.57,0,0,"['image', 'format']","[' Image', ' str ']","[None, ' ""JPEG""']",9,"['    """"""Convert a PIL.Image to a base64 encoded image str.""""""\n']","['BytesIO', 'image.save', 'cast', 'base64.b64encode']",4
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent,BaseComponent,class,39,159,90,1480,9.31,2,3,[],[],[],35,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode,BaseNode,class,61,347,159,3409,9.82,0,12,[],[],[],184,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document,Document,class,56,239,151,2572,10.76,0,1,[],[],[],591,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:ImageDocument,ImageDocument,class,2,7,7,59,8.43,0,0,[],[],[],722,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:ImageNode,ImageNode,class,23,80,53,681,8.51,0,1,[],[],[],454,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:IndexNode,IndexNode,class,12,35,25,284,8.11,0,0,[],[],[],494,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:MetadataMode,MetadataMode,class,4,8,8,45,5.62,0,0,[],[],[],162,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeRelationship,NodeRelationship,class,6,10,7,68,6.8,0,0,[],[],[],136,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore,NodeWithScore,class,29,134,72,1112,8.3,0,5,[],[],[],529,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:ObjectType,ObjectType,class,5,8,6,53,6.62,0,0,[],[],[],155,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:QueryBundle,QueryBundle,class,15,49,32,461,9.41,0,3,[],[],[],731,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:RelatedNodeInfo,RelatedNodeInfo,class,12,19,18,184,9.68,0,0,[],[],[],169,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode,TextNode,class,40,211,122,2063,9.78,3,6,[],[],[],359,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TransformComponent,TransformComponent,class,7,25,18,241,9.64,0,0,[],[],[],121,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent:__getstate__,BaseComponent:__getstate__,method,13,29,22,335,11.55,2,2,['self'],[None],[None],66,[],"['super', 'key.endswith', 'keys_to_remove.append', 'str']",4
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent:__setstate__,BaseComponent:__setstate__,method,2,8,8,96,12.0,0,0,"['self', 'state', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",88,[],"['self.__init__', 'super']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent:class_name,BaseComponent:class_name,method,1,2,2,22,11.0,0,0,['cls'],[None],[None],49,"['        """"""\n', '        Get the class name, used as a unique ID in serialization.\n', '\n', '        This provides a key that makes serialization robust against actual class\n', '        name changes.\n', '        """"""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent:dict,BaseComponent:dict,method,5,6,5,75,12.5,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",61,[],"['super', 'self.class_name']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent:from_dict,BaseComponent:from_dict,method,1,1,1,19,19.0,0,0,"['cls', 'data', 'Any]', '**kwargs', 'dict)']","[None, ' Dict[str', None, ' Any) -> Self:  # type: ignorekwargs', '']","[None, None, None, None, None]",108,[],['data.update'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent:from_json,BaseComponent:from_json,method,5,16,16,123,7.69,0,0,"['cls', 'data_str', '**kwargs', '**kwargs)']","[None, ' str', ' Any) -> Self:  # type: ignoredata_str)data', None]","[None, None, None, None]",116,[],"['from_json', 'json.loads', 'cls.from_dict']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent:json,BaseComponent:json,method,2,2,2,28,14.0,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",58,[],['self.to_json'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent:to_dict,BaseComponent:to_dict,method,5,6,5,72,12.0,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",97,[],"['self.dict', 'self.class_name']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseComponent:to_json,BaseComponent:to_json,method,4,4,4,50,12.5,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",102,[],"['self.to_dict', 'json.dumps']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:__str__,BaseNode:__str__,method,5,15,14,223,14.87,0,0,['self'],[None],[None],330,[],"['truncate_text', 'self.get_content', 'textwrap.fill']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:as_related_node_info,BaseNode:as_related_node_info,method,4,7,7,113,16.14,0,0,['self'],[None],[None],349,"['        """"""Get node as RelatedNodeInfo.""""""\n']",['RelatedNodeInfo'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:child_nodes,BaseNode:child_nodes,method,7,25,21,226,9.04,0,2,['self'],[None],[None],307,"['        """"""Child nodes.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:extra_info,BaseNode:extra_info,method,2,2,2,19,9.5,0,0,['self'],[None],[None],326,"['        """"""TODO: DEPRECATED: Extra info.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:get_content,BaseNode:get_content,method,42,249,104,2483,9.97,0,12,"['self', 'metadata_mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.ALL']",234,"['        """"""Get object content.""""""\n']","['get_metadata_str', 'set_content', 'hash', 'node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",20
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:get_embedding,BaseNode:get_embedding,method,3,10,9,80,8.0,0,1,['self'],[None],[None],339,"['        """"""Get embedding.\n', '\n', '        Errors if embedding is None.\n', '\n', '        """"""\n']",['ValueError'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:get_metadata_str,BaseNode:get_metadata_str,method,41,241,100,2400,9.96,0,12,"['self', 'mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.ALL']",238,"['        """"""Metadata string.""""""\n']","['set_content', 'hash', 'node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",19
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:get_type,BaseNode:get_type,method,43,257,106,2570,10.0,0,12,['cls'],[None],[None],230,"['        """"""Get Object type.""""""\n']","['get_content', 'get_metadata_str', 'set_content', 'hash', 'node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",21
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:hash,BaseNode:hash,method,39,228,96,2300,10.09,0,12,['self'],[None],[None],247,"['        """"""Get hash of node.""""""\n']","['node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",17
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:next_node,BaseNode:next_node,method,7,24,20,231,9.62,0,2,['self'],[None],[None],285,"['        """"""Next node.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:node_id,BaseNode:node_id,method,2,2,2,14,7.0,0,0,['self'],[None],[None],251,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:node_id,BaseNode:node_id,method,2,2,2,14,7.0,0,0,['self'],[None],[None],255,"['        """"""Source object node.\n', '\n', '        Extracted from the relationships field.\n', '\n', '        """"""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:parent_node,BaseNode:parent_node,method,7,24,20,237,9.88,0,2,['self'],[None],[None],296,"['        """"""Parent node.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:prev_node,BaseNode:prev_node,method,7,24,20,243,10.12,0,2,['self'],[None],[None],274,"['        """"""Prev node.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:ref_doc_id,BaseNode:ref_doc_id,method,4,10,8,86,8.6,0,1,['self'],[None],[None],318,"['        """"""Deprecated: Get ref doc id.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:set_content,BaseNode:set_content,method,40,234,98,2346,10.03,0,12,"['self', 'value']","[None, ' Any']","[None, None]",242,"['        """"""Set the content of the node.""""""\n']","['hash', 'node_id', 'source_node', 'isinstance', 'ValueError', 'prev_node', 'next_node', 'parent_node', 'child_nodes', 'ref_doc_id', 'extra_info', '__str__', 'truncate_text', 'self.get_content', 'textwrap.fill', 'get_embedding', 'as_related_node_info', 'RelatedNodeInfo']",18
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:BaseNode:source_node,BaseNode:source_node,method,7,23,20,223,9.7,0,2,['self'],[None],[None],259,"['        """"""Source object node.\n', '\n', '        Extracted from the relationships field.\n', '\n', '        """"""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:__setattr__,Document:__setattr__,method,4,8,7,91,11.38,0,1,"['self', 'name', 'value']","[None, ' str', ' object']","[None, None, None]",630,[],['super'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:__str__,Document:__str__,method,5,15,14,221,14.73,0,0,['self'],[None],[None],330,[],"['truncate_text', 'self.get_content', 'textwrap.fill']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:class_name,Document:class_name,method,1,2,2,16,8.0,0,0,['cls'],[None],[None],387,"['        """"""Get Object type.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:doc_id,Document:doc_id,method,2,2,2,14,7.0,0,0,['self'],[None],[None],613,"['        """"""Get document ID.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:example,Document:example,method,2,8,8,92,11.5,0,0,['cls'],[None],[None],711,[],['Document'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:from_embedchain_format,Document:from_embedchain_format,method,2,6,6,95,15.83,0,0,"['cls', 'doc', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",670,"['        """"""Convert struct from EmbedChain document format.""""""\n']",['cls'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:from_haystack_format,Document:from_haystack_format,method,2,7,7,82,11.71,0,0,"['cls', 'doc']","[None, ' ""HaystackDocument""']","[None, None]",656,"['        """"""Convert struct from Haystack document format.""""""\n']",['cls'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:from_langchain_format,Document:from_langchain_format,method,2,3,3,54,18.0,0,0,"['cls', 'doc']","[None, ' ""LCDocument""']","[None, None]",643,"['        """"""Convert struct from LangChain document format.""""""\n']",['cls'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:from_semantic_kernel_format,Document:from_semantic_kernel_format,method,3,15,15,168,11.2,0,0,"['cls', 'doc']","[None, ' ""MemoryRecord""']","[None, None]",691,"['        """"""Convert struct from Semantic Kernel document format.""""""\n']",['cls'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:get_doc_id,Document:get_doc_id,method,2,2,2,14,7.0,0,0,['self'],[None],[None],626,"['        """"""TODO: Deprecated: Get document ID.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:get_type,Document:get_type,method,2,2,2,25,12.5,0,0,['cls'],[None],[None],230,"['        """"""Get Object type.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:to_embedchain_format,Document:to_embedchain_format,method,1,10,10,84,8.4,0,0,['self'],[None],[None],662,"['        """"""Convert struct to EmbedChain document format.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:to_haystack_format,Document:to_haystack_format,method,6,13,13,151,11.62,0,0,['self'],[None],[None],647,"['        """"""Convert struct to Haystack document format.""""""\n']",['HaystackDocument'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:to_langchain_format,Document:to_langchain_format,method,9,13,13,151,11.62,0,0,['self'],[None],[None],635,"['        """"""Convert struct to LangChain document format.""""""\n']",['LCDocument'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:to_semantic_kernel_format,Document:to_semantic_kernel_format,method,8,19,18,230,12.11,0,0,['self'],[None],[None],678,"['        """"""Convert struct to Semantic Kernel document format.""""""\n']",['MemoryRecord'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:Document:to_vectorflow,Document:to_vectorflow,method,7,9,9,118,13.11,0,0,"['self', 'client']","[None, ' Any']","[None, None]",700,"['        """"""Send a document to vectorflow, since they don\'t have a document object.""""""\n']","['tempfile.NamedTemporaryFile', 'f.write', 'f.flush', 'client.embed']",4
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:ImageDocument:class_name,ImageDocument:class_name,method,1,2,2,21,10.5,0,0,['cls'],[None],[None],49,"['        """"""\n', '        Get the class name, used as a unique ID in serialization.\n', '\n', '        This provides a key that makes serialization robust against actual class\n', '        name changes.\n', '        """"""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:ImageNode:class_name,ImageNode:class_name,method,1,2,2,17,8.5,0,0,['cls'],[None],[None],473,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:ImageNode:get_type,ImageNode:get_type,method,2,2,2,22,11.0,0,0,['cls'],[None],[None],469,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:ImageNode:resolve_image,ImageNode:resolve_image,method,11,34,24,290,8.53,0,1,['self'],[None],[None],476,"['        """"""Resolve an image such that PIL can read it.""""""\n']","['BytesIO', 'requests.get', 'ValueError']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:IndexNode:class_name,IndexNode:class_name,method,1,2,2,17,8.5,0,0,['cls'],[None],[None],525,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:IndexNode:from_text_node,IndexNode:from_text_node,method,3,5,5,46,9.2,0,0,"['cls', 'node', 'index_id', '']","[None, ' TextNode', ' str', None]","[None, None, None, None]",508,"['        """"""Create index node from text node.""""""\n']",['cls'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:IndexNode:get_type,IndexNode:get_type,method,2,2,2,22,11.0,0,0,['cls'],[None],[None],521,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:__str__,NodeWithScore:__str__,method,3,12,12,100,8.33,0,1,['self'],[None],[None],533,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:class_name,NodeWithScore:class_name,method,1,2,2,21,10.5,0,0,['cls'],[None],[None],548,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:embedding,NodeWithScore:embedding,method,2,2,2,25,12.5,0,0,['self'],[None],[None],572,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:get_content,NodeWithScore:get_content,method,2,2,2,56,28.0,0,0,"['self', 'metadata_mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.NONE']",581,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:get_embedding,NodeWithScore:get_embedding,method,2,2,2,31,15.5,0,0,['self'],[None],[None],584,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:get_score,NodeWithScore:get_score,method,4,16,12,105,6.56,0,2,"['self', 'raise_error']","[None, ' bool ']","[None, ' False']",537,"['        """"""Get score.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:get_text,NodeWithScore:get_text,method,4,15,15,115,7.67,0,1,['self'],[None],[None],575,[],"['isinstance', 'ValueError']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:id_,NodeWithScore:id_,method,2,2,2,19,9.5,0,0,['self'],[None],[None],557,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:metadata,NodeWithScore:metadata,method,2,2,2,24,12.0,0,0,['self'],[None],[None],568,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:node_id,NodeWithScore:node_id,method,2,2,2,23,11.5,0,0,['self'],[None],[None],553,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:NodeWithScore:text,NodeWithScore:text,method,4,15,15,109,7.27,0,1,['self'],[None],[None],561,[],"['isinstance', 'ValueError']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:QueryBundle:__str__,QueryBundle:__str__,method,2,2,2,20,10.0,0,0,['self'],[None],[None],768,"['        """"""Convert to string representation.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:QueryBundle:embedding_image,QueryBundle:embedding_image,method,2,8,7,57,7.12,0,1,['self'],[None],[None],762,"['        """"""Use image path for image retrieval.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:QueryBundle:embedding_strs,QueryBundle:embedding_strs,method,2,14,10,132,9.43,0,2,['self'],[None],[None],752,"['        """"""Use custom embedding strs if specified, otherwise use query str.""""""\n']",['len'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:RelatedNodeInfo:class_name,RelatedNodeInfo:class_name,method,1,2,2,23,11.5,0,0,['cls'],[None],[None],176,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode:class_name,TextNode:class_name,method,1,2,2,16,8.0,0,0,['cls'],[None],[None],387,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode:get_content,TextNode:get_content,method,5,12,11,184,15.33,0,1,"['self', 'metadata_mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.NONE']",400,"['        """"""Get object content.""""""\n']",['self.get_metadata_str'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode:get_metadata_str,TextNode:get_metadata_str,method,14,47,27,508,10.81,3,5,"['self', 'mode']","[None, ' MetadataMode ']","[None, ' MetadataMode.ALL']",410,"['        """"""Metadata info string.""""""\n']","['set', 'usable_metadata_keys.remove', 'value=str']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode:get_node_info,TextNode:get_node_info,method,1,5,5,59,11.8,0,0,['self'],[None],[None],437,"['        """"""Get node info.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode:get_text,TextNode:get_text,method,2,2,2,55,27.5,0,0,['self'],[None],[None],441,[],['self.get_content'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode:get_type,TextNode:get_type,method,2,2,2,21,10.5,0,0,['cls'],[None],[None],396,"['        """"""Get Object type.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode:hash,TextNode:hash,method,3,6,6,122,20.33,0,0,['self'],[None],[None],391,[],['str'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode:node_info,TextNode:node_info,method,2,2,2,26,13.0,0,0,['self'],[None],[None],445,"['        """"""Deprecated: Get node info.""""""\n']",['self.get_node_info'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TextNode:set_content,TextNode:set_content,method,2,2,2,15,7.5,0,0,"['self', 'value']","[None, ' str']","[None, None]",433,"['        """"""Set the content of the node.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/schema.py:TransformComponent:__call__,TransformComponent:__call__,method,4,12,12,110,9.17,0,0,"['self', 'nodes', '**kwargs']","[None, ' List[""BaseNode""]', ' Any']","[None, None, None]",128,"['        """"""Transform nodes.""""""\n']","['acall', 'self.__call__']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:_get_default_node_parser,_get_default_node_parser,function,2,8,8,131,16.38,0,0,"['chunk_size', 'chunk_overlap', 'callback_manager', '']","[' int ', ' int ', ' Optional[CallbackManager] ', None]","[' DEFAULT_CHUNK_SIZE', ' SENTENCE_CHUNK_OVERLAP', ' None', None]",28,"['    """"""Get default node parser.""""""\n']","['SentenceSplitter', 'CallbackManager']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:_get_default_prompt_helper,_get_default_prompt_helper,function,6,16,12,191,11.94,0,2,"['llm_metadata', 'context_window', 'num_output', '']","[' LLMMetadata', ' Optional[int] ', ' Optional[int] ', None]","[None, ' None', ' None', None]",41,"['    """"""Get default prompt helper.""""""\n']",['PromptHelper.from_llm_metadata'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:set_global_service_context,set_global_service_context,function,2,2,2,57,28.5,0,0,['service_context'],[' Optional[ServiceContext]'],[None],388,"['    """"""Helper function to set the global service context.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:ServiceContext,ServiceContext,class,86,595,232,7919,13.31,3,21,[],[],[],63,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:ServiceContextData,ServiceContextData,class,6,10,7,90,9.0,0,0,[],[],[],54,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:ServiceContext:from_defaults,ServiceContext:from_defaults,method,41,190,105,2523,13.28,0,9,"['cls', 'llm_predictor', 'llm', 'prompt_helper', 'embed_model', 'node_parser', 'text_splitter', 'transformations', 'llama_logger', 'callback_manager', 'system_prompt', 'query_wrapper_prompt', 'pydantic_program_mode', 'chunk_size', 'chunk_overlap', 'context_window', 'num_output', 'chunk_size_limit', '']","[None, ' Optional[BaseLLMPredictor] ', ' Optional[LLMType] ', ' Optional[PromptHelper] ', ' Optional[Any] ', ' Optional[NodeParser] ', ' Optional[TextSplitter] ', ' Optional[List[TransformComponent]] ', ' Optional[LlamaLogger] ', ' Optional[CallbackManager] ', ' Optional[str] ', ' Optional[BasePromptTemplate] ', ' PydanticProgramMode ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', None]","[None, ' None', ' ""default""', ' None', ' ""default""', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' PydanticProgramMode.DEFAULT', ' None', ' None', ' None', ' None', ' None', None]",85,"['        """"""Create a ServiceContext from defaults.\n', '        If an argument is specified, then use the argument value provided for that\n', '        parameter. If an argument is not specified, then use the default value.\n', '\n', '        You can change the base defaults by setting llama_index.legacy.global_service_context\n', '        to a ServiceContext object with your desired settings.\n', '\n', '        Args:\n', '            llm_predictor (Optional[BaseLLMPredictor]): LLMPredictor\n', '            prompt_helper (Optional[PromptHelper]): PromptHelper\n', '            embed_model (Optional[BaseEmbedding]): BaseEmbedding\n', '                or ""local"" (use local model)\n', '            node_parser (Optional[NodeParser]): NodeParser\n', '            llama_logger (Optional[LlamaLogger]): LlamaLogger (deprecated)\n', '            chunk_size (Optional[int]): chunk_size\n', '            callback_manager (Optional[CallbackManager]): CallbackManager\n', '            system_prompt (Optional[str]): System-wide prompt to be prepended\n', '                to all input prompts, used to guide system ""decision making""\n', '            query_wrapper_prompt (Optional[BasePromptTemplate]): A format to wrap\n', '                passed-in input queries.\n', '\n', '        Deprecated Args:\n', '            chunk_size_limit (Optional[int]): renamed to chunk_size\n', '\n', '        """"""\n']","['cast', 'logger.warning', 'cls.from_service_context', 'CallbackManager', 'ValueError', 'resolve_llm', 'print', 'LLMPredictor', 'isinstance', 'resolve_embed_model', '_get_default_prompt_helper', '_get_default_node_parser', 'LlamaLogger', 'cls']",14
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:ServiceContext:from_dict,ServiceContext:from_dict,method,24,43,37,866,20.14,1,0,"['cls', 'data']","[None, ' dict']","[None, None]",359,[],"['ServiceContextData.parse_obj', 'load_predictor', 'load_embed_model', 'PromptHelper.from_dict', 'transformations.append', 'cls.from_defaults']",6
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:ServiceContext:from_service_context,ServiceContext:from_service_context,method,44,176,102,2089,11.87,1,11,"['cls', 'service_context', 'llm_predictor', 'llm', 'prompt_helper', 'embed_model', 'node_parser', 'text_splitter', 'transformations', 'llama_logger', 'callback_manager', 'system_prompt', 'query_wrapper_prompt', 'chunk_size', 'chunk_overlap', 'context_window', 'num_output', 'chunk_size_limit', '']","[None, ' ""ServiceContext""', ' Optional[BaseLLMPredictor] ', ' Optional[LLMType] ', ' Optional[PromptHelper] ', ' Optional[Any] ', ' Optional[NodeParser] ', ' Optional[TextSplitter] ', ' Optional[List[TransformComponent]] ', ' Optional[LlamaLogger] ', ' Optional[CallbackManager] ', ' Optional[str] ', ' Optional[BasePromptTemplate] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', ' Optional[int] ', None]","[None, None, ' None', ' ""default""', ' None', ' ""default""', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', None]",227,"['        """"""Instantiate a new service context using a previous as the defaults.""""""\n']","['cast', 'logger.warning', 'ValueError', 'resolve_llm', 'LLMPredictor', 'isinstance', 'resolve_embed_model', '_get_default_prompt_helper', '_get_default_node_parser', 'cls']",10
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:ServiceContext:llm,ServiceContext:llm,method,2,2,2,28,14.0,0,0,['self'],[None],[None],328,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:ServiceContext:node_parser,ServiceContext:node_parser,method,5,14,14,125,8.93,1,1,['self'],[None],[None],332,"['        """"""Get the node parser.""""""\n']","['isinstance', 'ValueError']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/service_context.py:ServiceContext:to_dict,ServiceContext:to_dict,method,11,22,22,423,19.23,0,0,['self'],[None],[None],339,"['        """"""Convert service context to dict.""""""\n']",['ServiceContextData'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/types.py:BaseOutputParser,BaseOutputParser,class,11,37,28,352,9.51,0,2,[],[],[],29,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/types.py:BasePydanticProgram,BasePydanticProgram,class,5,29,18,206,7.1,0,0,[],[],[],53,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/types.py:PydanticProgramMode,PydanticProgramMode,class,5,10,10,103,10.3,0,0,[],[],[],72,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/types.py:BaseOutputParser:format,BaseOutputParser:format,method,2,2,2,11,5.5,0,0,"['self', 'query']","[None, ' str']","[None, None]",36,"['        """"""Format a query with structured output formatting instructions.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/types.py:BaseOutputParser:format_messages,BaseOutputParser:format_messages,method,7,16,13,188,11.75,0,2,"['self', 'messages']","[None, ' List[ChatMessage]']","[None, None]",40,"['        """"""Format a list of messages with structured output formatting instructions.""""""\n']",['self.format'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/types.py:BaseOutputParser:parse,BaseOutputParser:parse,method,10,30,24,304,10.13,0,2,"['self', 'output']","[None, ' str']","[None, None]",33,"['        """"""Parse, validate, and correct errors programmatically.""""""\n']","['format', 'format_messages', 'self.format']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/types.py:BasePydanticProgram:__call__,BasePydanticProgram:__call__,method,0,1,1,4,4.0,0,0,"['self', '*args', '**kwds']","[None, ' Any', ' Any']","[None, None, None]",65,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/types.py:BasePydanticProgram:output_cls,BasePydanticProgram:output_cls,method,0,1,1,4,4.0,0,0,['self'],[None],[None],61,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:_get_colored_text,_get_colored_text,function,3,19,17,193,10.16,0,1,"['text', 'color']","[' str', ' str']","[None, None]",424,"['    """"""\n', '    Get the colored version of the input text.\n', '\n', '    Args:\n', '        text (str): Input text.\n', '        color (str): Color to be applied to the text.\n', '\n', '    Returns:\n', '        str: Colored version of the input text.\n', '    """"""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:add_sync_version,add_sync_version,function,6,17,16,192,11.29,0,0,['func'],[' Any'],[None],338,"['    """"""Decorator for adding sync version of an async function. The sync version\n', '    is added as a function attribute to the original function, func.\n', '\n', '    Args:\n', '        func(Any): the async function for which a sync variant will be built.\n', '    """"""\n']","['asyncio.iscoroutinefunction', '_wrapper', 'asyncio.get_event_loop']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:concat_dirs,concat_dirs,function,3,11,10,73,6.64,0,0,"['dirname', 'basename']","[' str', ' str']","[None, None]",260,"['    """"""\n', '    Append basename to dirname, avoiding backslashes when running on windows.\n', '\n', '    os.path.join(dirname, basename) will add a backslash before dirname if\n', '    basename does not end with a slash, so we make sure it does.\n', '    """"""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:count_tokens,count_tokens,function,4,6,6,66,11.0,0,0,['text'],[' str'],[None],286,[],"['get_tokenizer', 'tokenizer', 'len']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:get_cache_dir,get_cache_dir,function,10,43,36,514,11.95,0,2,[],[],[],308,"['    """"""Locate a platform-appropriate cache directory for llama_index,\n', ""    and create it if it doesn't yet exist.\n"", '    """"""\n']","['Path', 'os.makedirs', 'str']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:get_color_mapping,get_color_mapping,function,7,19,18,188,9.89,0,1,"['items', 'use_llama_index_colors']","[' List[str]', ' bool ']","[None, ' True']",400,"['    """"""\n', '    Get a mapping of items to colors.\n', '\n', '    Args:\n', '        items (List[str]): List of items to be mapped to colors.\n', '        use_llama_index_colors (bool, optional): Flag to indicate\n', '        whether to use LlamaIndex colors or ANSI colors.\n', '            Defaults to True.\n', '\n', '    Returns:\n', '        Dict[str, str]: Mapping of items to colors.\n', '    """"""\n']","['list', 'len', 'enumerate']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:get_new_id,get_new_id,function,5,12,10,70,5.83,1,1,['d'],[' Set'],[None],140,"['    """"""Get a new ID.""""""\n']",['str'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:get_new_int_id,get_new_int_id,function,5,13,11,82,6.31,1,1,['d'],[' Set'],[None],149,"['    """"""Get a new integer ID.""""""\n']",['random.randint'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:get_tokenizer,get_tokenizer,function,17,56,46,704,12.57,0,3,[],[],[],108,[],"['ImportError', 'tiktoken.encoding_for_model', 'partial', 'set_global_tokenizer']",4
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:get_tqdm_iterable,get_tqdm_iterable,function,9,17,15,129,7.59,0,1,"['items', 'show_progress', 'desc']","[' Iterable', ' bool', ' str']","[None, None, None]",271,"['    """"""\n', '    Optionally get a tqdm iterable. Ensures tqdm.auto is used.\n', '    """"""\n']",['tqdm'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:get_transformer_tokenizer_fn,get_transformer_tokenizer_fn,function,10,23,23,221,9.61,0,0,['model_name'],[' str'],[None],292,"['    """"""\n', '    Args:\n', '        model_name(str): the model name of the tokenizer.\n', '                        For instance, fxmarty/tiny-llama-fast-tokenizer.\n', '    """"""\n']","['ValueError', 'AutoTokenizer.from_pretrained']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:infer_torch_device,infer_torch_device,function,7,19,14,189,9.95,0,2,[],[],[],463,"['    """"""Infer the input to torch.device.""""""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:iter_batch,iter_batch,function,4,13,12,103,7.92,1,1,"['iterable', 'Generator]', 'size']","[' Union[Iterable', None, ' int']","[None, None, None]",246,"['    """"""Iterate over an iterable in batches.\n', '\n', '    >>> list(iter_batch([1,2,3,4,5], 3))\n', '    [[1, 2, 3], [4, 5]]\n', '    """"""\n']","['iter', 'list', 'len']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:print_text,print_text,function,3,12,12,96,8.0,0,0,"['text', 'color', 'end']","[' str', ' Optional[str] ', ' str ']","[None, ' None', ' """"']",445,"['    """"""\n', '    Print the text with the specified color.\n', '\n', '    Args:\n', '        text (str): Text to be printed.\n', '        color (str, optional): Color to be applied to the text. Supported colors are:\n', '            llama_pink, llama_blue, llama_turquoise, llama_lavender,\n', '            red, green, yellow, blue, magenta, cyan, pink.\n', '        end (str, optional): String appended after the last character of the text.\n', '\n', '    Returns:\n', '        None\n', '    """"""\n']","['_get_colored_text', 'print']",2
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:retry_on_exceptions_with_backoff,retry_on_exceptions_with_backoff,function,20,59,49,546,9.25,2,3,"['lambda_fn', 'errors_to_retry', 'max_tries', 'min_backoff_secs', 'max_backoff_secs', '']","[' Callable', ' List[ErrorToRetry]', ' int ', ' float ', ' float ', None]","[None, None, ' 10', ' 0.5', ' 60.0', None]",192,"['    """"""Execute lambda function with retries and exponential backoff.\n', '\n', '    Args:\n', '        lambda_fn (Callable): Function to be called and output we want.\n', '        errors_to_retry (List[ErrorToRetry]): List of errors to retry.\n', '            At least one needs to be provided.\n', '        max_tries (int): Maximum number of tries, including the first. Defaults to 10.\n', '        min_backoff_secs (float): Minimum amount of backoff time between attempts.\n', '            Defaults to 0.5.\n', '        max_backoff_secs (float): Maximum amount of backoff time between attempts.\n', '            Defaults to 60.\n', '\n', '    """"""\n']","['ValueError', 'tuple', 'lambda_fn', 'traceback.print_exc', 'error_checks.get', 'check_fn', 'time.sleep', 'min']",8
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:set_global_tokenizer,set_global_tokenizer,function,6,10,9,164,16.4,0,1,"['tokenizer', 'Callable[[str]', 'list]]']","[' Union[Tokenizer', None, None]","[None, None, None]",99,[],['isinstance'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:temp_set_attrs,temp_set_attrs,function,8,27,18,149,5.52,2,0,"['obj', '**kwargs']","[' Any', ' Any']","[None, None]",159,"['    """"""Temporary setter.\n', '\n', '    Utility class for setting a temporary value for an attribute on a class.\n', '    Taken from: https://tinyurl.com/2p89xymh\n', '\n', '    """"""\n']","['getattr', 'kwargs.items', 'setattr', 'prev_values.items']",4
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:truncate_text,truncate_text,function,4,10,9,67,6.7,0,1,"['text', 'max_length']","[' str', ' int']","[None, None]",239,"['    """"""Truncate text to a maximum length.""""""\n']",['len'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:unit_generator,unit_generator,function,1,2,2,6,3.0,0,0,['x'],[' Any'],[None],478,"['    """"""A function that returns a generator of a single element.\n', '\n', '    Args:\n', '        x (Any): the element to build yield\n', '\n', '    Yields:\n', '        Any: the single element\n', '    """"""\n']",[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:ErrorToRetry,ErrorToRetry,class,5,6,6,74,12.33,0,0,[],[],[],177,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:GlobalsHelper,GlobalsHelper,class,21,81,54,1030,12.72,0,2,[],[],[],32,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:Tokenizer,Tokenizer,class,1,11,11,63,5.73,0,0,[],[],[],94,[],[],0
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:GlobalsHelper:__init__,GlobalsHelper:__init__,method,9,30,25,520,17.33,0,1,['self'],[None],[None],43,"['        """"""Initialize NLTK stopwords and punkt.""""""\n']",['nltk.download'],1
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:GlobalsHelper:stopwords,GlobalsHelper:stopwords,method,12,36,31,373,10.36,0,1,['self'],[None],[None],70,"['        """"""Get stopwords.""""""\n']","['ImportError', 'nltk.download', 'stopwords.words']",3
repos/llama_index/llama-index-legacy/llama_index/legacy/utils.py:Tokenizer:encode,Tokenizer:encode,method,0,1,1,3,3.0,0,0,"['self', 'text', '*args', '**kwargs']","[None, ' str', ' Any', ' Any']","[None, None, None, None]",95,[],[],0
repos/llama_index/llama-index-legacy/tests/callbacks/test_llama_debug.py:test_flush_events,test_flush_events,function,6,22,15,472,21.45,0,0,[],[],[],58,"['    """"""Test flush events.""""""\n']","['LlamaDebugHandler', 'handler.on_event_start', 'handler.on_event_end', 'len', 'handler.flush_event_logs']",5
repos/llama_index/llama-index-legacy/tests/callbacks/test_llama_debug.py:test_get_event_stats,test_get_event_stats,function,9,19,17,355,18.68,0,0,[],[],[],43,"['    """"""Test get event stats.""""""\n']","['LlamaDebugHandler', 'handler.on_event_start', 'handler.on_event_end', 'len', 'handler.get_event_time_info']",5
repos/llama_index/llama-index-legacy/tests/callbacks/test_llama_debug.py:test_ignore_events,test_ignore_events,function,7,25,19,590,23.6,0,0,[],[],[],76,"['    """"""Test ignore event starts and ends.""""""\n']","['LlamaDebugHandler', 'CallbackManager', 'manager.on_event_start', 'manager.on_event_end', 'len']",5
repos/llama_index/llama-index-legacy/tests/callbacks/test_llama_debug.py:test_on_event_end,test_on_event_end,function,10,22,17,350,15.91,0,0,[],[],[],28,"['    """"""Test event end.""""""\n']","['LlamaDebugHandler', 'handler.on_event_end', 'len', 'isinstance']",4
repos/llama_index/llama-index-legacy/tests/callbacks/test_llama_debug.py:test_on_event_start,test_on_event_start,function,10,25,19,346,13.84,0,0,[],[],[],11,"['    """"""Test event start.""""""\n']","['LlamaDebugHandler', 'handler.on_event_start', 'len', 'isinstance']",4
repos/llama_index/llama-index-legacy/tests/callbacks/test_token_counter.py:test_on_event_end,test_on_event_end,function,7,32,17,556,17.38,0,0,[],[],[],29,"['    """"""Test event end.""""""\n']","['TokenCountingHandler', 'handler.on_event_end', 'len']",3
repos/llama_index/llama-index-legacy/tests/callbacks/test_token_counter.py:test_on_event_start,test_on_event_start,function,5,26,15,346,13.31,0,0,[],[],[],10,"['    """"""Test event start.""""""\n']","['TokenCountingHandler', 'handler.on_event_start', 'len']",3
repos/llama_index/llama-index-legacy/tests/chat_engine/test_condense_plus_context.py:override_predict,override_predict,function,2,2,2,34,17.0,0,0,"['self', 'prompt', '**prompt_args']","[' Any', ' BasePromptTemplate', ' Any']","[None, None, None]",15,[],['prompt.format'],1
repos/llama_index/llama-index-legacy/tests/chat_engine/test_condense_plus_context.py:test_condense_plus_context_chat_engine,test_condense_plus_context_chat_engine,function,32,174,108,2056,11.82,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",24,[],"['Mock', 'source_url', 'query.replace', 'override_retrieve', 'NodeWithScore', 'node=TextNode', 'CondensePlusContextChatEngine', 'llm=MockLLM', 'engine.reset', 'engine.chat', 'str']",11
repos/llama_index/llama-index-legacy/tests/chat_engine/test_condense_question.py:test_condense_question_chat_engine,test_condense_question_chat_engine,function,12,61,35,647,10.61,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",12,[],"['Mock', 'Response', 'CondenseQuestionChatEngine.from_defaults', 'engine.reset', 'engine.chat', 'str']",6
repos/llama_index/llama-index-legacy/tests/chat_engine/test_condense_question.py:test_condense_question_chat_engine_with_init_history,test_condense_question_chat_engine_with_init_history,function,13,43,36,554,12.88,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",37,[],"['Mock', 'Response', 'CondenseQuestionChatEngine.from_defaults', 'ChatMessage', 'print', 'engine.chat', 'str']",7
repos/llama_index/llama-index-legacy/tests/chat_engine/test_simple.py:test_simple_chat_engine,test_simple_chat_engine,function,6,48,23,443,9.23,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",6,[],"['SimpleChatEngine.from_defaults', 'engine.reset', 'engine.chat', 'str']",4
repos/llama_index/llama-index-legacy/tests/chat_engine/test_simple.py:test_simple_chat_engine_with_init_history,test_simple_chat_engine_with_init_history,function,5,34,25,375,11.03,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",27,[],"['SimpleChatEngine.from_defaults', 'ChatMessage', 'engine.chat', 'str']",4
repos/llama_index/llama-index-legacy/tests/conftest.py:allow_networking,allow_networking,function,1,1,1,18,18.0,0,0,['monkeypatch'],[' pytest.MonkeyPatch'],[None],33,[],['monkeypatch.undo'],1
repos/llama_index/llama-index-legacy/tests/conftest.py:mock_llm,mock_llm,function,2,2,2,15,7.5,0,0,[],[],[],100,[],['MockLLM'],1
repos/llama_index/llama-index-legacy/tests/conftest.py:mock_openai_credentials,mock_openai_credentials,function,2,7,7,82,11.71,0,1,[],[],[],105,[],[],0
repos/llama_index/llama-index-legacy/tests/conftest.py:mock_service_context,mock_service_context,function,2,2,2,63,31.5,0,0,"['patch_token_text_splitter', 'patch_llm_predictor', '']","[' Any', ' Any', None]","[None, None, None]",92,[],['ServiceContext.from_defaults'],1
repos/llama_index/llama-index-legacy/tests/conftest.py:patch_llm_predictor,patch_llm_predictor,function,1,35,13,475,13.57,0,0,['monkeypatch'],[' pytest.MonkeyPatch'],[None],52,[],"['monkeypatch.setattr', 'MockLLM', 'LLMMetadata']",3
repos/llama_index/llama-index-legacy/tests/conftest.py:patch_token_text_splitter,patch_token_text_splitter,function,1,16,13,358,22.38,0,0,['monkeypatch'],[' pytest.MonkeyPatch'],[None],38,[],['monkeypatch.setattr'],1
repos/llama_index/llama-index-legacy/tests/conftest.py:pytest_addoption,pytest_addoption,function,1,8,8,100,12.5,0,0,['parser'],[' pytest.Parser'],[None],152,[],['parser.addoption'],1
repos/llama_index/llama-index-legacy/tests/conftest.py:pytest_collection_modifyitems,pytest_collection_modifyitems,function,7,18,16,198,11.0,1,2,"['config', 'items']","[' pytest.Config', ' List[pytest.Item]']","[None, None]",165,[],"['config.getoption', 'item.add_marker']",2
repos/llama_index/llama-index-legacy/tests/conftest.py:pytest_configure,pytest_configure,function,1,6,6,70,11.67,0,0,['config'],[' pytest.Config'],[None],161,[],['config.addinivalue_line'],1
repos/llama_index/llama-index-legacy/tests/conftest.py:CachedOpenAIApiKeys,CachedOpenAIApiKeys,class,24,73,61,1040,14.25,0,1,[],[],[],110,[],[],0
repos/llama_index/llama-index-legacy/tests/conftest.py:CachedOpenAIApiKeys:__enter__,CachedOpenAIApiKeys:__enter__,method,12,20,18,373,18.65,0,1,['self'],[None],[None],132,[],['str'],1
repos/llama_index/llama-index-legacy/tests/conftest.py:CachedOpenAIApiKeys:__exit__,CachedOpenAIApiKeys:__exit__,method,7,8,8,196,24.5,0,0,"['self', '*exc']","[None, ' object']","[None, None]",145,[],['str'],1
repos/llama_index/llama-index-legacy/tests/conftest.py:CachedOpenAIApiKeys:__init__,CachedOpenAIApiKeys:__init__,method,10,10,10,190,19.0,0,0,"['self', 'set_env_key_to', 'set_library_key_to', 'set_fake_key', 'set_env_type_to', 'set_library_type_to', '# default value in openai package']","[None, ' Optional[str] ', ' Optional[str] ', ' bool ', ' Optional[str] ', ' str ', None]","[None, ' """"', ' None', ' False', ' """"', ' ""open_ai""', None]",118,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_azure_openai.py:test_custom_http_client,test_custom_http_client,function,9,16,14,292,18.25,0,0,['azure_openai_mock'],[' MagicMock'],[None],8,"['    """"""\n', '    Verify that a custom http_client set for AzureOpenAIEmbedding.\n', '    Should get passed on to the implementation from OpenAI.\n', '    """"""\n']","['httpx.Client', 'AzureOpenAIEmbedding', 'embedding.get_text_embedding', 'azure_openai_mock.assert_called']",4
repos/llama_index/llama-index-legacy/tests/embeddings/test_base.py:mock_get_text_embedding,mock_get_text_embedding,function,3,89,28,387,4.35,0,1,['text'],[' str'],[None],13,"['    """"""Mock get text embedding.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-legacy/tests/embeddings/test_base.py:mock_get_text_embeddings,mock_get_text_embeddings,function,1,6,6,51,8.5,0,0,['texts'],[' List[str]'],[None],35,"['    """"""Mock get text embeddings.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_base.py:test_embedding_similarity,test_embedding_similarity,function,6,16,14,168,10.5,0,0,[],[],[],72,"['    """"""Test embedding similarity.""""""\n']","['OpenAIEmbedding', 'embed_model.similarity']",2
repos/llama_index/llama-index-legacy/tests/embeddings/test_base.py:test_embedding_similarity_euclidean,test_embedding_similarity_euclidean,function,14,37,26,454,12.27,0,0,[],[],[],81,[],"['OpenAIEmbedding', 'embed_model.similarity']",2
repos/llama_index/llama-index-legacy/tests/embeddings/test_base.py:test_get_text_embeddings,test_get_text_embeddings,function,9,84,34,594,7.07,8,0,"['_mock_get_text_embeddings', '_mock_get_text_embedding']","[' Any', ' Any']","[None, None]",46,"['    """"""Test get queued text embeddings.""""""\n']","['OpenAIEmbedding', 'range', 'texts_to_embed.append', 'embed_model.get_text_embedding_batch']",4
repos/llama_index/llama-index-legacy/tests/embeddings/test_base.py:test_mean_agg,test_mean_agg,function,4,16,14,122,7.62,0,0,[],[],[],95,"['    """"""Test mean aggregation for embeddings.""""""\n']",['mean_agg'],1
repos/llama_index/llama-index-legacy/tests/embeddings/test_base.py:test_validates_api_key_is_present,test_validates_api_key_is_present,function,5,14,11,172,12.29,0,0,[],[],[],103,[],"['CachedOpenAIApiKeys', 'OpenAIEmbedding']",2
repos/llama_index/llama-index-legacy/tests/embeddings/test_bedrock.py:TestBedrockEmbedding,TestBedrockEmbedding,class,18,87,53,1505,17.3,0,0,[],[],[],11,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_bedrock.py:TestBedrockEmbedding:test_get_text_embedding_cohere,TestBedrockEmbedding:test_get_text_embedding_cohere,method,12,37,35,655,17.7,0,0,['self'],[None],[None],48,[],"['BytesIO', 'StreamingBody', 'len', 'BedrockEmbedding', 'bedrock_embedding.get_text_embedding', 'self.assertEqual']",6
repos/llama_index/llama-index-legacy/tests/embeddings/test_bedrock.py:TestBedrockEmbedding:test_get_text_embedding_titan,TestBedrockEmbedding:test_get_text_embedding_titan,method,12,37,35,645,17.43,0,0,['self'],[None],[None],15,[],"['BytesIO', 'StreamingBody', 'len', 'BedrockEmbedding', 'bedrock_embedding.get_text_embedding', 'self.assertEqual']",6
repos/llama_index/llama-index-legacy/tests/embeddings/test_elasticsearch.py:es_password,es_password,function,1,2,2,11,5.5,0,0,[],[],[],29,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_elasticsearch.py:es_url,es_url,function,1,2,2,29,14.5,0,0,[],[],[],17,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_elasticsearch.py:es_username,es_username,function,1,2,2,11,5.5,0,0,[],[],[],23,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_elasticsearch.py:model_id,model_id,function,1,2,2,21,10.5,0,0,[],[],[],11,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_elasticsearch.py:test_elasticsearch_embedding_constructor,test_elasticsearch_embedding_constructor,function,1,6,6,126,21.0,0,0,"['model_id', 'es_url', 'es_username', 'es_password']","[' str', ' str', ' str', ' str']","[None, None, None, None]",35,"['    """"""Test Elasticsearch embedding query.""""""\n']",['ElasticsearchEmbedding.from_credentials'],1
repos/llama_index/llama-index-legacy/tests/embeddings/test_fastembed.py:test_fastembed_embedding_texts_batch,test_fastembed_embedding_texts_batch,function,6,20,19,268,13.4,0,0,"['model_name', 'max_length', 'doc_embed_type', '""passage""]', 'threads', '']","[' str', ' int', ' Literal[""default""', None, ' int', None]","[None, None, None, None, None, None]",19,"['    """"""Test FastEmbed batch embedding.""""""\n']","['FastEmbedEmbedding', 'embedding.get_text_embedding_batch', 'len']",3
repos/llama_index/llama-index-legacy/tests/embeddings/test_fastembed.py:test_fastembed_query_embedding,test_fastembed_query_embedding,function,6,13,13,159,12.23,0,0,"['model_name', 'max_length']","[' str', ' int']","[None, None]",44,"['    """"""Test FastEmbed batch embedding.""""""\n']","['FastEmbedEmbedding', 'embedding.get_query_embedding', 'len']",3
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:gradient_access_token,gradient_access_token,function,1,2,2,25,12.5,0,0,[],[],[],21,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:gradient_host,gradient_host,function,1,2,2,32,16.0,0,0,[],[],[],11,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:gradient_model_slug,gradient_model_slug,function,1,2,2,17,8.5,0,0,[],[],[],16,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:gradient_workspace_id,gradient_workspace_id,function,1,2,2,25,12.5,0,0,[],[],[],26,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:test_gradientai_can_receive_multiple_text_embeddings,test_gradientai_can_receive_multiple_text_embeddings,function,5,22,20,376,17.09,0,0,"['gradient_access_token', 'gradient_model_slug', 'gradient_workspace_id']","[' str', ' str', ' str']","[None, None, None]",89,[],"['GradientEmbedding', 'test_object.get_text_embedding_batch', 'len']",3
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:test_gradientai_can_receive_query_embedding,test_gradientai_can_receive_query_embedding,function,4,17,17,284,16.71,0,0,"['gradient_access_token', 'gradient_model_slug', 'gradient_workspace_id']","[' str', ' str', ' str']","[None, None, None]",107,[],"['GradientEmbedding', 'test_object.get_query_embedding', 'len']",3
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:test_gradientai_can_receive_text_embedding,test_gradientai_can_receive_text_embedding,function,4,11,11,254,23.09,0,0,"['gradient_access_token', 'gradient_model_slug', 'gradient_workspace_id']","[' str', ' str', ' str']","[None, None, None]",74,[],"['GradientEmbedding', 'test_object.get_text_embedding', 'len']",3
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:test_gradientai_cannot_support_batches_larger_than_100,test_gradientai_cannot_support_batches_larger_than_100,function,3,8,8,204,25.5,0,0,"['gradient_access_token', 'gradient_model_slug', 'gradient_workspace_id']","[' str', ' str', ' str']","[None, None, None]",122,[],"['pytest.raises', 'GradientEmbedding']",2
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:test_gradientai_embedding_constructor,test_gradientai_embedding_constructor,function,2,11,10,190,17.27,0,0,"['gradient_access_token', 'gradient_model_slug', 'gradient_workspace_id']","[' str', ' str', ' str']","[None, None, None]",34,"['    """"""Test Gradient AI embedding query.""""""\n']",['GradientEmbedding'],1
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:test_gradientai_throws_if_not_installed,test_gradientai_throws_if_not_installed,function,3,7,7,183,26.14,0,0,"['gradient_access_token', 'gradient_model_slug', 'gradient_workspace_id']","[' str', ' str', ' str']","[None, None, None]",49,[],"['pytest.raises', 'GradientEmbedding']",2
repos/llama_index/llama-index-legacy/tests/embeddings/test_gradient.py:test_gradientai_throws_without_proper_auth,test_gradientai_throws_without_proper_auth,function,3,7,7,191,27.29,0,0,"['gradient_model_slug', 'gradient_workspace_id']","[' str', ' str']","[None, None]",61,"['    """"""Test Gradient AI embedding query.""""""\n']","['pytest.raises', 'GradientEmbedding']",2
repos/llama_index/llama-index-legacy/tests/embeddings/test_huggingface.py:fixture_hf_inference_api_embedding,fixture_hf_inference_api_embedding,function,4,5,5,125,25.0,0,0,[],[],[],14,[],"['patch.dict', 'huggingface_hub=MagicMock', 'HuggingFaceInferenceAPIEmbedding']",3
repos/llama_index/llama-index-legacy/tests/embeddings/test_huggingface.py:TestHuggingFaceInferenceAPIEmbeddings,TestHuggingFaceInferenceAPIEmbeddings,class,31,167,70,2877,17.23,0,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_huggingface.py:TestHuggingFaceInferenceAPIEmbeddings:test_class_name,TestHuggingFaceInferenceAPIEmbeddings:test_class_name,method,1,10,7,193,19.3,0,0,"['self', 'hf_inference_api_embedding']","[None, ' HuggingFaceInferenceAPIEmbedding']","[None, None]",20,[],"['HuggingFaceInferenceAPIEmbedding.class_name', 'hf_inference_api_embedding.class_name']",2
repos/llama_index/llama-index-legacy/tests/embeddings/test_huggingface.py:TestHuggingFaceInferenceAPIEmbeddings:test_embed_query,TestHuggingFaceInferenceAPIEmbeddings:test_embed_query,method,15,64,35,1132,17.69,0,0,"['self', 'hf_inference_api_embedding']","[None, ' HuggingFaceInferenceAPIEmbedding']","[None, None]",44,[],"['patch.object', 'AsyncMock', 'hf_inference_api_embedding.get_query_embedding', 'isinstance', 'len', 'np.all', 'np.array', 'mock_feature_extraction.assert_awaited_once_with']",8
repos/llama_index/llama-index-legacy/tests/embeddings/test_huggingface.py:TestHuggingFaceInferenceAPIEmbeddings:test_embed_query_one_dimension,TestHuggingFaceInferenceAPIEmbeddings:test_embed_query_one_dimension,method,10,29,26,541,18.66,0,0,"['self', 'hf_inference_api_embedding']","[None, ' HuggingFaceInferenceAPIEmbedding']","[None, None]",83,[],"['patch.object', 'AsyncMock', 'hf_inference_api_embedding.get_query_embedding', 'isinstance', 'len', 'np.all', 'np.array', 'mock_feature_extraction.assert_awaited_once_with']",8
repos/llama_index/llama-index-legacy/tests/embeddings/test_huggingface.py:TestHuggingFaceInferenceAPIEmbeddings:test_serialization,TestHuggingFaceInferenceAPIEmbeddings:test_serialization,method,4,11,9,187,17.0,0,0,"['self', 'hf_inference_api_embedding']","[None, ' HuggingFaceInferenceAPIEmbedding']","[None, None]",103,[],"['hf_inference_api_embedding.to_dict', 'isinstance']",2
repos/llama_index/llama-index-legacy/tests/embeddings/test_huggingface.py:TestHuggingFaceInferenceAPIEmbeddings:test_using_recommended_model,TestHuggingFaceInferenceAPIEmbeddings:test_using_recommended_model,method,9,17,16,380,22.35,0,0,['self'],[None],[None],32,[],"['MagicMock', 'patch.dict', 'HuggingFaceInferenceAPIEmbedding']",3
repos/llama_index/llama-index-legacy/tests/embeddings/test_llm_rails.py:api_key,api_key,function,1,2,2,20,10.0,0,0,[],[],[],12,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_llm_rails.py:model_id,model_id,function,1,2,2,21,10.5,0,0,[],[],[],6,[],[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_llm_rails.py:test_llm_rails_embedding_constructor,test_llm_rails_embedding_constructor,function,1,2,2,52,26.0,0,0,"['model_id', 'api_key']","[' str', ' str']","[None, None]",17,"['    """"""Test LLMRails embedding constructor.""""""\n']",['LLMRailsEmbedding'],1
repos/llama_index/llama-index-legacy/tests/embeddings/test_utils.py:mock_hf_embeddings,mock_hf_embeddings,function,1,1,1,6,6.0,0,0,"['*args', '**kwargs', 'Any]']","[' Any', ' Dict[str', None]","[None, None, None]",12,"['    """"""Mock HuggingFaceEmbeddings.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_utils.py:mock_openai_embeddings,mock_openai_embeddings,function,1,1,1,6,6.0,0,0,"['*args', '**kwargs', 'Any]']","[' Any', ' Dict[str', None]","[None, None, None]",17,"['    """"""Mock OpenAIEmbedding.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/embeddings/test_utils.py:test_resolve_embed_model,test_resolve_embed_model,function,4,28,16,598,21.36,0,0,['monkeypatch'],[' MonkeyPatch'],[None],22,[],"['monkeypatch.setattr', 'resolve_embed_model', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/evaluation/test_base.py:test_evaluator_basic,test_evaluator_basic,function,7,37,25,448,12.11,0,0,[],[],[],45,[],"['MockEvaluator', 'test_evaluator.evaluate', 'test_evaluator.evaluate_response', 'response=Response', 'NodeWithScore']",5
repos/llama_index/llama-index-legacy/tests/evaluation/test_base.py:MockEvaluator,MockEvaluator,class,13,61,47,617,10.11,0,0,[],[],[],10,[],[],0
repos/llama_index/llama-index-legacy/tests/evaluation/test_base.py:MockEvaluator:__init__,MockEvaluator:__init__,method,6,6,6,93,15.5,0,0,"['self', 'mock_score', 'mock_passing', 'mock_feedback', '']","[None, ' float ', ' bool ', ' str ', None]","[None, ' 1.0', ' True', ' ""test feedback""', None]",11,[],[],0
repos/llama_index/llama-index-legacy/tests/evaluation/test_base.py:MockEvaluator:_get_prompts,MockEvaluator:_get_prompts,method,1,2,2,8,4.0,0,0,['self'],[None],[None],21,"['        """"""Get prompts.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/evaluation/test_base.py:MockEvaluator:_update_prompts,MockEvaluator:_update_prompts,method,4,27,23,312,11.56,0,0,"['self', 'prompts']","[None, ' PromptDictType']","[None, None]",25,"['        """"""Update prompts.""""""\n']","['aevaluate', 'EvaluationResult']",2
repos/llama_index/llama-index-legacy/tests/evaluation/test_dataset_generation.py:test_dataset_generation,test_dataset_generation,function,3,5,5,102,20.4,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",10,"['    """"""Test dataset generation.""""""\n']","['TextNode', 'PromptTemplate']",2
repos/llama_index/llama-index-legacy/tests/extractors/test_metadata_extractor.py:test_metadata_extractor,test_metadata_extractor,function,16,44,41,592,13.45,0,0,[],[],[],55,"['    """"""Test metadata extraction.""""""\n']","['MockLLM', 'TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'TokenTextSplitter', 'TitleExtractor', 'QuestionsAnsweredExtractor', 'IngestionPipeline', 'pipeline.run']",10
repos/llama_index/llama-index-legacy/tests/indices/conftest.py:documents,documents,function,2,20,14,117,5.85,0,0,[],[],[],13,"['    """"""Get documents.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/indices/conftest.py:nodes,nodes,function,1,46,19,464,10.09,0,0,[],[],[],26,"['    """"""Get documents.""""""\n']","['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/indices/test_loading.py:test_load_index_from_storage_faiss_vector_store,test_load_index_from_storage_faiss_vector_store,function,18,37,29,840,22.7,0,0,"['documents', 'tmp_path', 'mock_service_context', '']","[' List[Document]', ' Path', ' ServiceContext', None]","[None, None, None, None]",146,[],"['StorageContext.from_defaults', 'docstore=SimpleDocumentStore', 'index_store=SimpleIndexStore', 'vector_store=FaissVectorStore', 'VectorStoreIndex.from_documents', 'index.as_retriever', 'storage_context.persist', 'load_index_from_storage', 'new_index.as_retriever']",9
repos/llama_index/llama-index-legacy/tests/indices/test_loading.py:test_load_index_from_storage_multiple,test_load_index_from_storage_multiple,function,21,68,38,988,14.53,2,0,"['nodes', 'tmp_path', 'mock_service_context', '']","[' List[BaseNode]', ' Path', ' ServiceContext', None]","[None, None, None, None]",58,[],"['StorageContext.from_defaults', 'VectorStoreIndex', 'SummaryIndex', 'storage_context.persist', 'pytest.raises', 'load_index_from_storage', 'load_indices_from_storage', 'len']",8
repos/llama_index/llama-index-legacy/tests/indices/test_loading.py:test_load_index_from_storage_retrieval_result_identical,test_load_index_from_storage_retrieval_result_identical,function,12,27,22,523,19.37,0,0,"['documents', 'tmp_path', 'mock_service_context', '']","[' List[Document]', ' Path', ' ServiceContext', None]","[None, None, None, None]",112,[],"['StorageContext.from_defaults', 'VectorStoreIndex.from_documents', 'index.as_retriever', 'storage_context.persist', 'load_index_from_storage', 'new_index.as_retriever']",6
repos/llama_index/llama-index-legacy/tests/indices/test_loading.py:test_load_index_from_storage_simple,test_load_index_from_storage_simple,function,10,19,18,445,23.42,0,0,"['documents', 'tmp_path', 'mock_service_context']","[' List[Document]', ' Path', ' ServiceContext']","[None, None, None]",31,[],"['StorageContext.from_defaults', 'VectorStoreIndex.from_documents', 'storage_context.persist', 'load_index_from_storage']",4
repos/llama_index/llama-index-legacy/tests/indices/test_loading.py:test_load_index_query_engine_service_context,test_load_index_query_engine_service_context,function,13,31,27,681,21.97,0,0,"['documents', 'tmp_path', 'mock_service_context', '']","[' List[Document]', ' Path', ' ServiceContext', None]","[None, None, None, None]",189,[],"['StorageContext.from_defaults', 'VectorStoreIndex.from_documents', 'storage_context.persist', 'load_index_from_storage', 'index.as_query_engine', 'new_index.as_query_engine', 'isinstance']",7
repos/llama_index/llama-index-legacy/tests/indices/test_loading_graph.py:test_load_graph_from_storage_simple,test_load_graph_from_storage_simple,function,22,59,44,1160,19.66,0,0,"['documents', 'tmp_path', 'mock_service_context', '']","[' List[Document]', ' Path', ' ServiceContext', None]","[None, None, None, None]",13,[],"['StorageContext.from_defaults', 'VectorStoreIndex.from_documents', 'SummaryIndex.from_documents', 'ComposableGraph.from_indices', 'graph.as_query_engine', 'query_engine.query', 'storage_context.persist', 'load_graph_from_storage', 'new_graph.as_query_engine', 'new_query_engine.query', 'str']",11
repos/llama_index/llama-index-legacy/tests/indices/test_prompt_helper.py:test_get_biggest_prompt,test_get_biggest_prompt,function,6,23,14,234,10.17,0,0,[],[],[],190,"['    """"""Test get_biggest_prompt from PromptHelper.""""""\n']","['PromptTemplate', 'get_biggest_prompt']",2
repos/llama_index/llama-index-legacy/tests/indices/test_prompt_helper.py:test_get_chunk_size,test_get_chunk_size,function,8,28,22,433,15.46,0,1,"['prompt', 'chunk_size_limit', 'num_chunks', 'padding', 'expected', 'Type[Exception]]', '']","[' str', ' Optional[int]', ' int', ' int', ' Union[int', None, None]","[None, None, None, None, None, None, None]",39,"['    """"""Test get chunk size given prompt.""""""\n']","['PromptHelper', 'isinstance', 'prompt_helper._get_available_chunk_size', 'PromptTemplate', 'pytest.raises']",5
repos/llama_index/llama-index-legacy/tests/indices/test_prompt_helper.py:test_get_numbered_text_from_nodes,test_get_numbered_text_from_nodes,function,13,41,38,494,12.05,0,0,[],[],[],153,"['    """"""Test get_text_from_nodes.""""""\n']","['PromptTemplate', 'PromptHelper', 'TextNode', 'prompt_helper.get_text_splitter_given_prompt', 'get_numbered_text_from_nodes', 'str']",6
repos/llama_index/llama-index-legacy/tests/indices/test_prompt_helper.py:test_get_text_splitter,test_get_text_splitter,function,19,71,50,876,12.34,0,0,[],[],[],66,"['    """"""Test get text splitter.""""""\n']","['PromptTemplate', 'PromptHelper', 'prompt_helper.get_text_splitter_given_prompt', 'text_splitter.split_text', 'truncate_text']",5
repos/llama_index/llama-index-legacy/tests/indices/test_prompt_helper.py:test_get_text_splitter_partial,test_get_text_splitter_partial,function,20,93,45,1099,11.82,0,0,[],[],[],98,"['    """"""Test get text splitter with a partially formatted prompt.""""""\n']","['PromptTemplate', 'PromptHelper', 'prompt_helper.get_text_splitter_given_prompt', 'text_splitter.split_text', 'truncate_text', 'test_prompt.partial_format', 'get_empty_prompt_txt']",7
repos/llama_index/llama-index-legacy/tests/indices/test_prompt_helper.py:test_repack,test_repack,function,8,29,27,410,14.14,0,0,[],[],[],174,"['    """"""Test repack.""""""\n']","['PromptTemplate', 'PromptHelper', 'prompt_helper.repack']",3
repos/llama_index/llama-index-legacy/tests/indices/test_prompt_helper.py:test_truncate,test_truncate,function,12,39,32,382,9.79,0,0,[],[],[],132,"['    """"""Test truncate.""""""\n']","['PromptTemplate', 'PromptHelper', 'prompt_helper.truncate']",3
repos/llama_index/llama-index-legacy/tests/indices/test_service_context.py:test_service_context_serialize,test_service_context_serialize,function,27,67,50,1320,19.7,0,0,[],[],[],16,[],"['SummaryExtractor', 'QuestionsAnsweredExtractor', 'TitleExtractor', 'SentenceSplitter', 'MockLLM', 'MockEmbedding', 'PromptHelper', 'ServiceContext.from_defaults', 'service_context.to_dict', 'ServiceContext.from_dict', 'isinstance', 'len']",12
repos/llama_index/llama-index-legacy/tests/indices/test_utils.py:test_expand_tokens_with_subtokens,test_expand_tokens_with_subtokens,function,4,26,22,188,7.23,0,0,[],[],[],6,"['    """"""Test expand tokens.""""""\n']",['expand_tokens_with_subtokens'],1
repos/llama_index/llama-index-legacy/tests/ingestion/test_cache.py:test_cache,test_cache,function,14,30,25,396,13.2,0,0,[],[],[],15,[],"['IngestionCache', 'DummyTransform', 'TextNode', 'get_transformation_hash', 'transformation', 'cache.put', 'cache.get']",7
repos/llama_index/llama-index-legacy/tests/ingestion/test_cache.py:test_cache_clear,test_cache_clear,function,13,25,22,286,11.44,0,0,[],[],[],33,[],"['IngestionCache', 'DummyTransform', 'TextNode', 'get_transformation_hash', 'transformation', 'cache.put', 'cache.get', 'cache.clear']",8
repos/llama_index/llama-index-legacy/tests/ingestion/test_cache.py:DummyTransform,DummyTransform,class,5,16,16,146,9.12,1,0,[],[],[],8,[],[],0
repos/llama_index/llama-index-legacy/tests/ingestion/test_cache.py:DummyTransform:__call__,DummyTransform:__call__,method,4,8,8,77,9.62,1,0,"['self', 'nodes', '**kwargs']","[None, ' List[BaseNode]', ' Any']","[None, None, None]",9,[],['node.set_content'],1
repos/llama_index/llama-index-legacy/tests/ingestion/test_pipeline.py:mock_hf_embeddings,mock_hf_embeddings,function,1,1,1,6,6.0,0,0,"['*args', '**kwargs', 'Any]']","[' Any', ' Dict[str', None]","[None, None, None]",12,"['    """"""Mock HuggingFaceEmbeddings.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/ingestion/test_pipeline.py:mock_openai_embeddings,mock_openai_embeddings,function,1,1,1,6,6.0,0,0,"['*args', '**kwargs', 'Any]']","[' Any', ' Dict[str', None]","[None, None, None]",17,"['    """"""Mock OpenAIEmbedding.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/ingestion/test_pipeline.py:test_resolve_embed_model,test_resolve_embed_model,function,4,28,16,598,21.36,0,0,['monkeypatch'],[' MonkeyPatch'],[None],22,[],"['monkeypatch.setattr', 'resolve_embed_model', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/llm_predictor/test_base.py:mock_llmpredictor_predict,mock_llmpredictor_predict,function,2,2,2,30,15.0,0,0,"['prompt', '**prompt_args']","[' BasePromptTemplate', ' Any']","[None, None]",27,"['    """"""Mock LLMPredictor predict.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/llm_predictor/test_base.py:test_struct_llm_predictor,test_struct_llm_predictor,function,8,26,17,391,15.04,0,0,"['mock_init', 'mock_predict']","[' Any', ' Any']","[None, None]",34,"['    """"""Test LLM predictor.""""""\n']","['StructuredLLMPredictor', 'MockOutputParser', 'PromptTemplate', 'llm_predictor.predict']",4
repos/llama_index/llama-index-legacy/tests/llm_predictor/test_base.py:MockOutputParser,MockOutputParser,class,4,18,11,102,5.67,0,0,[],[],[],15,[],[],0
repos/llama_index/llama-index-legacy/tests/llm_predictor/test_base.py:MockOutputParser:format,MockOutputParser:format,method,2,2,2,12,6.0,0,0,"['self', 'output']","[None, ' str']","[None, None]",22,"['        """"""Format output.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/llm_predictor/test_base.py:MockOutputParser:parse,MockOutputParser:parse,method,2,4,3,24,6.0,0,0,"['self', 'output']","[None, ' str']","[None, None]",18,"['        """"""Parse output.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/llms/test_ai21.py:mock_chat,mock_chat,function,2,312,98,2659,8.52,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",160,[],['construct_ai21_object'],1
repos/llama_index/llama-index-legacy/tests/llms/test_ai21.py:mock_completion,mock_completion,function,2,281,100,2375,8.45,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",20,[],['construct_ai21_object'],1
repos/llama_index/llama-index-legacy/tests/llms/test_ai21.py:test_completion_model_basic,test_completion_model_basic,function,13,48,43,529,11.02,0,0,['monkeypatch'],[' MonkeyPatch'],[None],319,[],"['monkeypatch.setattr', 'AI21', 'llm.complete', 'ChatMessage', 'llm.chat', 'print']",6
repos/llama_index/llama-index-legacy/tests/llms/test_anthropic.py:test_basic,test_basic,function,10,25,22,301,12.04,0,0,[],[],[],12,[],"['Anthropic', 'llm.complete', 'len', 'ChatMessage', 'llm.chat']",5
repos/llama_index/llama-index-legacy/tests/llms/test_anthropic.py:test_streaming,test_streaming,function,15,40,27,357,8.93,2,0,[],[],[],25,[],"['Anthropic', 'llm.stream_complete', 'ChatMessage', 'llm.stream_chat']",4
repos/llama_index/llama-index-legacy/tests/llms/test_anthropic_utils.py:test_anthropic_modelname_to_contextsize,test_anthropic_modelname_to_contextsize,function,3,4,4,76,19.0,0,0,[],[],[],28,[],"['pytest.raises', 'anthropic_modelname_to_contextsize']",2
repos/llama_index/llama-index-legacy/tests/llms/test_anthropic_utils.py:test_messages_to_anthropic_prompt,test_messages_to_anthropic_prompt,function,4,34,19,497,14.62,0,0,[],[],[],9,[],"['ChatMessage', 'messages_to_anthropic_prompt']",2
repos/llama_index/llama-index-legacy/tests/llms/test_azure_openai.py:test_custom_http_client,test_custom_http_client,function,13,22,20,431,19.59,0,0,['sync_azure_openai_mock'],[' MagicMock'],[None],10,"['    """"""\n', '    Verify that a custom http_client set for AzureOpenAI.\n', '    Should get passed on to the implementation from OpenAI.\n', '    """"""\n']","['httpx.Client', 'mock_chat_completion_v1', 'AzureOpenAI', 'azure_openai.complete', 'sync_azure_openai_mock.assert_called']",5
repos/llama_index/llama-index-legacy/tests/llms/test_bedrock.py:get_invoke_model_response,get_invoke_model_response,function,7,43,38,546,12.7,0,0,['payload'],[' str'],[None],26,[],"['payload.encode', 'BytesIO', 'len', 'StreamingBody']",4
repos/llama_index/llama-index-legacy/tests/llms/test_bedrock.py:test_model_basic,test_model_basic,function,16,51,37,705,13.82,0,0,"['model', 'complete_request', 'response_body', 'chat_request']","[' str', ' str', ' str', ' str']","[None, None, None, None]",125,[],"['Bedrock', 'Stubber', 'bedrock_stubber.add_response', 'get_invoke_model_response', 'bedrock_stubber.activate', 'llm.complete', 'ChatMessage', 'llm.chat', 'bedrock_stubber.deactivate']",9
repos/llama_index/llama-index-legacy/tests/llms/test_bedrock.py:test_model_streaming,test_model_streaming,function,14,49,40,808,16.49,0,0,['monkeypatch'],[' MonkeyPatch'],[None],163,[],"['monkeypatch.setattr', 'MockStreamCompletionWithRetry', 'Bedrock', 'llm.stream_complete', 'list', 'ChatMessage', 'llm.stream_chat']",7
repos/llama_index/llama-index-legacy/tests/llms/test_bedrock.py:MockEventStream,MockEventStream,class,4,30,28,268,8.93,1,0,[],[],[],13,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_bedrock.py:MockStreamCompletionWithRetry,MockStreamCompletionWithRetry,class,7,65,58,762,11.72,0,0,[],[],[],52,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_bedrock.py:MockEventStream:__iter__,MockEventStream:__iter__,method,3,24,22,222,9.25,1,0,['self'],[None],[None],14,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_bedrock.py:MockStreamCompletionWithRetry:__init__,MockStreamCompletionWithRetry:__init__,method,2,2,2,36,18.0,0,0,"['self', 'expected_prompt']","[None, ' str']","[None, None]",53,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_bedrock.py:MockStreamCompletionWithRetry:mock_stream_completion_with_retry,MockStreamCompletionWithRetry:mock_stream_completion_with_retry,method,3,47,41,594,12.64,0,0,"['self', 'request_body', '*args', '**kwargs']","[None, ' str', ' Any', ' Any']","[None, None, None, None]",56,[],"['json.loads', 'MockEventStream']",2
repos/llama_index/llama-index-legacy/tests/llms/test_cohere.py:mock_chat_with_retry,mock_chat_with_retry,function,2,55,44,612,11.13,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",52,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_cohere.py:mock_completion_with_retry,mock_completion_with_retry,function,2,30,28,314,10.47,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",14,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_cohere.py:test_completion_model_basic,test_completion_model_basic,function,13,37,30,531,14.35,0,0,['monkeypatch'],[' MonkeyPatch'],[None],113,[],"['monkeypatch.setattr', 'Cohere', 'llm.complete', 'ChatMessage', 'llm.chat']",5
repos/llama_index/llama-index-legacy/tests/llms/test_custom.py:test_basic,test_basic,function,7,11,11,129,11.73,0,0,[],[],[],51,[],"['TestLLM', 'ChatMessage', 'llm.complete', 'llm.chat']",4
repos/llama_index/llama-index-legacy/tests/llms/test_custom.py:test_streaming,test_streaming,function,7,11,11,143,13.0,0,0,[],[],[],61,[],"['TestLLM', 'ChatMessage', 'llm.stream_complete', 'llm.stream_chat']",4
repos/llama_index/llama-index-legacy/tests/llms/test_custom.py:TestLLM,TestLLM,class,13,76,48,612,8.05,1,0,[],[],[],12,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_custom.py:TestLLM:__init__,TestLLM:__init__,method,1,1,1,39,39.0,0,0,['self'],[None],[None],15,[],['super'],1
repos/llama_index/llama-index-legacy/tests/llms/test_custom.py:TestLLM:complete,TestLLM:complete,method,2,9,9,86,9.56,0,0,"['self', 'prompt', 'formatted', '**kwargs']","[None, ' str', ' bool ', ' Any']","[None, None, ' False', None]",22,[],['CompletionResponse'],1
repos/llama_index/llama-index-legacy/tests/llms/test_custom.py:TestLLM:metadata,TestLLM:metadata,method,2,2,2,19,9.5,0,0,['self'],[None],[None],19,[],['LLMMetadata'],1
repos/llama_index/llama-index-legacy/tests/llms/test_custom.py:TestLLM:stream_complete,TestLLM:stream_complete,method,6,27,25,201,7.44,1,0,"['self', 'prompt', 'formatted', '**kwargs']","[None, ' str', ' bool ', ' Any']","[None, None, ' False', None]",32,[],"['gen', 'CompletionResponse']",2
repos/llama_index/llama-index-legacy/tests/llms/test_gemini.py:test_gemini,test_gemini,function,9,17,15,242,14.24,0,0,[],[],[],66,[],"['MockGenaiPackage', 'Gemini', 'llm.complete', 'isinstance']",4
repos/llama_index/llama-index-legacy/tests/llms/test_gemini.py:test_gemini_stream,test_gemini_stream,function,7,17,15,252,14.82,0,0,[],[],[],80,[],"['MockGenaiPackage', 'Gemini', 'llm.stream_complete', 'isinstance']",4
repos/llama_index/llama-index-legacy/tests/llms/test_gemini.py:FakeGoogleDataclass,FakeGoogleDataclass,class,6,19,16,140,7.37,0,0,[],[],[],11,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_gemini.py:MockGenaiPackage,MockGenaiPackage,class,22,79,63,730,9.24,0,1,[],[],[],22,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_gemini.py:FakeGoogleDataclass:__init__,FakeGoogleDataclass:__init__,method,3,3,3,30,10.0,0,0,"['self', 'd', 'Any]', '*args', '**kwargs']","[None, ' Mapping[str', None, ' Any', ' Any']","[None, None, None, None, None]",14,[],['super'],1
repos/llama_index/llama-index-legacy/tests/llms/test_gemini.py:FakeGoogleDataclass:to_dict,FakeGoogleDataclass:to_dict,method,2,2,2,12,6.0,0,0,['self'],[None],[None],18,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_gemini.py:MockGenaiPackage:GenerativeModel,MockGenaiPackage:GenerativeModel,method,5,6,5,85,14.17,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",59,[],['mock.Mock'],1
repos/llama_index/llama-index-legacy/tests/llms/test_gemini.py:MockGenaiPackage:_gen_content,MockGenaiPackage:_gen_content,method,9,30,27,284,9.47,0,1,"['self', 'contents', '*', 'stream', '**kwargs']","[None, ' Any', None, ' bool ', ' Any']","[None, None, None, ' False', None]",35,[],"['mock.Mock', 'FakeGoogleDataclass']",2
repos/llama_index/llama-index-legacy/tests/llms/test_gemini.py:MockGenaiPackage:get_model,MockGenaiPackage:get_model,method,8,12,11,160,13.33,0,0,"['self', 'name', '**kwargs']","[None, ' str', ' Any']","[None, None, None]",27,[],['mock.Mock'],1
repos/llama_index/llama-index-legacy/tests/llms/test_gradient.py:test_gradient_adapter,test_gradient_adapter,function,9,24,22,371,15.46,0,0,[],[],[],70,[],"['patch.dict', 'MockGradientaiPackage', 'GradientModelAdapterLLM', 'gradientllm.complete', 'isinstance']",5
repos/llama_index/llama-index-legacy/tests/llms/test_gradient.py:test_gradient_base,test_gradient_base,function,9,24,22,364,15.17,0,0,[],[],[],54,"['    """"""Test Gradient.""""""\n']","['patch.dict', 'MockGradientaiPackage', 'GradientBaseModelLLM', 'gradientllm.complete', 'isinstance']",5
repos/llama_index/llama-index-legacy/tests/llms/test_gradient.py:GradientModel,GradientModel,class,7,29,16,319,11.0,0,0,[],[],[],15,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_gradient.py:MockGradient,MockGradient,class,7,27,19,284,10.52,0,0,[],[],[],31,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_gradient.py:MockGradientaiPackage,MockGradientaiPackage,class,2,2,2,21,10.5,0,0,[],[],[],48,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_gradient.py:GradientModel:complete,GradientModel:complete,method,4,6,5,92,15.33,0,0,"['self', 'query', 'max_generated_token_count']","[None, ' str', ' int']","[None, None, None]",18,"['        """"""Just duplicate the query m times.""""""\n']",['MagicMock'],1
repos/llama_index/llama-index-legacy/tests/llms/test_gradient.py:MockGradient:close,MockGradient:close,method,1,1,1,6,6.0,0,0,['self'],[None],[None],39,"['        """"""Mock Gradient completion.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/llms/test_gradient.py:MockGradient:get_base_model,MockGradient:get_base_model,method,3,5,5,63,12.6,0,0,"['self', 'base_model_slug']","[None, ' str']","[None, None]",34,[],['GradientModel'],1
repos/llama_index/llama-index-legacy/tests/llms/test_gradient.py:MockGradient:get_model_adapter,MockGradient:get_model_adapter,method,3,5,5,67,13.4,0,0,"['self', 'model_adapter_id']","[None, ' str']","[None, None]",43,[],['GradientModel'],1
repos/llama_index/llama-index-legacy/tests/llms/test_huggingface.py:fixture_hf_inference_api,fixture_hf_inference_api,function,4,5,5,116,23.2,0,0,[],[],[],11,[],"['patch.dict', 'huggingface_hub=MagicMock', 'HuggingFaceInferenceAPI']",3
repos/llama_index/llama-index-legacy/tests/llms/test_huggingface.py:TestHuggingFaceInferenceAPI,TestHuggingFaceInferenceAPI,class,38,270,157,3145,11.65,0,0,[],[],[],16,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_huggingface.py:TestHuggingFaceInferenceAPI:test_chat,TestHuggingFaceInferenceAPI:test_chat,method,11,94,70,907,9.65,0,0,"['self', 'hf_inference_api']","[None, ' HuggingFaceInferenceAPI']","[None, None]",39,[],"['ChatMessage', 'patch.object', 'hf_inference_api.chat', 'mock_conversational.assert_called_once_with']",4
repos/llama_index/llama-index-legacy/tests/llms/test_huggingface.py:TestHuggingFaceInferenceAPI:test_chat_text_generation,TestHuggingFaceInferenceAPI:test_chat_text_generation,method,15,75,55,910,12.13,0,0,"['self', 'hf_inference_api']","[None, ' HuggingFaceInferenceAPI']","[None, None]",74,[],"['MagicMock', 'ChatMessage', 'patch.object', 'hf_inference_api.chat', 'mock_complete.assert_called_once_with']",5
repos/llama_index/llama-index-legacy/tests/llms/test_huggingface.py:TestHuggingFaceInferenceAPI:test_class_name,TestHuggingFaceInferenceAPI:test_class_name,method,2,6,5,146,24.33,0,0,"['self', 'hf_inference_api']","[None, ' HuggingFaceInferenceAPI']","[None, None]",17,[],"['HuggingFaceInferenceAPI.class_name', 'hf_inference_api.class_name']",2
repos/llama_index/llama-index-legacy/tests/llms/test_huggingface.py:TestHuggingFaceInferenceAPI:test_complete,TestHuggingFaceInferenceAPI:test_complete,method,8,38,34,378,9.95,0,0,"['self', 'hf_inference_api']","[None, ' HuggingFaceInferenceAPI']","[None, None]",105,[],"['patch.object', 'hf_inference_api.complete', 'mock_text_generation.assert_called_once_with']",3
repos/llama_index/llama-index-legacy/tests/llms/test_huggingface.py:TestHuggingFaceInferenceAPI:test_instantiation,TestHuggingFaceInferenceAPI:test_instantiation,method,11,27,21,469,17.37,0,0,['self'],[None],[None],21,[],"['MagicMock', 'patch.dict', 'HuggingFaceInferenceAPI', 'isinstance']",4
repos/llama_index/llama-index-legacy/tests/llms/test_konko.py:teardown_module,teardown_module,function,4,4,4,39,9.75,0,0,[],[],[],46,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_konko.py:test_chat_model_basic_non_openai_model,test_chat_model_basic_non_openai_model,function,11,23,20,259,11.26,0,0,[],[],[],12,[],"['Konko', 'ChatMessage', 'llm.complete', 'llm.chat']",4
repos/llama_index/llama-index-legacy/tests/llms/test_konko.py:test_chat_model_basic_openai_model,test_chat_model_basic_openai_model,function,11,23,20,245,10.65,0,0,[],[],[],25,[],"['Konko', 'ChatMessage', 'llm.complete', 'llm.chat']",4
repos/llama_index/llama-index-legacy/tests/llms/test_konko.py:test_chat_model_streaming,test_chat_model_streaming,function,8,15,15,235,15.67,0,0,[],[],[],38,[],"['Konko', 'ChatMessage', 'llm.stream_chat', 'list']",4
repos/llama_index/llama-index-legacy/tests/llms/test_langchain.py:test_basic,test_basic,function,9,18,16,211,11.72,0,0,[],[],[],39,[],"['LC.FakeListLLM', 'LangChainLLM', 'ChatMessage', 'llm.complete', 'llm.chat']",5
repos/llama_index/llama-index-legacy/tests/llms/test_langchain.py:test_from_lc_messages,test_from_lc_messages,function,7,41,33,542,13.22,1,0,[],[],[],67,[],"['ChatMessage', 'to_lc_messages', 'range']",3
repos/llama_index/llama-index-legacy/tests/llms/test_langchain.py:test_metadata_sets_model_name,test_metadata_sets_model_name,function,7,24,17,448,18.67,0,0,[],[],[],93,[],['LangChainLLM'],1
repos/llama_index/llama-index-legacy/tests/llms/test_langchain.py:test_to_lc_messages,test_to_lc_messages,function,13,31,27,414,13.35,1,0,[],[],[],51,[],"['LC.SystemMessage', 'LC.HumanMessage', 'LC.AIMessage', 'LC.FunctionMessage', 'LC.ChatMessage', 'from_lc_messages', 'range']",7
repos/llama_index/llama-index-legacy/tests/llms/test_litellm.py:mock_chat_completion,mock_chat_completion,function,1,35,34,298,8.51,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",39,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_litellm.py:mock_chat_completion_stream,mock_chat_completion_stream,function,2,81,31,865,10.68,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",88,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_litellm.py:mock_completion,mock_completion,function,1,35,34,299,8.54,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",16,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_litellm.py:mock_completion_stream,mock_completion_stream,function,2,24,13,104,4.33,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",57,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_litellm.py:test_chat_model_basic,test_chat_model_basic,function,14,31,27,415,13.39,0,0,['monkeypatch'],[' MonkeyPatch'],[None],130,[],"['CachedOpenAIApiKeys', 'monkeypatch.setattr', 'LiteLLM', 'ChatMessage', 'llm.complete', 'llm.chat']",6
repos/llama_index/llama-index-legacy/tests/llms/test_litellm.py:test_deep_infra,test_deep_infra,function,7,19,19,250,13.16,0,0,[],[],[],155,[],"['LiteLLM', 'ChatMessage', 'llm.chat', 'print']",4
repos/llama_index/llama-index-legacy/tests/llms/test_litellm.py:test_metadata,test_metadata,function,3,5,5,84,16.8,0,0,[],[],[],149,[],"['LiteLLM', 'isinstance']",2
repos/llama_index/llama-index-legacy/tests/llms/test_litellm.py:test_openai,test_openai,function,7,16,16,209,13.06,0,0,[],[],[],167,[],"['LiteLLM', 'ChatMessage', 'llm.chat', 'print']",4
repos/llama_index/llama-index-legacy/tests/llms/test_litellm.py:test_tg_ai,test_tg_ai,function,7,19,19,266,14.0,0,0,[],[],[],176,[],"['LiteLLM', 'ChatMessage', 'llm.chat', 'print']",4
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:chat_messages_assistant_first,chat_messages_assistant_first,function,1,14,13,204,14.57,0,0,[],[],[],76,[],['ChatMessage'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:chat_messages_first_chat,chat_messages_first_chat,function,1,10,10,134,13.4,0,0,[],[],[],19,[],['ChatMessage'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:chat_messages_first_chat_no_system,chat_messages_first_chat_no_system,function,2,2,2,34,17.0,0,0,"['chat_messages_first_chat', '']","[' Sequence[ChatMessage]', None]","[None, None]",28,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:chat_messages_second_chat,chat_messages_second_chat,function,1,19,15,265,13.95,0,0,[],[],[],36,[],['ChatMessage'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:chat_messages_second_chat_no_system,chat_messages_second_chat_no_system,function,2,2,2,35,17.5,0,0,"['chat_messages_second_chat', '']","[' Sequence[ChatMessage]', None]","[None, None]",47,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:chat_messages_third_chat,chat_messages_third_chat,function,1,29,16,397,13.69,0,0,[],[],[],55,[],['ChatMessage'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:chat_messages_third_chat_no_system,chat_messages_third_chat_no_system,function,2,2,2,34,17.0,0,0,"['chat_messages_third_chat', '']","[' Sequence[ChatMessage]', None]","[None, None]",68,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:chat_messages_user_twice,chat_messages_user_twice,function,1,15,12,195,13.0,0,0,[],[],[],88,[],['ChatMessage'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_completion_to_prompt,test_completion_to_prompt,function,4,21,19,212,10.1,0,0,[],[],[],179,[],['completion_to_prompt'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_completion_to_prompt_default,test_completion_to_prompt_default,function,3,17,16,171,10.06,0,0,[],[],[],189,[],['completion_to_prompt'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_error_assistant_first,test_error_assistant_first,function,3,3,3,84,28.0,0,0,"['chat_messages_assistant_first', '']","[' Sequence[ChatMessage]', None]","[None, None]",164,[],"['pytest.raises', 'messages_to_prompt']",2
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_error_user_twice,test_error_user_twice,function,3,3,3,79,26.33,0,0,['chat_messages_user_twice'],[' Sequence[ChatMessage]'],[None],172,[],"['pytest.raises', 'messages_to_prompt']",2
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_first_chat,test_first_chat,function,2,16,15,137,8.56,0,0,['chat_messages_first_chat'],[' Sequence[ChatMessage]'],[None],99,[],['messages_to_prompt'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_first_chat_default,test_first_chat_default,function,2,15,14,165,11.0,0,0,"['chat_messages_first_chat_no_system', '']","[' Sequence[ChatMessage]', None]","[None, None]",107,[],['messages_to_prompt'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_second_chat,test_second_chat,function,2,28,23,204,7.29,0,0,['chat_messages_second_chat'],[' Sequence[ChatMessage]'],[None],118,[],['messages_to_prompt'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_second_chat_default,test_second_chat_default,function,2,26,22,228,8.77,0,0,"['chat_messages_second_chat_no_system', '']","[' Sequence[ChatMessage]', None]","[None, None]",128,[],['messages_to_prompt'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_third_chat,test_third_chat,function,2,40,24,266,6.65,0,0,['chat_messages_third_chat'],[' Sequence[ChatMessage]'],[None],140,[],['messages_to_prompt'],1
repos/llama_index/llama-index-legacy/tests/llms/test_llama_utils.py:test_third_chat_default,test_third_chat_default,function,2,38,23,290,7.63,0,0,"['chat_messages_third_chat_no_system', '']","[' Sequence[ChatMessage]', None]","[None, None]",151,[],['messages_to_prompt'],1
repos/llama_index/llama-index-legacy/tests/llms/test_localai.py:mock_chat_completion,mock_chat_completion,function,4,21,21,303,14.43,0,0,['text'],[' str'],[None],18,[],"['ChatCompletion', 'Choice', 'message=ChatCompletionMessage']",3
repos/llama_index/llama-index-legacy/tests/llms/test_localai.py:mock_completion,mock_completion,function,2,20,20,258,12.9,0,0,['text'],[' str'],[None],35,[],"['Completion', 'CompletionChoice']",2
repos/llama_index/llama-index-legacy/tests/llms/test_localai.py:test_chat,test_chat,function,10,16,15,332,20.75,0,0,['MockSyncOpenAI'],[' MagicMock'],[None],70,[],"['mock_chat_completion', 'LocalAI', 'llm.chat']",3
repos/llama_index/llama-index-legacy/tests/llms/test_localai.py:test_completion,test_completion,function,10,25,24,286,11.44,0,0,['MockSyncOpenAI'],[' MagicMock'],[None],54,[],"['mock_completion', 'LocalAI', 'llm.complete']",3
repos/llama_index/llama-index-legacy/tests/llms/test_localai.py:test_interfaces,test_interfaces,function,4,8,7,106,13.25,0,0,[],[],[],12,[],"['LocalAI', 'llm.class_name', 'type']",3
repos/llama_index/llama-index-legacy/tests/llms/test_localai.py:test_serialization,test_serialization,function,5,12,11,175,14.58,0,0,[],[],[],83,[],"['LocalAI', 'llm.to_dict']",2
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:mock_chat_completion,mock_chat_completion,function,1,35,34,298,8.51,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",65,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:mock_chat_completion_stream,mock_chat_completion_stream,function,2,81,31,865,10.68,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",165,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:mock_chat_completion_stream_v1,mock_chat_completion_stream_v1,function,3,52,24,965,18.56,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",206,[],"['ChatCompletionChunk', 'ChunkChoice', 'delta=ChoiceDelta']",3
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:mock_chat_completion_v1,mock_chat_completion_v1,function,3,23,23,328,14.26,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",83,[],"['ChatCompletion', 'usage=CompletionUsage', 'Choice', 'message=ChatCompletionMessage']",4
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:mock_completion,mock_completion,function,1,35,34,299,8.54,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",20,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:mock_completion_stream,mock_completion_stream,function,2,24,13,104,4.33,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",102,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:mock_completion_stream_v1,mock_completion_stream_v1,function,2,24,15,412,17.17,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",123,[],['Completion'],1
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:mock_completion_v1,mock_completion_v1,function,3,22,22,314,14.27,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",39,[],"['Completion', 'CompletionChoice', 'usage=CompletionUsage']",3
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:test_chat_model_basic,test_chat_model_basic,function,17,31,27,431,13.9,0,0,['MockSyncOpenAI'],[' MagicMock'],[None],270,[],"['CachedOpenAIApiKeys', 'mock_chat_completion_v1', 'OpenAI', 'ChatMessage', 'llm.complete', 'llm.chat']",6
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:test_chat_model_streaming,test_chat_model_streaming,function,19,38,31,659,17.34,0,0,['MockSyncOpenAI'],[' MagicMock'],[None],307,[],"['CachedOpenAIApiKeys', 'mock_chat_completion_stream_v1', 'OpenAI', 'ChatMessage', 'llm.stream_complete', 'list', 'llm.stream_chat']",7
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:test_completion_model_basic,test_completion_model_basic,function,17,33,28,434,13.15,0,0,['MockSyncOpenAI'],[' MagicMock'],[None],253,[],"['CachedOpenAIApiKeys', 'mock_completion_v1', 'OpenAI', 'ChatMessage', 'llm.complete', 'llm.chat']",6
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:test_completion_model_streaming,test_completion_model_streaming,function,19,31,28,577,18.61,0,0,['MockSyncOpenAI'],[' MagicMock'],[None],287,[],"['CachedOpenAIApiKeys', 'mock_completion_stream_v1', 'OpenAI', 'ChatMessage', 'llm.stream_complete', 'list', 'llm.stream_chat']",7
repos/llama_index/llama-index-legacy/tests/llms/test_openai.py:test_validates_api_key_is_present,test_validates_api_key_is_present,function,5,14,11,154,11.0,0,0,[],[],[],371,[],"['CachedOpenAIApiKeys', 'OpenAI']",2
repos/llama_index/llama-index-legacy/tests/llms/test_openai_like.py:mock_chat_completion,mock_chat_completion,function,4,21,21,298,14.19,0,0,['text'],[' str'],[None],27,[],"['ChatCompletion', 'Choice', 'message=ChatCompletionMessage']",3
repos/llama_index/llama-index-legacy/tests/llms/test_openai_like.py:mock_completion,mock_completion,function,2,20,20,249,12.45,0,0,['text'],[' str'],[None],44,[],"['Completion', 'CompletionChoice']",2
repos/llama_index/llama-index-legacy/tests/llms/test_openai_like.py:test_chat,test_chat,function,11,29,29,528,18.21,0,0,['MockSyncOpenAI'],[' MagicMock'],[None],106,[],"['mock_chat_completion', 'OpenAILike', 'tokenizer=StubTokenizer', 'llm.chat']",4
repos/llama_index/llama-index-legacy/tests/llms/test_openai_like.py:test_completion,test_completion,function,10,88,42,882,10.02,0,0,['MockSyncOpenAI'],[' MagicMock'],[None],62,[],"['mock_completion', 'OpenAILike', 'llm.complete', 'call', 'tokenizer=StubTokenizer']",5
repos/llama_index/llama-index-legacy/tests/llms/test_openai_like.py:test_interfaces,test_interfaces,function,5,9,8,134,14.89,0,0,[],[],[],21,[],"['OpenAILike', 'llm.class_name', 'type']",3
repos/llama_index/llama-index-legacy/tests/llms/test_openai_like.py:test_serialization,test_serialization,function,5,23,20,285,12.39,0,0,[],[],[],126,[],"['OpenAILike', 'tokenizer=StubTokenizer', 'llm.to_dict']",3
repos/llama_index/llama-index-legacy/tests/llms/test_openai_like.py:StubTokenizer,StubTokenizer,class,2,17,15,99,5.82,0,0,[],[],[],12,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_openai_like.py:StubTokenizer:encode,StubTokenizer:encode,method,1,11,9,62,5.64,0,0,"['self', 'text']","[None, ' str']","[None, None]",13,[],['text.split'],1
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:azure_chat_messages_with_function_calling,azure_chat_messages_with_function_calling,function,1,33,29,286,8.67,0,0,[],[],[],103,[],['ChatMessage'],1
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:azure_openai_message_dicts_with_function_calling,azure_openai_message_dicts_with_function_calling,function,1,26,25,306,11.77,0,0,[],[],[],78,"['    """"""\n', '    Taken from:\n', '    - https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/function-calling.\n', '    """"""\n']","['ChatCompletionMessage', 'ChatCompletionMessageToolCall', 'function=Function']",3
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:chat_messages_with_function_calling,chat_messages_with_function_calling,function,1,37,30,420,11.35,0,0,[],[],[],31,[],['ChatMessage'],1
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:openi_message_dicts_with_function_calling,openi_message_dicts_with_function_calling,function,1,32,28,424,13.25,0,0,[],[],[],55,[],"['ChatCompletionUserMessageParam', 'ChatCompletionAssistantMessageParam', 'function_call=FunctionCallParam', 'ChatCompletionFunctionMessageParam']",4
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:test_from_openai_message_dicts_function_calling,test_from_openai_message_dicts_function_calling,function,12,29,25,492,16.97,2,0,"['openi_message_dicts_with_function_calling', 'chat_messages_with_function_calling', '']","[' List[ChatCompletionMessageParam]', ' List[ChatMessage]', None]","[None, None, None]",156,[],"['from_openai_message_dicts', 'zip']",2
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:test_from_openai_messages_function_calling_azure,test_from_openai_messages_function_calling_azure,function,3,7,6,149,21.29,0,0,"['azure_openai_message_dicts_with_function_calling', 'azure_chat_messages_with_function_calling', '']","[' List[ChatCompletionMessage]', ' List[ChatMessage]', None]","[None, None, None]",174,[],['from_openai_messages'],1
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:test_to_openai_message_dicts_basic_enum,test_to_openai_message_dicts_basic_enum,function,3,25,19,307,12.28,0,0,[],[],[],124,[],"['ChatMessage', 'to_openai_message_dicts']",2
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:test_to_openai_message_dicts_basic_string,test_to_openai_message_dicts_basic_string,function,3,25,19,287,11.48,0,0,[],[],[],136,[],"['ChatMessage', 'to_openai_message_dicts']",2
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:test_to_openai_message_dicts_function_calling,test_to_openai_message_dicts_function_calling,function,3,5,4,137,27.4,0,0,"['chat_messages_with_function_calling', 'openi_message_dicts_with_function_calling', '']","[' List[ChatMessage]', ' List[ChatCompletionMessageParam]', None]","[None, None, None]",148,[],['to_openai_message_dicts'],1
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:test_to_openai_message_with_pydantic_description,test_to_openai_message_with_pydantic_description,function,6,22,20,212,9.64,0,0,[],[],[],199,[],"['TestOutput', 'to_openai_tool', 'TestOutput.schema']",3
repos/llama_index/llama-index-legacy/tests/llms/test_openai_utils.py:test_to_openai_tool_with_provided_description,test_to_openai_tool_with_provided_description,function,6,24,22,245,10.21,0,0,[],[],[],184,[],"['TestOutput', 'to_openai_tool', 'TestOutput.schema']",3
repos/llama_index/llama-index-legacy/tests/llms/test_palm.py:_mock_palm_completion,_mock_palm_completion,function,6,9,8,106,11.78,0,0,"['model_name', 'prompt', '**kwargs']","[' str', ' str', ' Any']","[None, None, None]",10,"['    """"""Mock PaLM completion.""""""\n']",['MagicMock'],1
repos/llama_index/llama-index-legacy/tests/llms/test_palm.py:test_palm,test_palm,function,8,15,14,227,15.13,0,0,[],[],[],41,"['    """"""Test palm.""""""\n']","['MockPalmPackage', 'PaLM', 'palm.complete', 'isinstance']",4
repos/llama_index/llama-index-legacy/tests/llms/test_palm.py:MockPalmPackage,MockPalmPackage,class,9,30,22,250,8.33,0,0,[],[],[],18,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_palm.py:MockPalmPackage:_mock_models,MockPalmPackage:_mock_models,method,4,6,6,55,9.17,0,0,['self'],[None],[None],21,[],['MagicMock'],1
repos/llama_index/llama-index-legacy/tests/llms/test_palm.py:MockPalmPackage:generate_text,MockPalmPackage:generate_text,method,2,4,4,50,12.5,0,0,"['self', 'model', 'prompt', '**kwargs']","[None, ' str', ' str', ' Any']","[None, None, None, None]",26,"['        """"""Mock PaLM completion.""""""\n']",['_mock_palm_completion'],1
repos/llama_index/llama-index-legacy/tests/llms/test_palm.py:MockPalmPackage:list_models,MockPalmPackage:list_models,method,2,2,2,25,12.5,0,0,['self'],[None],[None],30,[],['self._mock_models'],1
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:mock_chat_completion,mock_chat_completion,function,1,37,36,287,7.76,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",31,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:mock_chat_completion_stream,mock_chat_completion_stream,function,2,114,45,831,7.29,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",98,[],['str'],1
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:mock_chat_history,mock_chat_history,function,1,38,28,275,7.24,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",167,[],['ChatMessage'],1
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:mock_completion,mock_completion,function,1,34,34,250,7.35,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",17,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:mock_completion_stream,mock_completion_stream,function,2,94,42,700,7.45,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",49,[],['str'],1
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:test_chat,test_chat,function,10,20,19,249,12.45,0,0,['chat_history'],[' List[ChatMessage]'],[None],202,[],"['patch', 'mock_chat_completion', 'RunGptLLM', 'dummy.chat']",4
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:test_complete,test_complete,function,9,18,18,196,10.89,0,0,[],[],[],191,[],"['RunGptLLM', 'patch', 'mock_completion', 'dummy.complete']",4
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:test_init,test_init,function,5,14,12,162,11.57,0,0,[],[],[],184,[],"['RunGptLLM', 'isinstance']",2
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:test_stream_chat,test_stream_chat,function,17,38,37,551,14.5,0,0,['chat_history'],[' List[ChatMessage]'],[None],215,[],"['MagicMock', 'mock_chat_completion_stream', 'iter', 'patch', 'type', 'RunGptLLM', 'dummy.stream_chat', 'list']",8
repos/llama_index/llama-index-legacy/tests/llms/test_rungpt.py:test_stream_complete,test_stream_complete,function,18,43,41,553,12.86,0,0,[],[],[],235,[],"['MagicMock', 'mock_completion_stream', 'iter', 'patch', 'type', 'RunGptLLM', 'dummy.stream_complete', 'list']",8
repos/llama_index/llama-index-legacy/tests/llms/test_vertex.py:test_vertex_call,test_vertex_call,function,5,8,8,90,11.25,0,0,[],[],[],23,[],"['Vertex', 'llm.complete', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/llms/test_vertex.py:test_vertex_generate,test_vertex_generate,function,5,9,9,137,15.22,0,0,[],[],[],30,[],"['Vertex', 'llm.complete', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/llms/test_vertex.py:test_vertex_generate_code,test_vertex_generate_code,function,5,14,14,147,10.5,0,0,[],[],[],37,[],"['Vertex', 'llm.complete', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/llms/test_vertex.py:test_vertex_initialization,test_vertex_initialization,function,5,8,7,84,10.5,0,0,[],[],[],16,[],"['Vertex', 'llm.class_name']",2
repos/llama_index/llama-index-legacy/tests/llms/test_vertex.py:test_vertex_stream,test_vertex_stream,function,4,9,9,101,11.22,0,0,[],[],[],52,[],"['Vertex', 'list', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/llms/test_vllm.py:test_vllm_call,test_vllm_call,function,5,8,8,88,11.0,0,0,[],[],[],17,[],"['Vllm', 'llm.complete', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/llms/test_vllm.py:test_vllm_initialization,test_vllm_initialization,function,3,5,5,41,8.2,0,0,[],[],[],11,[],"['Vllm', 'llm.class_name']",2
repos/llama_index/llama-index-legacy/tests/llms/test_watsonx.py:test_model_basic,test_model_basic,function,13,39,31,457,11.72,0,0,[],[],[],54,[],"['WatsonX', 'llm.complete', 'ChatMessage', 'llm.chat']",4
repos/llama_index/llama-index-legacy/tests/llms/test_watsonx.py:test_model_streaming,test_model_streaming,function,15,43,35,553,12.86,0,0,[],[],[],77,[],"['WatsonX', 'llm.stream_complete', 'list', 'ChatMessage', 'llm.stream_chat']",5
repos/llama_index/llama-index-legacy/tests/llms/test_watsonx.py:MockIBMModelModule,MockIBMModelModule,class,7,58,43,433,7.47,0,0,[],[],[],22,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_watsonx.py:MockStreamResponse,MockStreamResponse,class,3,17,16,98,5.76,0,0,[],[],[],16,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_watsonx.py:MockStreamResponse:__iter__,MockStreamResponse:__iter__,method,2,11,10,54,4.91,0,0,['self'],[None],[None],17,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:mock_chat_stream_iterator,mock_chat_stream_iterator,function,2,3,3,25,8.33,0,0,[],[],[],75,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:test_chat,test_chat,function,9,18,15,238,13.22,0,0,['chat_history'],[' Sequence[ChatMessage]'],[None],148,[],"['MockXinference', 'dummy.chat', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:test_complete,test_complete,function,8,17,15,204,12.0,0,0,[],[],[],177,[],"['MockXinference', 'dummy.complete', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:test_init,test_init,function,14,62,40,825,13.31,0,0,[],[],[],119,[],"['MockXinference', 'isinstance']",2
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:test_stream_chat,test_stream_chat,function,15,44,33,611,13.89,1,0,['chat_history'],[' Sequence[ChatMessage]'],[None],158,[],"['MockXinference', 'dummy.stream_chat', 'enumerate', 'len', 'isinstance']",5
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:test_stream_complete,test_stream_complete,function,14,34,28,403,11.85,1,0,[],[],[],186,[],"['MockXinference', 'dummy.stream_complete', 'enumerate', 'len', 'isinstance']",5
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:MockRESTfulClient,MockRESTfulClient,class,3,6,6,68,11.33,0,0,[],[],[],100,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:MockXinference,MockXinference,class,7,29,27,207,7.14,0,0,[],[],[],105,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:MockXinferenceModel,MockXinferenceModel,class,7,56,42,478,8.54,1,2,[],[],[],79,[],[],0
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:MockRESTfulClient:get_model,MockRESTfulClient:get_model,method,2,2,2,27,13.5,0,0,['self'],[None],[None],101,[],['MockXinferenceModel'],1
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:MockXinference:load_model,MockXinference:load_model,method,6,16,15,124,7.75,0,0,"['self', 'model_uid', 'endpoint', '']","[None, ' str', ' str', None]","[None, None, None, None]",106,[],"['MockRESTfulClient', 'client.get_model']",2
repos/llama_index/llama-index-legacy/tests/llms/test_xinference.py:MockXinferenceModel:chat,MockXinferenceModel:chat,method,6,39,26,334,8.56,1,2,"['self', 'prompt', 'chat_history', 'Any]]', 'generate_config', 'Any]', '']","[None, ' str', ' List[Mapping[str', None, ' Dict[str', None, None]","[None, None, None, None, None, None, None]",80,[],"['isinstance', 'mock_chat_stream_iterator']",2
repos/llama_index/llama-index-legacy/tests/logger/test_base.py:test_logger,test_logger,function,8,36,23,253,7.03,1,0,[],[],[],6,"['    """"""Test logger.""""""\n']","['LlamaLogger', 'range', 'logger.add_log', 'logger.get_logs', 'logger.reset']",5
repos/llama_index/llama-index-legacy/tests/logger/test_base.py:test_logger_metadata,test_logger_metadata,function,9,65,31,462,7.11,3,0,[],[],[],25,"['    """"""Test logger metadata.""""""\n']","['LlamaLogger', 'range', 'logger.add_log', 'logger.set_metadata', 'logger.unset_metadata', 'logger.get_logs']",6
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_dict_save_load,test_dict_save_load,function,7,15,14,226,15.07,0,0,[],[],[],209,[],"['ChatMemoryBuffer.from_defaults', 'memory.to_dict', 'ChatMemoryBuffer.from_dict', 'len']",4
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_get_when_initial_tokens_exceed_limit_raises_value_error,test_get_when_initial_tokens_exceed_limit_raises_value_error,function,7,18,17,211,11.72,0,0,[],[],[],53,[],"['ChatMemoryBuffer.from_defaults', 'pytest.raises', 'memory.get', 'str']",4
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_get_when_initial_tokens_less_than_limit_returns_history,test_get_when_initial_tokens_less_than_limit_returns_history,function,7,15,14,200,13.33,0,0,[],[],[],36,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_get_when_initial_tokens_same_as_limit_removes_message,test_get_when_initial_tokens_same_as_limit_removes_message,function,5,12,12,174,14.5,0,0,[],[],[],66,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_get_when_space_for_all_but_first_message_removes_first_message_and_answer,test_get_when_space_for_all_but_first_message_removes_first_message_and_answer,function,11,32,28,501,15.66,0,0,[') -> (None'],[None],[None],131,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_get_when_space_for_assistant_message_removes_assistant_message_at_start_of_history,test_get_when_space_for_assistant_message_removes_assistant_message_at_start_of_history,function,7,16,15,245,15.31,0,0,[') -> (None'],[None],[None],82,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_get_when_space_for_second_message_and_answer_removes_only_first_message_and_answer,test_get_when_space_for_second_message_and_answer_removes_only_first_message_and_answer,function,10,29,25,431,14.86,0,0,[') -> (None'],[None],[None],102,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_max_tokens,test_max_tokens,function,4,27,18,406,15.04,0,0,[],[],[],174,[],"['ChatMemoryBuffer.from_defaults', 'memory.put', 'len', 'memory.get']",4
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_pickle,test_pickle,function,5,7,7,123,17.57,0,0,[],[],[],222,"['    """"""Unpickleable tiktoken tokenizer should be circumvented when pickling.""""""\n']","['ChatMemoryBuffer.from_defaults', 'pickle.dumps', 'isinstance']",3
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_put_get,test_put_get,function,6,10,9,166,16.6,0,0,[],[],[],24,[],"['ChatMemoryBuffer.from_defaults', 'memory.get', 'len']",3
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_set,test_set,function,4,10,8,187,18.7,0,0,[],[],[],163,[],"['ChatMemoryBuffer.from_defaults', 'memory.put', 'len', 'memory.set']",4
repos/llama_index/llama-index-legacy/tests/memory/test_chat_memory_buffer.py:test_sting_save_load,test_sting_save_load,function,7,15,14,228,15.2,0,0,[],[],[],196,[],"['ChatMemoryBuffer.from_defaults', 'memory.to_string', 'ChatMemoryBuffer.from_string', 'len']",4
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_answer,_mock_answer,function,2,4,4,61,15.25,0,0,['prompt_args'],[' Dict'],[None],86,"['    """"""Mock answer.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_choice_select,_mock_choice_select,function,1,5,5,25,5.0,0,0,['prompt_args'],[' Dict'],[None],141,"['    """"""Mock choice select prompt.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_conversation,_mock_conversation,function,2,4,4,55,13.75,0,0,['prompt_args'],[' Dict'],[None],160,[],[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_decompose_query,_mock_decompose_query,function,2,4,4,61,15.25,0,0,['prompt_args'],[' Dict'],[None],130,"['    """"""Mock decompose query.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_input,_mock_input,function,2,2,2,30,15.0,0,0,['prompt_args'],[' Dict'],[None],125,"['    """"""Mock input prompt.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_insert_predict,_mock_insert_predict,function,1,3,3,16,5.33,0,0,[],[],[],18,"['    """"""Mock insert predict.\n', '\n', '    Used in GPT tree index during insertion\n', '    to select the next node.\n', '\n', '    """"""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_keyword_extract,_mock_keyword_extract,function,2,2,2,57,28.5,0,0,['prompt_args'],[' Dict'],[None],96,"['    """"""Mock keyword extract.""""""\n']",['mock_extract_keywords_response'],1
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_kg_triplet_extract,_mock_kg_triplet_extract,function,2,2,2,25,12.5,0,0,['prompt_args'],[' Dict'],[None],120,"['    """"""Mock kg triplet extract.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_multi_select,_mock_multi_select,function,6,27,16,208,7.7,0,0,['prompt_args'],[' Dict'],[None],50,"['    """"""Mock single select.""""""\n']",['json.dumps'],1
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_pandas,_mock_pandas,function,4,4,4,61,15.25,0,0,['prompt_args'],[' Dict'],[None],135,"['    """"""Mock pandas prompt.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_query_keyword_extract,_mock_query_keyword_extract,function,2,2,2,61,30.5,0,0,['prompt_args'],[' Dict'],[None],101,"['    """"""Mock query keyword extract.""""""\n']",['mock_extract_keywords_response'],1
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_query_select,_mock_query_select,function,1,3,3,16,5.33,0,0,[],[],[],28,"['    """"""Mock query predict.\n', '\n', '    Used in GPT tree index during query traversal\n', '    to select the next node.\n', '\n', '    """"""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_refine,_mock_refine,function,2,4,4,67,16.75,0,0,['prompt_args'],[' Dict'],[None],91,"['    """"""Mock refine.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_schema_extract,_mock_schema_extract,function,2,2,2,25,12.5,0,0,['prompt_args'],[' Dict'],[None],106,"['    """"""Mock schema extract.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_single_select,_mock_single_select,function,2,11,11,56,5.09,0,0,[],[],[],38,"['    """"""Mock single select.""""""\n']",['json.dumps'],1
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_sql_response_synthesis,_mock_sql_response_synthesis,function,2,2,2,37,18.5,0,0,['prompt_args'],[' Dict'],[None],146,"['    """"""Mock sql response synthesis prompt.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_sql_response_synthesis_v2,_mock_sql_response_synthesis_v2,function,2,2,2,32,16.0,0,0,['prompt_args'],[' Dict'],[None],151,"['    """"""Mock sql response synthesis prompt.\n', '\n', '    TODO: deprecate the above\n', '\n', '    """"""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_sub_questions,_mock_sub_questions,function,3,17,17,141,8.29,0,0,[],[],[],72,"['    """"""Mock sub questions.""""""\n']",['json.dumps'],1
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_summary_predict,_mock_summary_predict,function,2,2,2,32,16.0,0,0,['prompt_args'],[' Dict'],[None],13,"['    """"""Mock summary predict.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:_mock_text_to_sql,_mock_text_to_sql,function,7,14,14,149,10.64,0,0,['prompt_args'],[' Dict'],[None],111,"['    """"""Mock text to sql.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:mock_llmpredictor_predict,mock_llmpredictor_predict,function,46,112,53,1901,16.97,0,1,"['prompt', '**prompt_args']","[' BasePromptTemplate', ' Any']","[None, None]",164,"['    """"""Mock predict method of LLMPredictor.\n', '\n', '    Depending on the prompt, return response.\n', '\n', '    """"""\n']","['_mock_summary_predict', '_mock_insert_predict', '_mock_query_select', '_mock_refine', '_mock_answer', '_mock_keyword_extract', '_mock_query_keyword_extract', '_mock_schema_extract', '_mock_text_to_sql', '_mock_kg_triplet_extract', '_mock_input', '_mock_single_select', '_mock_multi_select', '_mock_sub_questions', '_mock_pandas', '_mock_sql_response_synthesis', '_mock_sql_response_synthesis_v2', '_mock_decompose_query', '_mock_choice_select', '_mock_conversation', 'str']",21
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_predict.py:patch_llmpredictor_predict,patch_llmpredictor_predict,function,2,3,3,53,17.67,0,0,"['self', 'prompt', '**prompt_args']","[' Any', ' BasePromptTemplate', ' Any']","[None, None, None]",221,"['    """"""Mock predict method of LLMPredictor.\n', '\n', '    Depending on the prompt, return response.\n', '\n', '    """"""\n']",['mock_llmpredictor_predict'],1
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_text_splitter.py:mock_token_splitter_newline,mock_token_splitter_newline,function,3,7,6,43,6.14,0,1,"['text', 'metadata_str']","[' str', ' Optional[str] ']","[None, ' None']",15,"['    """"""Mock token splitter by newline.""""""\n']",['text.split'],1
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_text_splitter.py:patch_token_splitter_newline,patch_token_splitter_newline,function,3,7,6,43,6.14,0,1,"['self', 'text', 'metadata_str']","[' Any', ' str', ' Optional[str] ']","[None, None, ' None']",6,"['    """"""Mock token splitter by newline.""""""\n']",['text.split'],1
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_utils.py:mock_extract_keywords,mock_extract_keywords,function,2,6,6,92,15.33,0,0,"['text_chunk', 'max_keywords', 'filter_stopwords']","[' str', ' Optional[int] ', ' bool ']","[None, ' None', ' True']",22,"['    """"""Extract keywords (mock).\n', '\n', '    Same as simple_extract_keywords but without filtering stopwords.\n', '\n', '    """"""\n']",['simple_extract_keywords'],1
repos/llama_index/llama-index-legacy/tests/mock_utils/mock_utils.py:mock_tokenizer,mock_tokenizer,function,8,23,22,151,6.57,1,1,['text'],[' str'],[None],11,"['    """"""Mock tokenizer.""""""\n']","['re.split', 'token.strip', 'result.append']",3
repos/llama_index/llama-index-legacy/tests/multi_modal_llms/test_replicate_multi_modal.py:mock_completion,mock_completion,function,1,54,48,738,13.67,0,0,"['*args', '**kwargs']","[' Any', ' Any']","[None, None]",10,[],[],0
repos/llama_index/llama-index-legacy/tests/multi_modal_llms/test_replicate_multi_modal.py:test_completion_model_basic,test_completion_model_basic,function,6,19,19,269,14.16,0,0,['monkeypatch'],[' MonkeyPatch'],[None],40,[],"['monkeypatch.setattr', 'ReplicateMultiModal', 'llm.complete']",3
repos/llama_index/llama-index-legacy/tests/node_parser/metadata_extractor.py:test_metadata_extractor,test_metadata_extractor,function,13,41,33,582,14.2,0,0,['mock_service_context'],[' ServiceContext'],[None],15,[],"['TitleExtractor', 'QuestionsAnsweredExtractor', 'SummaryExtractor', 'KeywordExtractor', 'SentenceSplitter', 'Document', 'run_transformations']",7
repos/llama_index/llama-index-legacy/tests/node_parser/sentence_window.py:test_split_and_window,test_split_and_window,function,10,71,30,481,6.77,0,0,[],[],[],7,[],"['Document', 'SentenceWindowNodeParser.from_defaults', 'node_parser.get_nodes_from_documents', 'len']",4
repos/llama_index/llama-index-legacy/tests/node_parser/test_html.py:test_multiple_tags_splits,test_multiple_tags_splits,function,4,8,8,109,13.62,0,0,[],[],[],74,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_html.py:test_neighbor_tags_splits,test_neighbor_tags_splits,function,4,7,7,104,14.86,0,0,[],[],[],150,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_html.py:test_nesting_tags_splits,test_nesting_tags_splits,function,4,8,8,109,13.62,0,0,[],[],[],113,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_html.py:test_no_splits,test_no_splits,function,4,7,7,105,15.0,0,0,[],[],[],13,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_html.py:test_single_splits,test_single_splits,function,4,7,7,105,15.0,0,0,[],[],[],43,[],"['HTMLNodeParser', 'html_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_json.py:test_split_empty_text,test_split_empty_text,function,6,9,8,136,15.11,0,0,[],[],[],5,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_json.py:test_split_invalid_json,test_split_invalid_json,function,6,12,11,161,13.42,0,0,[],[],[],39,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_json.py:test_split_valid_dict_json,test_split_valid_dict_json,function,7,17,16,204,12.0,0,0,[],[],[],31,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents', 'len']",4
repos/llama_index/llama-index-legacy/tests/node_parser/test_json.py:test_split_valid_json,test_split_valid_json,function,8,28,24,275,9.82,0,0,[],[],[],12,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents', 'len']",4
repos/llama_index/llama-index-legacy/tests/node_parser/test_json.py:test_split_valid_json_defaults,test_split_valid_json_defaults,function,7,17,16,206,12.12,0,0,[],[],[],23,[],"['JSONNodeParser', 'Document', 'json_splitter.get_nodes_from_documents', 'len']",4
repos/llama_index/llama-index-legacy/tests/node_parser/test_markdown.py:test_header_metadata,test_header_metadata,function,4,9,9,117,13.0,0,0,[],[],[],65,[],"['MarkdownNodeParser', 'markdown_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_markdown.py:test_header_splits,test_header_splits,function,4,9,9,117,13.0,0,0,[],[],[],5,[],"['MarkdownNodeParser', 'markdown_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_markdown.py:test_non_header_splits,test_non_header_splits,function,4,9,9,114,12.67,0,0,[],[],[],28,[],"['MarkdownNodeParser', 'markdown_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_markdown.py:test_pre_header_content,test_pre_header_content,function,4,7,7,106,15.14,0,0,[],[],[],46,[],"['MarkdownNodeParser', 'markdown_parser.get_nodes_from_documents', 'Document']",3
repos/llama_index/llama-index-legacy/tests/node_parser/test_markdown_element.py:test_complex_md,test_complex_md,function,2,3,3,28,9.33,0,0,[],[],[],88,[],['Document'],1
repos/llama_index/llama-index-legacy/tests/node_parser/test_markdown_element.py:test_llama2_bad_md,test_llama2_bad_md,function,2,3,3,28,9.33,0,0,[],[],[],227,[],['Document'],1
repos/llama_index/llama-index-legacy/tests/node_parser/test_markdown_element.py:test_md_table_extraction,test_md_table_extraction,function,2,3,3,28,9.33,0,0,[],[],[],8,[],['Document'],1
repos/llama_index/llama-index-legacy/tests/node_parser/test_markdown_element.py:test_md_table_extraction_broken_table,test_md_table_extraction_broken_table,function,2,3,3,28,9.33,0,0,[],[],[],48,[],['Document'],1
repos/llama_index/llama-index-legacy/tests/node_parser/test_semantic_splitter.py:test_grouped_semantically,test_grouped_semantically,function,8,50,31,373,7.46,0,0,[],[],[],8,[],"['Document', 'MockEmbedding', 'SemanticSplitterNodeParser.from_defaults', 'node_parser.get_nodes_from_documents', 'len']",5
repos/llama_index/llama-index-legacy/tests/node_parser/test_semantic_splitter.py:test_split_and_permutated,test_split_and_permutated,function,13,109,41,811,7.44,0,0,[],[],[],26,[],"['Document', 'MockEmbedding', 'SemanticSplitterNodeParser.from_defaults', 'node_parser.sentence_splitter', 'node_parser._build_sentence_groups', 'len']",6
repos/llama_index/llama-index-legacy/tests/node_parser/test_unstructured.py:test_html_table_extraction,test_html_table_extraction,function,8,103,60,1039,10.09,0,0,[],[],[],20,[],"['Document', 'UnstructuredElementNodeParser', 'node_parser.get_nodes_from_documents', 'print', 'len', 'isinstance']",6
repos/llama_index/llama-index-legacy/tests/objects/test_base.py:test_object_index,test_object_index,function,6,24,17,315,13.12,0,0,['mock_service_context'],[' ServiceContext'],[None],11,"['    """"""Test object index.""""""\n']","['SimpleObjectNodeMapping.from_objects', 'ObjectIndex.from_objects', 'obj_index.as_retriever', 'obj_index.insert_object']",4
repos/llama_index/llama-index-legacy/tests/objects/test_base.py:test_object_index_persist,test_object_index_persist,function,11,41,25,834,20.34,0,0,['mock_service_context'],[' ServiceContext'],[None],25,"['    """"""Test object index persist/load.""""""\n']","['SimpleObjectNodeMapping.from_objects', 'ObjectIndex.from_objects', 'obj_index.persist', 'ObjectIndex.from_persist_dir']",4
repos/llama_index/llama-index-legacy/tests/objects/test_base.py:test_object_index_with_tools,test_object_index_with_tools,function,8,26,23,358,13.77,0,0,['mock_service_context'],[' ServiceContext'],[None],53,"['    """"""Test object index with tools.""""""\n']","['FunctionTool.from_defaults', 'SimpleToolNodeMapping.from_objects', 'ObjectIndex.from_objects', 'obj_retriever.as_retriever']",4
repos/llama_index/llama-index-legacy/tests/objects/test_node_mapping.py:test_simple_object_node_mapping,test_simple_object_node_mapping,function,6,24,19,452,18.83,0,0,[],[],[],36,"['    """"""Test simple object node mapping.""""""\n']","['SimpleObjectNodeMapping.from_objects', 'node_mapping.to_node', 'node_mapping.from_node', 'TestObject']",4
repos/llama_index/llama-index-legacy/tests/objects/test_node_mapping.py:test_simple_object_node_mapping_persist,test_simple_object_node_mapping_persist,function,8,12,12,234,19.5,0,0,[],[],[],49,"['    """"""Test persist/load.""""""\n']","['SimpleObjectNodeMapping.from_objects', 'node_mapping.persist', 'SimpleObjectNodeMapping.from_persist_dir']",3
repos/llama_index/llama-index-legacy/tests/objects/test_node_mapping.py:test_sql_table_node_mapping_to_node,test_sql_table_node_mapping_to_node,function,16,35,31,444,12.69,2,0,['mocker'],[' MockerFixture'],[None],92,"['    """"""Test to add node for sql table node mapping object to ensure no \'None\' values in metadata output to avoid issues with nulls when upserting to indexes.""""""\n']","['mocker.patch', 'SQLTableSchema', 'TestSQLDatabase', 'SQLTableNodeMapping', 'mapping.to_node', 'nodes.append']",6
repos/llama_index/llama-index-legacy/tests/objects/test_node_mapping.py:test_tool_object_node_mapping,test_tool_object_node_mapping,function,9,77,45,887,11.52,0,0,[],[],[],59,"['    """"""Test tool object node mapping.""""""\n']","['FunctionTool.from_defaults', 'SimpleToolNodeMapping.from_objects', 'node_mapping.to_node', 'node_mapping.from_node', 'recon_tool2', 'node_mapping.add_object']",6
repos/llama_index/llama-index-legacy/tests/objects/test_node_mapping.py:TestObject,TestObject,class,8,16,13,132,8.25,0,0,[],[],[],15,[],[],0
repos/llama_index/llama-index-legacy/tests/objects/test_node_mapping.py:TestSQLDatabase,TestSQLDatabase,class,1,5,5,29,5.8,0,0,[],[],[],29,[],[],0
repos/llama_index/llama-index-legacy/tests/objects/test_node_mapping.py:TestObject:__hash__,TestObject:__hash__,method,2,2,2,21,10.5,0,0,['self'],[None],[None],22,[],['hash'],1
repos/llama_index/llama-index-legacy/tests/objects/test_node_mapping.py:TestObject:__str__,TestObject:__str__,method,2,2,2,39,19.5,0,0,['self'],[None],[None],25,[],"['f""TestObject']",1
repos/llama_index/llama-index-legacy/tests/objects/test_node_mapping.py:TestSQLDatabase:__init__,TestSQLDatabase:__init__,method,0,1,1,4,4.0,0,0,['self'],[None],[None],32,[],[],0
repos/llama_index/llama-index-legacy/tests/output_parsers/test_base.py:test_lc_output_parser,test_lc_output_parser,function,13,65,45,751,11.55,0,0,[],[],[],19,"['    """"""Test langchain output parser.""""""\n']","['MockOutputParser', 'get_format_instructions', 'parse', 'ResponseSchema', 'LangchainOutputParser', 'output_parser.format']",6
repos/llama_index/llama-index-legacy/tests/output_parsers/test_pydantic.py:test_pydantic,test_pydantic,function,15,45,36,502,11.16,0,0,[],[],[],19,"['    """"""Test pydantic output parser.""""""\n']","['PydanticOutputParser', 'parser.parse', 'isinstance', 'pytest.raises']",4
repos/llama_index/llama-index-legacy/tests/output_parsers/test_pydantic.py:test_pydantic_format,test_pydantic_format,function,5,12,10,140,11.67,0,0,[],[],[],47,"['    """"""Test pydantic format.""""""\n']","['PydanticOutputParser', 'parser.format']",2
repos/llama_index/llama-index-legacy/tests/output_parsers/test_pydantic.py:AttrDict,AttrDict,class,3,4,4,21,5.25,0,0,[],[],[],8,[],[],0
repos/llama_index/llama-index-legacy/tests/output_parsers/test_pydantic.py:TestModel,TestModel,class,5,6,6,43,7.17,0,0,[],[],[],13,[],[],0
repos/llama_index/llama-index-legacy/tests/output_parsers/test_selection.py:output_parser,output_parser,function,2,2,2,29,14.5,0,0,[],[],[],7,[],['SelectionOutputParser'],1
repos/llama_index/llama-index-legacy/tests/output_parsers/test_selection.py:test_format,test_format,function,4,13,13,152,11.69,0,0,['output_parser'],[' SelectionOutputParser'],[None],11,[],"['output_parser.format', 'new_test_template.format']",2
repos/llama_index/llama-index-legacy/tests/output_parsers/test_utils.py:test_extract_json_str,test_extract_json_str,function,1,2,2,10,5.0,0,0,[],[],[],4,[],[],0
repos/llama_index/llama-index-legacy/tests/param_tuner/test_base.py:_mock_obj_function,_mock_obj_function,function,3,7,7,107,15.29,0,0,['param_dict'],[' Dict'],[None],8,"['    """"""Mock obj function.""""""\n']","['RunResult', 'score=int', 'int']",3
repos/llama_index/llama-index-legacy/tests/param_tuner/test_base.py:test_param_tuner,test_param_tuner,function,9,29,27,315,10.86,0,0,[],[],[],28,"['    """"""Test param tuner.""""""\n']","['ParamTuner', 'tuner.tune']",2
repos/llama_index/llama-index-legacy/tests/playground/test_base.py:test_from_docs,test_from_docs,function,7,35,31,489,13.97,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",117,"['    """"""Test initialization via a list of documents.""""""\n']","['MockEmbedding', 'Document', 'Playground.from_docs', 'len', 'pytest.raises']",5
repos/llama_index/llama-index-legacy/tests/playground/test_base.py:test_get_set_compare,test_get_set_compare,function,9,52,39,727,13.98,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",83,"['    """"""Test basic comparison of indices.""""""\n']","['MockEmbedding', 'VectorStoreIndex.from_documents', 'SummaryIndex.from_documents', 'TreeIndex.from_documents', 'Playground', 'len', 'playground.compare']",7
repos/llama_index/llama-index-legacy/tests/playground/test_base.py:test_validation,test_validation,function,4,32,16,337,10.53,0,0,[],[],[],142,"['    """"""Test validation of indices and modes.""""""\n']","['pytest.raises', 'Playground']",2
repos/llama_index/llama-index-legacy/tests/playground/test_base.py:MockEmbedding,MockEmbedding,class,13,285,58,1388,4.87,0,2,[],[],[],19,[],[],0
repos/llama_index/llama-index-legacy/tests/playground/test_base.py:MockEmbedding:_get_query_embedding,MockEmbedding:_get_query_embedding,method,3,8,7,26,3.25,0,0,"['self', 'query']","[None, ' str']","[None, None]",77,"['        """"""Mock get query embedding.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/playground/test_base.py:MockEmbedding:_get_text_embedding,MockEmbedding:_get_text_embedding,method,5,118,39,524,4.44,0,1,"['self', 'text']","[None, ' str']","[None, None]",52,"['        """"""Mock get text embedding.""""""\n']","['text.strip', 'print', 'ValueError']",3
repos/llama_index/llama-index-legacy/tests/playground/test_base.py:MockEmbedding:class_name,MockEmbedding:class_name,method,1,2,2,21,10.5,0,0,['cls'],[None],[None],21,[],[],0
repos/llama_index/llama-index-legacy/tests/postprocessor/test_base.py:test_embedding_recency_postprocessor,test_embedding_recency_postprocessor,function,10,85,53,1082,12.73,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",196,"['    """"""Test fixed recency processor.""""""\n']","['TextNode', 'EmbeddingRecencyPostprocessor', 'QueryBundle', 'postprocessor.postprocess_nodes', 'cast']",5
repos/llama_index/llama-index-legacy/tests/postprocessor/test_base.py:test_fixed_recency_postprocessor,test_fixed_recency_postprocessor,function,8,75,52,907,12.09,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",149,"['    """"""Test fixed recency processor.""""""\n']","['TextNode', 'FixedRecencyPostprocessor', 'QueryBundle', 'postprocessor.postprocess_nodes', 'len']",5
repos/llama_index/llama-index-legacy/tests/postprocessor/test_base.py:test_forward_back_processor,test_forward_back_processor,function,21,228,85,3159,13.86,1,2,['tmp_path'],[' Path'],[None],33,"['    """"""Test forward-back processor.""""""\n']","['TextNode', 'enumerate', 'RelatedNodeInfo', 'len', 'SimpleDocumentStore', 'docstore.add_documents', 'PrevNextNodePostprocessor', 'node_postprocessor.postprocess_nodes', 'pytest.raises']",9
repos/llama_index/llama-index-legacy/tests/postprocessor/test_base.py:test_keyword_postprocessor,test_keyword_postprocessor,function,10,107,49,1305,12.2,0,0,[],[],[],302,"['    """"""Test keyword processor.""""""\n']","['TextNode', 'KeywordNodePostprocessor', 'postprocessor.postprocess_nodes', 'len']",4
repos/llama_index/llama-index-legacy/tests/postprocessor/test_base.py:test_keyword_postprocessor_for_non_english,test_keyword_postprocessor_for_non_english,function,10,83,47,1362,16.41,0,0,[],[],[],339,"['    """"""Test keyword processor for non English.""""""\n']","['TextNode', 'KeywordNodePostprocessor', 'postprocessor.postprocess_nodes', 'len']",4
repos/llama_index/llama-index-legacy/tests/postprocessor/test_base.py:test_time_weighted_postprocessor,test_time_weighted_postprocessor,function,9,129,57,1402,10.87,0,0,[],[],[],256,"['    """"""Test time weighted processor.""""""\n']","['TextNode', 'TimeWeightedPostprocessor', 'postprocessor.postprocess_nodes', 'len', 'cast', 'NodeWithScore', 'enumerate']",7
repos/llama_index/llama-index-legacy/tests/postprocessor/test_llm_rerank.py:mock_format_node_batch_fn,mock_format_node_batch_fn,function,1,6,6,51,8.5,0,0,['nodes'],[' List[BaseNode]'],[None],44,"['    """"""Mock format node batch fn.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/postprocessor/test_llm_rerank.py:mock_llmpredictor_predict,mock_llmpredictor_predict,function,16,58,47,520,8.97,1,1,"['self', 'prompt', '**prompt_args']","[' Any', ' BasePromptTemplate', ' Any']","[None, None, None]",18,"['    """"""Patch llm predictor predict.""""""\n']","['context_str.split', 'enumerate', 'choices_and_scores.append']",3
repos/llama_index/llama-index-legacy/tests/postprocessor/test_llm_rerank.py:test_llm_rerank,test_llm_rerank,function,10,44,40,674,15.32,0,0,['mock_service_context'],[' ServiceContext'],[None],54,"['    """"""Test LLM rerank.""""""\n']","['TextNode', 'LLMRerank', 'llm_rerank.postprocess_nodes', 'QueryBundle', 'len']",5
repos/llama_index/llama-index-legacy/tests/postprocessor/test_longcontext_reorder.py:test_long_context_reorder,test_long_context_reorder,function,13,58,44,875,15.09,2,0,[],[],[],7,[],"['NodeWithScore', 'sorted', 'LongContextReorder', 'lcr.postprocess_nodes', 'set']",5
repos/llama_index/llama-index-legacy/tests/postprocessor/test_metadata_replacement.py:test_metadata_replacement,test_metadata_replacement,function,7,31,23,315,10.16,0,0,[],[],[],5,[],"['TextNode', 'MetadataReplacementPostProcessor', 'postprocessor.postprocess_nodes', 'len']",4
repos/llama_index/llama-index-legacy/tests/postprocessor/test_optimizer.py:mock_get_text_embedding,mock_get_text_embedding,function,3,51,20,242,4.75,0,1,['text'],[' str'],[None],23,"['    """"""Mock get text embedding.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-legacy/tests/postprocessor/test_optimizer.py:mock_get_text_embedding_chinese,mock_get_text_embedding_chinese,function,3,52,21,243,4.67,0,1,['text'],[' str'],[None],45,"['    """"""Mock get text embedding.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-legacy/tests/postprocessor/test_optimizer.py:mock_get_text_embeddings,mock_get_text_embeddings,function,1,6,6,51,8.5,0,0,['texts'],[' List[str]'],[None],40,"['    """"""Mock get text embeddings.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/postprocessor/test_optimizer.py:mock_get_text_embeddings_chinese,mock_get_text_embeddings_chinese,function,1,6,6,59,9.83,0,0,['texts'],[' List[str]'],[None],62,"['    """"""Mock get text embeddings.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/postprocessor/test_optimizer.py:mock_tokenizer_fn,mock_tokenizer_fn,function,2,3,3,20,6.67,0,0,['text'],[' str'],[None],11,"['    """"""Mock tokenizer function.""""""\n']",['text.split'],1
repos/llama_index/llama-index-legacy/tests/postprocessor/test_optimizer.py:mock_tokenizer_fn2,mock_tokenizer_fn2,function,2,2,2,21,10.5,0,0,['text'],[' str'],[None],17,"['    """"""Mock tokenizer function.""""""\n']",['text.split'],1
repos/llama_index/llama-index-legacy/tests/postprocessor/test_optimizer.py:test_optimizer,test_optimizer,function,9,125,39,1839,14.71,0,0,"['_mock_embeds', '_mock_embed']","[' Any', ' Any']","[None, None]",73,"['    """"""Test optimizer.""""""\n']","['SentenceEmbeddingOptimizer', 'QueryBundle', 'TextNode', 'optimizer.postprocess_nodes']",4
repos/llama_index/llama-index-legacy/tests/program/test_guidance.py:test_guidance_pydantic_program,test_guidance_pydantic_program,function,9,25,23,317,12.68,0,0,[],[],[],13,[],"['TestModel', 'GuidancePydanticProgram', 'guidance_llm=MockLLM', 'pytest.raises', 'program']",5
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:test_llm_program,test_llm_program,function,7,23,21,327,14.22,0,0,[],[],[],50,"['    """"""Test LLM program.""""""\n']","['PydanticOutputParser', 'LLMTextCompletionProgram.from_defaults', 'llm=MockLLM', 'llm_program', 'isinstance']",5
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:test_llm_program_with_messages,test_llm_program_with_messages,function,10,21,20,380,18.1,0,0,[],[],[],64,"['    """"""Test LLM program.""""""\n']","['ChatPromptTemplate', 'PydanticOutputParser', 'LLMTextCompletionProgram.from_defaults', 'llm=MockLLM', 'llm_program', 'isinstance']",6
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:test_llm_program_with_messages_and_chat,test_llm_program_with_messages_and_chat,function,10,21,20,383,18.24,0,0,[],[],[],80,"['    """"""Test LLM program.""""""\n']","['ChatPromptTemplate', 'PydanticOutputParser', 'LLMTextCompletionProgram.from_defaults', 'llm=MockChatLLM', 'llm_program', 'isinstance']",6
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:MockChatLLM,MockChatLLM,class,9,27,23,288,10.67,0,0,[],[],[],30,[],[],0
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:MockLLM,MockLLM,class,8,20,17,206,10.3,0,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:TestModel,TestModel,class,3,4,4,24,6.0,0,0,[],[],[],45,[],[],0
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:MockChatLLM:chat,MockChatLLM:chat,method,5,10,10,140,14.0,0,0,"['self', 'prompt']","[None, ' str']","[None, None]",31,[],"['json.dumps', 'ChatResponse', 'message=ChatMessage']",3
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:MockChatLLM:metadata,MockChatLLM:metadata,method,4,6,5,65,10.83,0,0,['self'],[None],[None],39,[],['LLMMetadata'],1
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:MockLLM:complete,MockLLM:complete,method,5,7,7,94,13.43,0,0,"['self', 'prompt']","[None, ' str']","[None, None]",20,[],"['json.dumps', 'CompletionResponse']",2
repos/llama_index/llama-index-legacy/tests/program/test_llm_program.py:MockLLM:metadata,MockLLM:metadata,method,2,2,2,19,9.5,0,0,['self'],[None],[None],26,[],['LLMMetadata'],1
repos/llama_index/llama-index-legacy/tests/program/test_lmformatenforcer.py:test_lmformatenforcer_pydantic_program,test_lmformatenforcer_pydantic_program,function,16,38,35,472,12.42,0,0,[],[],[],16,[],"['TestModel', 'MagicMock', 'CompletionResponse', 'LMFormatEnforcerPydanticProgram', 'program', 'isinstance']",6
repos/llama_index/llama-index-legacy/tests/program/test_multi_modal_llm_program.py:test_multi_modal_llm_program,test_multi_modal_llm_program,function,8,24,22,414,17.25,0,0,[],[],[],35,"['    """"""Test Multi Modal LLM Pydantic program.""""""\n']","['PydanticOutputParser', 'MultiModalLLMCompletionProgram.from_defaults', 'multi_modal_llm=MockMultiModalLLM', 'multi_modal_llm_program', 'isinstance']",5
repos/llama_index/llama-index-legacy/tests/program/test_multi_modal_llm_program.py:MockMultiModalLLM,MockMultiModalLLM,class,8,24,21,268,11.17,0,0,[],[],[],17,[],[],0
repos/llama_index/llama-index-legacy/tests/program/test_multi_modal_llm_program.py:TestModel,TestModel,class,3,4,4,24,6.0,0,0,[],[],[],30,[],[],0
repos/llama_index/llama-index-legacy/tests/program/test_multi_modal_llm_program.py:MockMultiModalLLM:complete,MockMultiModalLLM:complete,method,5,7,7,94,13.43,0,0,"['self', 'prompt', 'image_documents']","[None, ' str', ' Sequence[ImageDocument]']","[None, None, None]",18,[],"['json.dumps', 'CompletionResponse']",2
repos/llama_index/llama-index-legacy/tests/program/test_multi_modal_llm_program.py:MockMultiModalLLM:metadata,MockMultiModalLLM:metadata,method,2,2,2,29,14.5,0,0,['self'],[None],[None],26,[],['MultiModalLLMMetadata'],1
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:output_parser,output_parser,function,2,2,2,58,29.0,0,0,[],[],[],48,[],['MockOutputParser'],1
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:test_chat_template,test_chat_template,function,10,59,42,620,10.51,0,0,[],[],[],75,[],"['ChatPromptTemplate', 'ChatMessage', 'chat_template.partial_format', 'partial_template.format_messages', 'partial_template.format']",5
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:test_chat_template_output_parser,test_chat_template_output_parser,function,8,38,30,440,11.58,0,0,['output_parser'],[' BaseOutputParser'],[None],101,[],"['ChatPromptTemplate', 'ChatMessage', 'chat_template.format_messages']",3
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:test_function_mappings,test_function_mappings,function,24,136,93,1572,11.56,0,0,[],[],[],266,"['    """"""Test function mappings.""""""\n']","['_format_abc', 'PromptTemplate', 'test_prompt.format', 'test_prompt.partial_format', 'test_prompt_partial.format', '_format_abc_2', 'test_prompt_2.format', 'pytest.raises', '_format_prompt_key1', 'test_prompt_3.format', 'ChatPromptTemplate', 'ChatMessage', 'chat_template.format']",13
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:test_langchain_selector_template,test_langchain_selector_template,function,18,43,40,609,14.16,0,0,[],[],[],177,[],"['FakeListLLM', 'LangChainLLM', 'is_mock', 'LangchainTemplate.from_template', 'LangchainSelector', 'LangchainPromptTemplate', 'template.partial_format', 'isinstance', 'template_fmt.format']",9
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:test_langchain_template,test_langchain_template,function,13,37,32,589,15.92,0,0,[],[],[],155,[],"['LangchainTemplate.from_template', 'LangchainPromptTemplate', 'template.partial_format', 'isinstance', 'template_fmt.format', 'template_fmt.format_messages', 'ChatMessage', 'template_2.partial_format', 'template_2_partial.format']",9
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:test_selector_template,test_selector_template,function,17,62,48,762,12.29,0,0,[],[],[],123,[],"['PromptTemplate', 'ChatPromptTemplate', 'ChatMessage', 'SelectorPromptTemplate', 'isinstance', 'selector_template.partial_format', 'partial_template.format', 'partial_template.format_messages']",8
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:test_template,test_template,function,8,24,20,317,13.21,0,0,[],[],[],52,"['    """"""Test partial format.""""""\n']","['PromptTemplate', 'prompt.partial_format', 'isinstance', 'prompt_fmt.format', 'prompt_fmt.format_messages', 'ChatMessage']",6
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:test_template_output_parser,test_template_output_parser,function,5,15,13,194,12.93,0,0,['output_parser'],[' BaseOutputParser'],[None],67,[],"['PromptTemplate', 'prompt.format']",2
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:test_template_var_mappings,test_template_var_mappings,function,1,2,2,19,9.5,0,0,[],[],[],200,"['    """"""Test template variable mappings.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:MockOutputParser,MockOutputParser,class,7,27,19,201,7.44,0,0,[],[],[],34,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:MockOutputParser:__init__,MockOutputParser:__init__,method,2,2,2,33,16.5,0,0,"['self', 'format_string']","[None, ' str']","[None, None]",37,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:MockOutputParser:format,MockOutputParser:format,method,2,4,4,36,9.0,0,0,"['self', 'query']","[None, ' str']","[None, None]",43,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_base.py:MockOutputParser:parse,MockOutputParser:parse,method,1,3,3,23,7.67,0,0,"['self', 'output']","[None, ' str']","[None, None]",40,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_guidance_utils.py:test_convert_pydantic_to_guidance_output_template_nested,test_convert_pydantic_to_guidance_output_template_nested,function,3,5,4,102,20.4,0,0,[],[],[],51,[],['pydantic_to_guidance_output_template'],1
repos/llama_index/llama-index-legacy/tests/prompts/test_guidance_utils.py:test_convert_pydantic_to_guidance_output_template_simple,test_convert_pydantic_to_guidance_output_template_simple,function,3,5,4,102,20.4,0,0,[],[],[],46,[],['pydantic_to_guidance_output_template'],1
repos/llama_index/llama-index-legacy/tests/prompts/test_guidance_utils.py:test_convert_to_handlebars,test_convert_to_handlebars,function,3,23,16,165,7.17,0,0,[],[],[],10,[],['convert_to_handlebars'],1
repos/llama_index/llama-index-legacy/tests/prompts/test_guidance_utils.py:TestNestedModel,TestNestedModel,class,3,4,4,42,10.5,0,0,[],[],[],31,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_guidance_utils.py:TestSimpleModel,TestSimpleModel,class,4,6,6,34,5.67,0,0,[],[],[],17,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:test_prompt_mixin,test_prompt_mixin,function,7,48,25,585,12.19,0,0,[],[],[],49,[],"['MockObject1', 'mock_obj1.get_prompts', 'PromptTemplate', 'mock_obj1.update_prompts']",4
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject1,MockObject1,class,10,45,34,498,11.07,0,2,[],[],[],28,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject2,MockObject2,class,8,34,26,320,9.41,0,1,[],[],[],11,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject1:__init__,MockObject1:__init__,method,4,10,10,131,13.1,0,0,['self'],[None],[None],12,[],"['MockObject2', 'PromptTemplate']",2
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject1:_get_prompt_modules,MockObject1:_get_prompt_modules,method,1,3,3,42,14.0,0,0,['self'],[None],[None],20,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject1:_get_prompts,MockObject1:_get_prompts,method,2,2,2,25,12.5,0,0,['self'],[None],[None],17,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject1:_update_prompts,MockObject1:_update_prompts,method,1,12,9,131,10.92,0,2,"['self', 'prompts']","[None, ' PromptDictType']","[None, None]",23,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject2:__init__,MockObject2:__init__,method,2,6,6,59,9.83,0,0,['self'],[None],[None],12,[],['PromptTemplate'],1
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject2:_get_prompt_modules,MockObject2:_get_prompt_modules,method,1,2,2,8,4.0,0,0,['self'],[None],[None],20,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject2:_get_prompts,MockObject2:_get_prompts,method,2,2,2,25,12.5,0,0,['self'],[None],[None],17,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_mixin.py:MockObject2:_update_prompts,MockObject2:_update_prompts,method,1,6,6,59,9.83,0,1,"['self', 'prompts']","[None, ' PromptDictType']","[None, None]",23,[],[],0
repos/llama_index/llama-index-legacy/tests/prompts/test_utils.py:test_get_template_vars,test_get_template_vars,function,3,10,9,105,10.5,0,0,[],[],[],4,[],['get_template_vars'],1
repos/llama_index/llama-index-legacy/tests/query_engine/test_cogniswitch_query_engine.py:query_engine,query_engine,function,2,6,6,90,15.0,0,0,[],[],[],12,[],['CogniswitchQueryEngine'],1
repos/llama_index/llama-index-legacy/tests/query_engine/test_cogniswitch_query_engine.py:test_query_knowledge_successful,test_query_knowledge_successful,function,6,19,18,235,12.37,0,0,"['mock_post', 'query_engine']","[' Any', ' CogniswitchQueryEngine']","[None, None]",19,[],"['query_engine.query_knowledge', 'isinstance']",2
repos/llama_index/llama-index-legacy/tests/query_engine/test_cogniswitch_query_engine.py:test_query_knowledge_unsuccessful,test_query_knowledge_unsuccessful,function,6,17,15,231,13.59,0,0,"['mock_post', 'query_engine']","[' Any', ' CogniswitchQueryEngine']","[None, None]",30,[],"['query_engine.query_knowledge', 'isinstance']",2
repos/llama_index/llama-index-legacy/tests/query_engine/test_pandas.py:test_pandas_query_engine,test_pandas_query_engine,function,0,2,2,15,7.5,0,0,['mock_service_context'],[' ServiceContext'],[None],22,"['    """"""Test pandas query engine.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/query_engine/test_retriever_query_engine.py:test_query_engine_falls_back_to_inheriting_retrievers_service_context,test_query_engine_falls_back_to_inheriting_retrievers_service_context,function,15,69,37,1486,21.54,0,0,[],[],[],23,[],"['OpenAI', 'ServiceContext.from_defaults', 'TreeIndex.from_documents', 'TreeSelectLeafRetriever', 'RetrieverQueryEngine', 'Anthropic']",6
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:bar_fn,bar_fn,function,2,4,4,23,5.75,0,0,"['a', 'b']","[' Any', ' Any']","[None, None]",33,"['    """"""Bar function.""""""\n']",['str'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:foo_fn,foo_fn,function,4,4,4,11,2.75,0,0,"['a', 'b', 'c']","[' int', ' int ', ' int ']","[None, ' 1', ' 2']",28,"['    """"""Foo function.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:sum_fn,sum_fn,function,2,2,2,12,6.0,0,0,['a'],[' List[int]'],[None],38,"['    """"""Mock list function.""""""\n']",['sum'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:test_arg_component,test_arg_component,function,8,17,15,167,9.82,0,0,[],[],[],86,"['    """"""Test arg component.""""""\n']","['ArgPackComponent', 'arg_c.run_component', 'FnComponent', 'QueryPipeline', 'p.run']",5
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:test_fn_components,test_fn_components,function,7,39,27,539,13.82,0,0,[],[],[],43,"['    """"""Test components.""""""\n']","['FnComponent', 'foo_c.run_component', 'pytest.raises', 'bar_c.run_component']",4
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:test_fn_pipeline,test_fn_pipeline,function,8,37,29,412,11.14,0,0,[],[],[],66,"['    """"""Test pipeline with function components.""""""\n']","['QueryPipeline', 'FnComponent', 'p.run', 'p2.add_modules', 'InputComponent', 'p2.add_link', 'p2.run']",7
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:test_kwarg_component,test_kwarg_component,function,11,29,28,281,9.69,0,0,[],[],[],97,"['    """"""Test kwarg component.""""""\n']","['KwargPackComponent', 'arg_c.run_component', 'convert_fn', 'list', 'FnComponent', 'QueryPipeline', 'p.run']",7
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:test_selector_component,test_selector_component,function,15,40,33,490,12.25,0,0,[],[],[],137,"['    """"""Test selector component.""""""\n']","['bar1_fn', 'str', 'bar2_fn', 'MockSelector', 'RouterComponent', 'FnComponent', 'router.run_component', 'SelectorComponent', 'selector_c.run_component', 'SingleSelection']",10
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:MockSelector,MockSelector,class,8,42,27,405,9.64,0,0,[],[],[],113,[],[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:MockSelector:_get_prompts,MockSelector:_get_prompts,method,1,2,2,8,4.0,0,0,['self'],[None],[None],129,"['        """"""Get prompts.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:MockSelector:_select,MockSelector:_select,method,2,6,6,87,14.5,0,0,"['self', 'choices', 'query']","[None, ' Sequence[ToolMetadata]', ' QueryBundle']","[None, None, None]",116,"['        """"""Select.""""""\n']",['MultiSelection'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_components.py:MockSelector:_update_prompts,MockSelector:_update_prompts,method,0,0,0,0,0.0,0,0,[],[],[],133,"['        """"""Update prompts.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:test_query_pipeline_chain,test_query_pipeline_chain,function,4,8,7,90,11.25,0,0,[],[],[],137,"['    """"""Test query pipeline.""""""\n']","['QueryPipeline', 'p.run']",2
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:test_query_pipeline_chain_str,test_query_pipeline_chain_str,function,9,39,33,375,9.62,0,0,[],[],[],341,"['    """"""Test add_chain with only module strings.""""""\n']","['QueryPipeline', 'InputComponent', 'QueryComponent3', 'QueryComponent1', 'p.add_links', 'Link', 'p.add_chain', 'p.run']",8
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:test_query_pipeline_conditional_edges,test_query_pipeline_conditional_edges,function,13,83,60,739,8.9,0,1,[],[],[],364,"['    """"""Test conditional edges.""""""\n']","['choose_fn', 'QueryPipeline', 'InputComponent', 'FnComponent', 'QueryComponent1', 'QueryComponent2', 'p.add_links', 'Link', 'p.run']",9
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:test_query_pipeline_init,test_query_pipeline_init,function,13,90,49,893,9.92,0,0,[],[],[],290,"['    """"""Test query pipeline init params.""""""\n']","['QueryComponent1', 'QueryComponent2', 'InputComponent', 'QueryPipeline', 'Link', 'p.run', 'p.add_modules', 'p.add_links']",8
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:test_query_pipeline_input_component,test_query_pipeline_input_component,function,12,35,27,390,11.14,0,0,[],[],[],155,"['    """"""Test query pipeline input component.""""""\n']","['QueryComponent1', 'QueryComponent2', 'InputComponent', 'QueryPipeline', 'p.add_modules', 'p.add_link', 'p.run']",7
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:test_query_pipeline_multi,test_query_pipeline_multi,function,11,38,32,357,9.39,0,0,[],[],[],220,"['    """"""Test query pipeline.""""""\n']","['QueryComponent1', 'QueryComponent2', 'QueryPipeline', 'p.add_modules', 'p.add_link', 'p.run_multi']",6
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:test_query_pipeline_partial,test_query_pipeline_partial,function,14,48,33,599,12.48,0,0,[],[],[],176,"['    """"""Test query pipeline.""""""\n']","['QueryComponent1', 'QueryComponent2', 'qc2.partial', 'QueryPipeline', 'p.run', 'p.add_modules', 'p.add_link', 'Chainable2']",8
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:test_query_pipeline_single_arg_inp,test_query_pipeline_single_arg_inp,function,4,8,7,93,11.62,0,0,[],[],[],146,"['    """"""Test query pipeline with single arg input (no kwargs).""""""\n']","['QueryPipeline', 'QueryComponent3', 'p.run']",3
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:test_query_pipeline_sub,test_query_pipeline_sub,function,11,24,21,254,10.58,0,0,[],[],[],205,"['    """"""Test query pipeline.""""""\n']","['QueryComponent2', 'QueryComponent3', 'QueryPipeline', 'p.add_modules', 'p.add_link', 'p.run']",6
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:Chainable2,Chainable2,class,3,8,8,84,10.5,0,0,[],[],[],129,[],[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent1,QueryComponent1,class,13,70,40,608,8.69,0,2,[],[],[],20,[],[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent2,QueryComponent2,class,13,69,39,615,8.91,0,2,[],[],[],57,[],[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent3,QueryComponent3,class,13,59,38,537,9.1,0,1,[],[],[],94,[],[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:Chainable2:_as_query_component,Chainable2:_as_query_component,method,2,2,2,23,11.5,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",132,"['        """"""Get query component.""""""\n']",['QueryComponent2'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent1:_run_component,QueryComponent1:_run_component,method,1,4,4,50,12.5,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",38,"['        """"""Run component.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent1:_validate_component_inputs,QueryComponent1:_validate_component_inputs,method,3,22,12,127,5.77,0,2,"['self', 'input', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",30,"['        """"""Validate component inputs during run_component.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent1:input_keys,QueryComponent1:input_keys,method,2,3,3,46,15.33,0,0,['self'],[None],[None],47,"['        """"""Input keys.""""""\n']",['InputKeys.from_keys'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent1:output_keys,QueryComponent1:output_keys,method,2,2,2,38,19.0,0,0,['self'],[None],[None],52,"['        """"""Output keys.""""""\n']",['OutputKeys.from_keys'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent1:set_callback_manager,QueryComponent1:set_callback_manager,method,12,64,37,550,8.59,0,2,"['self', 'callback_manager']","[None, ' Any']","[None, None]",27,"['        """"""Set callback manager.""""""\n']","['_validate_component_inputs', 'ValueError', '_run_component', '_arun_component', 'self._run_component', 'input_keys', 'InputKeys.from_keys', 'output_keys', 'OutputKeys.from_keys']",9
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent2:_run_component,QueryComponent2:_run_component,method,1,3,3,57,19.0,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",38,"['        """"""Run component.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent2:_validate_component_inputs,QueryComponent2:_validate_component_inputs,method,3,22,12,127,5.77,0,2,"['self', 'input', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",30,"['        """"""Validate component inputs during run_component.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent2:input_keys,QueryComponent2:input_keys,method,2,3,3,46,15.33,0,0,['self'],[None],[None],47,"['        """"""Input keys.""""""\n']",['InputKeys.from_keys'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent2:output_keys,QueryComponent2:output_keys,method,2,2,2,38,19.0,0,0,['self'],[None],[None],52,"['        """"""Output keys.""""""\n']",['OutputKeys.from_keys'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent2:set_callback_manager,QueryComponent2:set_callback_manager,method,12,63,36,557,8.84,0,2,"['self', 'callback_manager']","[None, ' Any']","[None, None]",27,"['        """"""Set callback manager.""""""\n']","['_validate_component_inputs', 'ValueError', '_run_component', '_arun_component', 'self._run_component', 'input_keys', 'InputKeys.from_keys', 'output_keys', 'OutputKeys.from_keys']",9
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent3:_run_component,QueryComponent3:_run_component,method,1,4,4,48,12.0,0,0,"['self', '**kwargs']","[None, ' Any']","[None, None]",110,"['        """"""Run component.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent3:_validate_component_inputs,QueryComponent3:_validate_component_inputs,method,3,12,10,67,5.58,0,1,"['self', 'input', 'Any]']","[None, ' Dict[str', None]","[None, None, None]",30,"['        """"""Validate component inputs during run_component.""""""\n']",['ValueError'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent3:input_keys,QueryComponent3:input_keys,method,2,2,2,36,18.0,0,0,['self'],[None],[None],47,"['        """"""Input keys.""""""\n']",['InputKeys.from_keys'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent3:output_keys,QueryComponent3:output_keys,method,2,2,2,38,19.0,0,0,['self'],[None],[None],52,"['        """"""Output keys.""""""\n']",['OutputKeys.from_keys'],1
repos/llama_index/llama-index-legacy/tests/query_pipeline/test_query.py:QueryComponent3:set_callback_manager,QueryComponent3:set_callback_manager,method,12,53,35,479,9.04,0,1,"['self', 'callback_manager']","[None, ' Any']","[None, None]",27,"['        """"""Set callback manager.""""""\n']","['_validate_component_inputs', 'ValueError', '_run_component', '_arun_component', 'self._run_component', 'input_keys', 'InputKeys.from_keys', 'output_keys', 'OutputKeys.from_keys']",9
repos/llama_index/llama-index-legacy/tests/question_gen/test_guidance_generator.py:test_guidance_question_generator,test_guidance_question_generator,function,7,15,15,330,22.0,0,0,[],[],[],15,[],"['GuidanceQuestionGenerator.from_defaults', 'ToolMetadata', 'pytest.raises', 'question_gen.generate', 'query=QueryBundle']",5
repos/llama_index/llama-index-legacy/tests/question_gen/test_llm_generators.py:test_llm_question_gen,test_llm_question_gen,function,8,27,25,363,13.44,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",8,[],"['LLMQuestionGenerator.from_defaults', 'ToolMetadata', 'QueryBundle', 'question_gen.generate', 'isinstance']",5
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_error_if_not_dir_or_file,test_error_if_not_dir_or_file,function,4,16,12,285,17.81,0,0,[],[],[],420,[],"['pytest.raises', 'SimpleDirectoryReader', 'TemporaryDirectory']",3
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_exclude_hidden,test_exclude_hidden,function,11,172,67,1787,10.39,2,0,[],[],[],287,"['    """"""Test if exclude_hidden flag excludes hidden files and files in hidden directories.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'len', 'set']",6
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_excluded_files,test_excluded_files,function,9,168,59,1788,10.64,3,0,[],[],[],219,"['    """"""Tests if files are excluded properly.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'len', 'set']",6
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_file_metadata,test_file_metadata,function,16,65,45,641,9.86,2,0,[],[],[],186,"['    """"""Test if file metadata is added to Document.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'filename_to_metadata', 'SimpleDirectoryReader', 'reader.load_data']",6
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_filename_as_doc_id,test_filename_as_doc_id,function,11,59,39,640,10.85,1,0,[],[],[],357,"['    """"""Test if file metadata is added to Document.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'reader.load_data', 'str']",6
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_nonrecursive,test_nonrecursive,function,9,74,42,788,10.65,2,0,[],[],[],81,"['    """"""Test simple non-recursive directory reader.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'len']",5
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_num_files_limit,test_num_files_limit,function,9,102,45,1074,10.53,3,0,[],[],[],137,"['    """"""Test num files limit.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'len', 'set']",6
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_parallel_load,test_parallel_load,function,13,58,38,672,11.59,1,0,[],[],[],429,"['    """"""Test parallel load.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'min', 'cpu_count', 'reader.load_data', 'str']",8
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_recursive,test_recursive,function,10,167,51,1829,10.95,3,0,[],[],[],11,"['    """"""Test simple directory reader in recursive mode.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'len', 'set', 'print']",7
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_required_exts,test_required_exts,function,9,50,30,522,10.44,1,0,[],[],[],115,"['    """"""Test extension filter.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'len']",5
repos/llama_index/llama-index-legacy/tests/readers/test_file.py:test_specifying_encoding,test_specifying_encoding,function,11,58,39,676,11.66,1,0,[],[],[],389,"['    """"""Test if file metadata is added to Document.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'SimpleDirectoryReader', 'reader.load_data', 'str']",6
repos/llama_index/llama-index-legacy/tests/readers/test_html_reader.py:html_str,html_str,function,1,2,2,9,4.5,0,0,[],[],[],11,[],[],0
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader,TestJaguarReader,class,68,269,157,2680,9.96,0,9,[],[],[],21,[],[],0
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:setup_class,TestJaguarReader:setup_class,method,12,37,29,423,11.43,0,0,['cls'],[None],[None],29,[],"['JaguarVectorStore', 'JaguarReader']",2
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:teardown_class,TestJaguarReader:teardown_class,method,0,1,1,4,4.0,0,0,['cls'],[None],[None],59,[],[],0
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:test_add_texts,TestJaguarReader:test_add_texts,method,12,55,45,524,9.53,0,1,['self'],[None],[None],94,"['        """"""Add some text nodes through vectorstore.""""""\n']","['TextNode', 'len']",2
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:test_clear,TestJaguarReader:test_clear,method,4,7,7,84,12.0,0,1,['self'],[None],[None],166,"['        """"""Test cleanup of data in the store.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:test_create,TestJaguarReader:test_create,method,12,23,22,245,10.65,0,1,['self'],[None],[None],77,"['        """"""Create a vector with vector index \'v\' of vector_dimension.\n', '\n', ""        and 'v:text' to hold text and metadata fields author and category\n"", '        """"""\n']","['char', 'json.loads']",2
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:test_drop,TestJaguarReader:test_drop,method,3,4,4,49,12.25,0,1,['self'],[None],[None],174,"['        """"""Destroy the vector store.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:test_login,TestJaguarReader:test_login,method,6,15,11,110,7.33,0,1,['self'],[None],[None],62,"['        """"""Client must login to jaguar store server.\n', '\n', '        Environment variable JAGUAR_API_KEY or $HOME/.jagrc file must\n', '        contain the jaguar api key\n', '        """"""\n']",[],0
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:test_logout,TestJaguarReader:test_logout,method,4,5,5,72,14.4,0,1,['self'],[None],[None],181,"['        """"""Client must logout to disconnect from jaguar server.\n', '\n', '        and clean up resources used by the client\n', '        """"""\n']",[],0
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:test_query_data_filter,TestJaguarReader:test_query_data_filter,method,9,27,24,300,11.11,0,1,['self'],[None],[None],152,"['        """"""Test query date with filter(where condition).""""""\n']",['len'],1
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:test_query_data_limit,TestJaguarReader:test_query_data_limit,method,5,12,12,128,10.67,0,1,['self'],[None],[None],143,"['        """"""Test query date of 2 records.""""""\n']",['len'],1
repos/llama_index/llama-index-legacy/tests/readers/test_jaguar.py:TestJaguarReader:test_query_embedding,TestJaguarReader:test_query_embedding,method,9,27,24,283,10.48,0,1,['self'],[None],[None],125,"['        """"""Test that [0.4, 0.2, 0.8] will retrieve Slow Clouds.\n', '\n', '        This test case uses similarity search.\n', '        Here k is 1.\n', '        """"""\n']",['len'],1
repos/llama_index/llama-index-legacy/tests/readers/test_json.py:test_basic,test_basic,function,11,28,24,293,10.46,0,0,[],[],[],8,"['    """"""Test JSON reader in basic mode.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'JSONReader', 'reader.load_data', 'len', 'isinstance']",7
repos/llama_index/llama-index-legacy/tests/readers/test_json.py:test_collapse_length,test_collapse_length,function,15,45,36,491,10.91,0,0,[],[],[],39,"['    """"""Test JSON reader using the collapse_length function.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'JSONReader', 'reader1.load_data', 'isinstance', 'reader2.load_data']",7
repos/llama_index/llama-index-legacy/tests/readers/test_json.py:test_jsonl,test_jsonl,function,13,46,31,538,11.7,0,0,[],[],[],57,"['    """"""Test JSON reader using the is_jsonl function.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'JSONReader', 'reader.load_data', 'len', 'isinstance']",7
repos/llama_index/llama-index-legacy/tests/readers/test_json.py:test_levels_back0,test_levels_back0,function,14,35,32,332,9.49,0,0,[],[],[],23,"['    """"""Test JSON reader using the levels_back function.""""""\n']","['TemporaryDirectory', 'open', 'f.write', 'JSONReader', 'reader1.load_data', 'reader2.load_data']",6
repos/llama_index/llama-index-legacy/tests/readers/test_load_reader.py:test_loading_readers,test_loading_readers,function,24,38,35,689,18.13,0,0,[],[],[],16,[],"['NotionPageReader', 'StringIterableReader', 'BeautifulSoupWebReader', 'notion.to_dict', 'string_iterable.to_dict', 'soup.to_dict', 'cast', 'load_reader']",8
repos/llama_index/llama-index-legacy/tests/readers/test_mongo.py:test_load_data,test_load_data,function,11,31,27,388,12.52,0,0,[],[],[],15,"['    """"""Test Mongo reader using default field_names.""""""\n']","['patch', 'SimpleMongoReader', 'reader.load_data', 'len']",4
repos/llama_index/llama-index-legacy/tests/readers/test_mongo.py:test_load_data_with_field_name,test_load_data_with_field_name,function,11,53,44,661,12.47,0,0,[],[],[],54,"['    """"""Test Mongo reader using passed in field_names.""""""\n']","['patch', 'SimpleMongoReader', 'reader.load_data', 'len']",4
repos/llama_index/llama-index-legacy/tests/readers/test_mongo.py:test_load_data_with_max_docs,test_load_data_with_max_docs,function,14,46,42,479,10.41,0,1,[],[],[],32,"['    """"""Test Mongo reader with max_docs.""""""\n']","['patch', 'limit_fn', 'SimpleMongoReader', 'reader.load_data', 'len']",5
repos/llama_index/llama-index-legacy/tests/readers/test_mongo.py:test_load_data_with_metadata_name,test_load_data_with_metadata_name,function,11,77,53,991,12.87,0,0,[],[],[],77,"['    """"""Test Mongo reader using passed in metadata_name.""""""\n']","['patch', 'SimpleMongoReader', 'reader.load_data', 'len']",4
repos/llama_index/llama-index-legacy/tests/readers/test_simplewebreader.py:test_error_40x,test_error_40x,function,7,22,19,277,12.59,0,0,[],[],[],16,"['    """"""Test simple web reader for 40x error.""""""\n']","['SimpleWebPageReader', 'pytest.raises', 'reader.load_data', 'url_that_doesnt_exist.format', 'range']",5
repos/llama_index/llama-index-legacy/tests/readers/test_simplewebreader.py:test_url_metadata,test_url_metadata,function,6,16,14,226,14.12,0,0,[],[],[],33,"['    """"""Test simple web reader with metadata hook.""""""\n']","['SimpleWebPageReader', 'reader.load_data', 'len']",3
repos/llama_index/llama-index-legacy/tests/readers/test_string_iterable.py:test_load,test_load,function,4,15,15,124,8.27,0,0,[],[],[],6,"['    """"""Test loading data into StringIterableReader.""""""\n']","['StringIterableReader', 'reader.load_data', 'len']",3
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_google.py:test_get_response,test_get_response,function,25,105,70,1721,16.39,0,0,['mock_generate_answer'],[' MagicMock'],[None],40,[],"['genai.GenerateAnswerResponse', 'genai.GroundingAttribution', 'GoogleTextSynthesizer.from_defaults', 'genai.SafetySetting', 'synthesizer.get_response', 'pytest.approx', 'len']",7
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_google.py:test_set_google_config,test_set_google_config,function,5,6,6,127,21.17,0,0,['mock_credentials'],[' MagicMock'],[None],32,[],"['set_google_config', 'genaix.get_config']",2
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_google.py:test_synthesize,test_synthesize,function,31,118,78,1853,15.7,0,0,['mock_generate_answer'],[' MagicMock'],[None],112,[],"['genai.GenerateAnswerResponse', 'genai.GroundingAttribution', 'GoogleTextSynthesizer.from_defaults', 'synthesizer.synthesize', 'NodeWithScore', 'node=TextNode', 'len', 'pytest.approx']",8
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_google.py:test_synthesize_with_max_token_blocking,test_synthesize_with_max_token_blocking,function,12,34,32,454,13.35,0,0,['mock_generate_answer'],[' MagicMock'],[None],188,[],"['genai.GenerateAnswerResponse', 'GoogleTextSynthesizer.from_defaults', 'pytest.raises', 'synthesizer.synthesize', 'NodeWithScore', 'node=TextNode', 'str']",7
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_google.py:test_synthesize_with_recitation_blocking,test_synthesize_with_recitation_blocking,function,11,33,31,452,13.7,0,0,['mock_generate_answer'],[' MagicMock'],[None],246,[],"['genai.GenerateAnswerResponse', 'GoogleTextSynthesizer.from_defaults', 'pytest.raises', 'synthesizer.synthesize', 'NodeWithScore', 'node=TextNode', 'str']",7
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_google.py:test_synthesize_with_safety_blocking,test_synthesize_with_safety_blocking,function,11,33,31,444,13.45,0,0,['mock_generate_answer'],[' MagicMock'],[None],217,[],"['genai.GenerateAnswerResponse', 'GoogleTextSynthesizer.from_defaults', 'pytest.raises', 'synthesizer.synthesize', 'NodeWithScore', 'node=TextNode', 'str']",7
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_google.py:test_synthesize_with_unknown_blocking,test_synthesize_with_unknown_blocking,function,11,33,31,447,13.55,0,0,['mock_generate_answer'],[' MagicMock'],[None],275,[],"['genai.GenerateAnswerResponse', 'GoogleTextSynthesizer.from_defaults', 'pytest.raises', 'synthesizer.synthesize', 'NodeWithScore', 'node=TextNode', 'str']",7
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_refine.py:mock_refine_service_context,mock_refine_service_context,function,4,7,7,132,18.86,0,0,['patch_llm_predictor'],[' Any'],[None],58,[],"['CallbackManager', 'ServiceContext.from_defaults']",2
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_refine.py:refine_instance,refine_instance,function,2,7,7,125,17.86,0,0,['mock_refine_service_context'],[' ServiceContext'],[None],67,[],['Refine'],1
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_refine.py:test_constructor_args,test_constructor_args,function,4,16,12,303,18.94,0,0,['mock_refine_service_context'],[' ServiceContext'],[None],76,[],"['pytest.raises', 'Refine', 'MockRefineProgram']",3
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_refine.py:MockRefineProgram,MockRefineProgram,class,16,75,39,879,11.72,0,0,[],[],[],15,[],[],0
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_refine.py:MockRefineProgram:__call__,MockRefineProgram:__call__,method,9,14,13,205,14.64,0,0,"['self', '*args', 'context_str', 'context_msg', '**kwargs']","[None, ' Any', ' Optional[str] ', ' Optional[str] ', ' Any']","[None, None, ' None', ' None', None]",28,[],"['cast', 'StructuredRefineResponse']",2
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_refine.py:MockRefineProgram:__init__,MockRefineProgram:__init__,method,2,2,2,55,27.5,0,0,"['self', 'input_to_query_satisfied', 'bool]']","[None, ' Dict[str', None]","[None, None, None]",21,[],[],0
repos/llama_index/llama-index-legacy/tests/response_synthesizers/test_refine.py:MockRefineProgram:output_cls,MockRefineProgram:output_cls,method,2,2,2,30,15.0,0,0,['self'],[None],[None],25,[],[],0
repos/llama_index/llama-index-legacy/tests/retrievers/test_composable_retriever.py:test_composable_retrieval,test_composable_retrieval,function,12,39,33,465,11.92,0,0,[],[],[],5,"['    """"""Test composable retrieval.""""""\n']","['TextNode', 'IndexNode', 'obj=TextNode', 'SummaryIndex', 'index.as_retriever', 'retriever.retrieve', 'len']",7
repos/llama_index/llama-index-legacy/tests/selectors/test_llm_selectors.py:test_llm_multi_selector,test_llm_multi_selector,function,7,22,22,208,9.45,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",30,[],"['LLMMultiSelector.from_defaults', 'selector.select']",2
repos/llama_index/llama-index-legacy/tests/selectors/test_llm_selectors.py:test_llm_multi_selector_max_choices,test_llm_multi_selector_max_choices,function,7,24,24,222,9.25,0,0,"['mock_service_context', '']","[' ServiceContext', None]","[None, None]",46,[],"['LLMMultiSelector.from_defaults', 'selector.select']",2
repos/llama_index/llama-index-legacy/tests/selectors/test_llm_selectors.py:test_llm_single_selector,test_llm_single_selector,function,13,34,32,489,14.38,0,0,[],[],[],13,[],"['ServiceContext.from_defaults', 'LLMSingleSelector.from_defaults', 'patch.object', 'type', 'return_value=CompletionResponse', 'selector.select', 'mock_complete.assert_called_once']",7
repos/llama_index/llama-index-legacy/tests/storage/conftest.py:firestore_kvstore,firestore_kvstore,function,2,2,2,24,12.0,0,0,[],[],[],27,[],['FirestoreKVStore'],1
repos/llama_index/llama-index-legacy/tests/storage/conftest.py:mongo_client,mongo_client,function,2,2,2,23,11.5,0,0,[],[],[],17,[],['MockMongoClient'],1
repos/llama_index/llama-index-legacy/tests/storage/conftest.py:mongo_kvstore,mongo_kvstore,function,2,5,5,59,11.8,0,0,['mongo_client'],[' MockMongoClient'],[None],22,[],['MongoDBKVStore'],1
repos/llama_index/llama-index-legacy/tests/storage/conftest.py:postgres_container,postgres_container,function,17,54,50,751,13.91,0,1,[],[],[],48,[],"['docker.from_env', 'container.reload', 'time.sleep', 'container.stop', 'container.remove', 'client.close']",6
repos/llama_index/llama-index-legacy/tests/storage/conftest.py:postgres_kvstore,postgres_kvstore,function,7,23,22,341,14.83,1,1,"['postgres_container', 'Union[str', 'Container]]', '']","[' Dict[str', None, None, None]","[None, None, None, None]",91,[],"['PostgresKVStore', 'kvstore.get_all', 'kvstore.delete']",3
repos/llama_index/llama-index-legacy/tests/storage/conftest.py:redis_kvstore,redis_kvstore,function,9,14,13,209,14.93,0,0,[],[],[],37,[],"['Redis.from_url', 'RedisKVStore']",2
repos/llama_index/llama-index-legacy/tests/storage/conftest.py:simple_kvstore,simple_kvstore,function,2,2,2,21,10.5,0,0,[],[],[],32,[],['SimpleKVStore'],1
repos/llama_index/llama-index-legacy/tests/storage/test_storage_context.py:test_storage_context_dict,test_storage_context_dict,function,16,26,25,601,23.12,0,0,[],[],[],6,[],"['StorageContext.from_defaults', 'TextNode', 'IndexDict', 'storage_context.to_dict', 'StorageContext.from_dict']",5
repos/llama_index/llama-index-legacy/tests/test_exec_utils.py:test_contains_protected_access,test_contains_protected_access,function,1,92,32,802,8.72,0,0,[],[],[],4,[],"['_contains_protected_access', '_a', '_b', 'b']",4
repos/llama_index/llama-index-legacy/tests/test_schema.py:node_with_score,node_with_score,function,2,5,5,49,9.8,0,0,['text_node'],[' TextNode'],[None],15,[],['NodeWithScore'],1
repos/llama_index/llama-index-legacy/tests/test_schema.py:test_node_with_score_passthrough,test_node_with_score_passthrough,function,9,16,9,220,13.75,0,0,['node_with_score'],[' NodeWithScore'],[None],22,[],"['node_with_score.get_text', 'node_with_score.get_content', 'node_with_score.get_embedding']",3
repos/llama_index/llama-index-legacy/tests/test_schema.py:test_text_node_hash,test_text_node_hash,function,9,36,21,512,14.22,0,0,[],[],[],33,[],"['TextNode', 'node.set_content']",2
repos/llama_index/llama-index-legacy/tests/test_schema.py:text_node,text_node,function,2,10,10,85,8.5,0,0,[],[],[],6,[],['TextNode'],1
repos/llama_index/llama-index-legacy/tests/test_utils.py:fn_with_exception,fn_with_exception,function,3,11,11,77,7.0,0,1,"['exception_cls', 'Exception]]']","[' Optional[Union[Type[Exception]', None]","[None, None]",34,"['    """"""Return true unless exception is specified.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/test_utils.py:test_get_color_mapping,test_get_color_mapping,function,5,38,25,461,12.13,0,0,[],[],[],119,"['    """"""Test get_color_mapping function.""""""\n']","['get_color_mapping', 'len', 'set', 'all', 'color_mapping.values', 'color_mapping_ansi.values']",6
repos/llama_index/llama-index-legacy/tests/test_utils.py:test_get_colored_text,test_get_colored_text,function,6,40,22,469,11.72,1,0,[],[],[],133,"['    """"""Test _get_colored_text function.""""""\n']","['_get_colored_text', 'colored_text.startswith', 'colored_text.endswith']",3
repos/llama_index/llama-index-legacy/tests/test_utils.py:test_iter_batch,test_iter_batch,function,2,29,21,169,5.83,0,0,[],[],[],108,"['    """"""Check iter_batch works as expected on regular, lazy and empty sequences.""""""\n']","['list', 'range']",2
repos/llama_index/llama-index-legacy/tests/test_utils.py:test_print_text,test_print_text,function,8,47,24,592,12.6,2,0,['capsys'],[' CaptureFixture'],[None],151,"['    """"""Test print_text function.""""""\n']","['print_text', 'capsys.readouterr']",2
repos/llama_index/llama-index-legacy/tests/test_utils.py:test_retry_on_conditional_exceptions,test_retry_on_conditional_exceptions,function,6,36,20,537,14.92,0,0,[],[],[],84,"['    """"""Make sure retry function works on conditional exceptions.""""""\n']","['pytest.raises', 'retry_on_exceptions_with_backoff', 'fn_with_exception']",3
repos/llama_index/llama-index-legacy/tests/test_utils.py:test_retry_on_exceptions_with_backoff,test_retry_on_exceptions_with_backoff,function,7,39,21,518,13.28,0,0,[],[],[],53,"['    """"""Make sure retry function has accurate number of attempts.""""""\n']","['fn_with_exception', 'pytest.raises', 'retry_on_exceptions_with_backoff']",3
repos/llama_index/llama-index-legacy/tests/test_utils.py:test_tokenizer,test_tokenizer,function,6,10,10,79,7.9,0,0,[],[],[],20,"['    """"""Make sure tokenizer works.\n', '\n', '    NOTE: we use a different tokenizer for python >= 3.9.\n', '\n', '    """"""\n']","['get_tokenizer', 'len']",2
repos/llama_index/llama-index-legacy/tests/test_utils.py:ConditionalException,ConditionalException,class,3,8,8,73,9.12,0,0,[],[],[],45,[],[],0
repos/llama_index/llama-index-legacy/tests/test_utils.py:ConditionalException:__init__,ConditionalException:__init__,method,2,2,2,30,15.0,0,0,"['self', 'should_retry']","[None, ' bool']","[None, None]",48,"['        """"""Initialize with parameters.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:baz,baz,function,5,9,8,133,14.78,0,0,[],[],[],23,[],"['print', 'code_splitter.split_text', 'foo', 'baz']",4
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:baz,baz,function,5,9,8,133,14.78,0,0,[],[],[],23,[],"['print', 'code_splitter.split_text', 'foo', 'baz']",4
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:baz,baz,function,5,9,8,133,14.78,0,0,[],[],[],23,[],"['print', 'code_splitter.split_text', 'foo', 'baz']",4
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:foo,foo,function,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:foo,foo,function,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:foo,foo,function,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:test__py_custom_parser_code_splitter,test__py_custom_parser_code_splitter,function,9,21,21,222,10.57,0,1,[],[],[],168,"['    """"""Test case for code splitting using custom parser generated from tree_sitter_languages.""""""\n']","['get_parser', 'CodeSplitter']",2
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:test_cpp_code_splitter,test_cpp_code_splitter,function,4,14,14,130,9.29,0,1,[],[],[],145,"['    """"""Test case for code splitting using typescript.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:test_html_code_splitter,test_html_code_splitter,function,4,14,14,131,9.36,0,1,[],[],[],74,"['    """"""Test case for code splitting using typescript.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:test_python_code_splitter,test_python_code_splitter,function,4,14,14,133,9.5,0,1,[],[],[],10,"['    """"""Test case for code splitting using python.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:test_start_end_char_idx,test_start_end_char_idx,function,1,2,2,9,4.5,0,0,[],[],[],31,[],[],0
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:test_tsx_code_splitter,test_tsx_code_splitter,function,4,14,14,137,9.79,0,1,[],[],[],107,"['    """"""Test case for code splitting using typescript.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-legacy/tests/text_splitter/test_code_splitter.py:test_typescript_code_splitter,test_typescript_code_splitter,function,4,14,14,137,9.79,0,1,[],[],[],51,"['    """"""Test case for code splitting using typescript.""""""\n']",['CodeSplitter'],1
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_chinese_text,test_chinese_text,function,4,8,8,119,14.88,0,0,['chinese_text'],[' str'],[None],41,[],"['SentenceSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_contiguous_text,test_contiguous_text,function,4,8,8,123,15.38,0,0,['contiguous_text'],[' str'],[None],47,[],"['SentenceSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_edge_case,test_edge_case,function,6,193,85,1053,5.46,0,0,[],[],[],75,"['    """"""Test case from: https://github.com/jerryjliu/llama_index/issues/7287.""""""\n']","['SentenceSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_overlap,test_overlap,function,6,50,31,323,6.46,0,0,[],[],[],88,[],"['SentenceSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_paragraphs,test_paragraphs,function,7,23,15,269,11.7,0,0,[],[],[],8,"['    """"""Test case of a string with multiple paragraphs.""""""\n']","['SentenceSplitter', 'sentence_text_splitter.split_text']",2
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_sentences,test_sentences,function,7,25,16,268,10.72,0,0,[],[],[],30,"['    """"""Test case of a string with multiple sentences.""""""\n']","['SentenceSplitter', 'sentence_text_splitter.split_text']",2
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_split_texts_multiple,test_split_texts_multiple,function,13,45,22,459,10.2,0,0,[],[],[],111,"['    """"""Test case for a list of texts.""""""\n']","['SentenceSplitter', 'sentence_text_splitter.split_texts', 'print']",3
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_split_texts_singleton,test_split_texts_singleton,function,8,25,17,284,11.36,0,0,[],[],[],100,"['    """"""Test case for a singleton list of texts.""""""\n']","['SentenceSplitter', 'sentence_text_splitter.split_texts']",2
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_split_texts_with_metadata,test_split_texts_with_metadata,function,9,30,26,379,12.63,0,0,['english_text'],[' str'],[None],126,"['    """"""Test case for a list of texts with metadata.""""""\n']","['tiktoken.get_encoding', 'SentenceSplitter', 'splitter.split_texts', 'len', 'splitter.split_texts_metadata_aware']",5
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_split_with_metadata,test_split_with_metadata,function,11,35,30,440,12.57,1,0,['english_text'],[' str'],[None],57,[],"['tiktoken.get_encoding', 'SentenceSplitter', 'splitter.split_text', 'len', 'splitter.split_text_metadata_aware']",5
repos/llama_index/llama-index-legacy/tests/text_splitter/test_sentence_splitter.py:test_start_end_char_idx,test_start_end_char_idx,function,10,34,27,391,11.5,1,0,[],[],[],18,[],"['Document', 'SentenceSplitter', 'text_splitter.get_nodes_from_documents', 'len', 'node.get_content']",5
repos/llama_index/llama-index-legacy/tests/text_splitter/test_token_splitter.py:test_contiguous_text,test_contiguous_text,function,4,8,8,124,15.5,0,0,['contiguous_text'],[' str'],[None],70,[],"['TokenTextSplitter', 'splitter.split_text', 'len']",3
repos/llama_index/llama-index-legacy/tests/text_splitter/test_token_splitter.py:test_split_chinese,test_split_chinese,function,4,8,8,130,16.25,0,0,['chinese_text'],[' str'],[None],64,[],"['TokenTextSplitter', 'text_splitter.split_text', 'len']",3
repos/llama_index/llama-index-legacy/tests/text_splitter/test_token_splitter.py:test_split_long_token,test_split_long_token,function,7,38,26,438,11.53,0,0,[],[],[],44,"['    """"""Test split a really long token.""""""\n']","['tiktoken.get_encoding', 'TokenTextSplitter', 'text_splitter.split_text', 'len']",4
repos/llama_index/llama-index-legacy/tests/text_splitter/test_token_splitter.py:test_split_token,test_split_token,function,6,30,22,318,10.6,0,0,[],[],[],11,"['    """"""Test split normal token.""""""\n']","['TokenTextSplitter', 'text_splitter.split_text']",2
repos/llama_index/llama-index-legacy/tests/text_splitter/test_token_splitter.py:test_split_with_metadata,test_split_with_metadata,function,11,35,30,434,12.4,1,0,['english_text'],[' str'],[None],76,[],"['tiktoken.get_encoding', 'TokenTextSplitter', 'splitter.split_text', 'len', 'splitter.split_text_metadata_aware']",5
repos/llama_index/llama-index-legacy/tests/text_splitter/test_token_splitter.py:test_start_end_char_idx,test_start_end_char_idx,function,10,33,26,368,11.15,1,0,[],[],[],24,[],"['Document', 'TokenTextSplitter', 'text_splitter.get_nodes_from_documents', 'len', 'node.get_content']",5
repos/llama_index/llama-index-legacy/tests/text_splitter/test_token_splitter.py:test_truncate_token,test_truncate_token,function,6,12,11,134,11.17,0,0,[],[],[],36,"['    """"""Test truncate normal token.""""""\n']","['TokenTextSplitter', 'truncate_text']",2
repos/llama_index/llama-index-legacy/tests/token_predictor/test_base.py:test_token_predictor,test_token_predictor,function,18,49,34,667,13.61,0,0,['mock_split'],[' Any'],[None],18,"['    """"""Test token predictor.""""""\n']","['Document', 'MockLLM', 'ServiceContext.from_defaults', 'TreeIndex.from_documents', 'index.as_query_engine', 'query_engine.query', 'KeywordTableIndex.from_documents', 'index_keyword.as_query_engine', 'SummaryIndex.from_documents', 'index_list.as_query_engine']",10
repos/llama_index/llama-index-legacy/tests/tools/conftest.py:documents,documents,function,2,20,14,117,5.85,0,0,[],[],[],10,"['    """"""Get documents.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/tools/test_base.py:test_function_tool,test_function_tool,function,10,46,29,614,13.35,0,0,[],[],[],24,"['    """"""Test function tool.""""""\n']","['FunctionTool.from_defaults', 'str', 'function_tool']",3
repos/llama_index/llama-index-legacy/tests/tools/test_base.py:test_function_tool_to_langchain,test_function_tool_to_langchain,function,17,46,36,541,11.76,0,0,[],[],[],50,[],"['FunctionTool.from_defaults', 'function_tool.to_langchain_tool', 'langchain_tool.run', 'TestSchema', 'str', 'function_tool.to_langchain_structured_tool', 'langchain_tool2.run']",7
repos/llama_index/llama-index-legacy/tests/tools/test_base.py:test_retreiver_tool,test_retreiver_tool,function,16,69,51,766,11.1,0,0,[],[],[],163,[],"['Document', 'ServiceContext.from_defaults', 'embed_model=MockEmbedding', 'VectorStoreIndex.from_documents', 'vs_index.as_retriever', 'RetrieverTool', 'metadata=ToolMetadata', 'vs_ret_tool.call']",8
repos/llama_index/llama-index-legacy/tests/tools/test_base.py:test_tool_fn_schema,test_tool_fn_schema,function,9,21,21,268,12.76,0,0,[],[],[],196,[],"['TestSchema', 'ToolMetadata', 'json.loads', 'set']",4
repos/llama_index/llama-index-legacy/tests/tools/test_base.py:tmp_function,tmp_function,function,2,2,2,12,6.0,0,0,['x'],[' int'],[None],16,[],['str'],1
repos/llama_index/llama-index-legacy/tests/tools/test_ondemand_loader.py:test_ondemand_loader_tool,test_ondemand_loader_tool,function,3,10,10,93,9.3,0,0,"['tool', '']","[' OnDemandLoaderTool', None]","[None, None]",40,"['    """"""Test ondemand loader.""""""\n']","['tool', 'str']",2
repos/llama_index/llama-index-legacy/tests/tools/test_ondemand_loader.py:test_ondemand_loader_tool_langchain,test_ondemand_loader_tool_langchain,function,7,17,15,198,11.65,0,0,"['tool', '']","[' OnDemandLoaderTool', None]","[None, None]",49,[],"['tool.to_langchain_structured_tool', 'lc_tool.run', 'str']",3
repos/llama_index/llama-index-legacy/tests/tools/test_ondemand_loader.py:tool,tool,function,4,12,12,265,22.08,0,0,['mock_service_context'],[' ServiceContext'],[None],27,[],"['StringIterableReader', 'OnDemandLoaderTool.from_defaults']",2
repos/llama_index/llama-index-legacy/tests/tools/test_ondemand_loader.py:TestSchemaSpec,TestSchemaSpec,class,4,4,4,29,7.25,0,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-legacy/tests/tools/test_query_engine_tool.py:test_query_engine_tool,test_query_engine_tool,function,11,50,35,747,14.94,0,0,[],[],[],19,"['    """"""Test query engine tool.""""""\n']","['MockQueryEngine', 'QueryEngineTool.from_defaults', 'query_tool', 'str', 'cast', 'fn_schema_cls', 'pytest.raises']",7
repos/llama_index/llama-index-legacy/tests/tools/test_query_engine_tool.py:MockQueryEngine,MockQueryEngine,class,2,9,9,67,7.44,0,0,[],[],[],11,[],[],0
repos/llama_index/llama-index-legacy/tests/tools/test_query_engine_tool.py:MockQueryEngine:custom_query,MockQueryEngine:custom_query,method,1,3,3,25,8.33,0,0,"['self', 'query_str']","[None, ' str']","[None, None]",14,"['        """"""Query.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/tools/test_utils.py:test_create_schema_from_function,test_create_schema_from_function,function,9,55,36,617,11.22,0,0,[],[],[],9,"['    """"""Test create schema from function.""""""\n']","['test_fn', 'create_schema_from_function', 'SchemaCls.schema', 'test_fn2']",4
repos/llama_index/llama-index-legacy/tests/tools/test_utils.py:test_create_schema_from_function_with_field,test_create_schema_from_function_with_field,function,15,45,36,486,10.8,0,0,[],[],[],34,"['    """"""Test create_schema_from_function with pydantic.Field.""""""\n']","['tmp_function', 'Field', 'str', 'create_schema_from_function', 'schema.schema', 'schema']",6
repos/llama_index/llama-index-legacy/tests/utilities/test_sql_wrapper.py:sql_database,sql_database,function,12,35,34,435,12.43,0,0,['request'],[' pytest.FixtureRequest'],[None],10,[],"['create_engine', 'MetaData', 'Table', 'Column', 'metadata.create_all', 'getattr', 'SQLDatabase', 'metadata.drop_all']",8
repos/llama_index/llama-index-legacy/tests/utilities/test_sql_wrapper.py:test_get_single_table_info,test_get_single_table_info,function,1,19,17,141,7.42,0,0,['sql_database'],[' SQLDatabase'],[None],56,[],['sql_database.get_single_table_info'],1
repos/llama_index/llama-index-legacy/tests/utilities/test_sql_wrapper.py:test_get_table_columns,test_get_table_columns,function,2,10,10,108,10.8,0,0,['sql_database'],[' SQLDatabase'],[None],50,[],['sql_database.get_table_columns'],1
repos/llama_index/llama-index-legacy/tests/utilities/test_sql_wrapper.py:test_init,test_init,function,2,5,4,78,15.6,0,0,['sql_database'],[' SQLDatabase'],[None],36,[],['isinstance'],1
repos/llama_index/llama-index-legacy/tests/utilities/test_sql_wrapper.py:test_insert_and_run_sql,test_insert_and_run_sql,function,5,24,17,261,10.88,0,0,['sql_database'],[' SQLDatabase'],[None],66,[],"['sql_database.run_sql', 'sql_database.insert_into_table']",2
repos/llama_index/llama-index-legacy/tests/utilities/test_sql_wrapper.py:test_long_string_no_truncation,test_long_string_no_truncation,function,7,25,18,280,11.2,0,0,['sql_database'],[' SQLDatabase'],[None],92,[],"['sql_database.run_sql', 'sql_database.insert_into_table']",2
repos/llama_index/llama-index-legacy/tests/utilities/test_sql_wrapper.py:test_run_sql_truncation,test_run_sql_truncation,function,5,23,16,255,11.09,0,0,['sql_database'],[' SQLDatabase'],[None],79,[],"['sql_database.run_sql', 'sql_database.insert_into_table']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_astra.py:astra_db_store,astra_db_store,function,3,10,10,219,21.9,0,0,[],[],[],25,[],['AstraDBVectorStore'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_astra.py:test_astra_db_create_and_crud,test_astra_db_create_and_crud,function,3,23,20,213,9.26,0,0,['astra_db_store'],[' AstraDBVectorStore'],[None],42,[],"['astra_db_store.add', 'TextNode', 'RelatedNodeInfo', 'astra_db_store.delete']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_astra.py:test_astra_db_queries,test_astra_db_queries,function,3,7,7,95,13.57,0,0,['astra_db_store'],[' AstraDBVectorStore'],[None],64,[],"['VectorStoreQuery', 'astra_db_store.query']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azureaisearch.py:create_mock_vector_store,create_mock_vector_store,function,2,28,27,410,14.64,0,0,"['search_client', 'index_name', 'index_management', '']","[' Any', ' Optional[str] ', ' IndexManagement ', None]","[None, ' None', ' IndexManagement.NO_VALIDATION', None]",21,[],['AzureAISearchVectorStore'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azureaisearch.py:create_sample_documents,create_sample_documents,function,7,26,25,209,8.04,1,0,['n'],[' int'],[None],40,[],"['range', 'nodes.append', 'TextNode', 'RelatedNodeInfo']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azureaisearch.py:test_azureaisearch_add_one_batch,test_azureaisearch_add_one_batch,function,10,21,17,277,13.19,0,0,[],[],[],78,[],"['MagicMock', 'create_mock_vector_store', 'create_sample_documents', 'vector_store.add', 'len']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azureaisearch.py:test_azureaisearch_add_two_batches,test_azureaisearch_add_two_batches,function,10,21,17,277,13.19,0,0,[],[],[],60,[],"['MagicMock', 'create_mock_vector_store', 'create_sample_documents', 'vector_store.add', 'len']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azureaisearch.py:test_invalid_index_management_for_searchclient,test_invalid_index_management_for_searchclient,function,5,28,26,458,16.36,0,0,[],[],[],96,[],"['MagicMock', 'create_mock_vector_store', 'pytest.raises']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azureaisearch.py:test_invalid_index_management_for_searchindexclient,test_invalid_index_management_for_searchindexclient,function,5,25,24,396,15.84,0,0,[],[],[],123,[],"['MagicMock', 'pytest.raises', 'create_mock_vector_store']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azurecosmosmongo.py:node_embeddings,node_embeddings,function,4,53,34,706,13.32,0,0,[],[],[],33,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azurecosmosmongo.py:TestAzureMongovCoreVectorSearch,TestAzureMongovCoreVectorSearch,class,18,133,77,1403,10.55,0,0,[],[],[],69,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azurecosmosmongo.py:TestAzureMongovCoreVectorSearch:setup,TestAzureMongovCoreVectorSearch:setup,method,1,4,4,45,11.25,0,0,['self'],[None],[None],81,[],['collection.delete_many'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azurecosmosmongo.py:TestAzureMongovCoreVectorSearch:setup_class,TestAzureMongovCoreVectorSearch:setup_class,method,1,6,6,58,9.67,0,0,['cls'],[None],[None],71,[],['collection.count_documents'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azurecosmosmongo.py:TestAzureMongovCoreVectorSearch:teardown_class,TestAzureMongovCoreVectorSearch:teardown_class,method,1,4,4,45,11.25,0,0,['cls'],[None],[None],76,[],['collection.delete_many'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azurecosmosmongo.py:TestAzureMongovCoreVectorSearch:test_add_and_delete,TestAzureMongovCoreVectorSearch:test_add_and_delete,method,7,51,43,539,10.57,0,0,['self'],[None],[None],85,[],"['AzureCosmosDBMongoDBVectorSearch', 'sleep', 'vector_store.add', 'TextNode', 'RelatedNodeInfo', 'collection.count_documents', 'vector_store.delete']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_azurecosmosmongo.py:TestAzureMongovCoreVectorSearch:test_query,TestAzureMongovCoreVectorSearch:test_query,method,9,43,37,483,11.23,0,0,"['self', 'node_embeddings']","[None, ' List[TextNode]']","[None, None]",113,[],"['AzureCosmosDBMongoDBVectorSearch', 'vector_store.add', 'sleep', 'vector_store.query', 'VectorStoreQuery', 'print']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_cassandra.py:TestCassandraVectorStore,TestCassandraVectorStore,class,21,142,65,1796,12.65,0,0,[],[],[],21,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_cassandra.py:TestCassandraVectorStore:test_cassandra_create_and_crud,TestCassandraVectorStore:test_cassandra_create_and_crud,method,14,45,39,503,11.18,0,0,['self'],[None],[None],23,[],"['MagicMock', 'CassandraVectorStore', 'vector_store.add', 'TextNode', 'RelatedNodeInfo', 'vector_store.delete']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_cassandra.py:TestCassandraVectorStore:test_cassandra_queries,TestCassandraVectorStore:test_cassandra_queries,method,15,79,36,1080,13.67,0,0,['self'],[None],[None],58,[],"['MagicMock', 'CassandraVectorStore', 'VectorStoreQuery', 'vector_store.query', 'pytest.raises']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_chromadb.py:node_embeddings,node_embeddings,function,9,145,96,1538,10.61,0,0,[],[],[],74,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_chromadb.py:test_instance_creation_from_collection,test_instance_creation_from_collection,function,7,9,9,188,20.89,0,0,[],[],[],38,[],"['chromadb.HttpClient', 'connection.get_collection', 'ChromaVectorStore.from_collection', 'isinstance']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_chromadb.py:test_instance_creation_from_http_params,test_instance_creation_from_http_params,function,3,10,10,177,17.7,0,0,[],[],[],46,[],"['ChromaVectorStore.from_params', 'isinstance']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_chromadb.py:test_instance_creation_from_persist_dir,test_instance_creation_from_persist_dir,function,3,9,9,157,17.44,0,0,[],[],[],57,[],"['ChromaVectorStore.from_params', 'isinstance']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_chromadb.py:vector_store,vector_store,function,6,6,6,148,24.67,0,0,[],[],[],67,[],"['chromadb.HttpClient', 'connection.get_collection', 'ChromaVectorStore']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_docarray.py:node_embeddings,node_embeddings,function,4,53,34,706,13.32,0,0,[],[],[],21,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_docarray.py:test_hnsw,test_hnsw,function,15,43,30,660,15.35,0,0,"['node_embeddings', 'tmp_path']","[' List[TextNode]', ' Path']","[None, None]",55,[],"['DocArrayHnswVectorStore', 'docarray_vector_store.add', 'docarray_vector_store.num_docs', 'VectorStoreQuery', 'docarray_vector_store.query', 'len', 'docarray_vector_store.delete', 'new_vector_store.num_docs', 'new_vector_store.delete']",9
repos/llama_index/llama-index-legacy/tests/vector_stores/test_docarray.py:test_hnsw_filters,test_hnsw_filters,function,14,39,32,538,13.79,0,0,"['node_embeddings', 'tmp_path']","[' List[TextNode]', ' Path']","[None, None]",122,[],"['DocArrayHnswVectorStore', 'docarray_vector_store.add', 'docarray_vector_store.num_docs', 'MetadataFilters', 'VectorStoreQuery', 'docarray_vector_store.query', 'len']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_docarray.py:test_in_memory,test_in_memory,function,16,46,35,735,15.98,0,0,"['node_embeddings', 'tmp_path']","[' List[TextNode]', ' Path']","[None, None]",78,[],"['DocArrayInMemoryVectorStore', 'docarray_vector_store.add', 'docarray_vector_store.num_docs', 'VectorStoreQuery', 'docarray_vector_store.query', 'len', 'docarray_vector_store.delete', 'docarray_vector_store.persist', 'new_vector_store.num_docs', 'new_vector_store.delete']",10
repos/llama_index/llama-index-legacy/tests/vector_stores/test_docarray.py:test_in_memory_filters,test_in_memory_filters,function,14,38,31,514,13.53,0,0,['node_embeddings'],[' List[TextNode]'],[None],105,[],"['DocArrayInMemoryVectorStore', 'docarray_vector_store.add', 'docarray_vector_store.num_docs', 'MetadataFilters', 'VectorStoreQuery', 'docarray_vector_store.query', 'len']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_elasticsearch.py:elasticsearch_connection,elasticsearch_connection,function,17,50,45,655,13.1,1,2,[],[],[],52,[],"['Elasticsearch', 'index_name.startswith']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_elasticsearch.py:es_store,es_store,function,2,6,6,118,19.67,0,0,"['index_name', 'elasticsearch_connection']","[' str', ' Dict']","[None, None]",166,[],['ElasticsearchStore'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_elasticsearch.py:index_name,index_name,function,2,2,2,32,16.0,0,0,[],[],[],46,"['    """"""Return the index name.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_elasticsearch.py:node_embeddings,node_embeddings,function,9,145,96,1538,10.61,0,0,[],[],[],87,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_elasticsearch.py:test_check_user_agent,test_check_user_agent,function,24,65,56,793,12.2,0,0,"['index_name', 'node_embeddings', '']","[' str', ' List[TextNode]', None]","[None, None, None]",430,[],"['CustomTransport', 'perform_request', 'super', 'AsyncElasticsearch', 'ElasticsearchStore', 'es_store.add', 're.match']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_elasticsearch.py:test_instance_creation,test_instance_creation,function,3,8,8,127,15.88,0,0,"['index_name', 'elasticsearch_connection']","[' str', ' Dict']","[None, None]",157,[],"['ElasticsearchStore', 'isinstance']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_epsilla.py:node_embeddings,node_embeddings,function,4,33,24,367,11.12,0,0,[],[],[],18,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_epsilla.py:test_add_data_and_query,test_add_data_and_query,function,15,36,26,464,12.89,0,0,[],[],[],53,[],"['vectordb.Client', 'EpsillaVectorStore', 'node_embeddings', 'vector_store.add', 'VectorStoreQuery', 'vector_store.query']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_epsilla.py:test_initiate_store,test_initiate_store,function,6,15,14,222,14.8,0,0,[],[],[],42,[],"['vectordb.Client', 'EpsillaVectorStore']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_google.py:test_add,test_add,function,33,186,87,2870,15.43,0,0,"['mock_get_corpus', 'mock_get_document', 'mock_create_document', 'mock_batch_create_chunks', '']","[' MagicMock', ' MagicMock', ' MagicMock', ' MagicMock', None]","[None, None, None, None, None]",92,[],"['genai.Corpus', 'gapi_exception.NotFound', 'genai.Document', 'genai.BatchCreateChunksResponse', 'genai.Chunk', 'GoogleVectorStore.from_corpus', 'store.add', 'TextNode', 'RelatedNodeInfo', 'genai.CreateDocumentRequest', 'genai.CustomMetadata', 'genai.BatchCreateChunksRequest', 'genai.CreateChunkRequest']",13
repos/llama_index/llama-index-legacy/tests/vector_stores/test_google.py:test_class_name,test_class_name,function,2,5,4,79,15.8,0,0,[],[],[],79,[],['GoogleVectorStore.class_name'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_google.py:test_create_corpus,test_create_corpus,function,11,30,26,430,14.33,0,0,['mock_create_corpus'],[' MagicMock'],[None],46,[],"['fake_create_corpus', 'GoogleVectorStore.create_corpus', 'len']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_google.py:test_delete,test_delete,function,8,13,12,324,24.92,0,0,"['mock_get_corpus', 'mock_delete_document', '']","[' MagicMock', ' MagicMock', None]","[None, None, None]",238,[],"['genai.Corpus', 'GoogleVectorStore.from_corpus', 'store.delete', 'genai.DeleteDocumentRequest']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_google.py:test_from_corpus,test_from_corpus,function,5,7,7,143,20.43,0,0,['mock_get_corpus'],[' MagicMock'],[None],67,[],"['genai.Corpus', 'GoogleVectorStore.from_corpus']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_google.py:test_query,test_query,function,14,65,47,945,14.54,0,0,"['mock_get_corpus', 'mock_query_corpus', '']","[' MagicMock', ' MagicMock', None]","[None, None, None]",260,[],"['genai.Corpus', 'genai.QueryCorpusResponse', 'genai.RelevantChunk', 'GoogleVectorStore.from_corpus', 'store.query', 'query=VectorStoreQuery', 'filters=MetadataFilters', 'ExactMatchFilter', 'genai.QueryCorpusRequest', 'genai.MetadataFilter', 'genai.Condition']",11
repos/llama_index/llama-index-legacy/tests/vector_stores/test_google.py:test_set_google_config,test_set_google_config,function,5,6,6,127,21.17,0,0,['mock_credentials'],[' MagicMock'],[None],38,[],"['set_google_config', 'genaix.get_config']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore,TestJaguarVectorStore,class,80,394,180,3662,9.29,0,12,[],[],[],24,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:setup_class,TestJaguarVectorStore:setup_class,method,10,28,27,324,11.57,0,0,['cls'],[None],[None],31,[],['JaguarVectorStore'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:teardown_class,TestJaguarVectorStore:teardown_class,method,0,1,1,4,4.0,0,0,['cls'],[None],[None],52,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_add_texts,TestJaguarVectorStore:test_add_texts,method,12,55,45,524,9.53,0,1,['self'],[None],[None],86,"['        """"""Add some text nodes to the vector store.\n', '\n', '        Here the embeddings are given. In real-life applications,\n', '        the embeddings should be generated by an embedding model.\n', '        """"""\n']","['TextNode', 'len']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_clear,TestJaguarVectorStore:test_clear,method,4,7,7,84,12.0,0,1,['self'],[None],[None],229,"['        """"""Test cleanup of data in the store.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_create,TestJaguarVectorStore:test_create,method,12,23,22,245,10.65,0,1,['self'],[None],[None],70,"['        """"""Create a vector with vector index \'v\' of vector_dimension.\n', '\n', ""        and 'v:text' to hold text and metadata author and category\n"", '        """"""\n']","['char', 'json.loads']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_drop,TestJaguarVectorStore:test_drop,method,3,4,4,49,12.25,0,1,['self'],[None],[None],237,"['        """"""Destroy the vector store.""""""\n']",[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_load_documents_filter,TestJaguarVectorStore:test_load_documents_filter,method,10,37,32,348,9.41,0,1,['self'],[None],[None],170,"['        """"""Test loading documents with filter(where condition).""""""\n']",['len'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_login,TestJaguarVectorStore:test_login,method,4,17,12,111,6.53,0,2,['self'],[None],[None],55,"['        """"""Client must login to jaguar store server.\n', '        Environment variable JAGUAR_API_KEY or $HOME/.jagrc file must\n', '        contain the jaguar api key.\n', '        """"""\n']",[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_logout,TestJaguarVectorStore:test_logout,method,3,4,4,51,12.75,0,1,['self'],[None],[None],244,"['        """"""Client must logout to disconnect from jaguar server.\n', '\n', '        and clean up resources used by the client\n', '        """"""\n']",[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_query,TestJaguarVectorStore:test_query,method,11,40,26,359,8.97,0,1,['self'],[None],[None],121,"['        """"""Test that [0.4, 0.2, 0.8] will retrieve text Slow Clouds.\n', '        Here k is 1.\n', '        """"""\n']","['VectorStoreQuery', 'len']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_query_cutoff,TestJaguarVectorStore:test_query_cutoff,method,12,42,30,366,8.71,0,1,['self'],[None],[None],193,"['        """"""Test query with time cutoff.""""""\n']","['VectorStoreQuery', 'time.sleep', 'len']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_query_filter,TestJaguarVectorStore:test_query_filter,method,14,53,37,533,10.06,0,1,['self'],[None],[None],143,"['        """"""Test query with filter(where condition).""""""\n']","['VectorStoreQuery', 'len']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_jaguar.py:TestJaguarVectorStore:test_search_anomalous,TestJaguarVectorStore:test_search_anomalous,method,7,21,20,162,7.71,0,1,['self'],[None],[None],216,"['        """"""Test detection of anomalousness.""""""\n']",['TextNode'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lancedb.py:test_to_llama_similarities_from_df_w_distance,test_to_llama_similarities_from_df_w_distance,function,13,29,28,329,11.34,0,0,[],[],[],34,[],"['dict', 'np.log', 'distances.copy', 'natural_sort.sort', 'np.array_equal', 'pd.DataFrame', '_to_llama_similarities', 'np.allclose']",8
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lancedb.py:test_to_llama_similarities_from_df_w_score,test_to_llama_similarities_from_df_w_score,function,14,30,28,319,10.63,0,0,[],[],[],19,[],"['dict', 'np.log', 'scores.copy', 'reversed_sort.sort', 'np.array_equal', 'pd.DataFrame', '_to_llama_similarities', 'np.allclose']",8
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lancedb.py:test_to_llama_similarity_from_df_ordinal,test_to_llama_similarity_from_df_ordinal,function,6,11,11,130,11.82,0,0,[],[],[],49,[],"['dict', 'pd.DataFrame', '_to_llama_similarities', 'np.allclose']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lantern.py:_get_sample_vector,_get_sample_vector,function,1,5,5,36,7.2,0,0,['num'],[' float'],[None],50,"['    """"""\n', '    Get sample embedding vector of the form [num, 1, 1, ..., 1]\n', '    where the length of the vector is TEST_EMBED_DIM.\n', '    """"""\n']",[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lantern.py:conn,conn,function,4,7,7,59,8.43,0,0,[],[],[],59,[],['psycopg2.connect'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lantern.py:db,db,function,5,24,15,216,9.0,0,0,['conn'],[' Any'],[None],66,[],"['conn.cursor', 'c.execute', 'conn.commit']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lantern.py:hybrid_node_embeddings,hybrid_node_embeddings,function,1,50,36,722,14.44,0,0,[],[],[],130,[],"['TextNode', 'RelatedNodeInfo', 'embedding=_get_sample_vector']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lantern.py:index_node_embeddings,index_node_embeddings,function,1,32,29,331,10.34,0,0,[],[],[],162,[],"['TextNode', 'embedding=_get_sample_vector', 'IndexNode']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lantern.py:node_embeddings,node_embeddings,function,1,22,19,349,15.86,0,0,[],[],[],111,[],"['TextNode', 'RelatedNodeInfo', 'embedding=_get_sample_vector']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lantern.py:pg,pg,function,3,14,14,192,13.71,0,0,['db'],[' None'],[None],80,[],"['LanternVectorStore.from_params', 'asyncio.run']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lantern.py:pg_hybrid,pg_hybrid,function,3,15,15,212,14.13,0,0,['db'],[' None'],[None],95,[],"['LanternVectorStore.from_params', 'asyncio.run']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_lantern.py:test_hybrid_query_fails_if_no_query_str_provided,test_hybrid_query_fails_if_no_query_str_provided,function,8,22,22,236,10.73,0,0,"['pg_hybrid', 'hybrid_node_embeddings']","[' LanternVectorStore', ' List[TextNode]']","[None, None]",456,[],"['VectorStoreQuery', 'query_embedding=_get_sample_vector', 'pytest.raises', 'pg_hybrid.query', 'str']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_metadata_filters.py:test_legacy_filters,test_legacy_filters,function,9,26,22,381,14.65,0,0,[],[],[],23,[],"['ExactMatchFilter', 'MetadataFilters', 'metadata_filters.legacy_filters', 'len']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_metadata_filters.py:test_legacy_filters_value_error,test_legacy_filters_value_error,function,6,15,15,285,19.0,0,0,[],[],[],10,"['    """"""Test legacy filters.""""""\n']","['MetadataFilter', 'ExactMatchFilter', 'MetadataFilters', 'pytest.raises', 'metadata_filters.legacy_filters']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_milvus.py:embedded_milvus,embedded_milvus,function,5,11,11,182,16.55,0,0,[],[],[],24,[],"['default_server.cleanup', 'default_server.start', 'str', 'default_server.stop']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_milvus.py:node_embeddings,node_embeddings,function,4,36,28,477,13.25,0,0,[],[],[],35,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_milvus.py:test_add_stores_data,test_add_stores_data,function,5,9,9,204,22.67,0,0,"['node_embeddings', 'embedded_milvus']","[' List[TextNode]', ' str']","[None, None]",61,[],"['MilvusVectorStore', 'milvus_store.add']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_milvus.py:test_non_default_index_type,test_non_default_index_type,function,10,36,30,451,12.53,0,0,"['node_embeddings', 'embedded_milvus']","[' List[TextNode]', ' str']","[None, None]",124,[],"['MilvusVectorStore', 'milvus_store.add', 'milvus_store.query', 'VectorStoreQuery']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_milvus.py:test_search_data,test_search_data,function,10,27,22,342,12.67,0,0,"['node_embeddings', 'embedded_milvus']","[' List[TextNode]', ' str']","[None, None]",70,[],"['MilvusVectorStore', 'milvus_store.add', 'milvus_store.query', 'VectorStoreQuery']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_milvus.py:test_search_data_filter,test_search_data_filter,function,11,84,31,1017,12.11,0,0,"['node_embeddings', 'embedded_milvus']","[' List[TextNode]', ' str']","[None, None]",82,[],"['MilvusVectorStore', 'milvus_store.add', 'milvus_store.query', 'VectorStoreQuery', 'filters=MetadataFilters', 'print']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_mongodb.py:node_embeddings,node_embeddings,function,4,53,34,706,13.32,0,0,[],[],[],31,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_mongodb.py:TestMongoDBAtlasVectorSearch,TestMongoDBAtlasVectorSearch,class,17,123,68,1262,10.26,0,0,[],[],[],67,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_mongodb.py:TestMongoDBAtlasVectorSearch:setup,TestMongoDBAtlasVectorSearch:setup,method,1,4,4,45,11.25,0,0,['self'],[None],[None],79,[],['collection.delete_many'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_mongodb.py:TestMongoDBAtlasVectorSearch:setup_class,TestMongoDBAtlasVectorSearch:setup_class,method,1,6,6,58,9.67,0,0,['cls'],[None],[None],69,[],['collection.count_documents'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_mongodb.py:TestMongoDBAtlasVectorSearch:teardown_class,TestMongoDBAtlasVectorSearch:teardown_class,method,1,4,4,45,11.25,0,0,['cls'],[None],[None],74,[],['collection.delete_many'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_mongodb.py:TestMongoDBAtlasVectorSearch:test_add_and_delete,TestMongoDBAtlasVectorSearch:test_add_and_delete,method,7,48,40,487,10.15,0,0,['self'],[None],[None],83,[],"['MongoDBAtlasVectorSearch', 'sleep', 'vector_store.add', 'TextNode', 'RelatedNodeInfo', 'collection.count_documents', 'vector_store.delete']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_mongodb.py:TestMongoDBAtlasVectorSearch:test_query,TestMongoDBAtlasVectorSearch:test_query,method,8,36,30,394,10.94,0,0,"['self', 'node_embeddings']","[None, ' List[TextNode]']","[None, None]",110,[],"['MongoDBAtlasVectorSearch', 'vector_store.add', 'sleep', 'vector_store.query', 'VectorStoreQuery']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:get_version_attr_from_mock_classes,get_version_attr_from_mock_classes,function,4,22,21,169,7.68,0,1,['mock_class'],[' Type[Any]'],[None],46,[],"['hasattr', 'AttributeError']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:mock_import,mock_import,function,8,19,15,177,9.32,0,2,"['name', '*args', '**kwargs']","[' str', ' Any', ' Any']","[None, None, None]",54,[],['original_import'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:MockPineconePods,MockPineconePods,class,4,20,16,139,6.95,0,0,[],[],[],12,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:MockPineconeServerless,MockPineconeServerless,class,4,20,14,133,6.65,0,0,[],[],[],24,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:MockUnVersionedPineconeRelease,MockUnVersionedPineconeRelease,class,3,18,14,119,6.61,0,0,[],[],[],36,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:TestPineconeVectorStore,TestPineconeVectorStore,class,18,93,54,1253,13.47,0,0,[],[],[],60,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:MockPineconePods:init,MockPineconePods:init,method,0,1,1,4,4.0,0,0,"['api_key', 'environment']","[' str', ' str']","[None, None]",16,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:MockUnVersionedPineconeRelease:init,MockUnVersionedPineconeRelease:init,method,0,1,1,4,4.0,0,0,"['api_key', 'environment']","[' str', ' str']","[None, None]",16,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:TestPineconeVectorStore:setUp,TestPineconeVectorStore:setUp,method,4,7,7,83,11.86,0,0,['self'],[None],[None],61,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:TestPineconeVectorStore:tearDown,TestPineconeVectorStore:tearDown,method,4,5,5,61,12.2,0,0,['self'],[None],[None],65,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:TestPineconeVectorStore:test_pods_version,TestPineconeVectorStore:test_pods_version,method,9,22,21,381,17.32,0,0,['self'],[None],[None],68,[],"['patch', 'get_version_attr_from_mock_classes', 'PineconeVectorStore']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:TestPineconeVectorStore:test_serverless_version,TestPineconeVectorStore:test_serverless_version,method,9,21,20,365,17.38,0,0,['self'],[None],[None],84,[],"['patch', 'get_version_attr_from_mock_classes', 'PineconeVectorStore']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_pinecone.py:TestPineconeVectorStore:test_unversioned_pinecone_client,TestPineconeVectorStore:test_unversioned_pinecone_client,method,3,18,18,189,10.5,0,0,['self'],[None],[None],98,[],"['pytest.raises', 'get_version_attr_from_mock_classes']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_postgres.py:_get_sample_vector,_get_sample_vector,function,1,5,5,36,7.2,0,0,['num'],[' float'],[None],53,"['    """"""\n', '    Get sample embedding vector of the form [num, 1, 1, ..., 1]\n', '    where the length of the vector is TEST_EMBED_DIM.\n', '    """"""\n']",[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_postgres.py:conn,conn,function,4,7,7,59,8.43,0,0,[],[],[],62,[],['psycopg2.connect'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_postgres.py:db,db,function,5,24,15,216,9.0,0,0,['conn'],[' Any'],[None],69,[],"['conn.cursor', 'c.execute', 'conn.commit']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_postgres.py:hybrid_node_embeddings,hybrid_node_embeddings,function,1,50,36,722,14.44,0,0,[],[],[],140,[],"['TextNode', 'RelatedNodeInfo', 'embedding=_get_sample_vector']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_postgres.py:index_node_embeddings,index_node_embeddings,function,1,32,29,331,10.34,0,0,[],[],[],172,[],"['TextNode', 'embedding=_get_sample_vector', 'IndexNode']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_postgres.py:node_embeddings,node_embeddings,function,2,33,26,559,16.94,0,0,[],[],[],114,[],"['TextNode', 'RelatedNodeInfo', 'embedding=_get_sample_vector']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_postgres.py:pg,pg,function,3,14,14,187,13.36,0,0,['db'],[' None'],[None],83,[],"['PGVectorStore.from_params', 'asyncio.run']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_postgres.py:pg_hybrid,pg_hybrid,function,3,15,15,207,13.8,0,0,['db'],[' None'],[None],98,[],"['PGVectorStore.from_params', 'asyncio.run']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_postgres.py:test_hybrid_query_fails_if_no_query_str_provided,test_hybrid_query_fails_if_no_query_str_provided,function,8,22,22,236,10.73,0,0,"['pg_hybrid', 'hybrid_node_embeddings']","[' PGVectorStore', ' List[TextNode]']","[None, None]",497,[],"['VectorStoreQuery', 'query_embedding=_get_sample_vector', 'pytest.raises', 'pg_hybrid.query', 'str']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_qdrant.py:node_embeddings,node_embeddings,function,4,36,28,477,13.25,0,0,[],[],[],22,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_qdrant.py:test_add_stores_data,test_add_stores_data,function,8,20,20,289,14.45,0,0,['node_embeddings'],[' List[TextNode]'],[None],48,[],"['qdrant_client.QdrantClient', 'QdrantVectorStore', 'pytest.raises', 'client.count', 'qdrant_vector_store.add']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_qdrant.py:test_add_stores_data_multiple_connections,test_add_stores_data_multiple_connections,function,10,24,22,424,17.67,0,0,['node_embeddings'],[' List[TextNode]'],[None],61,[],"['qdrant_client.QdrantClient', 'QdrantVectorStore', 'pytest.raises', 'client.count', 'qdrant_vector_store_a.add', 'qdrant_vector_store_b.add']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_qdrant.py:test_build_query_filter_returns_combined_filter,test_build_query_filter_returns_combined_filter,function,26,150,74,1783,11.89,0,0,[],[],[],119,[],"['qdrant_client.QdrantClient', 'QdrantVectorStore', 'MetadataFilters', 'ExactMatchFilter', 'VectorStoreQuery', 'cast', 'qdrant_vector_store._build_query_filter', 'len', 'isinstance']",9
repos/llama_index/llama-index-legacy/tests/vector_stores/test_qdrant.py:test_build_query_filter_returns_empty_filter_on_query_str,test_build_query_filter_returns_empty_filter_on_query_str,function,12,26,24,366,14.08,0,0,[],[],[],105,[],"['qdrant_client.QdrantClient', 'QdrantVectorStore', 'VectorStoreQuery', 'cast', 'qdrant_vector_store._build_query_filter', 'len']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_qdrant.py:test_build_query_filter_returns_match_any,test_build_query_filter_returns_match_any,function,17,56,39,671,11.98,0,0,[],[],[],87,[],"['qdrant_client.QdrantClient', 'QdrantVectorStore', 'VectorStoreQuery', 'cast', 'qdrant_vector_store._build_query_filter', 'len', 'isinstance']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_qdrant.py:test_build_query_filter_returns_none,test_build_query_filter_returns_none,function,8,13,12,231,17.77,0,0,[],[],[],76,[],"['qdrant_client.QdrantClient', 'QdrantVectorStore', 'VectorStoreQuery', 'qdrant_vector_store._build_query_filter']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_qdrant.py:test_relative_score_fusion,test_relative_score_fusion,function,7,139,46,1686,12.13,0,0,[],[],[],166,[],"['TextNode', 'VectorStoreQueryResult', 'relative_score_fusion']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_rockset.py:collection_exists,collection_exists,function,3,8,7,122,15.25,0,0,"['client', 'collection_name']","[' Any', ' str ']","[None, ' ""test""']",38,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_rockset.py:collection_is_empty,collection_is_empty,function,1,8,8,73,9.12,0,0,"['client', 'collection_name']","[' Any', ' str ']","[None, ' ""test""']",34,[],['len'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_rockset.py:test_metadata_filter,test_metadata_filter,function,6,28,22,326,11.64,0,0,['vector_store'],[' RocksetVectorStore'],[None],90,[],"['vector_store.query', 'VectorStoreQuery', 'filters=MetadataFilters', 'len', 'isinstance']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_rockset.py:test_query,test_query,function,6,25,21,281,11.24,0,0,['vector_store'],[' RocksetVectorStore'],[None],78,[],"['vector_store.query', 'VectorStoreQuery', 'len', 'isinstance']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_rockset.py:vector_store,vector_store,function,9,68,46,689,10.13,2,0,[],[],[],47,[],"['RocksetVectorStore.with_new_collection', 'RocksetVectorStore', 'store.add', 'TextNode', 'collection_is_empty', 'sleep', 'collection_exists']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:_node_embeddings_for_test,_node_embeddings_for_test,function,1,42,25,591,14.07,0,0,[],[],[],17,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:SimpleVectorStoreTest,SimpleVectorStoreTest,class,22,192,72,3486,18.16,0,0,[],[],[],43,[],[],0
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_delete_removes_document_from_query_results,SimpleVectorStoreTest:test_delete_removes_document_from_query_results,method,9,15,15,323,21.53,0,0,['self'],[None],[None],140,[],"['SimpleVectorStore', 'simple_vector_store.add', 'simple_vector_store.delete', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_contradictive_filter_returns_no_matches,SimpleVectorStoreTest:test_query_with_contradictive_filter_returns_no_matches,method,12,25,24,379,15.16,0,0,['self'],[None],[None],113,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'ExactMatchFilter', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_exact_filters_returns_single_match,SimpleVectorStoreTest:test_query_with_exact_filters_returns_single_match,method,11,20,20,375,18.75,0,0,['self'],[None],[None],99,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'ExactMatchFilter', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_filter_applies_node_id_filter,SimpleVectorStoreTest:test_query_with_filter_applies_node_id_filter,method,10,18,18,389,21.61,0,0,['self'],[None],[None],85,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_filter_applies_top_k,SimpleVectorStoreTest:test_query_with_filter_applies_top_k,method,10,17,17,349,20.53,0,0,['self'],[None],[None],74,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_filter_on_unknown_field_returns_no_matches,SimpleVectorStoreTest:test_query_with_filter_on_unknown_field_returns_no_matches,method,11,21,21,345,16.43,0,0,['self'],[None],[None],128,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_with_filters_returns_multiple_matches,SimpleVectorStoreTest:test_query_with_filters_returns_multiple_matches,method,10,20,20,376,18.8,0,0,['self'],[None],[None],61,[],"['SimpleVectorStore', 'simple_vector_store.add', 'MetadataFilters', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertEqual']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_simple.py:SimpleVectorStoreTest:test_query_without_filters_returns_all_rows_sorted_by_similarity,SimpleVectorStoreTest:test_query_without_filters_returns_all_rows_sorted_by_similarity,method,10,24,24,404,16.83,0,0,['self'],[None],[None],44,[],"['SimpleVectorStore', 'simple_vector_store.add', 'VectorStoreQuery', 'simple_vector_store.query', 'self.assertCountEqual', 'self.assertEqual']",6
repos/llama_index/llama-index-legacy/tests/vector_stores/test_singlestoredb.py:test_metadata_filter,test_metadata_filter,function,6,28,22,326,11.64,0,0,['vector_store'],[' SingleStoreVectorStore'],[None],61,[],"['vector_store.query', 'VectorStoreQuery', 'filters=MetadataFilters', 'len', 'isinstance']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_singlestoredb.py:test_query,test_query,function,6,25,21,281,11.24,0,0,['vector_store'],[' SingleStoreVectorStore'],[None],49,[],"['vector_store.query', 'VectorStoreQuery', 'len', 'isinstance']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_singlestoredb.py:vector_store,vector_store,function,8,49,39,491,10.02,0,1,[],[],[],20,[],"['SingleStoreVectorStore', 'store.add', 'TextNode']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tair.py:get_tair_url,get_tair_url,function,2,3,3,54,18.0,0,0,[],[],[],47,[],['environ.get'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tair.py:node_embeddings,node_embeddings,function,6,42,26,633,15.07,0,0,[],[],[],21,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tair.py:test_add_stores_data,test_add_stores_data,function,7,11,11,230,20.91,0,0,['node_embeddings'],[' List[TextNode]'],[None],52,[],"['get_tair_url', 'TairVectorStore', 'tair_vector_store.add', 'int']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tair.py:test_delete,test_delete,function,14,36,28,531,14.75,0,0,[],[],[],121,[],"['get_tair_url', 'TairVectorStore', 'tair_vector_store.delete', 'int', 'VectorStoreQuery', 'tair_vector_store.query', 'len', 'tair_vector_store.delete_index']",8
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tair.py:test_query,test_query,function,15,120,44,1508,12.57,0,0,[],[],[],63,[],"['get_tair_url', 'TairVectorStore', 'VectorStoreQuery', 'tair_vector_store.query', 'len', 'MetadataFilters', 'ExactMatchFilter']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tencentvectordb.py:get_tencent_vdb_store,get_tencent_vdb_store,function,3,16,16,288,18.0,0,0,['drop_exists'],[' bool '],[' False'],55,[],"['FilterField', 'TencentVectorDB', 'collection_params=CollectionParams']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tencentvectordb.py:node_embeddings,node_embeddings,function,5,51,33,659,12.92,0,0,[],[],[],23,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tencentvectordb.py:test_add_stores_data,test_add_stores_data,function,6,12,12,218,18.17,0,0,['node_embeddings'],[' List[TextNode]'],[None],71,[],"['get_tencent_vdb_store', 'store.add', 'time.sleep', 'store.query_by_ids', 'len']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tencentvectordb.py:test_delete,test_delete,function,9,20,16,241,12.05,1,0,['node_embeddings'],[' List[TextNode]'],[None],112,[],"['get_tencent_vdb_store', 'store.query_by_ids', 'len', 'store.delete', 'time.sleep']",5
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tencentvectordb.py:test_query,test_query,function,8,25,23,301,12.04,0,0,[],[],[],83,[],"['get_tencent_vdb_store', 'VectorStoreQuery', 'store.query', 'len']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_tencentvectordb.py:test_query_with_filter,test_query_with_filter,function,9,31,28,318,10.26,0,0,['node_embeddings'],[' List[TextNode]'],[None],96,[],"['get_tencent_vdb_store', 'VectorStoreQuery', 'store.query', 'len']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_timescalevector.py:conn,conn,function,4,7,7,67,9.57,0,0,[],[],[],39,[],['psycopg2.connect'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_timescalevector.py:db,db,function,5,18,14,147,8.17,0,0,['conn'],[' Any'],[None],46,[],"['conn.cursor', 'c.execute', 'conn.commit']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_timescalevector.py:node_embeddings,node_embeddings,function,4,24,20,323,13.46,0,0,[],[],[],90,[],"['TextNode', 'RelatedNodeInfo']",2
repos/llama_index/llama-index-legacy/tests/vector_stores/test_timescalevector.py:test_add_to_db_query_and_delete,test_add_to_db_query_and_delete,function,10,32,27,528,16.5,0,0,"['tvs', 'node_embeddings']","[' TimescaleVectorStore', ' List[TextNode]']","[None, None]",211,[],"['tvs.add', 'isinstance', 'VectorStoreQuery', 'tvs.query', 'len', 'tvs.create_index', 'tvs.drop_index']",7
repos/llama_index/llama-index-legacy/tests/vector_stores/test_timescalevector.py:tvs,tvs,function,5,12,12,213,17.75,0,0,['db'],[' None'],[None],59,[],"['TimescaleVectorStore.from_params', 'asyncio.get_event_loop', 'asyncio.run']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_timescalevector.py:tvs_tp,tvs_tp,function,5,13,13,257,19.77,0,0,['db'],[' None'],[None],74,[],"['TimescaleVectorStore.from_params', 'time_partition_interval=timedelta', 'asyncio.get_event_loop', 'asyncio.run']",4
repos/llama_index/llama-index-legacy/tests/vector_stores/test_upstash.py:test_upstash_vector_add,test_upstash_vector_add,function,2,6,5,87,14.5,0,0,"['upstash_vector_store', 'text_nodes']","[' UpstashVectorStore', ' List[TextNode]']","[None, None]",47,[],['upstash_vector_store.add'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_upstash.py:test_upstash_vector_query,test_upstash_vector_query,function,6,15,14,190,12.67,0,0,"['upstash_vector_store', 'text_nodes']","[' UpstashVectorStore', ' List[TextNode]']","[None, None]",55,[],"['upstash_vector_store.add', 'upstash_vector_store.query', 'VectorStoreQuery']",3
repos/llama_index/llama-index-legacy/tests/vector_stores/test_upstash.py:text_nodes,text_nodes,function,1,19,14,225,11.84,0,0,[],[],[],29,[],['TextNode'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_upstash.py:upstash_vector_store,upstash_vector_store,function,3,9,7,123,13.67,0,0,[],[],[],21,[],['UpstashVectorStore'],1
repos/llama_index/llama-index-legacy/tests/vector_stores/test_weaviate.py:test_weaviate_add,test_weaviate_add,function,13,37,33,487,13.16,0,0,[],[],[],8,[],"['MagicMock', 'WeaviateVectorStore', 'vector_store.add', 'TextNode', 'RelatedNodeInfo']",5
repos/llama_index/llama-index-networks/tests/network/test_query_engine.py:test_network_query_engine,test_network_query_engine,function,14,26,23,358,13.77,0,0,['mock_contributor'],[None],[None],14,[],"['MockLLM', 'return_response', 'NetworkQueryEngine.from_args', 'network_query_engine.query']",4
repos/llama_index/llama-index-networks/tests/network/test_retriever.py:test_network_retriever,test_network_retriever,function,16,30,25,478,15.93,0,0,['mock_contributor'],[None],[None],11,[],"['return_nodes', 'NetworkRetriever', 'QueryBundle', 'network_retriever.retrieve', 'len']",5
repos/llama_index/llama-index-packs/llama-index-packs-agent-search-retriever/tests/test_packs_agent_search_retriever.py:test_class,test_class,function,4,10,8,124,12.4,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-agents-coa/tests/test_packs_agents.py:test_class,test_class,function,4,10,8,112,11.2,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-agents-lats/tests/test_packs_lats.py:test_pack,test_pack,function,5,8,8,76,9.5,0,0,[],[],[],13,[],"['MockLLM', 'LATSPack', 'isinstance']",3
repos/llama_index/llama-index-packs/llama-index-packs-agents-lats/tests/test_packs_lats.py:test_worker,test_worker,function,5,8,8,89,11.12,0,0,[],[],[],7,[],"['MockLLM', 'LATSAgentWorker', 'isinstance']",3
repos/llama_index/llama-index-packs/llama-index-packs-agents-llm-compiler/tests/test_packs_agents.py:test_class,test_class,function,4,10,8,120,12.0,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-amazon-product-extraction/tests/test_packs_amazon_product_extraction.py:test_class,test_class,function,4,10,8,127,12.7,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-arize-phoenix-query-engine/tests/test_packs_arize_phoenix_query_engine.py:test_class,test_class,function,4,10,8,127,12.7,1,0,[],[],[],7,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-auto-merging-retriever/tests/test_packs_auto_merging_retriever.py:test_class,test_class,function,4,10,8,124,12.4,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-chroma-autoretrieval/tests/test_packs_chroma_autoretrieval.py:test_class,test_class,function,4,10,8,123,12.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:test_cpp_code_splitter,test_cpp_code_splitter,function,4,13,13,133,10.23,0,1,[],[],[],605,"['    """"""Test case for code splitting using C++.""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:test_html_code_splitter,test_html_code_splitter,function,5,17,17,170,10.0,0,1,[],[],[],234,"['    """"""Test case for code splitting using HTML.""""""\n']","['CodeHierarchyNodeParser', 'chunk_min_characters=len']",2
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:test_python_code_splitter,test_python_code_splitter,function,4,13,13,133,10.23,0,1,[],[],[],9,"['    """"""Test case for code splitting using python.""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:test_python_code_splitter_with_decorators,test_python_code_splitter_with_decorators,function,4,13,13,133,10.23,0,1,[],[],[],131,"['    """"""Test case for code splitting using python.""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:test_tsx_code_splitter,test_tsx_code_splitter,function,4,13,13,137,10.54,0,1,[],[],[],508,"['    """"""Test case for code splitting using TypeScript JSX (TSX).""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:test_typescript_code_splitter,test_typescript_code_splitter,function,4,13,13,137,10.54,0,1,[],[],[],407,"['    """"""Test case for code splitting using TypeScript.""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Example,Example,class,0,0,0,0,0.0,0,0,[],[],[],421,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Example,Example,class,0,0,0,0,0.0,0,0,[],[],[],421,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Foo,Foo,class,34,245,96,3285,13.41,2,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Foo,Foo,class,34,245,96,3285,13.41,2,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Foo,Foo,class,34,245,96,3285,13.41,2,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:MyClass,MyClass,class,0,0,0,0,0.0,0,0,[],[],[],620,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:MyClass,MyClass,class,0,0,0,0,0.0,0,0,[],[],[],620,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_no_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:_handle_extra_radiation_types,_handle_extra_radiation_types,function,15,104,45,1149,11.05,0,2,"['datetime_or_doy', 'epoch_year']","[None, None]","[None, None]",539,[],"['np.isscalar', 'to_doy', 'partial', 'TextNode', 'code_splitter.get_nodes_from_documents', 'len', '_handle_extra_radiation_types']",7
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:test_html_code_splitter,test_html_code_splitter,function,5,17,17,169,9.94,0,1,[],[],[],254,"['    """"""Test case for code splitting using HTML.""""""\n']","['CodeHierarchyNodeParser', 'chunk_min_characters=len']",2
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:test_python_code_splitter,test_python_code_splitter,function,3,12,12,130,10.83,0,1,[],[],[],9,"['    """"""Test case for code splitting using python.""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:test_python_code_splitter_with_decorators,test_python_code_splitter_with_decorators,function,4,13,13,132,10.15,0,1,[],[],[],145,"['    """"""Test case for code splitting using python.""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:test_typescript_code_splitter,test_typescript_code_splitter,function,4,13,13,136,10.46,0,1,[],[],[],367,"['    """"""Test case for code splitting using TypeScript.""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:test_typescript_code_splitter_2,test_typescript_code_splitter_2,function,4,13,13,136,10.46,0,1,[],[],[],495,"['    """"""Test case for code splitting using TypeScript.""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Example,Example,class,0,0,0,0,0.0,0,0,[],[],[],381,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Example,Example,class,0,0,0,0,0.0,0,0,[],[],[],405,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Example,Example,class,0,0,0,0,0.0,0,0,[],[],[],405,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Example,Example,class,0,0,0,0,0.0,0,0,[],[],[],381,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Example,Example,class,0,0,0,0,0.0,0,0,[],[],[],405,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo,Foo,class,31,258,97,3348,12.98,2,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo,Foo,class,31,258,97,3348,12.98,2,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo,Foo,class,31,258,97,3348,12.98,2,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo,Foo,class,31,258,97,3348,12.98,2,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo,Foo,class,31,258,97,3348,12.98,2,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo,Foo,class,31,258,97,3348,12.98,2,0,[],[],[],19,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_hierarchy_with_skeleton.py:Foo:bar,Foo:bar,method,1,1,1,12,12.0,0,0,[],[],[],20,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_parse_nodes_special_characters.py:function_that_was_cut,function_that_was_cut,function,7,42,30,405,9.64,0,0,[],[],[],21,[],"['print', 'TextNode', 'code_splitter.get_nodes_from_documents', 'len', 'function_that_was_cut']",5
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_parse_nodes_special_characters.py:function_that_was_cut,function_that_was_cut,function,7,42,30,405,9.64,0,0,[],[],[],21,[],"['print', 'TextNode', 'code_splitter.get_nodes_from_documents', 'len', 'function_that_was_cut']",5
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_parse_nodes_special_characters.py:print_special_character,print_special_character,function,1,4,4,35,8.75,0,0,[],[],[],17,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_code_parse_nodes_special_characters.py:test_special_character,test_special_character,function,3,8,8,106,13.25,0,0,[],[],[],9,"['    """"""Test case for code splitting using python and add a special character in the code.""""""\n']",['CodeHierarchyNodeParser'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_query_engine.py:code_hierarchy_nodes,code_hierarchy_nodes,function,9,23,23,424,18.43,0,0,['request'],[None],[None],22,[],"['SimpleDirectoryReader', 'Path', 'reader.load_data', 'CodeHierarchyNodeParser', 'code_splitter=CodeSplitter']",5
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_query_engine.py:print_python,print_python,function,1,3,3,38,12.67,0,0,['python_text'],[' str'],[None],16,"['    """"""This function prints python text in ipynb nicely formatted.""""""\n']",['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_query_engine.py:test_code_splitter_NEXT_relationship_indention,test_code_splitter_NEXT_relationship_indention,function,5,24,18,204,8.5,1,1,"['code_hierarchy_nodes', '']","[' Sequence[BaseNode]', None]","[None, None]",43,"['    """"""When using jupyter I found that the final brevity comment was indented when it shouldn\'t be.""""""\n']",['last_line.startswith'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_query_engine.py:test_query_by_all_uuids,test_query_by_all_uuids,function,9,21,19,237,11.29,2,0,['code_hierarchy_nodes'],[' Sequence[BaseNode]'],[None],81,"['    """"""Test querying the index by signature.""""""\n']","['CodeHierarchyKeywordQueryEngine', 're.findall', 'index.query', 'len']",4
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_query_engine.py:test_query_by_item_name,test_query_by_item_name,function,5,12,12,182,15.17,0,0,"['name', 'code_hierarchy_nodes']","[' str', ' Sequence[BaseNode]']","[None, None]",71,"['    """"""Test querying the index by signature.""""""\n']","['CodeHierarchyKeywordQueryEngine', 'index.query', 'len']",3
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_query_engine.py:test_query_by_module_name,test_query_by_module_name,function,5,12,12,173,14.42,0,0,['code_hierarchy_nodes'],[' Sequence[BaseNode]'],[None],54,"['    """"""Test querying the index by filename.""""""\n']","['CodeHierarchyKeywordQueryEngine', 'index.query', 'len']",3
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:function,function,function,1,10,8,74,7.4,0,1,[],[],[],7,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:function,function,function,1,10,8,74,7.4,0,1,[],[],[],7,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:function,function,function,1,10,8,74,7.4,0,1,[],[],[],7,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:function,function,function,1,10,8,74,7.4,0,1,[],[],[],7,[],['print'],1
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:test_mixed_indentation,test_mixed_indentation,function,1,2,2,9,4.5,0,0,[],[],[],56,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:test_mixed_indentation_2,test_mixed_indentation_2,function,1,2,2,9,4.5,0,0,[],[],[],67,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:test_no_indentation,test_no_indentation,function,1,2,2,10,5.0,0,0,[],[],[],78,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:test_space_indentation,test_space_indentation,function,1,2,2,9,4.5,0,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:test_tab_indentation,test_tab_indentation,function,1,2,2,9,4.5,0,0,[],[],[],22,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:test_tab_indentation_2,test_tab_indentation_2,function,1,2,2,9,4.5,0,0,[],[],[],39,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:test_typescript,test_typescript,function,1,2,2,9,4.5,0,0,[],[],[],93,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:test_typescript_2,test_typescript_2,function,1,2,2,9,4.5,0,0,[],[],[],111,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:Example,Example,class,0,0,0,0,0,0,0,[],[],[],95,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-code-hierarchy/tests/test_utility_methods.py:Example,Example,class,0,0,0,0,0,0,0,[],[],[],95,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-cogniswitch-agent/tests/test_packs_cogniswitch_agent.py:test_class,test_class,function,4,10,8,120,12.0,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-cohere-citation-chat/tests/test_packs_cohere_citation_chat.py:test_class,test_class,function,4,10,8,128,12.8,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-corrective-rag/tests/test_packs_corrective_rag.py:test_class,test_class,function,4,10,8,117,11.7,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-deeplake-deepmemory-retriever/tests/test_packs_deeplake_deepmemory_retriever.py:test_class,test_class,function,4,10,8,123,12.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-deeplake-multimodal-retrieval/tests/test_packs_deeplake_multimodal_retrieval.py:test_class,test_class,function,2,12,10,133,11.08,0,0,[],[],[],7,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-dense-x-retrieval/tests/test_packs_dense_x_retrieval.py:test_class,test_class,function,4,10,8,119,11.9,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-diff-private-simple-dataset/tests/test_packs_diff_private_examples_gen.py:test_class,test_class,function,4,10,8,128,12.8,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-diff-private-simple-dataset/tests/test_templates.py:test_few_shot_template,test_few_shot_template,function,11,77,56,997,12.95,0,0,[],[],[],12,[],"['PromptBundle', 'LabelledSimpleDataExample', 'single_example_template.format', 'reduce', 'few_shot_completion_template.format']",5
repos/llama_index/llama-index-packs/llama-index-packs-evaluator-benchmarker/tests/test_packs_evaluator_benchmarker.py:test_class,test_class,function,4,10,8,124,12.4,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-fusion-retriever/tests/test_packs_fusion_retriever.py:test_class,test_class,function,5,20,10,253,12.65,2,0,[],[],[],8,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-fuzzy-citation/tests/test_packs_fuzzy_citation.py:test_class,test_class,function,4,10,8,123,12.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-gmail-openai-agent/tests/test_packs_gmail_openai_agent.py:test_class,test_class,function,4,10,8,120,12.0,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-gradio-agent-chat/tests/test_packs_gradio_agent_chat.py:test_class,test_class,function,4,10,8,119,11.9,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-gradio-react-agent-chatbot/tests/__init__.py:test_class,test_class,function,4,10,8,120,12.0,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-gradio-react-agent-chatbot/tests/test_packs_gradio_react_agent_chatbot.py:test_class,test_class,function,4,10,8,120,12.0,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-infer-retrieve-rerank/tests/test_packs_infer_retrieve_rerank.py:test_class,test_class,function,4,10,8,123,12.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-koda-retriever/tests/koda_mocking.py:KVMockLLM,KVMockLLM,class,19,62,52,587,9.47,0,2,[],[],[],23,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-koda-retriever/tests/koda_mocking.py:KVMockLLM:class_name,KVMockLLM:class_name,method,1,2,2,17,8.5,0,0,['cls'],[None],[None],31,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-koda-retriever/tests/koda_mocking.py:KVMockLLM:complete,KVMockLLM:complete,method,9,31,28,287,9.26,0,2,"['self', 'prompt', '**kwargs']","[None, ' str', None]","[None, None, None]",38,"['        """"""Returns a response that was matched from the given prompt.""""""\n']","['ValueError', 'CompletionResponse']",2
repos/llama_index/llama-index-packs/llama-index-packs-koda-retriever/tests/koda_mocking.py:KVMockLLM:random_prompt,KVMockLLM:random_prompt,method,2,2,2,55,27.5,0,0,['self'],[None],[None],34,"['        """"""Returns a random prompt from the prompt_responses dictionary.""""""\n']",['random.choice'],1
repos/llama_index/llama-index-packs/llama-index-packs-koda-retriever/tests/test_koda_retriever.py:test_a_retrieve,test_a_retrieve,function,8,16,16,179,11.19,0,0,['setup'],[None],[None],28,[],"['setup.get', 'llm.random_prompt', 'run', 'isinstance']",4
repos/llama_index/llama-index-packs/llama-index-packs-koda-retriever/tests/test_koda_retriever.py:test_categorize,test_categorize,function,9,32,25,323,10.09,0,0,['setup'],[None],[None],37,[],"['setup.get', 'llm.random_prompt', 'retriever.categorize', 'isinstance']",4
repos/llama_index/llama-index-packs/llama-index-packs-koda-retriever/tests/test_koda_retriever.py:test_category_retrieve,test_category_retrieve,function,10,22,22,262,11.91,0,0,['setup'],[None],[None],51,[],"['setup.get', 'llm.random_prompt', 'retriever.category_retrieve', 'isinstance']",4
repos/llama_index/llama-index-packs/llama-index-packs-koda-retriever/tests/test_koda_retriever.py:test_init,test_init,function,3,36,22,273,7.58,0,0,['setup'],[None],[None],7,[],"['setup.get', 'isinstance']",2
repos/llama_index/llama-index-packs/llama-index-packs-koda-retriever/tests/test_koda_retriever.py:test_retrieve,test_retrieve,function,8,16,16,172,10.75,0,0,['setup'],[None],[None],19,[],"['setup.get', 'llm.random_prompt', 'retriever.retrieve', 'isinstance']",4
repos/llama_index/llama-index-packs/llama-index-packs-llama-dataset-metadata/tests/test_packs_llama_dataset_metadata.py:test_class,test_class,function,4,10,8,124,12.4,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-llama-guard-moderator/tests/test_packs_llama_guard_moderator.py:test_class,test_class,function,4,10,8,123,12.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-llava-completion/tests/test_packs_llava_completion.py:test_class,test_class,function,4,10,8,119,11.9,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-multi-document-agents/tests/test_packs_multi_document_agents.py:test_class,test_class,function,4,10,8,123,12.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-multi-tenancy-rag/tests/test_packs_multi_tenancy_rag.py:test_class,test_class,function,4,10,8,119,11.9,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-multidoc-autoretrieval/tests/test_packs_multidoc_autoretrieval.py:test_class,test_class,function,4,10,8,125,12.5,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-nebulagraph-query-engine/tests/test_packs_nebulagraph_query_engine.py:test_class,test_class,function,4,10,8,126,12.6,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-neo4j-query-engine/tests/test_packs_neo4j_query_engine.py:test_class,test_class,function,4,10,8,120,12.0,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-node-parser-semantic-chunking/tests/test_packs_node_parser.py:test_class,test_class,function,2,12,10,133,11.08,0,0,[],[],[],7,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-ollama-query-engine/tests/test_packs_ollama_query_engine.py:test_class,test_class,function,4,10,8,121,12.1,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-panel-chatbot/tests/test_packs_panel_chatbot.py:test_class,test_class,function,4,10,8,113,11.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-query-understanding-agent/tests/test_packs_query_understanding_agent.py:test_class,test_class,function,4,10,8,127,12.7,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-raft-dataset/tests/test_packs_raft_dataset.py:test_class,test_class,function,4,10,8,115,11.5,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-rag-cli-local/tests/test_packs_rag_cli_local.py:test_class,test_class,function,4,10,8,115,11.5,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-rag-evaluator/tests/test_packs_rag_evaluator.py:test_class,test_class,function,4,10,8,116,11.6,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-rag-fusion-query-pipeline/tests/test_packs_query.py:test_class,test_class,function,4,10,8,121,12.1,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-ragatouille-retriever/tests/test_packs_ragatouille_retriever.py:test_class,test_class,function,4,10,8,124,12.4,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-raptor/tests/test_packs_raptor.py:test_raptor,test_raptor,function,7,32,28,510,15.94,0,0,[],[],[],6,[],"['RaptorRetriever', 'Document', 'embed_model=MockEmbedding', 'llm=MockLLM', 'len', 'retriever.retrieve']",6
repos/llama_index/llama-index-packs/llama-index-packs-recursive-retriever/tests/test_packs_recursive_retriever.py:test_class,test_class,function,2,24,12,276,11.5,0,0,[],[],[],8,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-redis-ingestion-pipeline/tests/test_packs_redis_ingestion_pipeline.py:test_class,test_class,function,4,10,8,126,12.6,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-resume-screener/tests/test_packs_resume_screener.py:test_class,test_class,function,4,10,8,118,11.8,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-retry-engine-weaviate/tests/test_packs_retry_engine_weaviate.py:test_class,test_class,function,4,10,8,123,12.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-searchain/tests/test_packs_searchain.py:test_class,test_class,function,4,10,8,113,11.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-self-discover/tests/test_packs_self_discover.py:test_class,test_class,function,4,10,8,116,11.6,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-self-rag/tests/test_packs_self_rag.py:test_class,test_class,function,4,10,8,111,11.1,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-sentence-window-retriever/tests/test_packs_sentence_window_retriever.py:test_class,test_class,function,4,10,8,127,12.7,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-snowflake-query-engine/tests/test_packs_snowflake_query_engine.py:test_class,test_class,function,4,10,8,124,12.4,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-stock-market-data-query-engine/tests/test_packs_stock_market_data_query_engine.py:test_class,test_class,function,4,10,8,130,13.0,1,0,[],[],[],7,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-streamlit-chatbot/tests/test_packs_streamlit_chatbot.py:test_class,test_class,function,4,10,8,117,11.7,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-sub-question-weaviate/tests/test_packs_sub_question_weaviate.py:test_class,test_class,function,4,10,8,123,12.3,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-tables/tests/test_packs_tables.py:test_class,test_class,function,5,20,10,239,11.95,2,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-trulens-eval-packs/tests/test_packs_trulens_eval_packs.py:test_class,test_class,function,6,30,11,358,11.93,3,0,[],[],[],9,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-vanna/tests/test_packs_vanna.py:test_class,test_class,function,4,10,8,109,10.9,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-vectara-rag/tests/test_packs_vectara_rag.py:test_class,test_class,function,4,10,8,114,11.4,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-voyage-query-engine/tests/test_packs_voyage_query_engine.py:test_class,test_class,function,4,10,8,121,12.1,1,0,[],[],[],5,[],[],0
repos/llama_index/llama-index-packs/llama-index-packs-zephyr-query-engine/tests/test_packs_zephyr_query_engine.py:test_class,test_class,function,4,10,8,121,12.1,1,0,[],[],[],5,[],[],0
